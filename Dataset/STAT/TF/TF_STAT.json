[
  {
    "qid": "statistic-posneg-744-4-1",
    "gold_answer": "FALSE. The paper explicitly mentions that the Wolters method provides more accurate density estimates for smaller sample sizes (15 and 30), but the proposed method is superior for larger samples (100 and 500), except for densities on the $[-1,1]$ support. Thus, the Wolters method does not outperform the proposed method across all sample sizes.",
    "question": "Does the Wolters method consistently outperform the proposed method across all sample sizes, including $n=100$ and $n=500$?",
    "question_context": "Density Estimation Performance Under Contamination\n\nFigs. 1–3 display plots of the average density estimates over the 1000 MC samples for each sample size for both methods. The plots are consistent with the numeric results such that the Wolters method provides more accurate density estimates for sample sizes of 15 and 30 but our method is superior for the larger samples of 100 and 500, with the exception of the densities on the $[-1,1]$ support. Both methods adeptly estimated the location of the mode for all proposed cases. <PARAGRAPH_SEPARATOR> # 3.6. Outliers <PARAGRAPH_SEPARATOR> One concern is how well our method performs when the data is contaminated with outliers. We explore this by generating data from a $\\mathcal{N}(0,1)$ density mixed with a $\\mathcal{N}(4,0.5)$ density. We set the mixture proportion to $95\\%$ , resulting in the following normal mixture, <PARAGRAPH_SEPARATOR> $$ p\\mathcal{N}(0,1)+(1-p)\\mathcal{N}(4,0.5), $$ <PARAGRAPH_SEPARATOR> where $p\\sim$ bernoulli(0.95). <PARAGRAPH_SEPARATOR> We generate 1000 MC samples from the mixture distribution with sizes $n=15$ , 30, and 100. Fig. 4 displays the average density estimates across the 1000 MC samples. Assuming the data is truly from a $\\mathcal{N}(0,1)$ distribution, the $5\\%$ contamination does not completely compromise the density estimate. For the $n=100$ sample, the density estimate still properly locates the mode at 0 and there is only a slightly heavier tail on the side of the contaminating distribution. <PARAGRAPH_SEPARATOR> # 4. Real data examples"
  },
  {
    "qid": "statistic-posneg-322-0-0",
    "gold_answer": "TRUE. The conditional stable tail dependence function L_y(x) is indeed defined as the limit of t^{-1} * (1 - C_y(1 - t x)) as t approaches 0. This captures the tail dependence structure of the multivariate distribution given the covariate Y = y, and C_y is the conditional copula that links the marginal distributions to the joint distribution.",
    "question": "Is the conditional stable tail dependence function L_y(x) defined as the limit of t^{-1} * (1 - C_y(1 - t x)) as t approaches 0, where C_y is the conditional copula?",
    "question_context": "Estimation of Extreme Multivariate Expectiles with Functional Covariates\n\nThe paper discusses the estimation of multivariate expectiles with functional covariates, focusing on conditional marginal distributions, quantile functions, and tail dependence functions. It introduces a framework for estimating extreme multivariate expectiles using conditional copulas and tail dependence functions, with applications in risk analysis and capital allocation."
  },
  {
    "qid": "statistic-posneg-101-0-2",
    "gold_answer": "FALSE. The estimator μ̃_{p,q} is √n₂-asymptotically normal when D = 0 only if n₂/N → 0. If the second-stage subsample includes the entire dataset (n₂ = N), the estimator becomes √N-asymptotically normal, as shown in Corollary 1. The convergence rate depends on the relationship between n₂ and N.",
    "question": "Is the estimator μ̃_{p,q} always √n₂-asymptotically normal when D = 0?",
    "question_context": "Two-Stage Optimal Subsampling for Missing Data Analysis\n\nThe paper discusses a two-stage subsampling method to estimate the parameter of interest μ=E[Y] in the presence of missing data. The method involves estimating model parameters in the first stage using a subsample and then estimating μ in the second stage using another subsample. The optimal subsampling probabilities are derived to minimize the asymptotic variance of the estimator."
  },
  {
    "qid": "statistic-posneg-42-0-0",
    "gold_answer": "TRUE. The definition is consistent with the properties required for $(h,\\phi)$-divergence measures, ensuring that $h$ is increasing and differentiable, and $\\phi$ is a convex function with specific smoothness and boundary conditions.",
    "question": "Is the condition for the $(h,\\phi)$-divergence measure $D_{\\phi,h}(\\pmb{r},\\pmb{q})=h(D_{\\phi}(\\pmb{r},\\pmb{q}))$ correctly defined with $h$ being a differentiable increasing function mapping from $[0,\\infty)$ into $[0,\\infty)$, with $h(0)=0$, $h^{\\prime}(0)>0$ and $\\phi\\in\\varPhi$?",
    "question_context": "Power Divergence Measures in Three-Way Contingency Tables\n\nThe paper discusses the use of power divergence measures in three-way contingency tables, focusing on hypotheses of independence, partial independence, and conditional independence. It introduces the $(h,\\phi)$-divergence measures and examines their properties and applications in statistical testing."
  },
  {
    "qid": "statistic-posneg-788-0-1",
    "gold_answer": "TRUE. The logistic growth model explicitly includes the term (1 - N/κ), which represents a linear decrease in the per capita growth rate as the population size N approaches the carrying capacity κ. This reflects density-dependent growth limitations due to factors like resource scarcity or overcrowding.",
    "question": "The logistic growth model dN/dt = v0N(1 - N/κ) assumes that the growth rate per capita decreases linearly with population size. True or False?",
    "question_context": "Deterministic and Stochastic Models of Population Growth\n\nThe text discusses deterministic and stochastic models of population growth. The deterministic approach assumes that the future population can be exactly predicted given initial conditions, often ignoring age-structure for simplicity. Key formulas include the exponential growth model N(t) = N(0)e^(νt) and the logistic growth model dN/dt = v0N(1 - N/κ). The stochastic approach considers chance fluctuations, particularly in the context of surname extinction, where the probability of extinction is derived from generating functions."
  },
  {
  "qid": "statistic-posneg-2055-1-0",
  "gold_answer": "TRUE. The decomposition is mathematically justified: V_I(β0) captures variability from the full cohort via E{∑W_1k(β)}^⊗2 (influence functions of complete data), while V_II(β0) represents additional uncertainty from subcohort sampling through E{∑∫Ω_1k(β,t)dΛ_0k(t)}^⊗2. The (1−α)/α term scales the subcohort contribution inversely with the sampling fraction α, as fewer sampled controls (lower α) increase variance. This aligns with survey-sampling theory and is validated by Theorem 1’s asymptotic normality result.",
  "question": "In a case–cohort study with bivariate failure times, the authors derive the large-sample covariance matrix of their updated weighted estimating-equation (UWEE) estimator as\n\nΣ(β₀) = V_I(β₀) + (1 − α)/α · V_II(β₀),\n\nwhere V_I(β₀) involves complete-data influence functions, V_II(β₀) reflects additional variability from subcohort sampling, and α ∈ (0, 1) is the limiting subcohort sampling fraction.  Does this expression correctly decompose the total asymptotic variance into full-cohort and subcohort components?  True or False?",
  "question_context": "Background.  The updated weighted estimating-equation (UWEE) method improves efficiency in case–cohort designs with correlated (bivariate) failure times by first solving standard inverse-probability-weighted equations and then applying an influence-function update that borrows risk-set information available on the entire cohort.  Under regularity conditions (finite second moments, independent Bernoulli subcohort sampling, and an α-stable sampling fraction), the UWEE estimator is √n-consistent and asymptotically normal with covariance matrix\n\nΣ(β₀) = V_I(β₀) + (1 − α)/α · V_II(β₀),\n\nwhere\n• V_I(β₀) = E{∑_k W_{1k}(β₀)}^{⊗2}\n• V_II(β₀) = E{∑_k ∫Ω_{1k}(β₀, t) dΛ_{0k}(t)}^{⊗2}\n\nThe scalar factor (1 − α)/α captures how variance inflates as the subcohort sampling fraction α decreases.  Simulation studies indicate that UWEE attains higher efficiency than existing pseudolikelihood and weighted estimating-equation methods, particularly when α ≤ 0.2 and the event-type correlation is moderate or low."
  },
  {
    "qid": "statistic-posneg-1209-0-0",
    "gold_answer": "FALSE. The pPAR formula integrates over the marginal distribution of confounders $\\mathbf{X}^{(2)}$ in the numerator (representing the counterfactual scenario where $\\mathbf{X}^{(1)}=0$) and the joint distribution of $(\\mathbf{X}^{(1)},\\mathbf{X}^{(2)})$ in the denominator (representing the observed scenario). This is mathematically distinct from integrating over the joint distribution in both terms, which would incorrectly double-count the exposure effect. The formula explicitly separates the role of $\\mathbf{X}^{(1)}$ (targeted for elimination) and $\\mathbf{X}^{(2)}$ (adjusted confounders), as seen in the definitions of $rr_{2x^{(2)}}$ (exposure fixed at 0) and $rr_{3x^{(1)}x^{(2)}}$ (varying exposure).",
    "question": "Is the statement 'The pPAR formula $p P A R=1-\\frac{\\int r r_{2x^{(2)}}f(\\mathbf{X}^{(2)}=\\mathbf{x}^{(2)})\\mathrm{d}\\mathbf{x}^{(2)}}{\\int r r_{3x^{(1)}x^{(2)}}f(\\mathbf{X}=\\pmb{x})\\mathrm{d}\\pmb{x}}$ correctly adjusts for confounding by integrating over the joint distribution of confounders $\\mathbf{X}^{(2)}$ and exposures $\\mathbf{X}^{(1)}$?'",
    "question_context": "Population Attributable Risk (PAR) Estimation with Misclassification Correction\n\nThe PAR represents the proportional reduction in disease prevalence that might occur if all other covariate distributions and associations of these with the outcome were unchanged, but the exposure was eliminated. We denote $Y$ the binary disease outcome, $\\mathbf{X}^{(1)}$ the targeted modifiable exposures, and $Y_{x}$ the counterfactual value of $Y$ if we had set $\\mathbf{X}^{(1)}=x$ in the population under study. A general form of the PAR can be written as $P A R=1-{\\frac{P(Y_{0}=1)}{P(Y=1)}}$. When there is no confounding of the estimated exposures-outcome relationship, $P(Y_{0}=1)=P(Y=1|\\mathbf{X}^{(1)}=0)$. However, when confounding exists, as would be the case in observational research, adjustment for confounders is necessary, and the pPAR estimates the proportion of cases that may have been avoided if the targeted modifiable exposures, $\\mathbf{X}^{(1)}$, were eliminated while the distributions of confounders and all associations in the hypothetical population remained unchanged."
  },
  {
    "qid": "statistic-posneg-1814-0-0",
    "gold_answer": "TRUE. The context explicitly states that for a stationary point process, $\\rho K(r)$ has the interpretation as the expected number of further points within distance $r$ from a typical point in $\\mathbf{X}$. This is a standard interpretation in spatial statistics, linking the $K$-function to the second-order properties of the point process.",
    "question": "For a stationary point process, the expected number of further points within distance $r$ from a typical point in $\\mathbf{X}$ is given by $\\rho K(r)$. Is this interpretation correct based on the provided context?",
    "question_context": "Point Process Summary Statistics and Likelihood Inference\n\nThis section considers the more classical summary statistics such as Ripley’s $K$ -function and the nearest-neighbour function $G$. Second order properties are described by the pair correlation function $g$, where it is convenient if $g(u,v)$ only depends on the distance $\\lVert u-v\\rVert$ or at least the difference $u-v$. The inhomogeneous reduced second moment measure is defined as ${\\mathcal{K}}(B)=\\int_{B}g(u)\\mathrm{d}u,\\quad B\\subseteq\\mathbb{R}^{2}$. For a stationary point process, $\\rho K(r)$ has the interpretation as the expected number of further points within distance $r$ from a typical point in $\\mathbf{X}$. The log likelihood function for a Poisson process with a parameterized intensity function $\\rho_{\\theta}$ is given by $l(\\theta){=}\\sum_{u\\in\\mathbf{x}}\\log\\rho_{\\theta}(u)-\\int_{W}\\rho_{\\theta}(u)\\mathrm{d}u$."
  },
  {
    "qid": "statistic-posneg-883-2-0",
    "gold_answer": "TRUE. The paper states that the estimator for the normalizing constants is consistent and asymptotically normal when the preliminary chains satisfy certain mixing conditions. This is supported by the asymptotic normality result provided in the paper, which shows that the estimator converges in distribution to a normal distribution as the number of regenerations increases.",
    "question": "Is the estimator for the normalizing constants consistent and asymptotically normal when the preliminary chains are generated under certain mixing conditions?",
    "question_context": "Multiple-Chain Importance Sampling with Unknown Normalizing Constants\n\nThe paper discusses a method for estimating unknown normalizing constants using multiple Markov chains. The key formulas include the log quasi-likelihood function, the estimator for the normalizing constants, and the asymptotic distributions of these estimators. The method involves a two-stage process where preliminary chains are used to estimate the normalizing constants, and these estimates are then used in shorter chains to compute the final estimates."
  },
  {
    "qid": "statistic-posneg-1368-3-0",
    "gold_answer": "TRUE. The context explicitly states that both theory and practice (citing Epanechnikov, 1969) suggest the choice of kernel is not crucial to the statistical performance of the method. The primary role of the kernel is to provide local weighting, and its exact form (e.g., Gaussian vs. Epanechnikov) has minimal effect on the estimator's asymptotic properties (bias and variance) compared to the window width $h$. Computational efficiency (e.g., using the Gaussian kernel for its smoothness and differentiability) is often prioritized over kernel type.",
    "question": "Is it true that the choice of kernel function $K$ has a negligible impact on the statistical performance of the kernel density estimator $f_n(x)$, as suggested by the context?",
    "question_context": "Kernel Density Estimation: Smoothing Parameter and Kernel Choice\n\nSuppose $X_{1},...,X_{n}$ are real observations from a probability density $f.$ The kernel estimate f. $f$ is defined by $$ f_{n}(x)=n^{-1}h^{-1}\\sum_{j=1}^{n}K\\{h^{-1}(x-X_{j})\\}, $$ where $\\pmb{K}$ is the kernel function and $h$ is the smoothing parameter or window width. The kernel estimator was introduced by Rosenblatt (1956) and has been the subject of much attention; for surveys see Rosenblatt (1971) and Fryer (1977). Density estimates are important for data analysis and presentation, among other applications; see, for example, Boneva et al. (1971) and Silverman (1978). Both theory and practice (see Epanechnikov, 1969) suggest that the choice of kernel is not crucial to the statistical performance of the method and therefore it is quite reasonable to choose a kernel for computational efficiency. The kernel we shall use is the standard Gaussian density. The character of the estimate is mainly governed by the choice of window width, which determines how much the data are smoothed to obtain the estimate. In this algorithm the choice of window width is left to the user. For exploratory purposes it is useful to examine estimates with several different window widths since these will highlight different features of the data; see Silverman (1981). If a single estimate is required then there are several methods available for choosing the window width; see Fryer (1977), Silverman (1978) and Habbema et al. (1974). The optimum window width depends on the unknown density being estimated, but"
  },
  {
    "qid": "statistic-posneg-1802-0-1",
    "gold_answer": "FALSE. Explanation: While the PoT model often assumes independence of exceedances for simplicity, this is not always the case in practice. The context explicitly mentions that if there is evidence of serial dependence, a common approach is to identify clusters of extremes and model only the frequency and magnitude of the cluster maxima. This is a theoretically justified method to handle dependence, as referenced by works like Davis et al. (2013) and others. Thus, independence is not an essential assumption and can be relaxed when necessary.",
    "question": "Is it correct to assume that the exceedances over a high threshold u are always independent in the Peaks-over-Threshold (PoT) model?",
    "question_context": "Extreme Value Theory: Generalized Pareto Distribution and Threshold Exceedances\n\nThe theory on which the vanilla versions of both block maxima and PoT models are based starts with the assumption of a sequence $Y_{1}$ , . . . , $Y_{m}$ of independent replicates of a random variable Y, with distribution $F(y)=\\operatorname*{Pr}[Y\\leq y]$ . Under some nonrestrictive conditions on $F$ , Von Mises (1936) and Gnedenko (1943) showed that in the limit as $m\\rightarrow\\infty$ , the distribution of the normalized block maximum $(M_{m}-b_{m})/a_{m}=(\\operatorname*{max}\\{Y_{1},...,Y_{m}\\}-b_{m})/a_{m}$ is one of the three types of max-stable distribution: Weibull, Fréchet, or Gumbel. While both the rate of convergence to the limiting dis­ tribution and the sequences $\\{a_{m}>0\\}$ and $\\{b_{m}\\}$ are determined by the underlying distribution $F$ which, in a modelling context is unknown, this result is nevertheless used to justify the use of the generalized extreme-value distribution (GEV) to model the maxima of large but finite blocks. The GEV is a class of distributions combining the three types of max-stable distribution. The resulting cumulative distribution function is, $$G(z)=\\exp\\left\\{-\\biggl[1+\\frac{\\xi}{\\sigma}(z-\\mu)\\biggr]^{-1/\\xi}\\right\\},~-\\infty<z<y^{+}$$ where $y^{+}<\\infty$ in the case $\\xi<0$ . This distribution has location $\\mu$ , scale $\\sigma>0$ and shape $\\xi$ parame­ ters, with the Weibull, Fréchet, and Gumbel distributions corresponding to $\\xi<0$ , $\\xi>0$ and $\\xi\\rightarrow$ 0, respectively. As a model the GEV has the advantage of increased flexibility over choosing one of the three types a priori. The vanilla PoT model is motivated by a result of Pickands (1975) who showed that, under some nonrestrictive regularity conditions, as $u\\to y^{+}$ for $y>u$ , $$F(y)=1-\\operatorname*{Pr}[Y>u]\\operatorname*{Pr}[Y>y\\mid Y>u]\\approx1-\\phi{\\bar{G}}(y)=1-\\phi{\\left[1+{\\frac{\\xi}{\\psi}}(y-u)\\right]}_{+}^{-1/\\xi}$$ where $a_{+}=\\operatorname*{max}{\\{a,0\\}},0\\leq\\phi\\leq1,\\psi$ $\\psi>0$ and $\\bar{G}(y)$ is the survival function of the generalized Pareto distribution. The parameters $\\phi,\\psi$ , and $\\xi$ are known as the rate, scale and shape, respectively; the rate $\\phi=\\operatorname*{Pr}[Y>u]$ is sometimes called the exceedance probability. The value of the shape param­ eter is determined by the rate at which $F(y)\\rightarrow1$ as $y\\to y^{+}$ ; exponential decay corresponds to $\\xi=0$ , and heavy (light) decay to $\\xi>0(\\xi<0)$ . For $\\xi\\ge0$ , there is no finite end-point, while $y^{+}=u-$ $\\sigma/\\xi<\\infty$ if $\\xi<0$ . By assuming that this limit result is a suitable approximation to the behaviour of the exceedan­ ces of a finite, but high, level $\\boldsymbol{\\mathscr{u}}$ , it can be used to motivate a model in which the magnitudes of the threshold exceedances $\\{y_{i}-u:y_{i}>u,i=1,...,n\\}$ are a random sample from the generalized Pareto distribution and the exceedance times a random sample from a homogeneous Poisson pro­ cess. While not essential an assumption of independence is often made about the exceedances. If there is evidence of serial dependence, a routine and theoretically justified approach is to identify all clusters of extremes and model only the frequency and magnitude of the cluster maxima (Davis et al., 2013; Drees, 2008; Laurini $\\&$ Tawn, 2003; Smith & Weissman, 1994). In either case, the parameters $(\\phi,\\psi,\\xi)$ are estimated with standard statistical inference procedures, e.g maximum likelihood or Bayesian inference. A crucial modelling consideration is the choice of threshold $\\boldsymbol{u}$ , which requires a compromise be­ tween retaining a sufficiently large number of exceedances such that estimation of the model pa­ rameters is possible, while still ensuring that the assumption of the limiting approximation is plausible. Various tools are available to aid threshold selection. Most of these use two key thresh­ old stability properties of the generalized Pareto distribution to identify the minimum plausible threshold above which the distribution is a suitable description of the data."
  },
  {
    "qid": "statistic-posneg-1710-3-2",
    "gold_answer": "TRUE. The context specifies that the acceptance probability assumes equal probabilities for proposing split and merge moves ('Assuming equal probabilities of proposing a split or a merge'). This symmetry simplifies the proposal ratio term in the Metropolis-Hastings acceptance rate, though the actual proposal probabilities may depend on object counts (e.g., n_A, n_B, n_C).",
    "question": "The split/merge move acceptance probability (Z_merge) assumes equal proposal probabilities for splitting object C into A and B and merging A and B into C. True or False?",
    "question_context": "Bayesian Object Identification via MCMC with Dimension-Changing Moves\n\nThe paper details a Markov chain Monte Carlo (MCMC) algorithm for Bayesian object identification, focusing on acceptance probabilities for dimension-changing moves such as resolution changes, object type changes, and split/merge operations. Key formulas include acceptance rates involving posterior density ratios, Jacobian terms for measure changes, and constraints on deformations and vertices."
  },
  {
    "qid": "statistic-posneg-1857-0-0",
    "gold_answer": "FALSE. The LNA approximation requires the solution of the ODE system given by (4), (7), and (11) to obtain the Gaussian distribution of the residual process. However, the condition is not sufficient on its own; it also requires the initial conditions and the Jacobian matrix to be correctly specified and integrated. The LNA is derived by linearizing the CLE around the deterministic trajectory, and the accuracy of the approximation depends on the validity of this linearization and the correct initialization of the ODE system.",
    "question": "Is the condition for the LNA approximation of the CLE given by the ODE system in (4), (7), and (11) correct for obtaining the Gaussian distribution of the residual process?",
    "question_context": "Stochastic Kinetic Models and Bayesian Inference\n\nThe paper discusses stochastic kinetic models (MJP, CLE, LNA) and Bayesian inference techniques, including particle MCMC and delayed-acceptance methods. It introduces the chemical reaction network (CRN) with species and reactions, and describes the inference task for rate constants given observations."
  },
  {
    "qid": "statistic-posneg-1436-0-0",
    "gold_answer": "TRUE. The context explicitly states that the performances of these affine-invariant test statistics did not change when the structure of the population’s variance–covariance matrix changed from $\\mathbf{I}_{3}$ to $\\mathbf{EE^{\\prime}}$. This is evidenced by comparing Table 3 with Table 4 and Table 5 with Table 6, which show consistent results across different variance–covariance matrices.",
    "question": "Is it true that the affine-invariant test statistics ($T^{2}$, $W_{w n p-1}$, $W_{l n p-1}$, PR, and $S_{n}$) showed consistent performance regardless of the change in the population variance–covariance matrix from $\\mathbf{I}_{3}$ to $\\mathbf{EE^{\\prime}}$?",
    "question_context": "Comparison of Test Statistics in Randomized Complete Block Designs\n\nTo compare the performances of $S_{n}$ and $W_{l n p-1}$ with respect to other procedures in the literature, a Monte Carlo study for dimension $p=3$ , sample size $n=40$ , and significance level $\\alpha=.05$ was conducted. Besides $S_{n}$ and $W_{l n p-1}$ , the study included $W_{w n p-1}$ , PR, the ANOVA $F$ test, Hotelling–Hsu $T^{2}$ , Friedman’s test $\\boldsymbol{\\cdot}\\boldsymbol{F}_{\\mathrm{R}})$ , Iman et al. [19] rank transformation version of the ANOVA $F$ test $(F_{\\mathrm{RT}})$ , the rank transformation test applied to centered within block data $(F_{\\mathrm{CRT}})$ and Quade’s test $(W_{\\mathrm{Q}})$ . Note that Quade’s test was computed using the range as the location-free statistic that measures the variability within the components of each observed vector. <PARAGRAPH_SEPARATOR> Five of the 10 procedures considered are computed on the transformed observations, that is the X’s. These are $T^{2}$ , $S_{n}$ , $W_{w n p-1}$ , PR, and $W_{l n p-1}$ , while the rest of the procedures use the original data, the Y’s. The test statistics $F$ , $F_{\\mathrm{RT}}$ , $F_{\\mathrm{CRT}}$ , and $W_{\\mathrm{Q}}$ are compared to upper \fth quantile of an $F$ distribution with $(p-1)$ and $(n-1)(p-1)$ , numerator and denominator degrees of freedom, respectively. No adjustment for the degrees of freedom was made since we noticed that doing so will only correct the $\\texttt{\\em}$ level of the test but will not have any significant effect on the power (see [20]). On the other hand, the tests $F_{\\mathrm{R}}$ , $S_{n}$ , $W_{w n p-1}$ , PR, and $W_{l n p-1}$ are compared to the upper \fth quantile of a chi-square distribution with $p-1$ degrees of freedom. The Hotelling–Hsu $T^{2}$ is compared to $(n-1)(p-1)/(n-p+1)$ times the upper \fth quantile of an $F$ distribution with $(p-1)$ and $(n-p+1)$ , numerator and denominator degrees of freedom, respectively. <PARAGRAPH_SEPARATOR> The test statistics were compared under elliptically symmetric distributions with densities from the power family and the family of multivariate $t\\cdot$ -distributions. From the power family, the heavy-tailed case $y=.1\\AA$ ), the multivariate normal case $(\\nu=1)$ ), and the light-tailed case $(\\nu=5)$ ) were used. From the family of multivariate $t\\cdot$ -distributions, distributions with degrees of freedom of 1, 3, and 10 were considered. For test statistics that use the original observations, the distributions were located at ${\\sf\\Psi}{\\sf\\Psi}=(k\\delta,k\\delta,0)$ , $k=0,1,2,3$ , and for the rest of the tests, the distributions were located at $\\mathbf{\\delta}\\mathbf{\\theta}=(k\\delta,k\\delta)$ , $k=0,1,2,3$ . The value $\\delta$ was adjusted for different distributions to obtain somewhat similar points on the power curve. Two different types of the population variance–covariance matrix were considered, the first is $\\pmb{\\Sigma}=\\mathbf{I}_{3}$ , the identity matrix and the second is <PARAGRAPH_SEPARATOR> $$ \\Sigma={\\bf E}{\\bf E}^{\\prime}=\\left[\\begin{array}{c c c}{1}&{-1}&{0}\\\\ {-1}&{2}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]. $$ <PARAGRAPH_SEPARATOR> The first choice satisfies the Huynh and Feldt condition, but $\\Sigma$ in (14) does not. This was done in order to show the dependence of the performances of $F_{\\cdot}$ , $F_{\\mathrm{R}}$ , $F_{\\mathrm{RT}}$ , $F_{\\mathrm{CRT}}$ , and $W_{\\mathrm{Q}}$ on the type of the population variance–covariance matrix. Tables 3 and 4 contain the results from the Monte Carlo study for distributions from the power family, while Tables 5 and 6 contain those for the distributions from the family of $t$ -distributions. Entries in each table are the proportion of times out of 3000 iterations in which each test statistic exceeded the upper $\\alpha$ -percentile of the corresponding null distribution. <PARAGRAPH_SEPARATOR> For the affine-invariant test statistics considered, that is, $T^{2}$ , $W_{w n p-1}$ , $W_{l n p-1}$ , PR, and $S_{n}$ , the results from the Monte Carlo studies agree with those obtained from the efficiency comparisons. Moreover, these performances did not change when the structure of the population’s variance–covariance matrix changed from ${\\bf I}_{3}$ to $\\mathbf{EE^{\\prime}}$ . This can be seen by comparing Table 3 with Table 4 and Table 5 with Table 6. <PARAGRAPH_SEPARATOR> Comparing all 10 tests, the following observations are made. When the underlying distribution is multivariate normal with $\\pmb{\\Sigma}=\\mathbf{I}_{3}$ , all 10 tests showed a good performance, with"
  },
  {
    "qid": "statistic-posneg-90-0-1",
    "gold_answer": "FALSE. The estimator $\\widehat{\\sigma}_{0k}^{2}$ does not require knowledge of the true $\theta_{i}$ values. Instead, it uses the unbiased estimator $\\widehat{\theta}_{i}$, which is derived from $\\widehat{c_{i}^{2}}(t,s)$, to consistently estimate $\\sigma_{0k}^{2}$. Proposition 2 states that $\\hat{\\sigma}_{0k}^{2}/\\sigma_{0k}^{2}\\xrightarrow{P}1$ under the given assumptions, confirming its consistency without requiring the true $\theta_{i}$ values.",
    "question": "Does the estimator $\\widehat{\\sigma}_{0k}^{2}$ require knowledge of the true $\theta_{i}$ values to be consistent?",
    "question_context": "Consistent Estimation of Null Variance in Hypothesis Testing\n\nProposition 1. Suppose that the assumptions in Lemma 1 and Assumptions 1–4 are fulfilled. Then (4) holds. <PARAGRAPH_SEPARATOR> # 4. Estimating the null variance <PARAGRAPH_SEPARATOR> In order to get a critical region for testing $H_{0}$ using the result in Corollary 1, we need a consistent estimator of $\\sigma_{0k}$ . Recall that $\begin{array}{r}{\\sigma_{0k}^{2}=k\times v a r_{0}(T_{k,L i n})=2/k\\sum_{i=1}^{k^{-}}\theta_{i}/n_{i}(\\stackrel{.}{n_{i}}-1).\\sigma_{0}^{2}}\\end{array}$ $\\sigma_{0k}^{2}$ is unknown because $\theta_{1},\\ldots,\theta_{k}$ are unknown quantities. From expression (1), $\theta_{i}$ can be unbiasedly  estimated by replacing $c_{i}^{2}(t,s)$ with an unbiased estimator. Assume that $n_{i}\\geq4$ $1\\leq i\\leq k$ , then $c_{i}^{2}(t,s)$ can be unbiasedly estimated by <PARAGRAPH_SEPARATOR> $$ \\widehat c_{i}^{2}(t,s)=\\frac{1}{n_{i}(n_{i}-1)(n_{i}-2)(n_{i}-3)}\\sum_{\\substack{1\\leq u\neq v\neq w\neq z\\leq n_{i}}}h(X_{i u},X_{i v};t,s)h(X_{i w},X_{i z};t,s), $$ <PARAGRAPH_SEPARATOR> where $\begin{array}{r}{h(X_{i u},X_{i v};t,s)=\\frac12\\{X_{i u}(t)-X_{i v}(t)\\}\\{X_{i u}(s)-X_{i v}(s)\\}.}\\end{array}$ Note that <PARAGRAPH_SEPARATOR> $$ \\widehat{c_{i}^{2}}(t,s)=\\frac{1}{n_{i}(n_{i}-1)}\\sum_{1\\leq u\neq v\\leq n_{i}}h(X_{i u},X_{i v};t,s)S_{i(u,v)}^{2}(t,s), $$ <PARAGRAPH_SEPARATOR> uwsheefrule $\begin{array}{r}{S_{i(u,v)}^{2}(t,s)=\\frac{1}{n_{i}-3}\\sum_{w\neq u,v}\\{X_{i w}(t)-\bar{X}_{i(u,v)}(t)\\}\\{X_{i w}(s)-\bar{X}_{i(u,v)}(s)\\}}\\end{array}$ , and $\begin{array}{r}{\bar{X}_{i(u,v)}(t)=\\frac{1}{n_{i}-2}\\sum_{w\neq u,v}X_{i w}(t).}\\end{array}$ Formula (6) is for computational purposes. Let $\begin{array}{r}{\\hat{\\sigma}_{0k}^{2}=\bar{(2/k)}\\sum_{i=1}^{k}\\hat{{\theta}}_{i}\bar{/}n_{i}({n}_{i}-1)}\\end{array}$ , where $\begin{array}{r c l}{\\widehat{\theta}_{i}}&{=}&{\\int\\int\\widehat{c_{i}^{2}}(t,s)d t d s}\\end{array}$ . Next proposition shows that, under some mild assumptions, the quotient 0/ onverges in probability ˆto 1. <PARAGRAPH_SEPARATOR> Proposition 2. Suppose that the assumptions in Lemma 1 and Assumptions 1–3 are fulfilled, and that $n_{i}\\geq4,1\\leq i\\leq k.$ Then $\\hat{\\sigma}_{0k}^{2}/\\sigma_{0k}^{2}\\xrightarrow{P}1$ . <PARAGRAPH_SEPARATOR> As an immediate consequence of Corollary 1 and Proposition 2, we have the following result."
  },
  {
    "qid": "statistic-posneg-114-1-3",
    "gold_answer": "FALSE. The paper's Definition explicitly states that for densities in family F, the Fisher information matrix I(X) must be non-singular (condition (c)). This is a regularity condition imposed on the family F, ensuring the inverse I⁻¹(X) exists for all subsequent results.",
    "question": "The Fisher information matrix I(X) for a continuous random vector X in family F is always singular. Is this statement true based on the paper's definition?",
    "question_context": "Fisher Information and Variance Inequalities in Statistical Models\n\nThe paper discusses the properties of Fisher information and its relationship with variance inequalities in both continuous and discrete random vectors. It defines a family of densities F with specific regularity conditions and derives various inequalities involving Fisher information matrices. Key results include variance lower bounds, convolution inequalities for Fisher information, and characterizations of normal and Poisson distributions based on equality conditions in these inequalities."
  },
  {
    "qid": "statistic-posneg-1264-0-0",
    "gold_answer": "TRUE. The proof shows that if an external point $l$ were to join a subset $P$ of mutual cluster $S$ before all points in $S$ are merged, it would require $\\operatorname*{min}_{j\\in S}d(l,j)<\\operatorname{diameter}(S)$. However, the definition of a mutual cluster requires $\\operatorname*{min}_{j\\in S}d(i,j)>\\operatorname{diameter}(S)$ for all external points $i$, leading to a contradiction. Thus, no external point can break the mutual cluster by merging with a subset of it prematurely.",
    "question": "In the context of mutual clusters in hierarchical clustering, the condition $\\operatorname*{min}_{j\\in S}d(i,j)>\\operatorname{diameter}(S),\\quad\\forall i\\notin S$ implies that no external point can join any subset of a mutual cluster before all internal points are merged. Is this statement true based on the provided proof?",
    "question_context": "Mutual Cluster Preservation in Hierarchical Clustering\n\nProof (by contradiction). Consider complete-linkage bottom-up clustering, in which the distance between two groups of points is the maximum of all pairwise inter-group distances. Let $S$ be a mutual cluster, and suppose that $l\\notin S$ is a point that is joined to a strict subset $P\\subset S$ . That is, $l$ breaks the mutual cluster $S$ by merging with part of it, $P$ , before all of the points in $S$ are joined. If $l$ joins $P$ before any point in $Q=S\\cap{\\overline{{P}}}$ , we must have that $d_{\\mathrm{com}}(l,P)<d_{\\mathrm{com}}(i,P),\\quad\\forall i\\in Q=S\\cap\\overline{{P}},$ where the complete linkage distance $d_{\\mathrm{com}}$ is given by $d_{\\mathsf{c o m}}(l,P)\\equiv\\operatorname*{max}_{j\\in P}d(l,j).$ Now since $Q\\subset S$ and $P\\subset S$ , we have $d_{\\mathsf{c o m}}(i,P)\\leqslant\\mathrm{diameter}(S),\\quad\\forall i\\in\\mathcal{Q}.$ Now (A.1) and (A.2) imply that $\\exists l\\not\\in S{\\mathrm{~such~that~}}\\operatorname*{max}_{j\\in P}d(l,j)<\\operatorname{diameter}(S).$ Now $\\operatorname*{min}_{j\\in S}d(l,j)<\\operatorname*{max}_{j\\in P}d(l,j),$ so (A.3) becomes $\\exists l\\not\\in S{\\mathrm{~such~that~}}\\operatorname*{min}_{j\\in S}d(l,j)<\\operatorname{diameter}(S).$ But because $S$ is a mutual cluster, we know that $\\operatorname*{min}_{j\\in S}d(i,j)>\\operatorname{diameter}(S),\\quad\\forall i\\notin S.$ Thus, (A.4) and (A.5) are a contradiction: $l$ cannot be joined to any point in mutual cluster $S$ before any member of $S$ . This proof applies equally well to single, average, or complete linkage."
  },
  {
    "qid": "statistic-posneg-1795-0-2",
    "gold_answer": "TRUE. Higher positive correlation $\\rho$ reduces the effective dimensionality of the problem because the variables become more linearly dependent. This increases the precision of estimating the largest mean $\\mu^{*}$ (since extreme values are less variable when components are highly correlated), allowing shorter intervals to achieve the same coverage probability $\\gamma$. The mathematical dependence arises from the $\\rho$-sensitive joint distribution $\\mathcal{F}_{\\nu}(t_{1},...,t_{k};\\rho)$, where higher $\\rho$ tightens the distribution of sample maxima.",
    "question": "Is the claim 'The length of the interval is decreasing as the correlation coefficient is increasing' consistent with the properties of multivariate normal distributions under the paper's assumptions?",
    "question_context": "Optimal Confidence Interval for the Largest Mean in Dependent Normal Populations\n\nIn statistical analysis the assumption of independence fits many experimental situations. However, in the biological and behavioural sciences the independence of populations cannot be guaranteed since it often happens that the experimental observations are collected at different times on the same experimental unit. For example, in medical research it is frequently necessary to subject the same patient, or other experimental unit, to each of several drugs at different times. In such a case, a dependence is induced by the characteristics of the patient rather than the drug. This type of experimental design is usually termed a repeated measurements design. In statistical analysis it is usually assumed that observations collected from repeated measurements design obey a multivariate normal rule, or come from dependent normal populations. The interest of this paper is to propose an interval estimate for the mean of thebest treatment.<PARAGRAPH_SEPARATOR>For normal populations that are dependent with complicated correlation structure, there is little work on the estimation of the ranked component means. In this paper we consider only the case where the correlation coefficients are common and known and the variances are equal, being known or unknown. An optimal confdence interval for the largest, or smallest, mean in the sense of minimizing the length of the interval with respect to a coverage of probability at least equal to $\\gamma(0<\\gamma<1)$ is obtained for both variance known and unknown. A single-stage sampling procedure is used. Percentage points to apply this procedure are also provided corresponding to various values of number of groups $\\pmb{k}$ correlationcoefficient $\\pmb{\\rho}_{1}$ variance degrees of freedom $\\pmb{\\nu}$ and coverageprobability $\\gamma$ . The length of the interval is decreasing as the correlation coeffcient is increasing. Asymptotically, a large class of important multivariate distributions converge to the multivariate normal distribution; hence the results obtained here would be applicable to these problems when the sample size is chosen to be sufficiently large.<PARAGRAPH_SEPARATOR>Let $\\pmb{X}^{\\prime}=(X_{1},...,X_{k})$ denote a $\\pmb{k}$ -variate $(k\\geqslant2)$ normal random vector with unknown mean vector $\\mu^{\\prime}=(\\mu_{1},\\ldots,\\mu_{k}),$ common variance ${\\pmb\\sigma}^{2}$ and a given correlation matrix $\\pmb{R}=((\\rho_{i j}))$ having $\\rho_{i j}=1$ for $\\pmb{\\mathscr{i}}=\\pmb{\\mathscr{j}}$ and $\\rho_{i j}=\\rho\\geqslant0$ for $i\\neq j$ .Let $\\mu^{*}$ denote the largest value of $\\mu_{i}\\mathbf{s}$ Let $X_{j}^{\\prime}=(X_{i j},...,X_{k j})$ for ${\\dot{\\boldsymbol{\\jmath}}}=1,...,{\\boldsymbol{n}}_{\\mathrm{;}}$ denote a sample of $\\pmb{\\mathscr{n}}$ independent vector observations on $\\pmb{X}^{\\prime}$ , let ${\\overline{{X}}}^{*}$ denote the largest value of $\\overline{{\\pmb{X}}}_{i}^{\\prime}{\\mathfrak{s}}$ where $\\overrightarrow{X}_{i}=\\Sigma_{j}X_{i j}/n(i=1,...,k)$ , and let $\\pmb{S}$ be the sample unbiased covariance matrix with $(i,j)$ th element $8_{i j}.$ where<PARAGRAPH_SEPARATOR>$$s_{i j}=\\Sigma_{l}(X_{u}-\\overline{{X}}_{i})(X_{n}-\\overline{{X}}_{j})/(n-1).$$<PARAGRAPH_SEPARATOR>We propose a conservative confidence interval $\\pmb{I}$ for estimating the largest component mean $\\mu^{*}$ based on the largest sample component mean $\\overleftrightarrow{X}^{*}$ and the sample covariance matrix $\\pmb{S}$ when ${\\pmb\\sigma^{2}}$ is unknown. Given a positive number $\\pmb{\\gamma}$ ,such that $0<\\gamma<1$ , we require that<PARAGRAPH_SEPARATOR>$$\\operatorname*{inf}_{\\Omega}\\operatorname{pr}_{\\gamma}(\\mu^{*}\\in I)\\geqslant\\gamma,$$<PARAGRAPH_SEPARATOR>where $\\Omega$ denotes the set of admissible values of the vector $\\pmb{\\mu}$ . We optimize the choice of $\\pmb{I}$ by minimizing the length of the interval $\\pmb{I}$ for which (1·1) holds.<PARAGRAPH_SEPARATOR>Denote by $\\Phi(z_{1},...,z_{k};\\rho)$ the cumulative distribution function of a $\\pmb{k}$ -variate normal random vector with means 0, variances l, and correlation coefficient $\\pmb\\rho$ , and by $\\mathcal{F}_{\\nu}(t_{1},...,t_{k};\\rho)$ the cumulative distribution function of a $\\pmb{k}$ -variate $\\pmb{\\ell}$ random vector with $\\pmb{\\nu}$ degrees of freedom and correlation coeffcients $\\pmb\\rho$ . When $k=1$ , the multivariate distributions reduce to the univariate standard normal distribution, $\\Phi(z)$ and to Student's $\\pmb{t}$ distribution, $\\pmb{\\mathcal{F}}_{p}(t)$<PARAGRAPH_SEPARATOR>In $\\S2$ we consider the optimal confidence interval $\\pmb{I}$ for $\\mu^{*}$ with varianceknown and unknown. In $\\S3$ the computational procedure is considered and tables are provided."
  },
  {
    "qid": "statistic-posneg-164-0-2",
    "gold_answer": "FALSE. The efficient score for length-biased data involves a different transformation of the density $g$ and uses $\\tilde{g}=u g(u)/\\int S(u)d(u)$ instead of $S(u)/\\int S(u)d(u)$. The score operator and the form of $R a(t)$ are adjusted accordingly, leading to a different efficient score expression.",
    "question": "The efficient score for $\\theta$ in the length-biased data setting is identical to that in the standard accelerated failure time model. Is this statement true?",
    "question_context": "Efficient Score Estimation in Semiparametric Accelerated Failure Time Models\n\nThe paper discusses efficient score estimation in semiparametric accelerated failure time models, focusing on the derivation of efficient scores and tangent spaces. The model is defined by the distribution $P_{\theta,g,h}^{*}$ with density $\\frac{d P_{\theta,g,h}^{*}}{d\\nu}(t,z)=e^{(\\theta-\\theta_{0})^{\\prime}z}g\\{e^{(\\theta-\\theta_{0})^{\\prime}z}t\\}h(z)$, where $\\theta_{0}$ is the true parameter value. The efficient score for $\\theta$ is derived using orthogonal projections onto tangent spaces for $S$ and $h$."
  },
  {
    "qid": "statistic-posneg-1836-0-1",
    "gold_answer": "TRUE. The context explicitly states that the mean similarity measure equals 1 if and only if there exists a partition $\\mathcal{P}=\\{P_{1},\\ldots,P_{k}\\}$ and warping functions $\\underline{{\\mathbf{h}}}$ such that $\\rho(\\mathbf{c}_{i}\\circ h_{i},\\mathbf{c}_{j}\\circ h_{j})=1$ for all $i$ and $j$ in the same cluster. This condition implies perfect alignment and clustering.",
    "question": "The mean similarity measure $\\frac{1}{N}\\sum_{i=1}^{N}\\rho(\\pmb{\\varphi}_{\\lambda(\\underline{{\\varphi}},\\mathbf{c}_{i})},\\mathbf{c}_{i}\\circ h_{i})$ can only achieve a value of 1 if all curves can be perfectly aligned and clustered into $k$ groups. True or False?",
    "question_context": "k-Means Alignment for Curve Clustering with Unknown Templates\n\nCase of unknown templates. Ideally, in order to cluster and align the set of $N$ curves $\\{\\mathbf{c}_{1},\\hdots,\\mathbf{c}_{N}\\}$ with respect to $k$ unknown templates we should first solve the following optimization problem: (i) find $\\underline{{\\varphi}}=\\{\\pmb{\\varphi}_{1},\\dots,\\pmb{\\varphi}_{k}\\}\\subset\\mathcal{C}$ and $\\mathbf{\\underline{{h}}}=\\{h_{1},\\dots,h_{N}\\}\\subset W$ such that $\\frac{1}{N}\\sum_{i=1}^{N}\\rho(\\varphi_{\\lambda(\\underline{{\\varphi}},\\mathbf{c}_{i})},\\mathbf{c}_{i}\\circ h_{i})\\geq\\frac{1}{N}\\sum_{i=1}^{N}\\rho(\\psi_{\\lambda(\\underline{{\\psi}},\\mathbf{c}_{i})},\\mathbf{c}_{i}\\circ g_{i}),$ for any other set of $k$ templates $\\underline{{\\psi}}=\\{\\pmb{\\psi}_{1},\\dots,\\pmb{\\psi}_{k}\\}\\subset\\mathcal{C}$ and any other set of $N$ warping functions $\\mathbf{\\underline{{g}}}=\\{g_{1},\\ldots,g_{N}\\}\\subset W$ ; and then, for $i=1,\\ldots,N$, (ii) assign ${\\bf c}_{i}$ to the cluster $\\lambda(\\underline{{\\pmb\\varphi}},\\mathbf{c}_{i})$ , and align it to the corresponding template, ${\\pmb\\varphi}_{\\lambda(\\underline{{\\varphi}},{\\bf c}_{i})}$ , using the warping function $h_{i}$. warping functions. Note that the solution $(\\underline{{\\pmb{\\varphi}}},\\underline{{\\pmb{\\ h}}})$ to (i) has mean similarity $\\frac{1}{N}\\sum_{i=1}^{N}\\rho(\\pmb{\\varphi}_{\\lambda(\\underline{{\\varphi}},\\mathbf{c}_{i})},\\mathbf{c}_{i}\\circ h_{i})$ $k$ equal to 1 if and on $N$ if it is possible to perfectly align and cluster in $k$ groups the set of curves, i.e., if and only if there exist $\\underline{{\\mathbf{h}}}$ and a partition $\\mathcal{P}=\\{P_{1},\\ldots,P_{k}\\}$ of $\\{1,\\ldots,N\\}$ in $k$ elements, such that $\\rho(\\mathbf{c}_{i}\\circ h_{i},\\mathbf{c}_{j}\\circ h_{j})=1$ for all i and $j$ belonging to the same element of $\\mathcal{P}$."
  },
  {
    "qid": "statistic-posneg-1573-6-0",
    "gold_answer": "TRUE. The estimator $\\mathsf{L}_{T,N}^{\\times}$ explicitly includes the terms $\\sigma_{+}$ and $\\sigma_{-}$ in its formula, meaning these volatility coefficients must be known or estimated beforehand for the estimator to be computed.",
    "question": "Is it true that the estimator $\\mathsf{L}_{T,N}^{\\times}$ requires knowledge of $\\sigma_{+}$ and $\\sigma_{-}$ for its implementation?",
    "question_context": "Convergence and Estimation of Local Time in Oscillatory Brownian Motion\n\nThe convergence of $\\mathsf{Q}_{T,N}^{\\pm}$ was discussed in Lejay and Pigato (2018) for the OBM (see also Lemma 1). We prove that the speed of convergence is strictly better than $\\sqrt{N}$ , meaning that $\\sqrt{N}(\\mathsf{Q}_{T,N}^{+}-\\mathsf{Q}_{T}^{+})\\xrightarrow[n\\to\\infty]{\\mathbb{P}}0$ . This result can be extended to the drifted process $\\xi$ via the Girsanov theorem.<PARAGRAPH_SEPARATOR>Alternatively to computing $\\mathsf{R}_{T,N}^{\\pm}$ , we may approximate the local time:<PARAGRAPH_SEPARATOR>• The approximation $\\mathsf{L}_{T,N}$ to the local time given by Equation (10) is easily implemented. • In Lejay and Pigato (2018); Lejay et al. (2019), we have also considered two other consistent estimators for the local time. For the OBM, they are<PARAGRAPH_SEPARATOR>$$\\overset{\\mathtt{\\xi}_{\\mathtt{{-}},N}}{\\neg}:=\\frac{-3}{2}\\sqrt{\\frac{\\pi}{2\\Delta t}}\\frac{\\sigma_{+}+\\sigma_{-}}{\\sigma_{+}\\sigma_{-}}\\sum_{k=1}^{N}((\\sharp_{k T/N}\\mathfrak{h}^{+}-\\mathbb{(}(\\xi_{(k-1)T/N}\\mathfrak{h}^{+})\\cdot(\\mathbb{(} \\xi_{k T/N}\\mathfrak{h}^{-}-\\mathbb{(} \\xi_{(k-1)T/N}\\mathfrak{h}^{-}),$$<PARAGRAPH_SEPARATOR>and<PARAGRAPH_SEPARATOR>$$\\mathsf{L}_{T,N}^{\\times}:=\\frac{4}{\\sqrt{2\\pi}}\\times\\frac{1}{\\sigma_{+}+\\sigma_{-}}\\times\\frac{1}{\\sqrt{N}}\\sum_{k=0}^{N-1}1_{\\xi_{k T/N}\\xi_{(k+1)T/N+1}<0}.$$<PARAGRAPH_SEPARATOR>• The expressions of $\\mathsf{L}_{T,N}^{\\dagger}$ and $\\mathsf{L}_{T,N}^{\\times}$ require $\\sigma_{+}$ and $\\sigma_{-}$ , unlike the one of $\\mathsf{L}_{T,N}$ . To the best of our knowledge, this is the first time that such estimator is considered.   • For the Brownian motion, these estimators converge at rate $N^{1/4}$ , thanks to the results of Jacod (1998). The proof has not been adapted to OBM because of the technical difficulties due to the discontinuity of the coefficients at 0. Anyway, we conjecture a rate of $1/4$ .   • The three different estimators for the local time seem to have all comparable accuracy on numerical simulations if the volatility coefficients $\\sigma_{\\pm}$ are known. It looks like estimator $\\mathsf{L}_{T,N}^{\\times}$ is the best one when $\\sigma_{+}\\approx\\sigma_{-}$ , whereas $\\mathsf{L}_{T,N}$ and $\\mathsf{L}_{T,N}^{\\dagger}$ look more accurate when $\\sigma_{+}\\neq\\sigma_{-}$ .   • Anyway, if these coefficients are not known, only $\\mathsf{L}_{T,N}$ can be implemented directly, whereas to implement the other estimators we have first to estimate $\\sigma_{\\pm}$ from observations of $\\xi$ . This problem has been thoroughly investigated in Lejay and Pigato (2018). As a consequence, a good estimation of the local time relies on the estimation of $\\sigma_{\\pm}$ . Therefore, we choose to show here the implementation of the estimator using $\\mathsf{L}_{T,N}$ , which does not suffer of this possible additional problem.<PARAGRAPH_SEPARATOR>In practice, this means that we are actually showing estimator $\\beta_{T,N}^{\\pm}$ in (11) in place of $\\beta_{T}^{\\pm}$ . The numerical results using an approximation of $\\beta_{T}^{\\pm}$ using $\\mathsf{Q}_{T,N}^{\\pm}$ and $\\mathsf{L}_{T,N}^{\\dagger}$ for the local time are very similar, if $\\sigma_{\\pm}$ is supposed to be known.<PARAGRAPH_SEPARATOR>We do not push further in this paper the theoretical discussion on the quality of these discrete-time approximations. Some more insights, based on numerical results, are given in the following section."
  },
  {
    "qid": "statistic-posneg-1448-3-0",
    "gold_answer": "FALSE. The context proves injectivity (via the integral equality $\\int c_{x}1_{E}(x)\\eta_{\\sigma}(d x) = \\sigma(D)$) and surjectivity (via constructing $\\sigma$ such that $\\eta = \\eta_{\\sigma}$), but the bijectivity claim requires both properties to hold simultaneously under consistent definitions of $F$ and $\\mathfrak{L}$. The text does not explicitly confirm that the same $F$ satisfies both conditions without additional constraints, leaving potential gaps in the bijectivity proof.",
    "question": "Is the mapping $F$ from ${\\mathfrak{M}}^{1}(C)$ to $\\mathfrak{L}$ bijective based on the provided derivation and justification in the context?",
    "question_context": "Operator-Stable Measures and Injectivity of Mapping in Convex Analysis\n\n[For $f\\in\\Re(V^{*})$ the function $x\\to\\int f d\\eta_{x}$ on $C$ is in $\\mathfrak{R}(C)$ in view of (iv). Hence the assertion.] <PARAGRAPH_SEPARATOR> (vi) $F$ is an injective mapping of ${\\mathfrak{M}}^{1}(C)$ onto $\\mathfrak{L}$ <PARAGRAPH_SEPARATOR> [Let $D\\in{\\mathfrak{B}}(C)$ and define $E=\\left\\{e^{t A}y;y\\in D,0<t<1\\right\\}$ . Then in view of (i) we have for every $\\pmb{\\sigma}\\in\\mathfrak{M}^{1}(C)$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{\\lefteqn{\\int c_{x}1_{E}(x)\\eta_{\\sigma}(d x)=\\int_{C}\\left(\\int c_{x}1_{E}(x)\\eta_{y}(d x)\\right)\\sigma(d y)}}\\ &{=\\int_{C}c_{y}^{-1}\\left(\\int_{\\mathbb{R}}c_{(\\exp{t}A)y}1_{E}(e^{t A}y)e^{-t}d t\\right)\\sigma(d y)}\\ &{=\\int_{C}\\left(\\int_{\\mathbb{R}}1_{E}(e^{t A}y)d t\\right)\\sigma(d y)=\\sigma(D).}\\end{array} $$ <PARAGRAPH_SEPARATOR> This proves the injectivity of $F.$ <PARAGRAPH_SEPARATOR> Now let $\\pmb{\\eta}\\in\\mathfrak{L}$ Then by $\\nu(D):=\\eta(\\{e^{\\iota A}x\\colon x\\in D,t>0\\}$ $D\\in{\\mathfrak{B}}(C);$ there is defined a finite measure $\\pmb{\\nu}$ on ${\\mathfrak{B}}(C)$ such that $\\eta=\\int_{C}\\eta_{x}^{\\prime}\\nu(d x)$ [4, Theorem 2]. Denote by $\\pmb{\\sigma}$ the measure on $\\mathfrak{B}(C)$ with $v$ density $x\\rightarrow c_{x}$ (cf. (i)). Then we   have $\\eta=\\int_{C}\\eta_{x}\\sigma(d x)$ and $\\sigma(C)=\\int_{C}\\left(\\int\\varphi d\\eta_{x}\\right)\\sigma(d x)=\\int\\varphi d\\eta=1$ i.e., $\\sigma\\in\\mathfrak{M}^{1}(C)$ and $\\eta=\\eta_{\\sigma}$ . This proves the surjectivity of $F$ 门 <PARAGRAPH_SEPARATOR> For the theory of compact convex sets we refer to [1]. In particular let us recall that a (compact) simplex with closed extreme boundary is said to be aBauer simplex."
  },
  {
    "qid": "statistic-posneg-911-0-1",
    "gold_answer": "FALSE. The posterior variance (3.7) is a monotone increasing function of τ. This means that as τ increases, the posterior variance also increases, reflecting greater uncertainty about the response function. This is because a larger τ indicates less certainty a priori about the adequacy of the low-degree polynomial approximation, leading to higher posterior uncertainty. The variance is smallest when τ=0, corresponding to the standard least squares regression case.",
    "question": "Does the posterior variance of the response function g(X) decrease as the prior variance parameter τ increases?",
    "question_context": "Bayesian Hermite Expansion Model for Polynomial Regression\n\nThe paper presents a Bayesian approach to flexible modeling using Hermite polynomial expansions. It contrasts standard polynomial regression (with coefficients estimated by ordinary least squares) with a more flexible model that allows for intermediate prior variances. The model is expressed as a combination of terms with proper and improper priors, and the posterior distribution of the response function is derived, including its mean and variance. The approach allows for interpolation as the prior variance parameter τ approaches infinity, and discusses the implications of different choices for τ and w on the posterior variance."
  },
  {
    "qid": "statistic-posneg-1714-1-0",
    "gold_answer": "FALSE. The calculation provided in the formula is incorrect. The residual covariance should not be zero, as the terms ${\\boldsymbol{x}}_{1}^{2}{\\boldsymbol{x}}_{2}$ and $\\scriptstyle{x_{2}x_{3}^{2}}$ are not necessarily uncorrelated after regressing on $x_{(1)}$. The correct calculation would involve evaluating the expectations and the inverse matrix $I_{3}^{-1}$ properly, which would not necessarily yield zero.",
    "question": "Is the residual covariance between ${\\boldsymbol{x}}_{1}^{2}{\\boldsymbol{x}}_{2}$ and $\\scriptstyle{x_{2}x_{3}^{2}}$ after regressing on $x_{(1)}$ correctly calculated as zero in the given formula?",
    "question_context": "Rao's Score Test for Goodness-of-Fit and Independence in Multivariate Normality\n\nTHEOREM 1. Let $x\\sim N_{p}(0,I_{p})$ :After regressing on ${\\pmb u}({\\pmb x})$ , the elements of ${\\pmb v}({\\pmb x})$ are residually uncorrelated. Further, the residual variance of an element of ${\\pmb v}({\\pmb x})$ of degree $d$ andpartitiontype $L$ is $d!/r(L)$ <PARAGRAPH_SEPARATOR> Outline of proof. From the symmetry of the normal distribution it follows immediately that all odd order moments are uncorrelated with all even order moments. In particular, when evaluating the residual variance of $\\pmb{x}_{(3)}$ , regressing on $x_{(1)}$ and $x_{(2)}$ is the same as the regressing on $x_{(1)}$ . Similarly when evaluating the residual variance of $\\pmb{x}_{(4)}$ , regressing on $x_{(1)}$ and $x_{(2)}$ is the same as regressing on $x_{(2)}$ . Further all the elements of $x_{(3)}$ are residually uncorrelated with all the elements of $\\pmb{x}_{(4)}$ , after regressing on $x_{(1)}$ and $x_{(2)}$ <PARAGRAPH_SEPARATOR> For the remaining calculations it seems necessary to use brute force. Although the answer is elegant, there does not seem to be a simple method of proof. As an example of the sort of calculation needed, we evaluate the residual covariance of ${\\boldsymbol{x}}_{1}^{2}{\\boldsymbol{x}}_{2}$ and $\\scriptstyle{x_{2}x_{3}^{2}}$ after regressing on $x_{(1)}$ . This quantity is given by <PARAGRAPH_SEPARATOR> $$ E(x_{1}^{2}x_{2}^{2}x_{3}^{2})-E[x_{1}^{3}x_{2},x_{1}^{2}x_{2}^{2},x_{1}^{2}x_{2}x_{3}]I_{3}^{-1}E{\\left[\\begin{array}{l}{x_{1}x_{2}x_{3}^{2}}\\ {x_{2}^{2}x_{3}^{2}}\\ {x_{2}x_{3}^{3}}\\end{array}\\right]}=1-[0,1,0]{\\left[\\begin{array}{l}{0}\\ {1}\\ {0}\\ {0}\\end{array}\\right]}=0. $$ <PARAGRAPH_SEPARATOR> It should be noted that this theorem does not generalize to $d>4$ , at least not in a form relevant for our purpose here. For use below let $A_{d}$ denote the residual covariance matrix of $x_{(d)}$ after regressing on $x_{(1)}$ and $x_{(2)}$ , for $d=3$ , 4. Thus $A_{d}$ is a $\\pmb{p}(d)\\times\\pmb{p}(d)$ diagonal matrix with typical element $d!/r(L)$ , where $L$ is the partition type of the corresponding element of x(d)· <PARAGRAPH_SEPARATOR> We now have all the information needed to compute Rao's score statistic. Let <PARAGRAPH_SEPARATOR> $$ \\{x_{r};r=1,\\ldots,n\\} $$ <PARAGRAPH_SEPARATOR> be a sample of $\\pmb{p}$ -vectors with elements $x_{r i}$ $(i=1,\\ldots,p)$ and dth order powers $x_{(d),r}$ and ${\\bf{\\boldsymbol{x}}_{[d],\\boldsymbol{\\ r}}}$ . Transform the data to $z_{r}=S^{-1}(x_{r}-\\bar{x})$ ,where $\\bar{x}$ and $s$ are the sample mean vector and sample covariance matrix. Let $\\bar{z}_{(d)}=n^{-1}\\Sigma z_{(d),\\prime}$ and $\\begin{array}{r}{\\tilde{z}_{[d]}=n^{-1}\\sum z_{[d],r}}\\end{array}$ . Then Rao's score statistic takes theform <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\displaystyle{T=n\\{\\bar{z}_{(3)}^{\\mathrm{T}}A_{(3)}^{-1}\\bar{z}_{(3)}+(\\bar{z}_{(4)}-\\mu_{(4)})^{\\mathrm{T}}A_{(4)}^{-1}(\\bar{z}_{(4)}-\\mu_{(4)})\\}}}}\\ {{\\displaystyle{\\quad=n\\bigg\\{\\frac{1}{3!}\\bar{z}_{[3]}^{\\mathrm{T}}\\bar{z}_{[3]}+\\frac{1}{4!}(\\bar{z}_{[4]}-\\mu_{[4]})^{\\mathrm{T}}\\big(\\bar{z}_{[4]}-\\mu_{[4]}\\big)\\bigg\\}}}}\\ {{\\displaystyle{\\quad=T_{3}+T_{4},}}}\\end{array} $$ <PARAGRAPH_SEPARATOR> say. Under $H_{0}$ , $T_{3}\\sim\\chi_{p(3)}^{2}$ and $\\ensuremath{T_{4}}\\sim\\ensuremath{\\chi_{p(4)}}^{2}$ , independently of one another so $T\\sim\\chi_{p(3)+p(4)}^{2}$ The representation for $T$ using square-bracket subscripts is the simplest to write down. The second line follows from the first line in (4.1) because the diagonal elements of $d!A_{d}^{-1}$ just count the number of replications of each component of $x_{(d)}$ in $x_{[d]}$ so that <PARAGRAPH_SEPARATOR> $$ d!x_{(d)}^{\\mathsf{T}}A_{d}^{-1}x_{(d)}=x_{[d]}^{\\mathsf{T}}x_{[d]}. $$ <PARAGRAPH_SEPARATOR> It can be shown that <PARAGRAPH_SEPARATOR> $$ T_{3}=n b_{1,p}/6,T_{4}=n\\{b_{2p}^{*}-6b_{2,p}+3p(p+2)\\}/24, $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ b_{1,p}=\\frac{1}{n^{2}}\\sum_{r=1}^{n}\\sum_{s=1}^{n}D_{r s}^{3},\\quad b_{2,p}=\\frac{1}{n}\\sum_{r=1}^{n}D_{r r}^{2},\\quad b_{2,p}^{*}=\\frac{1}{n}\\sum_{r=1}^{n}\\sum_{s=1}^{n}D_{r s}^{4}, $$ <PARAGRAPH_SEPARATOR> with $D_{r s}=(x_{r}-{\\bar{x}})^{\\mathsf{T}}{S}^{-1}(x_{s}-{\\bar{x}})=z_{r}^{\\mathsf{T}}z_{s}$ .The quantities $b_{1,p}$ and $b_{2,p}$ are themultivariate skewness and kurtosis introduced by Mardia (1970). These expressions are useful in computing thevalues of $T_{3}$ and $T_{4}$ . Also $b_{2,p}$ is asymptotically normally distributed with mean $\\gamma_{1}=p(p+2)$ and variance $\\gamma_{2}=8p(p+1)/n.$ <PARAGRAPH_SEPARATOR> Note that $\\pmb{b_{2,p}}$ depends on the fourth-order moments only through the 'radial part of the data; that is, on the elements $D_{r r}^{2}$ . Thus we might expect $b_{2,p}$ to be more powerful than $T_{4}$ in picking up departures from multinormality in elliptical families. Further, it is possible to partition the chi-squared statistic $T_{4}$ into two asymptotically independent pieces, <PARAGRAPH_SEPARATOR> $$ T_{4}=Z^{2}+(T_{4}-Z^{2}), $$ <PARAGRAPH_SEPARATOR> where $Z=(b_{2,p}-\\gamma_{1})\\gamma_{2}^{-1}$ has one degree of freedom and $\\scriptstyle T_{4}-Z^{2}$ has $\\pmb{p(4)-1}$ degrees of freedom. The validity of this decomposition follows from Cochran's theorem. <PARAGRAPH_SEPARATOR> For $\\pmb{p=1}$ , the statistic $\\pmb{T}$ reduces to the statistic of Bowman & Shenton (1975, p. 243), mentioned in passing. For $\\pmb{p=2}$ $\\pmb{T}$ reduces to the projection pursuit index introduced by Jones & Sibson (1987) as pointed out by Mardia (1987) while introducing $T.$ <PARAGRAPH_SEPARATOR> Other invariant procedures for testing multivariate normality using the first four moments have also been proposed in the literature. However, for the most part these other approaches are not based on quadratic forms in the third and fourth moments, and hence the test statistics do not follow asymptotic chi-squared distributions. Two of the key proposals are those of Cox & Small (1978) and Malkovich & Affi (1973). For a review see Mardia (1980)."
  },
  {
    "qid": "statistic-posneg-293-2-1",
    "gold_answer": "TRUE. The formula for AIC in the context of bivariate copula selection is correctly specified as AIC_{jk;S}(θ_{jk}) = -2ℓ_{jk;S}(θ_{jk}) + 2|θ_{jk}|, where ℓ_{jk;S}(θ_{jk}) is the log-likelihood of the bivariate copula C_{j,k;S} and |θ_{jk}| is the number of parameters in the copula. This is consistent with standard model selection criteria, penalizing model complexity to avoid overfitting.",
    "question": "Is the AIC criterion for bivariate copula selection given by the formula AIC_{jk;S}(θ_{jk}) = -2ℓ_{jk;S}(θ_{jk}) + 2|θ_{jk}| correct, where ℓ_{jk;S}(θ_{jk}) is the log-likelihood and |θ_{jk}| is the number of copula parameters?",
    "question_context": "Conditional Quantile Functions in Vine Copula Regression Models\n\nThe paper discusses the construction of truncated R-vines for regression models, focusing on the conditional quantile functions of the response variable given explanatory variables. It presents theoretical results on the shapes of these conditional quantile functions, especially in the extremes of the predictor space. The paper also details the selection of bivariate copulas and the prediction of conditional distributions using vine copula models."
  },
  {
    "qid": "statistic-posneg-304-3-2",
    "gold_answer": "FALSE. The paper explicitly mentions that the method of least squares does not lead to exact significance tests and fiducial limits for the proportional regression model because the equations of estimation for ξ are not linear. Instead, the paper adopts an alternative method involving F-tests and fiducial limits based on the null hypothesis of proportional regressions.",
    "question": "Does the paper suggest that the method of least squares provides exact significance tests for the proportional regression model?",
    "question_context": "Proportional Regressions in Simultaneous Equations\n\nThe paper discusses fitting proportional regression equations where coefficients are proportional across different dependent variables. For instance, in studying coal properties like carbon content and calorific value, each property is linearly related to the percentage ash content. The regression equations are restricted to have a common constant of proportionality, denoted as ξ. The paper provides methods to test the validity of this assumption and to estimate ξ using least squares and F-tests."
  },
  {
    "qid": "statistic-posneg-749-0-0",
    "gold_answer": "FALSE. While the condition s0 << p is a standard assumption in high-dimensional settings to ensure sparsity, sign consistency of the Lasso estimator depends on additional conditions such as the mutual incoherence condition and the minimum signal strength of the non-zero coefficients. The paper explicitly states that these conditions are essential for correct sign recovery, implying that s0 << p alone is not sufficient.",
    "question": "In the Cox model with Lasso penalty, the condition s0 << p is necessary to ensure that the Lasso estimator achieves sign consistency. True or False?",
    "question_context": "High-dimensional Cox Model with Lasso Penalty: Sign Consistency and Estimation\n\nThe paper discusses the Cox proportional hazards model in a high-dimensional setting where the number of covariates p is much larger than the number of non-zero coefficients s0. The model uses a counting process framework with a Lasso penalty to induce sparsity. Key components include the partial likelihood function, its gradient and Hessian, and population counterparts for theoretical analysis. The primal-dual witness technique is employed to establish sign consistency and estimation error bounds."
  },
  {
    "qid": "statistic-posneg-1904-3-3",
    "gold_answer": "TRUE. The formula shows that the difference between the estimator $\\widetilde{\\pmb{\\phi}}_{K1}$ and the true value $\\widetilde{\\pmb{\\phi}}_{0K1}$ is dominated by the term $(\\mathbb{X}_{1}^{\\top}\\mathbb{X}_{1})^{-1}\\mathbb{X}_{1}^{\\top}\\pmb{\\epsilon}$, which converges in probability to zero, and the remainder term $o_{p}(1/\\sqrt{N_{\\operatorname*{min}}})$ also converges to zero, indicating consistency.",
    "question": "The formula $\\widetilde{\\pmb{\\phi}}_{K1} - \\widetilde{\\pmb{\\phi}}_{0K1} = (\\mathbb{X}_{1}^{\\top}\\mathbb{X}_{1})^{-1}\\mathbb{X}_{1}^{\\top}\\pmb{\\epsilon} + o_{p}(1/\\sqrt{N_{\\operatorname*{min}}})$ implies that the estimator $\\widetilde{\\pmb{\\phi}}_{K1}$ is consistent. Is this statement true?",
    "question_context": "Convergence Analysis of High-Dimensional Integrative Models with Homogeneity\n\nThe paper discusses the convergence properties of high-dimensional integrative models with homogeneity. It presents several lemmas and theorems to establish the convergence of the proposed algorithm. Key formulas include the updates for the variables in the optimization process and the conditions under which convergence is guaranteed."
  },
  {
    "qid": "statistic-posneg-302-0-1",
    "gold_answer": "TRUE. The time-discrete version of the CAR(3) process includes the term σ(t)ε(t), where ε(t) = B(t+1) - B(t) are i.i.d. standard normally distributed random variables, as explicitly stated in the context.",
    "question": "Does the time-discrete version of the CAR(3) process include a stochastic term σ(t)ε(t) where ε(t) are i.i.d. standard normally distributed random variables?",
    "question_context": "CAR(3) Model Estimation and Stationarity Analysis\n\nTable 4. Fitted CAR(3)-parameters <PARAGRAPH_SEPARATOR> <html><body><table><tr><td></td><td>a2</td><td>C.3</td></tr><tr><td>2.04</td><td>1.34</td><td>0.177</td></tr></table></body></html> <PARAGRAPH_SEPARATOR> of the time dynamics of the CAR(3) process, we are led to the time-discrete version for $t=0,1,2,\\ldots$ , <PARAGRAPH_SEPARATOR> $$\\begin{array}{c}{{X_{1}(t+3)\\approx(3-\\alpha_{1})X_{1}(t+2)+(2\\alpha_{1}-\\alpha_{2}-3)X_{1}(t+1)}}\\ {{{}}}\\ {{+(\\alpha_{2}+1-(\\alpha_{1}+\\alpha_{3}))X_{1}(t)+\\sigma(t)\\epsilon(t).}}\\end{array}$$ <PARAGRAPH_SEPARATOR> Here $\\epsilon(t)=B(t+1)-B(t)$ are i.i.d. standard normally distributed random variables. From the estimated values of $\\beta_{1},\\beta_{2}$ and $\\beta_{3}$ reported in Table 2 and matching with the parameters in (13), we find the corresponding CAR(3)-parameters. The results are presented in Table 4. The parameters for the seasonal function $\\Lambda(t)$ and the variance function $\\sigma(t)$ are taken directly from the AR(3)-case, as here we fitted functions that were well-defined and were on a timecontinuous scale. <PARAGRAPH_SEPARATOR> The stationarity condition of the CAR(3)-model says that the eigenvalues of the matrix $A$ need to have negative real parts. The eigenvalues of the fitted matrix in the case of Stockholm are $\\lambda_{1}=-0.1748$ and $\\lambda_{2,3}=-0.9341\\pm0.374i$ . Thus, the condition for stationarity is fulfilled. <PARAGRAPH_SEPARATOR> # 4. Analysis of temperature derivatives"
  },
  {
    "qid": "statistic-posneg-287-0-0",
    "gold_answer": "FALSE. The formula presented is $\\sigma_{\\log\\rho}^{2}=\\frac{H\\left(\\displaystyle\\int_{0}^{\\infty}(\\log\\rho)^{2}e^{-\\frac{1}{\\Gamma_{0}}x_{0}\\rho}d\\rho\\right)}{H\\left(\\displaystyle\\int_{0}^{\\infty}e^{-\\frac{1}{\\Gamma_{0}}x_{0}\\rho}d\\rho\\right)}-(\\log\\overline{{\\rho}})^{2}$. While the structure resembles the standard variance formula (second moment minus the square of the mean), the application of the operator $H$ to both the numerator and denominator is not standard in statistical derivations. Typically, the operator would be applied to the entire expression or not at all, depending on the context. The correct derivation should clarify the role of $H$ in transforming the integrals.",
    "question": "Is the formula for $\\sigma_{\\log\\rho}^{2}$ correctly derived as the difference between the second moment of $\\log\\rho$ and the square of its mean, using the operator $H$?",
    "question_context": "Standard Deviation of Log-Transformed Variable in Statistical Modeling\n\nThe standard deviation of $\\log\\rho$ is the square root of the second moment of $\\log\\rho$ about its mean $\\log\\overline{{\\rho}}$ . Using the operator-notation of the previous section, with standard formulae of statistics, we obtain <PARAGRAPH_SEPARATOR> Now, as before, <PARAGRAPH_SEPARATOR> $$ \\sigma_{\\log\\rho}^{2}=\\frac{H\\left(\\displaystyle\\int_{0}^{\\infty}(\\log\\rho)^{2}e^{-\\frac{1}{\\Gamma_{0}}x_{0}\\rho}d\\rho\\right)}{H\\left(\\displaystyle\\int_{0}^{\\infty}e^{-\\frac{1}{\\Gamma_{0}}x_{0}\\rho}d\\rho\\right)}-(\\log\\overline{{\\rho}})^{2}. $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\displaystyle\\int_{0}^{\\infty}(\\log\\rho)^{2}e^{-\\gamma_{0}x_{*}\\rho}d\\rho=\\left[\\frac{d^{2}}{d{u}^{2}}\\int_{0}^{\\infty}\\rho^{u}e^{-\\gamma_{0}x_{0}\\rho}d\\rho\\right]_{u=0}}}\\\\ {{\\displaystyle~=\\frac{10}{x_{0}}[\\{F(1)\\}^{2}+F(1)+\\{\\log10\\}^{2}+2F(1)\\log10}}\\\\ {{\\displaystyle~-2\\{\\log10+F(1)\\}\\log x_{0}+\\{\\log x_{0}\\}^{2}]},}}\\end{array} $$ <PARAGRAPH_SEPARATOR> whence we easily find <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{\\sigma_{10\\oplus\\rho}^{2}=}&{-(\\log\\bar{\\rho})^{2}+[F(1)+\\log10]^{2}+F(1)}\\\\ &{-2[F(1)+\\log10]\\frac{H\\left\\{\\displaystyle\\frac{\\log Z_{0}}{x_{0}}\\right\\}}{H\\left\\{\\displaystyle\\frac{1}{x_{0}}\\right\\}}}\\\\ &{+\\frac{H\\left\\{\\displaystyle\\frac{(\\log x_{0})^{2}}{x_{0}}\\right\\}}{H\\left\\{\\displaystyle\\frac{1}{x_{0}}\\right\\}},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ \\bar{F}(1)=-0{\\cdot}577218,\\quad\\bar{F}(1)=1{\\cdot}644934. $$ <PARAGRAPH_SEPARATOR> # V. TRANSFORMATIONS <PARAGRAPH_SEPARATOR> In the fractional terms in (13) and (16), the numerators and denominators each represent finite alternating series; but these series are useless for computing purposes, because the terms are so nearly equal that each would have to be expressed with at least twenty significant figures to yield sufficiently accurate results. Hence we carry out the following transformations: <PARAGRAPH_SEPARATOR> Referring to (10) we take out $(1-E)^{\\varSigma n}=(-A)^{\\varSigma n}$ 88 a factor from the operator $\\pmb{H}$ , &nd obtain <PARAGRAPH_SEPARATOR> $$ H=H_{0}(-A)^{N}, $$ <PARAGRAPH_SEPARATOR> where $N=\\varSigma n=n_{10}+n_{1}+n_{01}$ &nd <PARAGRAPH_SEPARATOR> $$ {\\cal H}_{0}=(1+{\\cal E}+\\ldots+{\\cal E}^{99})^{n_{10}}(1+{\\cal E}+\\ldots+{\\cal E}^{9})^{n_{1}}. $$"
  },
  {
    "qid": "statistic-posneg-2111-0-0",
    "gold_answer": "TRUE. The halfspace depth function is affine invariant, as shown by the equation $l d e p t h(A\\Theta+\\ensuremath{\\mathbf{b}};A X_{n}+\\ensuremath{\\mathbf{b}})=l d e p t h(\\ensuremath{\\mathbf{\\Theta}};X_{n})$. This property holds because affine transformations map halfspaces to halfspaces, preserving the relative positions of the point and the data.",
    "question": "Is the halfspace depth function affine invariant, meaning that applying an affine transformation to the data and the point does not change the depth value?",
    "question_context": "Halfspace Depth and Its Properties in Multivariate Data Analysis\n\nTake any data set $X_{n}=\\{\\mathbf{x}_{i};i=1,...,n\\}$ with data points $\\mathbf{x}_{i}=(x_{i1},...,x_{i p})^{\\prime}$ $\\in\\mathbb{R}^{p}$ . This data set determines an empirical distribution ${\\hat{P}}_{n}$ which is the discrete probability distribution on $\\mathbb{R}^{p}$ given by $\\hat{P}_{n}(A)=\\#\\left\\{\\mathbf{x}_{i}\\in A\\right\\}/n$ . When the sample size $n$ is given, ${\\hat{P}}_{n}$ characterizes the data set $X_{n}$ . <PARAGRAPH_SEPARATOR> Tukey (1975) and Donoho and Gasko (1992) defined the halfspace depth of an arbitrary point ${\\bf\\uptheta}=(\\theta_{1},...,\\theta_{p})^{\\prime}\\in\\mathbb{R}^{p}$ relative to $X_{n}$ as the smallest number of data points in any closed halfspace with boundary hyperplane through %. We also call this the location depth, and it can be written as <PARAGRAPH_SEPARATOR> $$ l d e p t h(\\ \\ 6;X_{n})=\\operatorname*{min}_{\\|\\mathbf{u}\\|=1}\\#\\left\\{i;\\mathbf{u}^{\\prime}\\mathbf{x}_{i}\\leqslant\\mathbf{u}^{\\prime}\\mathbf{\\uptheta}\\right\\}, $$ <PARAGRAPH_SEPARATOR> where u ranges over all vectors in $\\mathbb{R}^{p}$ with $\\left\\|\\mathbf{u}\\right\\|=1$ . Interestingly, (1.1) is affine invariant. That is, if we consider a regular matrix $A\\in\\mathbb{R}^{p\\times p}$ and some vector $\\ensuremath{\\mathbf{b}}\\in\\mathbb{R}^{p}$ , it holds that <PARAGRAPH_SEPARATOR> $$ l d e p t h(A\\Theta+\\ensuremath{\\mathbf{b}};A X_{n}+\\ensuremath{\\mathbf{b}})=l d e p t h(\\ensuremath{\\mathbf{\\Theta}};X_{n}) $$ <PARAGRAPH_SEPARATOR> due to the fact that halfspaces are mapped to halfspaces. <PARAGRAPH_SEPARATOR> Since (1.1) is defined for any $\\mathbf{\\uptheta}\\in\\mathbb{R}^{p}$ we call it the depth function. Its values are nonnegative integers. When $p=1$ we have $u\\in\\{-1,1\\}$ so we can write (1.1) as <PARAGRAPH_SEPARATOR> $$ l d e p t h_{1}(\\theta;X_{n})=\\operatorname*{min}\\{n\\hat{F}_{n}(\\theta;X_{n}),n\\hat{F}_{n}(-\\theta;-X_{n})\\}, $$ <PARAGRAPH_SEPARATOR> where $\\hat{F}_{n}(\\theta;X_{n})=\\#\\left\\{x_{i}\\leqslant\\theta\\right\\}/n$ is the usual empirical cdf. Figure 1(a) shows the depth function of a univariate data set with $n=30$ . The data values were randomly generated from a $\\chi_{6}^{2}$ -distribution. The depth function clearly reflects the skewness. The increasing part of the function coincides with $\\hat{F}_{n}$ , whereas the decreasing part coincides with the empirical cdf of the image of the data under an affine transformation $x\\to a x+b$ with $a<0$ . Figure 1(b) is the depth function of a bivariate data set ( $p=2,$ ). The two variables are the batting average and the number of home runs of 14 baseball teams in the American League in 1987 (Moore and McCabe 1989; the data are also available at http:\\\\\\\\lib.stat.cmu.edu\\\\\\\\DASL\\\\\\\\). For any dimension $p\\geqslant2$ it holds that <PARAGRAPH_SEPARATOR> $$ l d e p t h}(\\ \\mathbf{\\Theta};X_{n})=\\operatorname*{min}_{\\|\\mathbf{u}\\|=1}{l d e p t h}_{1}(\\mathbf{u}^{\\prime}\\mathbf{\\Theta}\\mathbf{\\Theta};\\mathbf{u}^{\\prime}X_{n}), $$ <PARAGRAPH_SEPARATOR> which can also be written as <PARAGRAPH_SEPARATOR> $$ \\operatorname*{min}_{g\\in\\mathcal{A}}l d e p t h_{1}([g(\\mathbf{\\theta})]_{1};[g(X_{n})]_{1})=\\operatorname*{min}_{g\\in\\mathcal{A}}n\\hat{F}_{n}([g(\\mathbf{\\theta})]_{1};[g(X_{n})]_{1}), $$ <PARAGRAPH_SEPARATOR> where $\\mathcal{A}$ is the set of all affine transformations of $\\mathbb{R}^{p}$ and $[g(\\mathbf{\\theta})]_{1}$ denotes the first component of $g({\\bf\\theta})$ . Therefore, location depth can be seen as a natural affine equivariant generalization of the univariate empirical cdf. The usual multivariate empirical cdf is not affine equivariant because it depends on the coordinate system used. <PARAGRAPH_SEPARATOR> The depth contours defined as <PARAGRAPH_SEPARATOR> $$ D_{k}=\\{\\pmb{\\uptheta}\\in\\mathbb{R}^{p};l d e p t h(\\pmb{\\uptheta};X_{n})\\geqslant k\\} $$ <PARAGRAPH_SEPARATOR> are convex, and $D_{k+1}\\subseteq D_{k}$ for each $k$ . The outermost contour $D_{1}$ is the convex hull of the data set. Each data set also has an innermost depth contour $D_{k^{*}}$ where $k^{*}$ is the maximum of the function ldepth $\\iota(\\boldsymbol{\\mathbf{\\theta}};\\boldsymbol{X}_{n})$ over all $\\mathbf{\\uptheta}\\in\\mathbb{R}^{p}$ . Therefore, the complete set of contours is $D_{k^{*}}\\subseteq D_{k^{*}-1}\\subseteq\\cdots\\subseteq$ $D_{2}\\subseteq D_{1}$ . Figure 2 shows such a collection of contours. The depicted data set (from the Wall Street Journal of March 1, 1984, and provided at http:\\\\\\\\lib.stat.cmu.edu\\\\\\\\DASL\\\\\\\\) gives the 1983 TV advertising budget of several well-known companies, in millions of dollars. The second variable is based on a survey, where people had to cite a commercial for that product they had seen in the past week. The number of retained impressions (in millions) are plotted on the vertical axis. We see that the depth contours reflect the shape of the data set. <PARAGRAPH_SEPARATOR> From definition (1.1) we can derive an equivalent expression for the $k$ th depth contour: <PARAGRAPH_SEPARATOR> Lemma 1. <PARAGRAPH_SEPARATOR> $$ D_{k}=\\bigcap_{\\scriptstyle A\\in{\\mathcal{A}}(n-k+1)}A, $$ <PARAGRAPH_SEPARATOR> where $\\mathcal{A}(m)$ is the set of all closed halfspaces containing at least m data points. <PARAGRAPH_SEPARATOR> Proof. We first prove $\\subseteq$ . If % does not belong to the intersection there exists a closed halfspace in $\\mathcal{A}(n-k+1)$ which does not contain %, hence % belongs to an open halfspace containing at most $n-(n-k+1)=k-1$ observations. Therefore ldepth $\\iota(\\boldsymbol\\Theta;X_{n})\\leqslant k-1$ , and thus $\\mathbf{\\uptheta}\\notin D_{k}$ . On the other hand, suppose that $\\mathbf{\\uptheta}\\notin D_{k}$ . Then ldepth $(\\mathbf{\\theta}\\in X_{n})<k$ , hence % belongs to a closed halfspace $A$ containing fewer than $k$ data points. The complement of $A$ is an open halfspace containing at least $n-k+1$ data points, from which we immediately obtain a closed halfspace that does not contain % and has at least $n-k+1$ data points. Therefore also $\\supseteq$ holds. K"
  },
  {
    "qid": "statistic-posneg-451-0-0",
    "gold_answer": "TRUE. The condition ensures that the measure $\\mu$ assigns positive mass to any interval where the true distribution function $F_0$ is increasing, which is crucial for identifying $F_0$ and thus for the consistency of the GMLE. Without this condition, there could be intervals where $F_0$ increases but $\\mu$ assigns zero mass, making it impossible to consistently estimate $F_0$ on those intervals.",
    "question": "Is the condition $\\mu((a,b))>0$ for all $0<F_{0}(a)<F_{0}(b)<1$ necessary for the uniform strong consistency of the GMLE on $[\\tau_{1},\\tau_{2}]$?",
    "question_context": "Consistency of the Generalized Maximum Likelihood Estimator (GMLE) in Interval Censored Data\n\nProposition 5 Suppose the following four conditions hold for real numbers $\\tau_{1}<\\tau_{2}$ (1) $F_{0}$ is continuous at every point in the interval $(\\tau_{1},\\tau_{2}]$ (2) either $\\mu(\\{\\tau_{1}\\})>0$ or $F_{0}(\\tau_{1})=0$ (3) either $\\mu(\\{\\tau_{2}\\})>0$ or $F_{0}(\\tau_{2}-)=1$ (4) for all a and $^b$ in $(\\tau_{1},\\tau_{2})$ $0<F_{0}(a)<F_{0}(b)<1$ implies $\\mu((a,b))>0$. Then the GMLE is uniformly strongly consistent on $[\\tau_{1},\\tau_{2}]$ i.e. $\\operatorname*{sup}_{x\\in[\\tau_{1},\\tau_{2}]}\\left|{\\hat{F}}_{n}(x)-F_{0}(x)\\right|\\rightarrow0\\quad a.s.$"
  },
  {
    "qid": "statistic-posneg-1720-0-0",
    "gold_answer": "TRUE. The formula (1-p^T)/(1-p)p^T is derived from the expected number of trials until the first run of T successes in a Bernoulli process. When T=1, the formula simplifies to (1-p)/(1-p)p = 1/p, which is the expected number of trials until the first success in a Bernoulli process with success probability p.",
    "question": "Is the claim that the average number of samples before stoppage is (1-p^T)/(1-p)p^T correct, and does it reduce to 1/p when T=1?",
    "question_context": "Quality Control Runs Analysis: Average Run Length and Detection Probability\n\nThe paper examines quality control procedures where production is stopped when T successive means fall beyond a specified control limit. The average number of samples before stoppage is given by (1-p^T)/(1-p)p^T, and for equivalent schemes across different T values, the condition (1-p_T^T)/(1-p_T)p_T^T = 1/p is used. The paper also discusses the probability of detecting a mean shift using runs of T successive means, with approximations for the probability of no runs of length T given by q_m ∼ (1-μx)/(T+1-μx) * 1/x^(m+1), where x is a root of (1-μ)s(1+μs+...+μ^(T-1)s^(T-1))=1."
  },
  {
    "qid": "statistic-posneg-1461-0-1",
    "gold_answer": "FALSE. The probability formula $$\\mathbb{P}\\left(P_{i}=\\frac{j}{n+1},P_{i^{\\prime}}=\\frac{j^{\\prime}}{n+1}\\right)=\\frac{1}{n+2}\\cdot\\frac{1}{n+1}\\mathbb{1}_{\\{j=j^{\\prime}\\}}+\\left(1-\\frac{1}{n+2}\\right)\\cdot\\frac{1}{n+1}\\cdot\\frac{1}{n+1}$$ does not directly imply an invariant correlation of $1/(n+2)$. The invariant correlation is a property of the exchangeable case of the model (14), but the formula itself does not explicitly derive or confirm this correlation value. The reasoning provided in the context is insufficient to conclude that the correlation is indeed $1/(n+2)$ based solely on this probability expression.",
    "question": "Does the probability $\\mathbb{P}\\left(P_{i}=\\frac{j}{n+1},P_{i^{\\prime}}=\\frac{j^{\\prime}}{n+1}\\right)$ correctly reflect an invariant correlation of $1/(n+2)$ when $m=2$? Explain the reasoning.",
    "question_context": "Tail-Dependence Matrix and Conformal p-Values Analysis\n\nProof of Proposition 10. Since the statement on $\\kappa_{g}$ -matrices is obvious, we will show that $\\mathbf{X}$ has the tail-dependence matrix $R$ . Without loss of generality, we can assume that the identical marginal of $\\mathbf{X}$ is the standard uniform distribution. Fix $i,j\\in[d]$ Since $\\lambda_{i j}=\\lambda_{j i}$ , Proposition 7 leads to $$\\lambda_{i j}=\\frac{\\lambda_{i j}+\\lambda_{j i}}{2}=\\operatorname*{lim}_{u\\downarrow0}\\frac{C_{i j}(u,u)+C_{j i}(u,u)}{2u}=\\operatorname*{lim}_{u\\downarrow0}\\frac{r_{i j}M(u,u)+(1-r_{i j}){\\cal{I}}(u,u)}{u}=r_{i j}.\\quad\\bigtriangledown$$ Proof of Proposition 11. Since $S_{n+1},\\ldots,S_{n+d}$ are independent conditional on the null training sample ${\\mathcal{D}}=\\{S_{i}:i\\in[n]\\}$ , we have $$\\mathbb{P}\\left(P_{i}=\\frac{j_{i}}{n+1},i\\in\\mathcal{K}\\right)=\\mathbb{E}_{\\mathcal{D}}\\left[\\mathbb{P}\\left(P_{i}=\\frac{j_{i}}{n+1},i\\in\\mathcal{K}\\bigg|\\mathcal{D}\\right)\\right]=\\mathbb{E}_{\\mathcal{D}}\\left[\\prod_{i\\in\\mathcal{K}}\\mathbb{P}\\left(P_{i}=\\frac{j_{i}}{n+1}\\bigg|\\mathcal{D}\\right)\\right]=\\mathbb{E}_{\\mathcal{D}}\\left[\\prod_{i\\in\\mathcal{K}}\\left(S_{(j_{i})}-\\sum_{i,i=1}\\nu_{i}\\right)\\right]$$ for $j_{i}\\in[n+1],$ where $0=S_{(0)}<S_{(1)}<\\cdots<S_{(n)}<S_{(n+1)}=1$ are the order statistics of the null training sample $(S_{1}\\ldots,S_{n})$ . Since the conformal $\\mathfrak{p}$ -values are independent of the distribution of scores, we can assume without loss of generality that $S_{1},\\ldots,S_{n}$ are independently distributed of the standard uniform distribution. Let $T_{j}=S_{(j)}-S_{(j-1)}$ , $j\\in[n+1]$ . Then $(T_{1},\\dots,T_{n+1})$ follows the Dirichlet distribution with the parameter vector $\\mathbf{1}_{n+1}\\in\\mathbb{R}^{n+1}$ . Therefore, $$\\mathbb{E}_{\\mathcal{D}}\\left[\\prod_{i\\in\\mathcal{K}}\\left(S_{(j_{i})}-S_{(j_{i}-1)}\\right)\\right]=\\mathbb{E}_{\\mathcal{D}}\\left[T_{1}^{N_{1}}\\dots T_{n+1}^{N_{n+1}}\\right]=\\frac{B(1+N_{1},\\dots,1+N_{n+1})}{B(\\mathbf{1}_{n+1})}=\\frac{N_{1}!\\dots N_{n+1}!}{(n+m)(n+m-1)\\dots(n+1)},$$ where $B$ is the $(n+1)$ -dimensional Beta function. In particular, when $m=1$ , we have $\\mathbb{P}(P_{i}=j/(n+1))=1/(n+1),j\\in[n+1]$ . When $m=2$ , we have $$\\mathbb{P}\\left(P_{i}=\\frac{j}{n+1},P_{i^{\\prime}}=\\frac{j^{\\prime}}{n+1}\\right)=\\frac{1}{n+2}\\cdot\\frac{1}{n+1}\\mathbb{1}_{\\{j=j^{\\prime}\\}}+\\left(1-\\frac{1}{n+2}\\right)\\cdot\\frac{1}{n+1}\\cdot\\frac{1}{n+1},$$ which is the exchangeable case of the model (14) and thus has the invariant correlation $1/(n+2)$ . □"
  },
  {
    "qid": "statistic-posneg-860-1-1",
    "gold_answer": "TRUE. The limit (β+2)/(β+3) decreases as β decreases (heavier tails), meaning the ratio (κ_SBAG - μ_π)/(κ* - μ_π) becomes smaller. This indicates that sequential bagging's approximation to the Bayes rule is less effective for heavier-tailed distributions. The improvement becomes more significant as β increases (lighter tails), eventually reaching full agreement (ratio → 1) for exponential tails.",
    "question": "Does the condition (κ_SBAG - μ_π)/(κ* - μ_π) → (β+2)/(β+3) for polynomial tails imply that sequential bagging is less effective for heavier tails (smaller β)?",
    "question_context": "Sequential Bagging for Classification: Bias Reduction and Approximation to Bayes Rule\n\nWe illustrate the possible effects of sequential bagging on unsupervised classification by applying it to a simple hard-thresholding classifier. Unlike Bihlmann & $\\mathrm{Yu}\\left(2002\\right)$ , who have used the same example for studying stability of bagged predictors, we investigate the extent to which nonparametric and sequential baggings imitate the Bayes rule under the two-class setting studied in $\\S3$ , where each $X_{i}$ in ${\\mathcal{L}}\\subset\\mathbb{R}$ is drawn from either $f$ or $g$ with prior probability $\\pi_{f}$ or $\\pi_{g}$ , respectively. Assume that $f$ and $g$ differ only in location such that $g(\\cdot+\\mu_{g})=f(\\cdot+\\mu_{f})=h(\\cdot)$ for some $\\mu_{f}<\\mu_{g}$ , where $h$ denotes a smooth, symmetric and zero-mean density function that is strictly decreasing on $[0,\\infty)$ . For classification of $x_{0}\\in\\mathbb{R}$ , we consider a hard-thresholding classifier $T$ which assigns $x_{0}$ to $f$ if and only if $x_{0}<\\bar{X}\\equiv n^{-1}\\textstyle\\sum_{i=1}^{n}X_{i}$ . Define $\\Delta=$ $\\mu_{g}-\\mu_{f}$ , $\\mu_{\\pi}=\\pi_{f}\\mu_{f}+\\pi_{g}\\mu_{g}$ and $\\epsilon=\\log(\\pi_{g}/\\pi_{f})$ . We consider only the case of a small $\\epsilon$ , under which $T$ approximates the Bayes rule (3) to some extent. <PARAGRAPH_SEPARATOR> Denote by $(X_{1}^{*},\\ldots,X_{n}^{*})$ a generic bootstrap resample drawn from $\\mathcal{L}$ , and denote by $\\bar{X}^{*}$ the corresponding sample mean. Nonparametric bagging of $T$ assigns $x_{0}$ to $f$ if $\\mathrm{pr}(x_{0}<\\bar{X}^{\\ast}\\mid\\mathcal{L})>\\dot{1}/2$ , which is asymptotically equivalent to $n^{1/2}(x_{0}-\\bar{X})<0$ by the bootstrap central limit theorem. Thus, the bagged predictor shares with $T$ the same large-sample property in relation to the Bayes rule, in which sense bagging makes no improvement on $T$ . In their study of the same bagged predictor, Bihlmann & $\\mathrm{Yu}$ (2002) focus instead on the stability of $\\mathrm{pr}(x_{0}<\\bar{X}^{\\ast}\\mid\\mathcal{L})$ and $\\mathbb{I}(x_{0}<\\bar{X})$ , a feature not directly related to their respective prediction powers. <PARAGRAPH_SEPARATOR> Setting $M=m=n$ in our sequential bagging algorithm, we obtain from the first round of bagging the class 1 probabilities $\\hat{p}_{1}(X_{i})$ for $f$ , which are given to first order by $\\mathrm{pr}(\\bar{X}^{*}>X_{i}\\mid\\mathcal{L})$ . In the second round, we draw resamples $\\tilde{\\mathcal{L}}^{*}$ of the form $\\{(X_{i},Y_{i}^{*}):i=1,\\dots,n\\}$ , where $Y_{i}^{*}=1$ with probability $\\hat{p}_{1}(X_{i})$ and $Y_{i}^{*}=2$ with probability $1-\\hat{p}_{1}(X_{i})$ . We choose for the second round of bagging a more natural classifier $T^{\\prime}$ good for supervised learning, such that $T^{\\prime}(\\tilde{\\mathcal{L}}^{*},x_{0})=1$ if and only if $x_{0}$ is closer to the mean of class 1 observations in $\\tilde{\\mathcal{L}}^{*}$ . Thus, our sequential bagging procedure predicts $f$ for $x_{0}$ if and only if <PARAGRAPH_SEPARATOR> $$ \\operatorname{pr}\\left\\{\\left|x_{0}-{\\frac{\\sum_{i=1}^{n}X_{i}\\mathbb{I}(Y_{i}^{*}=1)}{\\sum_{i=1}^{n}\\mathbb{I}(Y_{i}^{*}=1)}}\\right|<\\left|x_{0}-{\\frac{\\sum_{i=1}^{n}X_{i}\\mathbb{I}(Y_{i}^{*}=2)}{\\sum_{i=1}^{n}\\mathbb{I}(Y_{i}^{*}=2)}}\\right||{\\mathcal{L}}\\right\\}>{\\frac{1}{2}}. $$ <PARAGRAPH_SEPARATOR> The following proposition, proved in the Appendix, evaluates the relative proximities of the sequentially bagged and Bayes predictors to $T$ , under different $\\Delta$ and tail conditions on $h$ . <PARAGRAPH_SEPARATOR> PROPOSITION 2. The Bayes rule (3) is equivalent to $x_{0}<\\kappa^{*}$ , while the corresponding condition (4) under sequential bagging is equivalent, in probability as $n\\to\\infty$ , to $x_{0}<K_{\\mathrm{SBAG}}$ , where $\\kappa^{*}$ and $\\kappa$ SBAG satisfy the following conditions. <PARAGRAPH_SEPARATOR> (i) As $\\Delta\\rightarrow0$ and $\\epsilon\\Delta^{-1}\\to0$ , <PARAGRAPH_SEPARATOR> $$ 0<\\frac{\\kappa_{\\mathrm{SBAG}}-\\mu_{\\pi}}{\\kappa^{*}-\\mu_{\\pi}}=\\Delta^{4}\\left\\{\\frac{h^{\\prime\\prime}(0)^{2}}{12h(0)}\\right\\}\\int_{0}^{\\infty}u h(u)\\mathrm{d}u\\{1+O(\\epsilon^{2}\\Delta^{-2}+\\Delta^{2})\\}\\rightarrow0. $$ <PARAGRAPH_SEPARATOR> (ii) Given any fixed $C$ , $\\beta>0$ and polynomial $P$ , as $\\Delta\\rightarrow\\infty$ and $\\epsilon\\Delta\\rightarrow0$ , <PARAGRAPH_SEPARATOR> $$ \\frac{\\kappa_{\\mathrm{SBAG}}-\\mu_{\\pi}}{\\kappa^{*}-\\mu_{\\pi}}\\to\\left\\{\\begin{array}{l l}{(\\beta+2)/(\\beta+3)}&{i f h(x)=C|x|^{-\\beta-2}f o r l a r g e|x|,}\\\\ {1}&{i f h(x)=P(|x|)\\exp(-C|x|^{\\beta})f o r l a r g e|x|.}\\end{array}\\right. $$ <PARAGRAPH_SEPARATOR> Some interesting conclusions can be drawn from Proposition 2. Under condition (i) we have, for sufficiently small $\\Delta$ and $\\epsilon\\Delta^{-1}$ , that $\\operatorname*{min}(\\mu_{\\pi},\\kappa^{*})<\\kappa_{\\mathrm{SBAG}}<\\operatorname*{max}(\\mu_{\\pi},\\kappa^{*})$ , so κSBAG is closer to the Bayes threshold $\\kappa^{*}$ than is $\\mu_{\\pi}$ , the threshold used by $T$ or its bagged version. Under condition (ii), sequential bagging reduces the discrepancy between $\\mu_{\\pi}$ and $\\kappa^{*}$ by more than two-thirds. As the polynomial tail exponent $\\beta$ increases, the sequentially bagged predictor moves closer to the Bayes rule, and the two eventually agree to second order when the tail becomes exponential. We do not have a general result for $\\mathrm{lim}_{\\epsilon\\to0}(\\kappa_{\\mathrm{SBAG}}-\\mu_{\\pi})/(\\kappa^{*}-\\mu_{\\pi})$ under a fixed $\\Delta>0$ . Nevertheless, numerical investigation of examples under different tail conditions shows that the limit increases monotonically from 0 to an upper limit bounded above by 1 as $\\Delta$ increases, implying that, as approximations to the Bayes rule, sequential bagging improves upon nonparametric bagging. Such improvement becomes more significant as $\\Delta$ increases. Although set in a didactic context, Proposition 2 exposes a noteworthy scenario where sequential bagging improves a bagged predictor by effecting a bias reduction, in contrast to its variance-reducing effect on nearest neighbour classifiers shown by Proposition 1. By repeatedly assigning random labels to $\\mathcal{L}$ , sequential bagging admits more varied bipartitions of the data for positioning $\\kappa_{\\mathrm{SBAG}}$ , which explains the improvement in bias against the Bayes threshold $\\kappa^{*}$ . <PARAGRAPH_SEPARATOR> ![](images/755e8e7c57322ff4d9cf45b4219de52dd2277d321b4e625fa19fb9fda73c7536.jpg)  Fig. 2. Probabilities of assigning $x_{0}$ to density $g$ , plotted against $x_{0}$ , given by simple hard thresholding (dashed lines), nonparametric bagging (dotted lines) and sequential bagging (solid lines). Panels (a)– (d) correspond to the simulation settings (a)–(d) described in $\\S4$ ; Bayes thresholds $\\kappa^{*}$ are indicated by vertical lines. <PARAGRAPH_SEPARATOR> For an empirical comparison, we calculated the probabilities of assigning $x_{0}$ to $g$ using different classifiers. The probabilities, given by $\\mathrm{pr}(\\bar{X}\\leqslant x_{0})$ for simple hard-thresholding, $\\operatorname{pr}\\{\\operatorname{pr}({\\bar{X}}^{*}\\leqslant x_{0}\\mid{\\mathcal{L}})\\geqslant1/2\\}$ for nonparametric bagging and $\\mathrm{pr}[\\mathrm{pr}\\{T^{\\prime}(\\tilde{\\mathcal{L}}^{*},x_{0})=1\\mid\\mathcal{L}\\}\\leqslant1/2]$ for sequential bagging, were obtained by averaging over 1000 replications. Four simulation settings were considered. In scenario (a), $f$ and $g$ are unit-variance normal densities with means 0 and 3, respectively, $n=200$ , and $\\pi_{g}/\\pi_{f}=1$ ; scenario (b) is similar to (a) except that $n=250$ and $\\pi_{g}/\\pi_{f}=2/3$ . In scenario (c), $f$ and $g$ are Student’s $t_{3}$ densities with means recentred at 1 and 4, respectively, $n=200$ , and $\\pi_{g}/\\pi_{f}=1$ ; scenario (d) is similar to (c) except that $g$ has mean 5, $n=300$ and $\\pi_{g}/\\pi_{f}=1/2$ . Figure 2 shows the probabilities together with the locations of the Bayes threshold $\\kappa^{*}$ . As expected, in cases (a) and (c), for which $\\epsilon=0$ , all three methods approximate well the Bayes threshold. In cases (b) and (d), where $\\epsilon<0$ , the probabilities given by simple hard-thresholding and nonparametric bagging cross the 0·5 level at a location markedly different from $\\kappa^{*}$ . Sequential bagging moves this location closer to $\\kappa^{*}$ , resulting in a better approximation to the Bayes rule. Our simulation findings are in general agreement with the asymptotic results contained in Proposition 2. The probability curves given by sequential bagging have gradients slightly less steep than those given by the other two methods, indicating that the vote proportions calculated by sequential bagging are slightly less stable. This small disadvantage, which may be attributed to the extra variability introduced by random labelling, is more than compensated for by the big reduction in bias."
  },
  {
    "qid": "statistic-posneg-1056-2-1",
    "gold_answer": "TRUE. Explanation: The Brownian bridge process is a transformed Wiener process where each realization is pinned to zero at both ends of the interval [0, 1]. The process $\\varepsilon_{t}(\\upsilon)$ is constructed by subtracting a linear trend from the Wiener process, ensuring that $\\varepsilon_{t}(0) = \\varepsilon_{t}(1) = 0$. Since the Wiener process $W_{t}(v)$ itself has independent increments and the transformation is applied independently for each $t$, the resulting Brownian bridge process $\\varepsilon_{t}(\\upsilon)$ is serially uncorrelated and thus qualifies as a functional white noise process.",
    "question": "Does the Brownian bridge process $\\varepsilon_{t}(\\upsilon)=W_{t}(\\upsilon)-\\upsilon W_{t}(1)$ qualify as a functional white noise process?",
    "question_context": "Functional Time Series White Noise Validation\n\nThe FACF and FPACF introduced above can be used to evaluate the whiteness of a given FTS. In order to validate the proposed method, we consider several synthetic functional white noise processes. This section investigates the finite-sample properties of the method when applied to such processes. <PARAGRAPH_SEPARATOR> Each functional observation has been discretized in a grid of 100 equi-spaced points in the interval [0, 1]. Four different white noise processes have been simulated, and the methods employed to generate each process are described below. <PARAGRAPH_SEPARATOR> The first 3 series were generated by selecting a functional basis $\\phi_{k}(v)$ and i.i.d. coefficients $b_{t,k}$ , thus obtaining the elements of the process as <PARAGRAPH_SEPARATOR> $$ \\varepsilon_{t}(v)=\\sum_{k=1}^{K}b_{t,k}\\phi_{k}(v),\\quad t=1,\\ldots,T. $$ <PARAGRAPH_SEPARATOR> As the coefficients of the series are i.i.d with respect to $t$ , the FTS should not exhibit any kind of serial correlation. By arying the functional basis $\\phi_{k}$ and the distributions of the coefficients $b_{t,k}$ , different processes have been generated: <PARAGRAPH_SEPARATOR> • εt $\\varepsilon_{t}^{(1)}$ is obtained using the first 7 elements of the Fourier basis, due to the fact that this basis is often used when dealing with periodic functional data. The coefficients $b_{t,k}$ follow a standard normal distribution $N(0,1)$ . <PARAGRAPH_SEPARATOR> • εt $\\varepsilon_{t}^{(2)}$ is obtained using the first 7 elements of the B-spline basis, due to the fact that this basis is often used when dealing with non-periodic functional data. The coefficients are drawn from a Beta(2, 5) distribution. • $\\varepsilon_{t}^{(3)}$ is obtained selecting 3 functions and applying the Gram–Schmidt method to obtain 3 orthonormal functions that will be used as a basis in (19). In this case, the functions selected were $\\theta_{1}(v)=\\sin(v)$ , $\\theta_{2}(v)\\:=\\:\\exp v$ and $\\theta_{3}(v)=\\cos(v)$ . In this case, the coefficients of (19) follow a Exp(1) distribution. <PARAGRAPH_SEPARATOR> The last white noise process has been obtained simulating a brownian bridge random process, where each innovation function is defined as <PARAGRAPH_SEPARATOR> $$ \\varepsilon_{t}(\\upsilon)=W_{t}(\\upsilon)-\\upsilon W_{t}(1), $$ <PARAGRAPH_SEPARATOR> where $W_{t}(v)$ is a Wiener processes defined on the domain [0, 1]. <PARAGRAPH_SEPARATOR> • $\\varepsilon_{t}^{(4)}$ is generated by a brownian bridge process where the variance of the Wiener process is 1. <PARAGRAPH_SEPARATOR> Fig. 3 shows a representative example of the first 50 values of the FACF of each white noise process, including the $99\\%$ prediction bound provided by Theorem 1 that has been obtained using the obtain_FACF function of the R package fdaACF. The value of $d$ was selected as the number of eigenvalues $\\hat{\\lambda}_{i}$ of $\\hat{C}_{0}(\\bar{u},v)$ such that $\\hat{\\lambda}_{i}/\\hat{\\lambda}_{1}>0.0001$ Evidently most of the coefficients of the FACF fall below this limit, which is consistent with the white noise hypothesis."
  },
  {
    "qid": "statistic-posneg-1475-0-2",
    "gold_answer": "FALSE. While the ReLU activation function has a first derivative that is 1 for x > 0 and 0 for x < 0, it is not differentiable at x = 0. The condition |D^k φ_l| ≤ N_l assumes differentiability everywhere, which is not the case for ReLU at x = 0. However, the set of discontinuities is a null set, so in practice, the proof of Proposition A.1 still holds.",
    "question": "Is it correct that the ReLU activation function, defined as φ_l(x) = max{0, x}, satisfies the condition |D^k φ_l| ≤ N_l for all k = 1, ..., p with N_l = 1?",
    "question_context": "Quasi-Random Sampling and Discrepancy Analysis in GMMN QMC Estimators\n\nThe paper discusses the discrepancy of point sets on the unit hypercube [0,1]^p, defining the discrepancy function D(I; P_{n_gen}) as the difference between the relative frequency of points in an interval I and the probability of a p-dimensional standard uniform random vector falling in I. The star discrepancy D*(P_{n_gen}) is defined as the supremum of |D(I; P_{n_gen})| over all intervals I in A. The paper also analyzes the GMMN QMC estimator, deriving conditions under which the error in approximating E(Ψ(Y)) is small, using the Koksma–Hlawka inequality which bounds the error by V(g)D*(P_{n_gen})."
  },
  {
    "qid": "statistic-posneg-1726-5-0",
    "gold_answer": "FALSE. The formula shows that the tail behavior of real wage growth is a product of the tail behavior of real wages and the probability that $\\psi_{t} > 1$. This means that real wage growth does not simply inherit the tail properties of real wages but is also influenced by the distribution of $\\psi_{t}$. The context suggests that real wage growth should display Pareto-like tails, but this is a result of the combined effect of $P(w_{t}>x)$ and $P(\\psi_{t}>1)$, not a direct inheritance.",
    "question": "The formula $$\\operatorname*{lim}_{x,t\\to+\\infty}P(w_{t}-w_{t-1}>x)=\\operatorname*{lim}_{x,t\\to+\\infty}P(w_{t}>x)P(\\psi_{t}>1)$$ implies that real wage growth inherits the tail properties of real wages. Is this statement true based on the context provided?",
    "question_context": "Tail Index Estimation in Wage Growth Processes\n\nSeveral techniques can be used to estimate the tail index of time series. As none of these methods has been precisely designed for RC-ARMA models, we present the estimations resulting from the various methods. First, we propose three techniques to derive inference about the tail index for stationary time series. Second, we propose an alternative approach based on the specific features of our model. <PARAGRAPH_SEPARATOR> The most common estimator derives from the Hill plot. The consistency of the Hill estimator is established in various cases, but the convergence may be really slow and results in large imprecision, especially if dependence is very strong and/or if the distribution departs from the Pareto case. This problem has been discussed by several authors and has be coined as ‘Hill’s horror plots’ issue (see Embrechts et al., 1997). Additionally, the Hill plot, as well as other tail estimators, derives from high-order statistics. Applying this technique to our sequence of wages, will select only the last part of the sample (see Fig. 2), leading to large dependence. We thus suggest using real wage growth (i.e. increments in $w_{t}$ ) instead of real wages to derive inference about the presence of a fat tail. Indeed, we established in Proposition 3 that <PARAGRAPH_SEPARATOR> $$\\operatorname*{lim}_{x,t\\to+\\infty}P(w_{t}-w_{t-1}>x)=\\operatorname*{lim}_{x,t\\to+\\infty}P(w_{t}>x)P(\\psi_{t}>1)$$ <PARAGRAPH_SEPARATOR> so that real wage growth should also display Pareto-like tails. <PARAGRAPH_SEPARATOR> The results seem to favor a fat-tail index between 2 and 5. Using the ‘evir’ package of R, we obtain the Hill plot in Fig. 5. The dotted lines in the figure indicate the end points of confidence intervals. We also perform the estimation method proposed"
  },
  {
    "qid": "statistic-posneg-478-0-0",
    "gold_answer": "TRUE. Explanation: The indicator variable $X_{i j}=I(j>1)$ only differentiates between the first visit (j=1) and subsequent visits (j>1). It captures a one-time learning effect between the first and second visits but does not account for potential additional learning effects beyond the second visit. The model's structure assumes that any learning effect is fully captured by this binary distinction, implying no further learning adjustments are needed for visits beyond the second one.",
    "question": "In the model $$ Y_{i j}=\\alpha+X_{i j}\\beta+b_{i}+\\int_{0}^{1}W_{i j}(s)\\gamma(s)\\mathrm{d}s+\\varepsilon_{i j} $$, the term $X_{i j}=I(j>1)$ is included to account for a learning effect. Does this imply that the model assumes no learning effect occurs after the second visit?",
    "question_context": "Longitudinal Functional Regression Model for Cognitive Function Analysis\n\nRecall that, in this study, 100 patients are scanned approximately once per year and undergo a collection of tests to assess cognitive and motor function; patients are seen between two and eight times, with a median of three visits per subject. Here we focus on the mean diffusivity profile of the corpus callosum tract and the parallel diffusivity profile of the right corticospinal tract as our functional predictors and the paced auditory serial addition test (PASAT), which is a commonly used examination of cognitive function affected by MS with scores ranging between 0 and 60, as our scalar outcome. <PARAGRAPH_SEPARATOR> We begin by fitting models of the form <PARAGRAPH_SEPARATOR> $$ Y_{i j}=\\alpha+X_{i j}\\beta+b_{i}+\\int_{0}^{1}W_{i j}(s)\\gamma(s)\\mathrm{d}s+\\varepsilon_{i j},\\qquadb_{i}\\sim N(0,\\sigma_{\\mathbf{b}}^{2}),\\quad\\varepsilon_{i j}\\sim N(0,\\sigma_{\\mathbf{Y}}^{2}) $$ <PARAGRAPH_SEPARATOR> where $Y_{i j}$ is the PASAT score for subject $i$ at visit $j$ , $W_{i j}(s)$ is the functional predictor for subject $i$ at visit $j$ and the variable $X_{i j}=I(j>1)$ is used to account for a learning effect that causes PASAT scores generally to rise between the first and second visit. Two such models are fitted: one using the mean diffusivity profile of the corpus callosum and another using the parallel diffusivity profile of the right corticospinal tract. Next, we fit additional models that include the subject-specific time since first visit as a fixed effect and random slopes on this variable. These models illustrate that more complex random-effect structures are possible within our proposed framework with a minimal increase in computation time and allow the treatment of irregularly timed observations. However, these models gave results that were indistinguishable from the random-intercept-only model and are not discussed further. The random-intercept models are fitted using both likelihood-based and Bayesian implementations of the method that was described in Section 2. Because our model uses a continuous outcome and identity link, the coefficient function has a marginal interpretation, i.e. the effect of the predictor function on the outcome (as mediated by the coefficient function) does not depend on a subject’s random effect. <PARAGRAPH_SEPARATOR> Table 3. Percentage of the variance in the PASAT outcome explained by a random-interceptonly model, a model without functional effects (labelled the ‘non-functional model’), two models with single functional predictors (labelled ‘single: mean diffusivity’ and ‘single: parallel diffusivity’ for the models using the mean diffusivity and parallel diffusivity tract profiles respectively), and a model with multiple functional predictors (labelled ‘multiple’) <PARAGRAPH_SEPARATOR> <html><body><table><tr><td>Parameter</td><td colspan=\"5\">Results(%)for the following models:</td></tr><tr><td></td><td>Random interceptonly</td><td>Non-functional model</td><td>Single:mean diffusivity</td><td>Single:parallel diffusivity</td><td>Multiple</td></tr><tr><td>PVE</td><td>81.2</td><td>83.8</td><td>88.6</td><td>88.7</td><td>88.8</td></tr></table></body></html>"
  },
  {
    "qid": "statistic-posneg-1915-2-0",
    "gold_answer": "FALSE. While the true model M2 was indeed selected due to having the smallest DIC values, the coefficient of variation (CV) being under 1% is not the reason for its selection. The CV indicates that the standard deviations of the DIC values are relatively small compared to their means, but the selection criterion is solely based on the DIC values being the smallest for M2. The CV being under 1% merely suggests that the DIC values are stable and reliable, not that they are the smallest.",
    "question": "Is the statement 'The true model M2 was selected because it had the smallest DIC values and the smallest coefficient of variation (CV) under 1%' correct based on the provided context?",
    "question_context": "Mixture of Generalized Latent Variable Models: Model Selection and Structural Equation Analysis\n\nThe paper discusses a mixture of generalized latent variable models (GLVMs) applied to the National Longitudinal Surveys of Youth (NLSY79) data set. The study investigates how latent variables such as 'home environment' and 'friendship' influence 'academic achievement' and 'behavior problem'. The model uses a non-overlapping loading matrix and a structural equation to assess these influences. Model selection is performed using the modified Deviance Information Criterion (DIC), with the true model M2 consistently selected due to its smallest DIC values. The paper also details the prior inputs, convergence checks, and identifiability constraints used in the analysis."
  },
  {
    "qid": "statistic-posneg-1062-0-2",
    "gold_answer": "FALSE. The paper mentions that numerical formulae for $L^{\\prime}$ and diagonal elements of $L^{\\prime\\prime}$ are centered about $\\beta$, but slightly less accurate noncentred formulae are used for cross derivatives to save calculations.",
    "question": "Is the cross derivative approximation formula centered about $\\beta$?",
    "question_context": "Recursive Likelihood Calculations in Matched Case-Control Studies\n\nNot long after this paper had been submitted for publication, a reader pointed out that the recursion (3) below had been presented by Miss Susannah Howard in her discussion of Cox (l972). The relevance of her contribution to case-control studies has not been recognized in subsequent literature. We are under the impression that her methods have never even received the recognition they deserve for survival calculations based on the partial likelihood. Miss Howard presented recursions for the log likelihood $L$ and for the derivatives $L^{\\prime}$ and $L^{\\prime\\prime}$ . We include a derivation of Miss Howard's results, in simplified notation, in the Appendix. It turns out, however, that numerical approximation of $L^{\\prime}$ and $L^{\\prime\\prime}$ , as suggested in the original version of this manuscript, is faster than exact recursive calculation. In the main example of this paper, the latter required $67\\%$ morecomputer time than the numerical methods in $\\S2$ <PARAGRAPH_SEPARATOR> # 2. METHODS <PARAGRAPH_SEPARATOR> For notational convenience, set $U_{i}=\\exp{(\\beta^{\\prime}X_{i})}$ , and define the denominator of (l) as <PARAGRAPH_SEPARATOR> $$ B(m,n)=\\sum\\prod_{i_{\\pm}=1}^{m}U_{i_{\\pm}}. $$ <PARAGRAPH_SEPARATOR> Let $B(m,n-1)$ and $B(m-1,n-1)$ denote sums like (2) defined on the reduced set of elements $(U_{1},...,U_{n-1})$ . The basic recursion is <PARAGRAPH_SEPARATOR> $$ B(m,n)=B(m,n-1)+U_{n}B(m-1,n-1). $$ <PARAGRAPH_SEPARATOR> The frst term on the right-hand side of (3) arises from those summands in (2) which exclude $U_{\\pmb{\"}}$ and the second term from those summands which include $U_{\\mathfrak{n}}$ . Initial conditions are $B(0,n)=1$ for $\\pmb{n\\geqslant0}$ and $B(m,n)=0$ for $m>n$ . The recursion can be propagated along the diagonals to yield the desired value $B(m,n)$ as indicated in Table 1. Note that the desired quantity $B(2,3)$ is indeed given by the element in row $\\pmb{m}=\\pmb{2}$ and column ${\\mathbf{\\nabla}}n=3$ <PARAGRAPH_SEPARATOR> ![](images/67fd172e5e1d9f663dc191a47b7be8abc2c2ceb7f2c96d23fa51fce9f5431e2d.jpg)  Table 1. Recursive generation for $B(m,n)$ for $\\pmb{m}=\\pmb{2}$ $\\mathbf{\\nabla}{\\pmb{n}}=\\mathbf{3}$ <PARAGRAPH_SEPARATOR> Note that direct enumeration of (2) requires $\\left\\{n!/(n-m)!\\left(m-1\\right)!\\right\\}-1$ arithmetic operations, whereas the recursion requires only $2m(n-m+1)$ arithmetic operations. For each fixed $\\beta$ one can evaluate the log likelihood <PARAGRAPH_SEPARATOR> $$ L(\\beta)=\\sum_{k=1}^{K}L_{k}(\\beta), $$ <PARAGRAPH_SEPARATOR> corresponding to $K$ sets of matched cases and controls, by computing $L_{\\star}(\\beta)$ separately for each $\\bar{\\pmb{k}}=\\mathbf{l},...,\\pmb{K}$ from the recursion (3). We used simple numerical methods to approximate $L^{\\prime}$ and $L^{\\prime\\prime}$ , the first and second derivatives of $L(\\beta)$ , and then employed a quasi Newton-Raphson procedure to solve for $\\hat{\\beta}_{c}$ , the conditional maximum likelihood estimate. For the two-dimensional case, the first component of $L^{\\prime}$ at $\\beta^{\\prime}=(\\beta_{1},\\beta_{2})$ was <PARAGRAPH_SEPARATOR> taken to be <PARAGRAPH_SEPARATOR> $$ \\big\\{L(\\beta_{1}+h,\\beta_{2})-L(\\beta_{1}-h,\\beta_{2})\\big\\}/(2h) $$ <PARAGRAPH_SEPARATOR> and the corresponding second derivative was <PARAGRAPH_SEPARATOR> $$ \\{L(\\beta_{1}+h,\\beta_{2})+L(\\beta_{1}-h,\\beta_{2})-2L(\\beta_{1},\\beta_{2})\\}/h^{2}. $$ <PARAGRAPH_SEPARATOR> The cross derivative was approximated by <PARAGRAPH_SEPARATOR> $$ \\{L(\\beta_{1}+h,\\beta_{2})+L(\\beta_{1},\\beta_{2}-h)-L(\\beta_{1}+h,\\beta_{2}-h)-L(\\beta_{1},\\beta_{2})\\}/h^{2}. $$ <PARAGRAPH_SEPARATOR> Similar formulae were used for more than two dimensions. Note that numerical formulae for $L^{\\prime}$ and diagonal elements of $L^{\\prime\\prime}$ are centred about $\\beta$ , whereas, to save calculations, slightly less accurate noncentred formulae are used for cross derivatives. To estimate $L^{\\prime}$ and $L^{\\prime}$ for $\\pmb{\\mathcal{P}}$ parameters requires $0{\\cdot}5p^{2}+1{\\cdot}5p+1$ evaluations of $L(\\beta)$ in the neighbourhood of $\\beta$ . It was found in the example in $\\S3$ that values $h=0{\\cdot}001$ and $h=0{\\cdot}0\\dot{0}01$ yielded estimates of $\\hat{\\beta}_{\\epsilon}$ which agreed to at least four decimal places and estimates of cov $(\\widehat{\\beta}_{c})$ the inverse of the observed $-L^{\\prime}$ , which differed by at most 2 units in the third decimal place. We used double precision Fortran. <PARAGRAPH_SEPARATOR> An alternative is to calculate $L^{\\prime}$ and $L^{\\prime\\prime}$ recursively as in the Appendix and to solve for $\\hat{\\beta}_{c}$ using the Newton-Raphson procedure. A hybrid approach, which uses the recursion to compute $\\mathbf{\\delta}_{L^{\\prime}}$ and the numerical methods above to approximate $L^{\\prime\\prime}$ , may also be used. We compared all three approaches. <PARAGRAPH_SEPARATOR> # 3. EXAMPLE AND DISCUSSION"
  },
  {
    "qid": "statistic-posneg-1078-0-1",
    "gold_answer": "FALSE. For a VARMA model to be invertible, the roots of det{Θ(z)} must lie outside the unit disk, not inside. This ensures that the MA component can be inverted to express the innovations ε_t as a convergent series of past observations Y_t.",
    "question": "For a VARMA model to be invertible, the roots of det{Θ(z)} must lie inside the unit disk. True or False?",
    "question_context": "Structured VARMA Models and Exact Maximum Likelihood Estimation\n\nThe paper discusses structured VARMA models, focusing on echelon forms based on Kronecker indices and scalar component models (SCM). It covers I(1) VARMA models, their transformation to stationary VARMA models, and the evaluation of exact likelihood using recurrence equations. The paper also compares exact maximum likelihood estimation with conditional methods."
  },
  {
    "qid": "statistic-posneg-1941-0-0",
    "gold_answer": "TRUE. The separation measure T(x,y;h_n,s) is defined as the ratio of between-group variability (numerator) to within-group variability (denominator). A higher value of T(x,y;h_n,s) indicates that the between-group variability is large relative to the within-group variability, which implies better separation between the two clusters. This is consistent with the paper's approach of selecting the threshold s that maximizes T(x,y;h_n,s) to achieve optimal clustering.",
    "question": "The separation measure T(x,y;h_n,s) used for clustering is defined as the ratio of between-group variability to within-group variability. Is it correct to say that a higher value of T(x,y;h_n,s) indicates a better separation between the two clusters?",
    "question_context": "Image Denoising via Local Clustering and Adaptive Smoothing\n\nThe paper presents a method for image denoising using a local clustering framework. The observed image is modeled as a 2D regression surface with discontinuities at object boundaries. The method involves clustering pixels into two groups based on intensity values within a neighborhood, using a separation measure to determine the optimal threshold. The denoised image is then estimated by weighted averaging within the identified clusters, with weights based on local similarity measures."
  },
  {
    "qid": "statistic-posneg-2132-2-0",
    "gold_answer": "FALSE. While the MLE is theoretically optimal for uncontaminated exponential distributions, Table 4 shows that the difference in RMSE between MLE and MGFEs (particularly those using ADR and AD statistics) is small. This suggests that MGFEs perform nearly as well as MLE in this scenario, not significantly inferior.",
    "question": "Is the statement 'MGFEs are expected to be inferior to MLE in terms of RMSEs for uncontaminated exponential distributions' supported by the simulation results in Table 4?",
    "question_context": "Robustness of MGF Estimators vs. MLE in Exponential and Shifted Exponential Models\n\nThe exponential $\\operatorname{Exp}(\\theta)$ CDF is a particular case of the GPD, obtained using $k=0$ in (1). Suppose that $(x_{1},\\ldots,x_{n})$ is a sample of $n$ IID observations on an $\\operatorname{Exp}(\\theta)$ random variable $X.$ . The MLE $\\hat{\\boldsymbol{\\theta}}=\\bar{\\boldsymbol{x}}$ is unbiased, asymptotically optimal and var $\\left({\\hat{\\theta}}\\right)=\\theta^{2}/n$ . Therefore, MGFEs are expected to be inferior to $\\hat{\\boldsymbol{\\theta}}$ in terms of RMSEs. We have simulated 1000 samples of size 100 for the $\\mathrm{Exp}(1)$ distribution and have computed the corresponding MGFEs. (The results are, obviously, invariant with respect to the value of $\\theta$ .) Table 4 provides the estimated bias and RMSEs for the MGFEs corresponding to the eight EDF statistics of Section 2, together with the bias and RMSE of the MLE, ordered according to their RMSEs. Although the MLE is optimal, the difference is small with respect to the MGFEs using ADR and AD statistics.<PARAGRAPH_SEPARATOR>Suppose now that the samples of size 100 are contaminated so that 95 observations come from the $\\operatorname{Exp}(\\theta)$ distribution with $\\theta=1$ , but five observations correspond to the $\\mathrm{Exp}(4\\theta)$ distribution. Table 5 provides the new values of the bias and RMSEs for the ML and MGF estimators of $\\theta$ . One can see that the MLE is now outperformed by five of the eight MGFEs. Consequently, MGFEs may be more robust than MLEs.<PARAGRAPH_SEPARATOR>As a third model, suppose that the random variable $X$ has the shifted exponential CDF<PARAGRAPH_SEPARATOR>$$F_{\\alpha,\\theta}(x)={\\left\\{\\begin{array}{l l}{1-\\exp\\{-(x-\\alpha)/\\theta\\}}&{{\\mathrm{if~}}x\\geqslant\\alpha,}\\ {0}&{{\\mathrm{if~}}x<\\alpha.}\\end{array}\\right.}$$<PARAGRAPH_SEPARATOR>Table 3 Estimated bias and RMSEs for the MGFEs corresponding to the eight EDF statistics of Section 2, together with the bias and RMSE of the MLE, based on 1000 samples of size 100 on a uniform $U[0,\\theta]$ distribution with $\\theta=1$ , in ascending order of the RMSEs<PARAGRAPH_SEPARATOR><html><body><table><tr><td></td><td>AD2R</td><td>AD2</td><td>MLE</td><td>ADR</td><td>AD</td><td>CM</td><td>KS</td><td>ADL</td><td>AD2L</td></tr><tr><td>Bias</td><td>0.000</td><td>0.000</td><td>0.010</td><td>0.005</td><td>0.007</td><td>0.013</td><td>0.016</td><td>0.015</td><td>0.026</td></tr><tr><td>RMSE</td><td>0.013</td><td>0.013</td><td>0.014</td><td>0.020</td><td>0.023</td><td>0.032</td><td>0.036</td><td>0.038</td><td>0.061</td></tr></table></body></html><PARAGRAPH_SEPARATOR>Table 4 Estimated bias and RMSEs for the MGFEs corresponding to the eight EDF statistics of Section 2, together with the bias and RMSE of the MLE, based on 1000 samples of size 100 on an exponential $\\mathrm{Exp}(\\theta)$ distribution with $\\theta=1$ , in ascending order of the RMSEs"
  },
  {
    "qid": "statistic-posneg-940-0-0",
    "gold_answer": "FALSE. Explanation: The text explicitly states that in binary regression models with measurement errors, the functional maximum likelihood estimate of β is not consistent when ΩM is known, which contrasts with linear regression where the functional maximum likelihood estimate is consistent under certain conditions (e.g., known ratio of error variances or finite replication of predictors). This inconsistency in binary regression arises due to the nonlinear nature of the model and the presence of measurement errors in the predictors.",
    "question": "In the context of binary regression models with measurement errors, is it true that the functional maximum likelihood estimate of β is consistent when ΩM is known, similar to linear regression?",
    "question_context": "Errors-in-Variables in Binary Regression Models\n\nIt is well known that many baseline risk factors are measured with error; systolic blood pressure is a good example (Armitage & Rose, 1966; Rosner & Polk, 1979). One of us was asked by a number of investigators whether such measurement errors could substantially affect the binary regression estimates and, if so, what could be done to correct for the measurement error. The present study is an outgrowth of these questions, although there are many important practical facets of the problem yet to be investigated.<PARAGRAPH_SEPARATOR>Michalek & Tripathi (1980) show that ordinary logistic regression will not be too badly disturbed by measurement error as long as such error is moderate; see also Ahmed $\\&$ Lachenbruch (1975). While our model is different, our methods provide alternatives to ordinary binary regression which will help the experimenter to get a more precise understanding of the effect of the measurement errors, especially if they are severe and the sample size is large.<PARAGRAPH_SEPARATOR>The model is similar to that of Halperin, Wu & Gordon (1979). We have a sample of $N$ persons from a particular population, e.g. males aged 45-54. The ith person in the sample is assumed to have a $\\pmb{\\mathcal{P}}$ -vector of baseline risk factors $\\pmb{x}_{i}$ .with the probability of developing disease given $x_{i}$ taking the form<PARAGRAPH_SEPARATOR>$$\\mathrm{pr}(Y_{i}=1|x_{i})=G(x_{i}^{\\prime}\\beta)\\quad(i=1,...,N),$$<PARAGRAPH_SEPARATOR>where $G(.)$ is a  known distribution  function such  as for  logistic regression $G(a)=(1+e^{-a})^{-1}$ , and for probit regression ${\\cal G}(a)=\\Phi(a)$ .Here $\\Phi(.)$ isthe standard normal distribution function. We return to probit regression later, but it is important to remember that probit and logistic regression often given similar results (Finney, 1964).<PARAGRAPH_SEPARATOR>We partition the risk factors $\\pmb{x}_{t}$ into components observed without and with error, so that<PARAGRAPH_SEPARATOR>$$\\begin{array}{r}{x_{i}^{\\prime}=(w_{i}^{\\prime},z_{i}^{\\prime}),\\quad\\beta^{\\prime}=(\\beta_{1}^{\\prime},\\beta_{2}^{\\prime}).}\\end{array}$$<PARAGRAPH_SEPARATOR>In (1·2), the vectors $\\left\\{w_{i}\\right\\}$ can be observed at nearly exact levels, while the $\\pmb q$ vectors $\\left\\{z_{i}\\right\\}$ are measured with nontrivial error and cannot be observed; rather, we observe<PARAGRAPH_SEPARATOR>$$Z_{i}=z_{i}+u_{i}.$$<PARAGRAPH_SEPARATOR>The $\\left\\{u_{i}\\right\\}$ are assumed independently and normally distributed with mean zero and nonsingular covariance $\\Omega_{M}$<PARAGRAPH_SEPARATOR>When the risk factors $\\left\\{z_{i}\\right\\}$ observed with error are unknown constants, we have a functional model (Kendall $\\&$ Stuart, 1979, Chapter 29). In this instance, classical maximum likelihood theory does not apply. In fact, even in simple binary regression models, the functional maximum likelihood estimate of $\\beta$ is not consistent when $\\Omega_{M}$ is known; details are available from the authors. This is in contrast to linear regression, where the functional maximum likelihood estimate is consistent if the ratio of the error variances is known or if there is finite replication of the predictors. Consistent and asymptotically normal estimates for the functional logistic regression model can be constructed when the measurement errors in (1·3) are normally distributed; this work will be reported elsewhere.<PARAGRAPH_SEPARATOR>In $\\S2$ we study the structural model, wherein the $\\left\\{z_{i}\\right\\}$ are themselves independent with common distribution function $\\pmb{F}$ , which we will suppose is that of a normal random vector with mean $\\mu_{x}$ and covariance $\\Omega_{\\pm}$ . In effect, we condition on the observed values $(w_{i},Z_{i})$ and replace (1·l) by pr( $Y_{i}=1|w_{i},Z_{i})$ , the probability of an event given the observed outcomes; see Armstrong & Oakes (1982) for a similar idea."
  },
  {
    "qid": "statistic-posneg-760-0-1",
    "gold_answer": "FALSE. Explanation: While Lemma 2.7 states that $\\mathbf{P}_{\\mathbf{K};\\mathbf{X};\\Sigma}$ (the projection matrix for BLUE) is unique if and only if $r[\\mathbf{X}, \\boldsymbol{\\Sigma}]=n$, the BLUE itself is unique with probability 1 if and only if the model $\\mathcal{M}$ is consistent (i.e., $\\mathbf{y}$ lies in the column space of $[\\mathbf{X}, \\boldsymbol{\\Sigma}]$ with probability 1). Uniqueness of the projection matrix does not necessarily imply uniqueness of the estimator in all cases, as the arbitrary matrix $\\mathbf{U}$ in the general expression of BLUE can lead to multiple representations unless the model is consistent.",
    "question": "Is it correct that the BLUE of $\\mathbf{K}\\beta$ under (1.1) is unique if and only if $r[\\mathbf{X}, \\boldsymbol{\\Sigma}]=n$?",
    "question_context": "Estimability and BLUE/OLSE Comparisons in Constrained Linear Models\n\nLemma 2.3. Let $\\mathcal{P}_{r}$ be as given in (1.4). Then, the following statements are equivalent: (a) $\\mathbf{X}_{1}\\boldsymbol{\\beta}_{1}$ is estimable under (1.4), i.e., there exists a linear statistic $\\mathbf{L}\\mathbf{y}+\\mathbf{d}$ such that $E(\\mathbf{Ly}+\\mathbf{d})=\\mathbf{X}_{1}\\pmb{\\beta}_{1}$ holds under (1.4). (b) $\\mathbf{X}_{2}\\boldsymbol{\\beta}_{2}$ is estimable under (1.4), i.e., there exists a linear statistic $\\mathbf{L}\\mathbf{y}+\\mathbf{d}$ such that $E(\\mathbf{Ly}+\\mathbf{d})=\\mathbf{X}_{2}\\pmb{\\beta}_{2}$ holds under (1.4). (c) $\\mathcal{R}(\\widetilde{\\mathbf{X}}_{1}^{\\prime})\\subseteq\\mathcal{R}(\\mathbf{X}_{c}^{\\prime})$ $(\\mathcal{N}(\\widetilde{\\mathbf{X}}_{1})\\supseteq\\mathcal{N}(\\mathbf{X}_{c}))$. (d) $\\mathcal{R}(\\widetilde{\\mathbf{X}}_{2}^{\\prime})\\subseteq\\mathcal{R}(\\mathbf{X}_{c}^{\\prime})$ $(\\mathcal{N}(\\widetilde{\\mathbf{X}}_{2})\\supseteq\\mathcal{N}(\\mathbf{X}_{c}))$. (e) $\\mathcal{R}(\\mathbf{X}_{c1})\\cap\\mathcal{R}(\\mathbf{X}_{c2})=\\{\\mathbf{0}\\}$. (f) $r(\\mathbf{X}_{c})=r(\\mathbf{X}_{c1})+r(\\mathbf{X}_{c2})$. (g) $\\mathcal{R}(\\mathbf{X}_{1}\\mathbf{F}_{\\mathbf{A}_{1}})\\cap\\mathcal{R}(\\mathbf{X}_{2}\\mathbf{F}_{\\mathbf{A}_{2}})=\\{\\pmb{0}\\}$. (h) $r(\\mathbf{X}\\mathbf{F}_{\\mathbf{A}})=r(\\mathbf{X}_{1}\\mathbf{F}_{\\mathbf{A}_{1}})+r(\\mathbf{X}_{2}\\mathbf{F}_{\\mathbf{A}_{2}})$."
  },
  {
    "qid": "statistic-posneg-203-1-2",
    "gold_answer": "FALSE. The density is $f(z,c) = D''(z)z(1-z)$ only for $(z,c)$ with $-1/D(z) < c < 0$, but this is not universally true for all $(z,c)$. The paper specifies this density under the condition that $D$ is twice continuously differentiable and not the constant function 1.",
    "question": "Is the density of the Pickands coordinates $(Z,C)$ for a GP with dependence function $D$ given by $f(z,c) = D''(z)z(1-z)$ for all $(z,c)$ with $-1/D(z) < c < 0$?",
    "question_context": "Distribution of Pickands Coordinates in Bivariate Extreme Value Models\n\nThe paper models a bivariate extreme value distribution with Pickands coordinates $(Z,C) = (V/(U+V), U+V)$, where $(U,V)$ is a random vector with df $H$ having support in $(-\\infty,0]^2$. The transformation $T(x,y) := \\left(\\frac{y}{x+y}, x+y\\right)$ is introduced to define the Pickands coordinates. The density of $(Z,C)$ is derived under the assumption that $H$ has continuous partial derivatives of second order near $(0,0)$."
  },
  {
    "qid": "statistic-posneg-1071-0-1",
    "gold_answer": "FALSE. The paper explicitly states that $\\hat{Z}_i$ can exceed 1 and thus truncates it to 1 when it is greater than 1. The modification helps stabilize the values but does not inherently bound them below 1.",
    "question": "The paper proposes a modified version of the E-step when $f$ is unknown, given by: $$\\hat{Z}_{i}=\\frac{2\\pi\\phi_{\\sigma}\\left(X_{i}\\right)}{\\pi\\phi_{\\sigma}\\left(X_{i}\\right)+\\widehat{m\\left(X_{i}\\right)}},$$ where $\\widehat{m(x)}$ is the KDE of the mixture density. Is it true that this modification ensures $\\hat{Z}_i$ never exceeds 1?",
    "question_context": "Semiparametric Mixture Model Estimation via EM Algorithm and Kernel Density Estimation\n\nThe paper discusses a semiparametric mixture model where observations are drawn from a mixture of a known normal distribution and an unknown distribution. The model is given by: $$X_{i}\\sim\\pi\\phi(\\mu,\\sigma^{2}I_{p})+(1-\\pi)f,\\quad i=1,\\ldots,n,$$ where $\\phi(\\mu,\\sigma^{2}I_{p})$ is the density of $N_{p}(\\mu,\\sigma^{2}I_{p})$, $\\mu$ is known (assumed to be 0), and $f$ is an unknown density. The EM algorithm is used for estimation, with the E-step involving the conditional expectation of the latent variables $Z_i$ given the current parameter estimates and the data. The M-step updates the parameters $\\pi$ and $\\sigma$ based on these expectations. When $f$ is unknown, a kernel density estimator (KDE) is used to approximate $f$."
  },
  {
    "qid": "statistic-posneg-223-0-1",
    "gold_answer": "FALSE. The paper clearly indicates that if some or all of $\\pmb{k}$ samples are of unequal size, even moderate variations in variance-covariance matrices can have a large effect on the level of significance and the power of the test. This suggests that unequal sample sizes exacerbate the impact of unequal variance-covariance matrices on the test's performance.",
    "question": "Does the paper conclude that the $\\pmb{T_{0}^{2}}$ test is unaffected by unequal sample sizes when variance-covariance matrices are unequal?",
    "question_context": "Robustness of the T02 Test in MANOVA with Unequal Variance-Covariance Matrices\n\nCurrent interest in multivariate analysis in biometrics and other areas of applied statistics must inevitably lead to a scrutiny of the robustness of the procedure used. One widely used test isthe $\\pmb{T_{0}^{2}}$ test in multivariate analysis of variance (MANOVA). Among the assumptions underlying MANOVA are: (1) multivariate normal distributions, (2) equality of variancecovariance matrices, and (3) serially uncorrelated observations. The purpose of the present paper is to explore, in a preliminary fashion, the consequence to this test of violation of the assumption of equality of variance-covariance matrices. It is shown that in the case of two samples of equal size, the test is well behaved when there are variations of variancecovariance matrices if the samples are very large; further that in the case of two samples of nearly equal size or $\\pmb{k}$ samples of equal size, the test is not affected seriously by moderate inequality of variance-covariance matrices as long as samples are very large. However, if some or all of $\\pmb{k}$ samples are of unequal size, quite a large effect occurs on the level of significance and the power of the test from even moderate variations. It is to be noted that among numerous works in univariate analysis on the effect of heteroscedasticity of variances (see, e.g. David $\\&$ Johnson, 1951; Gronow, 1951; Horsnell, 1953; Box, 1954; Schef6, 1959, Pp. 334-58), the present investigation is related to and has extended the work by Scheffé and Box as far as the effect on the level of significance of the test is concerned."
  },
  {
    "qid": "statistic-posneg-1315-1-2",
    "gold_answer": "FALSE. Explanation: The ISNI method avoids fitting complicated nonignorable models. Instead, it uses a Taylor-series approximation around the MAR model, requiring only the MAR estimates and derivatives evaluated at γ1 = 0. The paper emphasizes that this approach simplifies sensitivity analysis by avoiding high-dimensional integrations and complex model fitting.",
    "question": "The ISNI (Index of Sensitivity to Nonignorability) method requires fitting a nonignorable missing data model to compute the sensitivity of parameter estimates to nonignorable missingness. Is this statement true?",
    "question_context": "Nonignorable Missing Data Mechanism in Longitudinal Clinical Trials\n\nThe paper discusses modeling nonignorable missing data mechanisms in longitudinal clinical trials, distinguishing between intermittent missingness and dropout. The missing data model assumes that the missingness status at each visit follows a multinomial distribution, conditional on past missingness patterns and observed outcomes. The model allows for different missing data mechanisms for different past missingness patterns and includes parameters to account for the dependence of missingness on unobserved outcomes."
  },
  {
    "qid": "statistic-posneg-1468-0-0",
    "gold_answer": "FALSE. The formula $$x_{i}=g_{i}\\lambda_{i}(\\gamma(s_{i})+\\varepsilon_{i})$$ includes additional multiplicative factors $g_i$ and $\\lambda_i$ that are not explained in the context. The deterministic geodesic path $\\gamma(s_i)$ and random perturbation $\\varepsilon_i$ are only part of the additive component within the parentheses. The full interpretation must account for the role of $g_i$ and $\\lambda_i$, which might represent scaling or transformation factors not detailed in the provided context.",
    "question": "Is the geodesic perturbation model formula $$x_{i}=g_{i}\\lambda_{i}(\\gamma(s_{i})+\\varepsilon_{i})$$ correctly interpreted as a model where $x_i$ is a combination of a deterministic geodesic path $\\gamma(s_i)$ and a random perturbation $\\varepsilon_i$?",
    "question_context": "Geodesic Perturbation Model for Shape Analysis\n\nWithin the discussion of ‘consistency’, the modelling of Dryden et al. (2009a) for diffusion tensor imaging has been extended to diffusion tensors mildly deficient in rank. To the knowledge of the author, this is the first framework allowing for statistical inference on non-regular diffusion tensors without utilizing a Euclidean embedding, say, in the space of symmetric matrices. Of course in many applications, the interest lies specifically in degenerate tensors because these indicate a strong directional flow. The finding that mildly rank-deficient diffusion tensors are only ‘mildly perturbation inconsistent’ may provide for another motivation for the approach of Dryden et al. (2009a, b). Following Dryden et al. (2009a), we have used Procrustes means. For the underlying reflection shape space, extrinsic Schoenberg means qualify as well, they compute much faster than Procrustes or Ziezold means. As the most important advantage, the two-sample tests of Bhattacharya (2008) can then be performed even including rank 1 diffusion tensors. A drawback, however, may result from a possible insensitivity of Schoenberg means to degeneracy thus yielding a smaller discrimination power (cf. Huckemann (2010) for a detailed discussion). These issues certainly warrant further research.<PARAGRAPH_SEPARATOR>Briefly compiling the findings on tree boles we can add to the well known fact that longest boles can be found in dense stands, it seems that the most cylindrical boles can be found in young stands with low competition.<PARAGRAPH_SEPARATOR>A geodesic perturbation model. As demonstrated in this work, for most practical applications in 3D avoiding degenerate configurations, perturbation models may be considered compatible with the shape space’s geometry for isotropic error. Note that the shapes of entire tree boles are nearly singular (cf. Huckemann et al. (2010)). For short bole frusta, however, such models may still serve intuition and for sufficiently small error provide an adequate approximation. As one of such consider the geodesic perturbation model<PARAGRAPH_SEPARATOR>$$x_{i}=g_{i}\\lambda_{i}(\\gamma(s_{i})+\\varepsilon_{i})$$<PARAGRAPH_SEPARATOR>with a straight line $s\\to\\gamma(s)=\\mu+s\\nu$ in configuration space $F_{m}^{k}$ such that locally all points on \u0011 are in optimal position to each other. For 3D frusta, the geodesic of cylinders has been used here, any other geodesic of specific frusta can be used similarly. Moreover, based on larger samples, a geodesic perturbation model within the space of elliptical-like frusta can be used to assess specific model parameters. In particular, a close investigation of such model parameters may lead to inference on the impact of environmental effects on the shape of tree boles building on artificially induced modifications of biological tree parameters as in Gaffrey & Sloboda (2004).<PARAGRAPH_SEPARATOR># Acknowledgements<PARAGRAPH_SEPARATOR>The author would like to express his sincerest gratitude to the late Herbert Ziezold (1942 to 2008) whose untimely death was most unfortunate. He would also like to thank Dieter Gaffrey and the colleagues from the Institute for Forest Biometry and Informatics at the University of Göttingen for discussion of and supplying with the tree-bole data. Also he is grateful for helpful discussions with Thomas Hotz, John Kent and Axel Munk and support by the DFG. Another thank goes to the two anonymous referees’ and the associate editor’s valuable comments improving this work considerably."
  },
  {
    "qid": "statistic-posneg-1038-3-4",
    "gold_answer": "FALSE. While the simultaneous perturbation scheme includes formulas for both yi = 0 and yi > 0 cases, the actual influence measures (λti) for the zero-inflation portion still show zero influence when yi > 0 (as seen in earlier appendices). The non-zero components for yi > 0 primarily relate to the count portion of the model. Thus, not all observations contribute equally to both components - zero counts influence both portions while positive counts mainly influence the count component.",
    "question": "For the simultaneous explanatory variable perturbation (Appendix E), the influence measures are non-zero for both yi = 0 and yi > 0 cases. Does this indicate that all observations contribute to the assessment of model sensitivity to perturbations in this scheme?",
    "question_context": "Zero-Inflated Negative Binomial Regression: Influence Diagnostics\n\nThe authors derive formulas for obtaining the second-order partial derivatives of the log-likelihood function in a zero-inflated negative binomial (ZINB) regression model. The model includes functions for zero-inflation probabilities (g1, g2), a combined hazard function (h), and logistic components (v). The perturbation schemes (case-weight, explanatory variable, and simultaneous) are used to assess the influence of observations on parameter estimates."
  },
  {
    "qid": "statistic-posneg-608-6-2",
    "gold_answer": "TRUE. The paper explicitly states that each algorithm was run for 10 replicates of $10^5$ iterations using the optimal tuning parameters, and the efficiencies were calculated and compared. The conclusion that AAPS is slightly less efficient than blurred HMC and slightly more efficient than the no U-turn sampler is based on these results.",
    "question": "The paper states that AAPS is slightly less efficient than blurred HMC and slightly more efficient than the no U-turn sampler on this high-dimensional posterior. Is it true that this conclusion is based on running each algorithm for 10 replicates of $10^5$ iterations using optimal tuning parameters?",
    "question_context": "Stochastic Volatility Model: Parameter Estimation and Algorithm Efficiency\n\nTable 3 of Appendix I provides the equivalent efficiencies when the algorithms are tuned according to recommended guidelines, and shows AAPS remaining competitive with blurred HMC and the no U-turn sampler. <PARAGRAPH_SEPARATOR> # 4.2. Stochastic Volatility Model <PARAGRAPH_SEPARATOR> Consider the following model for zero-centered, Gaussian data $y=(y_{1},\\dots,y_{T})$ where the variance depends on a zero-mean, Gaussian AR(1) process started from stationarity (e.g., Girolami and Calderhead 2011; Wu et al. 2019): <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{Y_{t}\\sim\\mathrm{N}(0,\\kappa^{2}\\exp x_{t}),t=1,\\ldots,T,}}\\ {{X_{1}\\sim\\mathrm{N}\\left(0,\\displaystyle\\frac{\\sigma^{2}}{1-\\phi^{2}}\\right),X_{t}|(X_{t-1}=x_{t-1})}}\\ {{\\qquad\\sim\\mathrm{N}(\\phi x_{t-1},\\sigma^{2}),t=2,\\ldots,T.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Parameter priors are $\\pi_{0}(\\kappa)\\propto1/\\kappa,1/\\sigma^{2}\\sim\\mathrm{Gamma}(10,0.05)$ ) and $(1+\\phi)/2\\sim\\mathrm{Beta}(20,1.5)$ , and we reparameterise to give a parameter vector in $\\mathbb{R}^{3}$ : <PARAGRAPH_SEPARATOR> $$ \\alpha=\\log\\frac{1+\\phi}{1-\\phi},\\beta=\\log\\kappa,\\mathrm{and}\\gamma=2\\log\\sigma. $$ <PARAGRAPH_SEPARATOR> As in Girolami and Calderhead (2011) and Wu et al. (2019) we generate $T~=~1000$ observations using parameters $\\phi=$ $0.98,\\kappa=0.65$ and $\\sigma=0.15$ . We then apply blurred HMC, AAPS and the no U-turn sampler to perform inference on the 1003-dimensional posterior. We ran AAPS using two different $K$ values, one found by optimizing the choice of $(\\epsilon,K)$ over a numerical grid and one by using the tuning mechanism mentioned in Section 3.5. Tuning standard HMC (not blurred HMC) on such a high-dimensional target was extremely difficult; due to the algorithm’s sensitivity to the integration time, we could not identify a suitable range for $L$ . Widely used statistical packages such as Stan (Stan Development Team 2020) and PyMC3 (Salvatier et al. 2016) perform the blurring by default, and so we only present the results for HMC-bl. <PARAGRAPH_SEPARATOR> Each algorithm was then run for 10 replicates of $10^{5}$ iterations using the optimal tuning parameters. Efficiency was calculated for each parameter, and the minimum efficiency over the latent variables and over all 1003 components were also calculated. For each algorithm the mean and standard deviation (over the replicates) of these efficiencies were ascertained; Table 2 reports these values normalized by the mean efficiency for AAPS for that parameter or parameter combination. Overall, on this complex, high-dimensional posterior, AAPS is slightly less efficient than blurred HMC, and slightly more efficient than the no U-turn sampler. <PARAGRAPH_SEPARATOR> # 4.3. Multimodality"
  },
  {
    "qid": "statistic-posneg-723-0-0",
    "gold_answer": "FALSE. The definition of L is not standard for Bernstein polynomial density estimation. Typically, the weight matrix in such contexts would be based on variance stabilization or other considerations, but the given form involving F_n(x_i) and 3/(8n) is not a conventional choice. The paper might be using a specific weighting scheme, but without further justification, this form seems arbitrary and not directly linked to the core principles of Bernstein polynomial estimation.",
    "question": "The diagonal weight matrix L is defined as Diag(n[(F_n(x_i) + 3/(8n))(1 + 3/(8n) - F_n(x_i))]^{-1}). Is this definition correct for the given context of unimodal density estimation using Bernstein polynomials?",
    "question_context": "Bernstein Polynomial Method for Unimodal Density Estimation\n\nThe paper discusses the Bernstein polynomial method for unimodal density estimation. The minimization problem is expressed as (f - Bω)^T L (f - Bω), where B is a matrix of Bernstein basis functions, f is the empirical distribution function, and L is a diagonal weight matrix. The expression expands to f^T L f - 2 f^T L B ω + ω^T B^T L B ω, which is then decomposed into components for optimization."
  },
  {
    "qid": "statistic-posneg-1127-0-0",
    "gold_answer": "TRUE. Proposition S1 in the Supplementary Material shows that under Conditions 1–4, the probability that the estimated bandwidth $\\hat{k}$ equals the true bandwidth $k_{0}$ and the estimated order $\\hat{d}$ equals the true order $d$ converges to 1 as $n, p \\to \\infty$. This consistency is achieved by minimizing the modified BIC criterion $\\widetilde{\\mathrm{BIC}}_{i}(k,\\ell)$, which accounts for both the bandwidth and the autoregressive order.",
    "question": "Is it true that the proposed BIC-based estimator for the bandwidth $k_{0}$ is consistent under the given conditions, as stated in Proposition S1 in the Supplementary Material?",
    "question_context": "Banded Vector Autoregressive Models: Estimation and Bandwidth Determination\n\nThe paper discusses banded vector autoregressive models where the coefficient matrices $A_{1},\\ldots,A_{d}$ are banded, meaning $a_{i j}^{(\\ell)}=0$ for $|i-j|>k_{0}$. The goal is to estimate the banded coefficient matrices and determine the bandwidth $k_{0}$. The model is formulated as $y_{t}=A_{1}y_{t-1}+\\cdots+A_{d}y_{t-d}+\\varepsilon_{t}$, with $\\varepsilon_{t}$ being the innovation at time $t$. The paper proposes using the marginal Bayesian information criterion (BIC) to estimate $k_{0}$ and the coefficient matrices, with the estimator given by $\\widehat{k}=\\operatorname*{max}_{1\\leqslant i\\leqslant p}\\Big\\{\\underset{1\\leqslant k\\leqslant K}{\\arg\\operatorname*{min}}\\scriptscriptstyle\\mathrm{BIC}_{i}(k)\\Big\\}$."
  },
  {
    "qid": "statistic-posneg-958-1-0",
    "gold_answer": "FALSE. Although the simulated data satisfy the assumption of no unmeasured confounders (since treatment depends only on $W$), Scenario 3 introduces confounding through $W_{1}$, which is related to both treatment and outcome. This means that $W_{1}$ acts as a confounder, and failing to adjust for it would lead to biased treatment effect estimates. The presence of $W_{3}$, which predicts treatment but not outcome, does not introduce confounding but does not eliminate the confounding caused by $W_{1}.",
    "question": "In Scenario 3, where treatment is confounded through $W_{1}$ ($\\beta_{1}=1$) and $W_{3}$ predicts treatment ($\\beta_{3}=1$), but $W_{3}$ is unrelated to the outcome ($\\alpha_{3}=0$), does the model still satisfy the assumption of no unmeasured confounders?",
    "question_context": "Semiparametric Model Selection in Treatment Effect Estimation\n\nFor each simulation experiment we simulate data from one of three general scenarios. These scenarios differ in terms of how the covariates are related to outcome and treatment. For all scenarios, we take W to be a 3-dimensional covariate, $A$ to be a dichotomous treatment, and $Y$ to be a either a continuous or dichotomous outcome. We generate data using the following laws for the observed data: $W$ is multivariate normal with mean 0 and $\\Sigma=I$. $A$ is conditionally Bernoulli with mean $E[A|W]=\\left(1+\\exp\\{-(\\beta_{0}+\\beta_{1}W_{1}+\\beta_{2}W_{2}+\\beta_{3}W_{3})\\}\\right)^{-1}$. The conditional distribution of $Y$ is either Gaussian with conditional mean $E[Y\\vert A,W]=\\alpha_{0}+\\alpha_{1}W_{1}+\\alpha_{2}W_{2}+\\alpha_{3}W_{3}+\\alpha_{4}A$ and variance $\\sigma_{y}=1$ or Bernoulli with conditional expectation $E[Y\\vert A,W]=(1+\\exp\\{-(\\alpha_{0}+\\alpha_{1}W_{1}+\\alpha_{2}W_{2}+\\alpha_{3}W_{3}+\\alpha_{4}A)\\})^{-1}$. Given this described set-up, we consider three scenarios based on different settings of the parameters of the data generating distribution. In all three scenarios, the covariates $W_{1}$ and $W_{2}$ are related to the outcome, while $W_{3}$ is not ($\\alpha_{1}=\\alpha_{2}=1$, $\\alpha_{3}=0$). For all scenarios, the treatment has the same effect on the outcome ($\\alpha_{4}=1$). The three scenarios differ in how the covariates are related to treatment: Scenario $I$: Treatment is completely randomized as it would be in a clinical trial ($\\beta_{1}=\\beta_{2}=\\beta_{3}=0$). Scenario 2: Treatment depends on $W_{3}$ ($\\beta_{3}=1$), but since this covariate is unrelated to the outcome there is no confounding. Scenario 3: Treatment is confounded through $W_{1}$ while $W_{3}$ still predicts treatment ($\\beta_{1}=\\beta_{3}=1$). Since treatment depends only on $W$, the simulated data satisfy the assumption of no unmeasured confounders for treatment."
  },
  {
    "qid": "statistic-posneg-311-0-0",
    "gold_answer": "TRUE. The efficiency E is defined as the ratio of the variance of the multivariate estimate β̂_1 to the variance of the univariate estimate β̃_1. Since the multivariate estimate β̂_1 is the maximum likelihood estimate, it is fully efficient, meaning its variance is minimized. Therefore, E ≤ 1 by definition. The specific formula E = Ω_11 ∑ σ_i^{-2} = (1/n) V̄_11 ∑ (1^T V_i 1)^{-1} reflects this property, as it compares the variance of the multivariate estimate (which includes information from all outcomes) to the univariate estimate (which only includes the primary outcome).",
    "question": "Is the efficiency E = Ω_11 ∑ σ_i^{-2} = (1/n) V̄_11 ∑ (1^T V_i 1)^{-1} always less than or equal to 1, as claimed in the paper?",
    "question_context": "Multivariate Meta-Analysis: Efficiency and Borrowing of Strength\n\nThe paper discusses a multivariate meta-analysis of n independent studies, each measuring a p×1 vector y of treatment effect estimates. The standard multivariate fixed effects model is y_i ∼ N(β, V_i), where V_i is the variance matrix specific to each study, and β is the common mean parameter. The maximum likelihood estimate of β is given by β̂ = Ω ∑ V_i^{-1} y_i, where Ω = (∑ V_i^{-1})^{-1}. The efficiency E is defined as the ratio of the variance of the multivariate estimate β̂_1 to the variance of the univariate estimate β̃_1, with E ≤ 1. The borrowing of strength B(V_i^{-1}) measures the contribution of secondary outcomes to the variance of β̂_1."
  },
  {
    "qid": "statistic-posneg-1500-0-3",
    "gold_answer": "TRUE. The context provides the expansion $\\log\\left(\\frac{f+\\frac{1}{2}}{m-f+\\frac{1}{2}}\\right) = \\log\\left(\\frac{p}{1-p}\\right) + \\gamma^{1/2}z m^{-1/2} + \\frac{1}{2}\\gamma(1-2p)(1-z^{2})m^{-1} + O_{p}(m^{-3/2})$, which explicitly includes the $O_{p}(m^{-3/2})$ term.",
    "question": "Does the approximation for $\\log\\left(\\frac{f+\\frac{1}{2}}{m-f+\\frac{1}{2}}\\right)$ include a term of order $O_{p}(m^{-3/2})$?",
    "question_context": "Meta-Analysis Approximations and Radial Plot Statistics\n\nThe paper discusses the asymptotic properties of estimates in meta-analysis, focusing on the joint normality of $(\\hat{\\theta}, \\hat{\\sigma}^2)$ with mean $(\\theta, \\sigma^2)$. It assumes biases of order $O(n^{-1})$, and provides various approximations and expansions for statistics like $\\widetilde{\\theta}$, $Z_1$, and $Q$. The context includes detailed derivations and expectations of these statistics, as well as special cases involving binomial distributions and radial plot statistics."
  },
  {
    "qid": "statistic-posneg-526-0-0",
    "gold_answer": "FALSE. The condition $r^{\\prime}\\leqslant2r$ is not necessarily a strict requirement for all $V$ paths of type (4.20). The actual bound depends on the specific constraints and free indices in the path, and it can vary based on the structure of the matrix and the indices involved.",
    "question": "Is it true that for a $V$ path of type (4.20), the number of distinct elements $r^{\\prime}$ must satisfy $r^{\\prime}\\leqslant2r$?",
    "question_context": "Eigenvector Analysis in Large Dimensional Matrices\n\nIt is evident that each nonzero term in (4.10)-(4.19) is the result of a $V$ path with $r_{1}=r_{2}=r$ <PARAGRAPH_SEPARATOR> Let $r_{0}$ and $c_{0}$ be, respectively, the number of rows and columns of $\nu_{n}$ entered by a given $V$ path. Let $l=r_{0}+c_{0}$ . It is clear that $l$ is the number of distinct indices in the $V.$ path. We will first determine a bound on $l$ for each type of $V$ -path in (4.10)-(4.19) resulting in a nonzero term. There are essentially six different types: <PARAGRAPH_SEPARATOR> Vik…: Uyk,: U)k,:. : Vu,) i# j appearing in (4.14) and (4.17), which can be seen by reversing the order on a set of indices, say, $\\underline{{i}}_{2},...,\\underline{{i}}_{r_{2}};\\underline{{k}}_{1},...,\\underline{{k}}_{r_{2}}$ $v_{1k_{1}}:\\cdots:v_{1k_{r_{1}}}:v_{1\\underline{{k}}_{1}}:\\cdots:v_{1\\underline{{k}}_{r_{2}}}$ appearing in (4.10) (4.20) and (4.21) are those $V.$ paths studied in [4]), <PARAGRAPH_SEPARATOR> $$ v_{i k_{1}}:\\cdots:v_{j k_{r_{1}}}:v_{j\\underline{{k}}_{1}}:\\cdots:v_{j\\underline{{k}}_{r_{2}}},\\quad i\not=j, $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{v_{i k_{1}}:\\cdots:v_{i k_{r_{1}}}:v_{j\\underline{{k}}_{1}}:\\cdots:v_{j\\underline{{k}}_{r_{2}}},i\\not=j,\\mathrm{app}}\\ &{v_{i k_{1}}:\\cdots:v_{i k_{r_{1}}}:v_{\\underline{{i}}\\underline{{k}}_{1}}:\\cdots:v_{\\underline{{j}}\\underline{{k}}_{r_{2}}},i,\\underline{{i}},\\underline{{j}}}\\end{array} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> For a $V$ path of type (4.20) ler $r^{\\prime}\\leqslant2r$ be the number of distinct elements of $V_{n}$ appearing in the path and let $(a_{1},b_{1}),...,(a_{r^{\\prime}},b_{r^{\\prime}})$ be a listing of the indices on the distinct elements where the a's $(b^{\\flat}\\mathfrak{s})$ correspond to the rows (columns) of $V_{n}$ . Each $\\pmb{a}_{m}\\left(\\pmb{b}_{m}\\right)$ is either constrained, that is equal, to at least one other $a_{m}$ $\\left(b_{m^{\\prime}}\\right)$ , or is free, that is, not constrained to any other index. Notice that any pair cannot have both indices free. <PARAGRAPH_SEPARATOR> We have three cases:"
  },
  {
    "qid": "statistic-posneg-1384-2-0",
    "gold_answer": "FALSE. While the condition $E(\\varphi_A f) = \\varphi_A E(f)$ is necessary for $E$ to be a conditional expectation operator, it is not sufficient on its own. The operator $E$ must also satisfy additional properties such as being a contractive projection and preserving the measure structure, as detailed in the paper's theorems and proofs.",
    "question": "Is the condition $E(\\varphi_A f) = \\varphi_A E(f)$ for $f \\in L^1(\\Sigma)$ and $A \\in \\Lambda$ sufficient to conclude that $E$ is a conditional expectation operator?",
    "question_context": "Contractive Projections and Conditional Expectations in L^p Spaces\n\nThe paper discusses contractive projections and conditional expectations in L^p spaces, focusing on operators that preserve certain properties under contraction. The main theorem characterizes such projections and their relationship with conditional expectations, emphasizing conditions under which these operators coincide."
  },
  {
    "qid": "statistic-posneg-953-0-0",
    "gold_answer": "TRUE. The context explicitly states that $\\mathscr{E}(\tilde{c}_{2}^{\\mathrm{adj}})\\approx c_{2}$ while $\\mathscr{E}(\tilde{c}_{2})=\\infty$, indicating that the adjusted estimator is designed to avoid the infinite expectation issue of the original estimator. The adjustment term $\\frac{(t-1)n_{1}(n_{1}-1)}{2t(n_{2}+1)}$ ensures finite estimates by incorporating the observed counts ($n_1$, $n_2$) and the number of sampling occasions ($t$) in a way that bounds the estimator's expectation.",
    "question": "Is the adjusted estimator $\tilde{c}_{2}^{\\mathrm{adj}}=n+\\frac{(t-1)n_{1}(n_{1}-1)}{2t(n_{2}+1)}$ guaranteed to always provide a finite estimate for the population size, unlike the original estimator $\tilde{c}_{2}$?",
    "question_context": "Adjusted Estimator for Population Size in Capture-Recapture Studies\n\nLink (2003) concluded that “the examples strongly call into question the usefulness of nonparametric methods; they simply cannot be reliable in all cases”. It is not necessary to be so pessimistic. The proposed lower bounds to the population size are useful in the sense that they provide sensible partial information and their estimators yield lower confidence limits for the population size when genuine upper confidence limits do not exist (Mao and Lindsay, 2006). <PARAGRAPH_SEPARATOR> Although the appropriate choice of the order of the proposed lower bounds depends on particular applications, there are some recommendations. When $t$ is small, there are only a few lower bounds and the sharpest one is recommended. When $t$ gets larger, although more lower bounds are available, as it is often difficult to precisely and accurately estimate higher order lower bounds, including the sharpest one, the second and third order ones are recommended. <PARAGRAPH_SEPARATOR> Finally, note that there is an adjusted version of $\tilde{c}_{2}$ in (12) given by <PARAGRAPH_SEPARATOR> $$ \tilde{c}_{2}^{\\mathrm{adj}}=n+\\frac{(t-1)n_{1}(n_{1}-1)}{2t(n_{2}+1)}. $$ <PARAGRAPH_SEPARATOR> It is easily shown that $\\mathscr{E}(\tilde{c}_{2}^{\\mathrm{adj}})\\approx c_{2}$ while $\\mathscr{E}(\tilde{c}_{2})=\\infty$ . Further investigation is required to improve the finite sample properties of other lower bound estimators. <PARAGRAPH_SEPARATOR> # Appendix. Two technical points <PARAGRAPH_SEPARATOR> The nonidentifiability of $\theta(\boldsymbol{Q})$ can be shown using the techniques in Mao (2006a). Let $\rho_{k}(\\pi)=(\rho_{0}(\\pi),\rho_{1}(\\pi),...,$ $\rho_{k}(\\pi))^{\\prime}$ with $k<t$ , where $\rho_{0}(\\pi)=\theta(\\delta(\\pi))$ and $\rho_{x}(\\pi)=f_{\\delta(\\pi)}(x)$ for $x\\geqslant1$ . For $k+1$ distinct $\\pi_{0},\\pi_{1},\\ldots,\\pi_{k}$ in $(0,1)$ , <PARAGRAPH_SEPARATOR> $$ |\\rho_{i}(\\pi_{j}))_{i,j=0}^{k}|=|(\\{\\pi_{i}/(1-\\pi_{i})\\}^{j})_{i,j=0}^{k}|\\cdot\\prod_{i=0}^{k}\\rho_{0}(\\pi_{i})\\cdot\\prod_{x=1}^{k}\\left({\\binom{t}{x}}\\neq0,\\right. $$ <PARAGRAPH_SEPARATOR> as a Vandermonde determinant is non-zero. This means that $\\{\\pmb{\\rho}_{k}(\\pi_{i})\\}_{i=0}^{k}$ are linearly independent and the set of functions $\\{\\rho_{0}(\\pi),\\rho_{1}(\\pi),\\ldots,\\rho_{k}(\\pi)\\}$ is a Tchebysheff system. The nonidentifiability of $\\theta(\\boldsymbol{Q})$ can be established from the fact that $\\{\\rho_{0}(\\pi),\\rho_{1}(\\pi),\\ldots,\\rho_{t-1}(\\pi)\\}$ is a Tchebysheff system (Mao, 2006a). <PARAGRAPH_SEPARATOR> The closed-form expression of $\\varphi_{2k}(f_{Q})$ is based on the facts that $\\textstyle\\theta(Q)=\\int\\mathrm{d}\\nu(\\lambda)$ and $\\begin{array}{r}{f_{Q}(x)/(\\binom{t}{x}=\\int\\lambda^{x}\\mathrm{d}\\nu(\\lambda)}\\end{array}$ , where $\\nu$ is a measure given by $\\mathrm{d}\\nu(\\lambda)=\\rho_{0}(\\lambda/(1+\\lambda))\\mathrm{d}Q(\\lambda/(1+\\lambda))$ , which allows the use of the truncated Stieltjes moment problem (Curto and Fialkow, 1991; Mao and Lindsay, 2006; Mao (2006b))."
  },
  {
    "qid": "statistic-posneg-218-0-0",
    "gold_answer": "TRUE. The formula explicitly shows that the predictive distribution $p(X_{n+1},\\ldots,X_{n+f}|Y)$ is obtained by integrating over the joint posterior distribution $p(X_{n},U,\\theta|Y)$, which includes all possible values of the parameters $\\theta$ and latent variables $U$. This integration inherently accounts for the uncertainty in these parameters, as opposed to using point estimates like maximum likelihood. The method of composition further ensures that the predictive draws reflect this uncertainty by first sampling from the posterior and then propagating these samples through the model to generate forecasts.",
    "question": "Is it true that the forecasting method described integrates over all possible parameter values of $\\theta$ and $U$ to account for parameter uncertainty, as implied by the integral in the formula for $p(X_{n+1},\\ldots,X_{n+f}|Y)$?",
    "question_context": "Bayesian Forecasting of Aggregated Traffic Volume\n\nThe MCMC sampler is readily extended to give predictive distributions of $Y_{n+1}$ $\\quad_{1},Y_{n+2},\\ldots$ , given $Y_{1},\\dots,Y_{n}$ . The idea is as follows. Let $f>0$ be some forecast horizon. Since $$\\begin{array}{r l}&{p(X_{n+1},\\ldots,X_{n+f}|Y)=\\displaystyle\\int p(X_{n+1},\\ldots,X_{n+f}|Y,X,U,\\theta)p(X,U,\\theta|Y)\\mathrm{d}(X,U,\\theta)}\\ &{\\phantom{p(X_{n+1},\\ldots,X_{n+f}|Y)}\\quad=\\displaystyle\\int p(X_{n+1},\\ldots,X_{n+f}|X_{n},U,\\theta)p(X_{n},U,\\theta|Y)\\mathrm{d}(X_{n},U,\\theta),}\\end{array}$$ the method of composition can be used to draw a sample from $p(X_{n+1},\\ldots,X_{n+f}|Y)$ , by first drawing a sample from $p(X_{n},U,\\theta|Y)$ , and then, conditioned on this, drawing a sample from $p(X_{n+1},\\ldots,X_{n+f}|X_{n},U,\\theta)$ . Our predictive sample for the observation process is then $\\{Y_{n+k}=$ $g(h^{\\mathrm{T}}X_{n+k}+s_{n+k})$ , $k=1,2,\\ldots,f\\}$ : Thus, at the end of step 3, we simply ‘simulate into the future’ using the current values of $X_{n}^{(k)},~U^{(k)}$ and ${\\theta}^{(k)}$ . The resulting simulation can be regarded as a draw from the predictive distribution of $Y_{n+1},\\dots,Y_{n+f}$ , given $Y_{1},\\dots,Y_{n}$ . It is important to remember that draws at successive iterations of $k$ are not necessarily independent. A convenient property of this approach is that forecasts are not based on some fixed (e.g. maximum likelihood) estimate of model parameters. Rather, they take into account the uncertainty in parameter estimation, since the integral in equation (6) is taken over all possible parameter values $\\theta$ and $U$ ."
  },
  {
    "qid": "statistic-posneg-1123-0-0",
    "gold_answer": "FALSE. The characteristic function of P₁ in (4.2) includes a non-zero cubic term β(it)³, where β = {g(-½Δ)}³(Δ/64)(1/n₁ + 1/n₂)². This implies the third moment about the mean is non-zero (6β + O₃), violating the symmetry requirement for normality. The normal approximation only holds when O₂ terms are ignored, as shown in (4.1). The presence of skewness (captured by β) confirms the non-normality when second-order terms are retained.",
    "question": "Is the statement 'The conditional error rate P₁ follows a normal distribution when second-order terms (O₂) are included in the asymptotic expansion' supported by the derived characteristic function in equation (4.2)?",
    "question_context": "Asymptotic Distributions in Discriminant Analysis: Conditional Error Rates and Risk\n\nThe paper examines the asymptotic distributions of conditional error rates (P₁, P₂) and risk (R) in discriminant analysis under multivariate normality assumptions. It derives characteristic functions and variance expansions for these statistics using higher-order asymptotic expansions, showing that P₁ is not normally distributed when second-order terms are considered, while R remains normal even when third-order terms are included."
  },
  {
    "qid": "statistic-posneg-630-0-1",
    "gold_answer": "TRUE. The Poisson Law of Small Numbers is derived as an approximation to the binomial distribution when q is small and n is large, keeping the product m = nq finite. The paper shows that the binomial N(p + q)^n can be approximated by the Poisson series M e^(-m)(1 + m + m²/2! + m³/3! + ...), which is valid under these conditions. This approximation is useful because it simplifies calculations and is easier to work with than the binomial distribution for large n and small q.",
    "question": "Does the Poisson Law of Small Numbers accurately approximate the binomial distribution when the probability of success q is small and the number of trials n is large?",
    "question_context": "Binomial Distribution and Poisson Law of Small Numbers\n\nThe paper discusses the binomial distribution and its limitations, particularly when the probability of success is small. It introduces the Poisson Law of Small Numbers as an approximation for the binomial distribution when the number of trials is large and the probability of success is small. The paper also explores the conditions under which the binomial distribution can be approximated by the Gaussian distribution and discusses the probable errors of the constants in the binomial frequency distribution."
  },
  {
    "qid": "statistic-posneg-1287-0-0",
    "gold_answer": "TRUE. The moments $\\mu_{1}^{\\prime}(T)$, $\\mu_{2}^{\\prime}(T)$, $\\mu_{8}^{\\prime}(T)$, and $\\mu_{4}^{\\prime}(T)$ are highly complex and depend on $n$ in a non-linear fashion, which suggests that the distribution of $\\pmb{T}$ (and by extension $\\pmb{S}$) is not normal. The complexity of these moments indicates skewness and kurtosis that deviate from those of a normal distribution, supporting the claim that the distribution of $\\pmb{S}$ is far from normal even for large $\\pmb{\\mathscr{n}}$.",
    "question": "Is the claim that the distribution of $\\pmb{S}$ is far from normal for large values of $\\pmb{\\mathscr{n}}$ supported by the moments $\\mu_{1}^{\\prime}(T)$, $\\mu_{2}^{\\prime}(T)$, $\\mu_{8}^{\\prime}(T)$, and $\\mu_{4}^{\\prime}(T)$ provided in the context?",
    "question_context": "Moments and Distribution of Transformed Variables in Infinite Plane Theory\n\nUnder infinite plane theory $-2n\\log G$ is a special case of Bartlett's statistic, $\\pmb{M}$ , which has been extensively studied by Nair (1938), Hartley (1940), Glaser (1976) and others. However, since large values of $\\pmb{M}$ correspond to small values of ${\\pmb G}$ , the tabulated upper tail percentage points of M (Pearson $\\&$ Hartley, 1958, p. 180) are not appropriate to the test for local regularity. We obtained upper tail percentage points of $\\pmb{G}$ using the well-known $x_{n-1}^{2}$ approximation to M with Bartlett's correction. The accuracy of these percentage points was confrmed using computer simulation and Pearson curve approximation. <PARAGRAPH_SEPARATOR> Pearson curve approximation has been used to obtain lower percentage points for $s$ Calculation of the moments of $s$ and of $\\pmb{G}$ , can be simplified by transforming the squared nearest neighbour distances to a set of Dirichlet variates (Hogg $\\&$ Craig, 1965, p. 130). <PARAGRAPH_SEPARATOR> Here $\\pmb{S}_{i}$ . or more conveniently $\\pmb{T}$ given by $\\ S=\\pmb{n}(\\pmb{n}T-1)/(\\pmb{n}-1)$ . can be simply expressed in terms of the transformed variables. The frst four moments of $\\pmb{T}$ about the origin are, in the usual factorial power notation, <PARAGRAPH_SEPARATOR> $$ \\mu_{1}^{\\prime}(T)=2/(n+1),\\quad\\mu_{2}^{\\prime}(T)=4(n+5)/(n+3)^{(3)}, $$ <PARAGRAPH_SEPARATOR> $$ \\mu_{8}^{\\prime}(T)=8(n^{2}+18n+74)/(n+\\tilde{\\delta})^{(5)},\\quad\\mu_{4}^{\\prime}(T)=16(n^{3}+30n^{2}+371n+2118)/(n+7)^{(7)}. $$ <PARAGRAPH_SEPARATOR> The Pearson curve $5\\%$ and $2\\%$ points have been checked by computer simulation and for sample sizes of 15 and above are adequate. <PARAGRAPH_SEPARATOR> Table 2 gives means, standard deviations, coefficients of skewness and kurtosis and approximate $5\\%$ and $2\\%$ points for $\\pmb{S}$ for four sample sizes. The intermediate Pearson curve percentage points can be obtained to within 0.0002 by fitting & cubic in log n to these four points. Table 2 shows that even for large values of $\\pmb{\\mathscr{n}}$ , the distribution of $\\pmb{S}$ is far from normal. For $\\pmb{n}$ larger than 100, $1/\\sqrt{s}$ is approximately normal with mean $(1+\\frac{3}{8}c^{2})/\\sqrt{\\mu}$ and variance $\\yen1\\mu$ ,where $\\pmb{\\mu}$ and $\\pmb{c}$ are the mean and coefficient of variation of $\\pmb{S}$ <PARAGRAPH_SEPARATOR> Table 2. Mean, $\\pmb{\\mu}$ , 8tandard deviation, $\\pmb{\\upsigma}$ , coeficient of skewne88, $\\sqrt{\\beta_{1}}$ , and coeficient of kurtosi8, $\\beta_{\\mathbf{2}},$ of $\\pmb{S}$ for infinite plane theory; lower $5\\%$ and $2\\%$ points are based on Pearson curve approximation"
  },
  {
    "qid": "statistic-posneg-1015-0-1",
    "gold_answer": "FALSE. The description is partially correct but incomplete. The generative process involves two distinct steps: (1) For each topic $k$, a perturbed version $\\beta_{k}^{i}$ is selected uniformly from $\\{\\tilde{\\beta}_{k}^{1},\\dots,\\tilde{\\beta}_{k}^{R_{k}}\\}$, forming the topic matrix $B_{i}$. (2) The count data $x_{i}$ is then drawn from a multinomial distribution with parameters $n_{i}$ (total counts) and $B_{i}\\gamma_{i}$ (linear combination of the selected perturbed topics weighted by $\\gamma_{i}$). The uniform selection of perturbed versions is a critical step not mentioned in the original claim.",
    "question": "The multinomial sampling model $x_{i}\\sim\\mathrm{Mult}(n_{i},B_{i}\\gamma_{i})$ assumes that the count data for sample $i$ is generated by first selecting perturbed topic versions uniformly at random and then combining them linearly with weights $\\gamma_{i}$. Is this a correct description of the generative process?",
    "question_context": "Multiscale Topic Modeling for Detecting Functional Equivalence in Microbiota\n\nOur final simulation studies whether alignment can detect mis-specifications in topic modeling due to highly correlated topics. Our setup is motivated by the strain switching phenomenon observed in some microbiota environments (Jeganathan and Holmes, 2021). In this situation, there are strains that can be exchanged between what are otherwise similar communities. These strains can be thought of as being functionally equivalent, competing for a niche within an ecosystem. The consequence is that two nearly identical communities may be present in the ecosystem, but with systematic differences for some strains. From a topic modeling perspective, these communities have anticorrelated topic memberships—only one of the competing strains can be present in a sample at a time. <PARAGRAPH_SEPARATOR> The existence of these communities can be detected by comparing topics estimated at different scales. At a coarse scale, two communities may be indistinguishable from one another, swamped by larger variations in species signatures across the ecosystem. At a finer scale, the subsets of strains that distinguish them may become apparent after close inspection of the estimated topics. <PARAGRAPH_SEPARATOR> Our goal is to study the extent to which alignment can support this multiscale analysis. Our simulation mechanism first draws $\\gamma_{i}$ and $\\beta_{k}$ as in the simulations above. Instead of directly using $\\beta_{k}$ , however, perturbed versions $\\tilde{\\beta}_{k}^{r}$ are generated for $r\\le R_{k}$ , a prespecified number of perturbed replicates $R_{k}$ . The perturbation mechanism is given in Algorithm S1 of the Supplementary material available at Biostatistics online. The resulting $\\tilde{\\beta}_{k}^{r}$ and $\\tilde{\\beta}_{k}^{r^{\\prime}}$ differ only on a subset of $S$ coordinates and can be viewed as functionally equivalent subcommunities. Given perturbed topics, sample $i$ is drawn by first randomly selecting one perturbed version from each of the $K$ topics, <PARAGRAPH_SEPARATOR> $$ \\beta_{k}^{i}\\sim\\operatorname{Unif}\\left(\\left\\{\\tilde{\\beta}_{k}^{1},\\dots,\\tilde{\\beta}_{k}^{R_{k}}\\right\\}\\right) $$ <PARAGRAPH_SEPARATOR> binding the results into a $K$ column matrix $B_{i}$ , and then drawing <PARAGRAPH_SEPARATOR> $$ x_{i}\\sim\\mathrm{Mult}(n_{i},B_{i}\\gamma_{i}) $$ <PARAGRAPH_SEPARATOR> as in standard LDA. <PARAGRAPH_SEPARATOR> We set $K=5$ and $(R_{1},...,R_{5})=(2,2,1,1,1)$ . We draw $N=250$ samples with dimension $D=1000$ . We use $S=230$ ; results with varying $S$ are given in Supplementary material available at Biostatistics online. In the microbiota interpretation, our samples include counts of 1000 species each, and five underlying community types are present. Two versions of the first two types are present, differing on a subset of $S$ competing species. <PARAGRAPH_SEPARATOR> The resulting alignment is given in Figures 5(a) and (b). The learned topics for $K=5{-}7$ are given in the right panel. We note that, at $K=6$ , the purple and blue topics have similar weights across many, but not all species. Likewise, at $K=7$ , the brown-orange and brown-green topic signatures are similar. The accompanying flow diagram shows that, in both cases, the pairs of similar topics had been merged when $K=5$ , suggesting that the model begins to detect perturbed versions of the same topics once $K$ is increased."
  },
  {
    "qid": "statistic-posneg-326-0-0",
    "gold_answer": "FALSE. The case $\\alpha=1$ corresponds to the minimum trace method, not the maximum determinant method. The maximum determinant method is obtained by letting $\\alpha\\rightarrow0$.",
    "question": "Is it true that the case $\\alpha=1$ in the generalized weighted minimum trace method corresponds to the maximum determinant method?",
    "question_context": "Generalized Weighted Minimum Trace Method in Factor Analysis\n\nThere are similarities between the problem in (10), (11) and the minimum trace method of fitting factor analysis models introduced to psychometrics by [2]. Given an observed covariance matrix C, the minimum trace method finds a diagonal positive semidefinite matrix $\\Sigma$ such that $\\pmb{\\Sigma}\\leqslant\\pmb{\\mathrm{\\mathbf{C}}}$ and trace $[\\mathbf{C}-\\mathbf{\\pmb{\\Sigma}}]$ is minimised. The method thus seeks to maximise trace $\\Sigma$ while keeping $\\pmb{\\Sigma}\\leqslant\\pmb{\\mathrm{\\bfC}}.$ . It is parallel to the problem in (11) and (12), but maximises trace rather than determinant. The maximisation of trace $\\Sigma$ was considered using optimisation theory by [7,10], and in a manner closer to that used in this paper by [12]. Much of the interest has been in the algorithms to find the solutions, see [6]. <PARAGRAPH_SEPARATOR> The matrices $\\hat{\\Sigma},\\hat{\\mathbf{D}}$ giving the solution for the minimum trace problem are not in general the same as for the problem considered in this paper. One could propose a ‘maximum determinant’ method for fitting factor analysis models, and use the methods developed in this paper carry out the fitting, but there are perhaps too many ways to fit factor analysis models already. As it happens, the matrices $\\hat{\\Sigma}$ , $\\hat{\\mathbf{D}}$ leading to the solutions for the maximum determinant for equally correlated normal variables given in Section 3 also provide a solution for the minimum trace problem. <PARAGRAPH_SEPARATOR> It is possible to construct a family of criteria for fitting factor analysis models that includes the maximum determinant method and the minimum trace method as special cases. All that is necessary is to maximise over $\\Sigma=\\operatorname{diag}(\\sigma_{11},\\sigma_{22},\\dots,\\sigma_{q q})$ <PARAGRAPH_SEPARATOR> $$ \\frac{1}{\\alpha}\\ln\\sum\\sigma_{i i}^{\\alpha}/q $$ <PARAGRAPH_SEPARATOR> for some $x,0<\\alpha\\leqslant1$ , where as before $\\pmb{\\Sigma}\\leqslant\\pmb{\\mathrm{\\mathbf{C}}}$ . The case $\\alpha=1$ gives the minimum trace method, while letting $\\alpha\\rightarrow0$ gives the maximum determinant method. It is even possible <PARAGRAPH_SEPARATOR> to generalise the weighted minimum trace method introduced by Shapiro [10] by using the family of criteria <PARAGRAPH_SEPARATOR> $$ {\\frac{1}{\\alpha}}\\ln\\sum p_{i}\\sigma_{i i}^{\\alpha}, $$ <PARAGRAPH_SEPARATOR> where $p_{i}$ are non-zero probabilities that sum to 1. Results for this extension corresponding to Theorem 1 are given in Theorems 2 and 3, which are offered without proof, but which can be proved in a manner not too dissimilar to Theorem 1. <PARAGRAPH_SEPARATOR> Theorem 2 (Primal generalised weighted minimum trace). For $0<\\alpha<1$ , $\\textstyle\\beta={\\frac{\\alpha}{\\alpha-1}}$ \u0005\u00051 and $\\mathbf{W}=\\mathrm{diag}(\\sqrt{p_{1}},\\dots,\\sqrt{p_{q}})$ , where $p_{i}$ are non-zero probabilities adding to 1, (18), (19), (20) are a set of sufficient conditions for a diagonal positive semidefinite matrix $\\hat{\\boldsymbol{\\Sigma}}~=$ $\\mathrm{diag}(\\hat{\\sigma}_{11},\\hat{\\sigma}_{22},\\dots,\\hat{\\sigma}_{q q})$ to give a solution for <PARAGRAPH_SEPARATOR> $$ {\\frac{1}{\\alpha}}\\operatorname*{sup}\\ln[\\operatorname{trace}\\mathbf{W}\\Sigma^{\\alpha}\\mathbf{W}] $$ <PARAGRAPH_SEPARATOR> over all positive semidefinite diagonal $\\pmb{\\Sigma}\\leqslant\\mathbf{C}$ . <PARAGRAPH_SEPARATOR> $$ \\hat{\\pmb{\\Sigma}}\\leqslant\\mathbf{C} $$ <PARAGRAPH_SEPARATOR> $$ \\hat{{\\cal\\Sigma}}={\\frac{(\\mathrm{diag}\\hat{{\\bf D}})^{\\beta-1}}{\\mathrm{trace}{\\bf W}(\\mathrm{diag}({\\bf D}))^{\\beta}{\\bf W}}}, $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\mathrm{trace}[\\mathbf{W}(\\hat{\\pmb{\\Sigma}}-\\mathbf{C})\\mathbf{W}\\hat{\\mathbf{D}}]=0,}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\hat{\\bf D}$ is a positive semidefinite matrix. <PARAGRAPH_SEPARATOR> Theorem 3 (Dual generalised weighted minimum trace). For $0<\\alpha<1$ , $\\beta=\\alpha/(\\alpha-1)$ and $\\mathbf{W}=\\mathrm{diag}(\\sqrt{p_{1}},\\dots,\\sqrt{p_{q}})$ , where $p_{i}$ are non-zero probabilities adding to 1, (18), (19), (20) are a set of sufficient conditions for a positive semidefinite matrix with at least one non-zero diagonal element to give a solution for <PARAGRAPH_SEPARATOR> $$ {\\displaystyle\\operatorname*{inf}[\\operatorname{trace}[{\\bf W}{\\bf C}{\\bf W}{\\bf D}]-1-\\frac{1}{\\beta}\\ln[\\operatorname{trace}[{\\bf W}(\\mathrm{diag}({\\bf D}))^{\\beta}{\\bf W}]]]} $$ <PARAGRAPH_SEPARATOR> over all positive semidefinite D. Condition (19) can also be written: <PARAGRAPH_SEPARATOR> $$ \\hat{\\bf D}={\\frac{(\\mathrm{diag}(\\hat{\\Sigma}))^{\\alpha-1}}{\\mathrm{trace}{\\bf W}\\Sigma^{\\alpha}{\\bf W}}}. $$"
  },
  {
    "qid": "statistic-posneg-1585-5-1",
    "gold_answer": "TRUE. Explanation: The variance $\\kappa$ is calculated as $\\Sigma\\pi a^{2}$, where $a$ represents the scores. If all scores are zero for certain recessivity types (e.g., $[22^{*}]$ and [4]), then $\\kappa = 0$. This means these types contribute no variability to the linkage test statistic, making them uninformative for detecting linkage. The linkage test relies on non-zero variances to assess significance, so these types would not contribute to the total score or variance in the analysis.",
    "question": "For recessivity types $[22^{*}]$ and [4], the variance per sib-pair $\\kappa$ is zero because all scores are zero. Does this imply that these types are uninformative for linkage detection?",
    "question_context": "Linkage Detection in Human Genetics: Sib-pair Analysis\n\nThis table is arranged so as to give also the actual probability $p(G^{i}G^{j},T^{r}T^{s})$ of any sib-pair classified as $G^{i}G^{j}$ in the main character, and $T^{r}T^{s}$ in the test character.Write <PARAGRAPH_SEPARATOR> $$ p(G^{i}G^{j},T^{r}T^{s})=\\pi(G^{i}G^{j})\\pi(T^{r}T^{s})\\:\\Psi, $$ <PARAGRAPH_SEPARATOR> where $\\pi(G^{i}G^{j})$ is the probability of pair $G^{i}G^{j}$ when $\\zeta=0$ $\\pi(T^{r}T^{s})$ the same for $T^{r}T^{s}$ and $\\downarrow$ a correcting factor for the effect of linkage. The value of $\\pi(G^{i}G^{j})$ for all kinds of pairs is given in Table 5._The value of $\\pi(T^{r}T^{s})$ follows from symmetry. The value of $\\updownarrow$ can be found from the entry in Table 5 by the use of Table 6. <PARAGRAPH_SEPARATOR> The variances per sib-pair can be found from the formula $\\kappa=\\Sigma\\pi a^{2}$ and are given in Table 7. For recessivity types $[22^{*}]$ and [4] in either character $\\pmb{\\kappa}=\\mathbf{0}$ , since all scores are zero. <PARAGRAPH_SEPARATOR> These tables enable a test for linkage to be made in any set of families in which the parental genotypes are known.For each family a score $\\lambda$ is found, by adding the scores for each sib-pair as given in Table 5, and a variance by multiplying the value of $\\pmb{\\kappa}$ in Table 7 by $\\frac{1}{2}n(n-1)$ ，the number of sib-pairs (where $_n$ is the number of sibs). The scores and variances for the different families are summed to give a total score and total variance. If the total score is positive and exceeds (say) 3 times the root of the variance it may be considered significant. Note that any one phenotype may be given different symbols in different families: e.g., blood-group $\\pmb{A}$ is of type $\\mathbf{G_{2}}$ when themating is $\\mathbf{AA}\\times\\mathbf{AO}$ ,but $\\mathbf{G_{3}}$ when themating is $\\mathbf{AO}\\times\\mathbf{AO}$"
  },
  {
    "qid": "statistic-posneg-756-3-1",
    "gold_answer": "FALSE. The condition $R S S_{2M} \\leqslant R S S_{P}$ does not imply that treating two observations as missing values always results in a lower residual sum of squares than interchanging them. Rather, it means that the residual sum of squares when treating the two observations as missing values ($R S S_{2M}$) is less than or equal to the residual sum of squares obtained by interchanging them ($R S S_{P}$). This is because $R S S_{2M}$ is derived by minimizing the residual sum of squares, which ensures it is at least as low as any other method, including interchange.",
    "question": "Does the condition $R S S_{2M} \\leqslant R S S_{P}$ imply that treating two observations as missing values always results in a lower residual sum of squares than interchanging them?",
    "question_context": "Factorial Experiments and Outlier Detection via Residual Sum of Squares\n\nWhen there are two interchanged observations, the application of the outliers search to these values will result in a reduced residual mean square, unless the interchanged values are only marginally different from each other. Hence, in the search for interchanges, instead of examining all possible pairs of response values, only those which have lowered the residual mean square in the outliers search are investigated. This selection reduces the amount of calculation considerably, and there is a negligible risk of failing to discover a true interchange of any importance.<PARAGRAPH_SEPARATOR>The interchanges search procedure therefore consists of the following sequence of steps:<PARAGRAPH_SEPARATOR>(i)  The selected observations are interchanged in pairs and the factorial ANovA is completed. The least of the residual sums of squares, which will be denoted by $R S S_{P}$ , is computed: it has $\\pmb{r}$ degrees of freedom.   (ii) The two observations which give rise to $R S S_{P}$ are next regarded as a pair of missing values to be estimated simultaneously by minimizing the residual sum of squares. Let $R S S_{2M}$ denote this minimum residual: it has $(r-2)$ associated degrees of freedom. By the method of derivation it is clear that $R S S_{2M}{\\leqslant}R S S_{P}$   (i)The value of $R S S_{2M}$ is now tested to see if it gives a significant reduction from the residual sum of squares $R S S_{C}$ for the complete set of original data. This is accomplished in the manner of Section 2.1 by computing the ratio<PARAGRAPH_SEPARATOR>$$\\frac{(R S S_{C}-R S S_{2M})/2}{R S S_{2M}/(r-2)}$$<PARAGRAPH_SEPARATOR>and comparing it with the $0{\\cdot}8P/(r-2)$ per centpoint of the $F\\cdot$ distribution with 2 and $(r-2)$ degrees of freedom. (However this procedure has not been justified by simulation.) If the ratio (3) is not significant at the chosen (overall) level $P_{:}$ , then neither is $R S S_{P}$ significantly less than $R S S_{C}$ and the likelihood of interchanges in this set of data is discredited. However when (3) is significant, we go on to test whether it is more plausible that the two suspect observations are interchanged than that they are two unrelated outliers.<PARAGRAPH_SEPARATOR>(iv) Hence finally the ratio<PARAGRAPH_SEPARATOR>$$\\frac{(R S S_{P}-R S S_{2M})/2}{R S S_{2M}/(r-2)}$$<PARAGRAPH_SEPARATOR>is computed. Under the null hypothesis of no interchanges or outliers, the ratio (4) is distributed approximately as $F(2,r-2)$ .Since we are now trying to decide whether an interchange or a pair of unrelated outliers is more likely and are not undertaking a significance test, we have arbitrarily decided to conclude that the latter case applies when the ratio (4) exceeds the 75 per cent point of the $F(2,r-2)$ distribution, and the former otherwise. The choice of this percentage point is a kind of prior weighting that an interchange is more likely than that this pair of observations are unrelated Outliers.<PARAGRAPH_SEPARATOR>If the search for interchanged observations is to take place, it is evident that the size of the experiment and the fitted model must be such that $r\\geqslant3$<PARAGRAPH_SEPARATOR># 4. COMPUTATIONAL PROCEDURE"
  },
  {
    "qid": "statistic-posneg-1370-0-4",
    "gold_answer": "FALSE. Explanation: As γ → ∞, the allocation probabilities π_j converge to π_j ∝ exp(G_j), not necessarily to equal allocation. Only if all gains G_j are equal (G_j = G for all j) does π_j = 1/t. The paper explicitly states that when G_j are equal, the rule reduces to equal randomization, but this is not the case if the gains differ. Thus, the limit depends on the specific values of G_j.",
    "question": "For γ → ∞, the allocation probabilities π_j converge to equal allocation (π_j = 1/t) regardless of the gains G_j.",
    "question_context": "Optimal Response and Covariate-Adaptive Biased Coin Designs in Clinical Trials\n\nThe paper discusses the design of clinical trials where patients are sequentially allocated to one of t treatments based on prognostic factors, previous allocations, and responses. The model involves a regression analysis with treatment effects, covariates, and period effects. The design aims to balance efficient parameter estimation with randomization, using a utility function that combines variance reduction and randomness. The allocation probabilities are derived from a combination of information measures and treatment gains, with a focus on D_A-optimality and skewed allocations towards more effective treatments."
  },
  {
    "qid": "statistic-posneg-1603-0-0",
    "gold_answer": "TRUE. The QUT methodology is specifically designed to control the probability of correctly recognizing the null model, which is reflected in the definitions of FWER and FDR under $\\beta^{0}=0$. The threshold $\\tau_{\\alpha}^{\\mathrm{QUT}}$ is set such that $\\mathbb{P}(\\hat{\\beta}_{\\tau}^{\\mathrm{lass0}}(y)=0\\mid\\tau=\\tau_{\\alpha}^{\\mathrm{QUT}}(y))=1-\\alpha$ under $\\beta^{0}=0$, ensuring control over both FWER and FDR in this scenario.",
    "question": "Is it true that the QUT for Lasso-Zero, defined as $\\tau_{\\alpha}^{\\mathrm{QUT}}=F_{T}^{-1}(1-\\alpha)$, ensures control of both FWER and FDR under the null model ($\\beta^{0}=0$)?",
    "question_context": "Quantile Universal Threshold (QUT) for Lasso-Zero Model Selection\n\nWe close this section with a short comparison of LassoZero and resampling methods such as stability selection (Meinshausen and Bühlmann 2010), with which Lasso-Zero presents some similarity. Indeed, in the same vein as stability selection, our aggregation strategy aims at discarding variables that are rarely selected. Nevertheless, the aggregation procedures differ in that stability selection focuses on the selection probability, independently of the sign, whereas the median will favor coefficients with unchanging sign. Moreover, the two methods perform resampling on different things: stability selection resamples observations, discarding a part of them in each replication, whereas Lasso-Zero resamples noise dictionaries, always keeping all observations. Lasso-Zero also has a resemblance to the knockoff methodology (Barber and Candès 2015; Candès et al. 2018), in that both methods augment the regression matrix $X$ with columns that are unrelated to y. However, the goal of this augmentation differs completely: the knockoff variables are designed to mimic the correlation structure in the original features to control false discoveries, whereas the noise dictionaries of Lasso-Zero are designed to fit the noise term and depend neither on the marginals, nor on the correlation structure of the features. Lasso-Zero is compared to stability selection and the knockoffs in numerical experiments in Section 4. <PARAGRAPH_SEPARATOR> # 3. QUT for Lasso-Zero <PARAGRAPH_SEPARATOR> Common methods for selecting tuning parameters include cross-validation, which is driven by prediction goals, and information criteria like SURE (Stein 1981), BIC (Schwarz 1978), AIC (Akaike 1998), which look for a trade-off between the fit and the complexity of the model. According to the practitioner’s needs, any of these methods could be used to select the threshold of Lasso-Zero. In this work we rather opt for QUT, a general methodology proposed by Giacobino et al. (2017) and which is only directed toward model selection. It consists in controlling the probability of correctly recognizing the null model. In other words, when $\\beta^{0}=0$ , QUT controls the FWER, defined by <PARAGRAPH_SEPARATOR> $$ \\mathrm{FWER}=\\mathbb{P}(\\left|\\hat{S}\\cap\\overline{{S^{0}}}\\right|>0), $$ <PARAGRAPH_SEPARATOR> as well as the FDR <PARAGRAPH_SEPARATOR> $$ \\mathrm{FDR}:=\\mathbb{E}(\\mathrm{FDP}), $$ <PARAGRAPH_SEPARATOR> where FDP is defined in (13), since under the null model $\\mathrm{{FWER}=\\mathrm{{FDR}}}$ . <PARAGRAPH_SEPARATOR> If $\\hat{\\beta}_{\\lambda}$ is a regularized estimator tuned by a parameter $\\lambda$ , Giacobino et al. (2017) defined its $\\alpha$ -QUT αQUT by the value of $\\lambda$ for which $\\mathbb{P}(\\hat{\\beta}_{\\lambda}(\\epsilon)=0)=1-\\alpha$ . By Definition 1, $\\hat{\\beta}_{\\tau}^{\\mathrm{lass0}}=0$ if and only if $\\tau\\geq\\left\\|\\hat{\\beta}^{\\mathrm{med}}\\right\\|_{\\infty}$ med  , hence the α-QUT for Lasso-Zero is given by <PARAGRAPH_SEPARATOR> $$ \\tau_{\\alpha}^{\\mathrm{QUT}}=F_{T}^{-1}(1-\\alpha), $$ <PARAGRAPH_SEPARATOR> where $F_{T}$ denotes the cdf of $T:=\\left\\lVert\\hat{\\beta}^{\\mathrm{med}}(\\epsilon)\\right\\rVert_{\\infty}$ <PARAGRAPH_SEPARATOR> If the value of $\\sigma$ is known, the threshold (18) can be estimated by Monte Carlo simulations. In practice, the noise level is not known however, and it is generally estimated beforehand, for example using a refitted cross-validation procedure (Fan, Guo, and Hao 2012) or based on the Lasso tuned by cross-validation like in Reid, Tibshirani, and Friedman (2016). To bypass the need to estimate $\\sigma$ , we propose to exploit the noise coefficients $\\gamma^{(k)}$ , which have not been used so far, to pivotize the statistic $\\left\\|\\hat{\\beta}^{\\mathrm{med}}(\\epsilon)\\right\\|_{\\infty}$ . We consider <PARAGRAPH_SEPARATOR> $$ P:=\\frac{\\left\\|\\hat{\\beta}^{\\mathrm{med}}(\\epsilon)\\right\\|_{\\infty}}{s(\\epsilon)}, $$ <PARAGRAPH_SEPARATOR> with $s(y)$ := $\\mathrm{MAD}^{\\neq0}(\\hat{\\gamma}^{(1)}(y),\\dots,\\hat{\\gamma}^{(M)}(y))$ , where $\\mathrm{MAD}^{\\ne0}(\\nu):=\\mathrm{MAD}\\{\\nu_{i}|\\nu_{i}\\ne0\\}$ is the Median Absolute Deviation of all nonzero components. The numerator and denominator in (19) being both equivariant with respect to $\\sigma$ , the distribution $F_{P}$ does not depend on $\\sigma$ . Then choosing the data-dependent QUT <PARAGRAPH_SEPARATOR> $$ \\tau_{\\alpha}^{\\mathrm{QUT}}(y):=s(y)F_{P}^{-1}(1-\\alpha) $$ <PARAGRAPH_SEPARATOR> ensures $\\mathbb{P}(\\hat{\\beta}_{\\tau}^{\\mathrm{lass0}}(y)=0\\mid\\tau=\\tau_{\\alpha}^{\\mathrm{QUT}}(y))=1-\\alpha$ under $\\beta^{0}=0$ <PARAGRAPH_SEPARATOR> When $\\beta^{0}\\neq0$ , we observed that the data-dependent QUT defined in (20) tends to be larger than (18), leading to more conservative results. This is related to the fact that when the coefficient vector has a lot of nonzero entries, the distribution of the BP entries βBI corresponding to truly zero coefficients is stochastically larger (Tardivel and Bogdan 2018). This motivated the choice of the median absolute deviation, which is more robust to large $\\gamma$ entries compared to other variability measures like the variance for example. <PARAGRAPH_SEPARATOR> Let us emphasize here that QUT is designed to control the FWER and FDR under the null model only, and does not generally control them in models where $\\beta^{0}\\neq0^{\\dot{2}}$ . However, numerical experiments of Giacobino et al. (2017) showed that the Lasso tuned by QUT provides a generally good trade-off between low FDR and high TPR. Our simulations of Section 4 show that this is also the case for Lasso-Zero."
  },
  {
    "qid": "statistic-posneg-335-3-1",
    "gold_answer": "FALSE. The correct convergence rate under the given assumptions is $O(n^{-s_{*}p/(2s_{*}+1)})$, as shown in the proof of Theorem 3.3. The initial rate mentioned is derived under different conditions and is not applicable here.",
    "question": "The paper claims that the expected Lp error of the estimator $\\hat{g}_{L}(x)$ converges at the rate $O(n^{-s_{*}p/(2s_{*}+4)})$. Is this convergence rate correct under the given assumptions?",
    "question_context": "Nonparametric Estimation of a Quantile Density Function Using Wavelets\n\nThe paper discusses nonparametric estimation of a quantile density function using wavelet methods. It presents several lemmas and theorems that establish bounds on the estimation error and convergence rates. Key formulas include bounds on the probability of large deviations of wavelet coefficient estimates and the expected Lp error of the wavelet-based estimator."
  },
  {
    "qid": "statistic-posneg-1646-0-1",
    "gold_answer": "TRUE. Explanation: The definition $P(L_p) = p$ directly states that $L_p$ is the level of $x$ (dose or stimulus) at which the probability of observing a positive response ($Y=1$) is exactly $p$. This is a standard definition in quantal response analysis, where $L_p$ is often referred to as the $p$-level effective dose (ED$_{100p}$).",
    "question": "The level $L_p$ is defined by $\\operatorname*{Pr}\\{Y(L_{p})=1\\}=P(L_{p})=p$. Does this imply that $L_p$ is the dose level at which the probability of a positive response is exactly $p$?",
    "question_context": "Quantal Response Curve Estimation Using Logit Model\n\nThis paper studies methods given by Wetherill (1963) for estimating general percentage points of & quantal response curve. A new method of estimation is proposed which is both simple to compute and highly eficient. The use of this with the Up and Down Transformed Response Rule (UDTR) is investigated, by exact calculation of probability distributions and by empirical sampling trials. The use of the UDTR to estimate the slope of a response curveisoutlined. <PARAGRAPH_SEPARATOR> # 1. INTRODUCTION <PARAGRAPH_SEPARATOR> Wetherill (1963) congidered the following problem. Random variables $\\pmb{Y}(\\pmb{x})$ can be observed which take on the values l or O with probabilities $P(x)$ and $\\scriptstyle{\\mathbf{1}}-P({\\pmb{x}})$ respectively. The quantity $\\pmb{\\mathscr{x}}$ is assumed to be under the control of the experimenter and is referred to as the level at which the observation $\\pmb{Y}(\\pmb{x})$ is taken. The experimenter may require point and interval estimates for the level of $\\pmb{x}$ $L_{p}$ defined by <PARAGRAPH_SEPARATOR> $$ \\operatorname*{Pr}\\{Y(L_{p})=1\\}=P(L_{p})=p. $$ <PARAGRAPH_SEPARATOR> In this paper we consider some simple but highly effcient methods for point estimation of $\\pmb{L_{p}}$ . For examples of some practical situations where such problems arise, see Wetherill (1963) and the discussion on that paper, and Wetherill $\\pmb{\\&}$ Levitt (1966). <PARAGRAPH_SEPARATOR> It will be convenient to assume the logit model for $\\scriptstyle P(x)$ <PARAGRAPH_SEPARATOR> $$ P(x)=\\{1+\\exp{[-\\alpha+\\beta x)]}\\}^{-1}. $$ <PARAGRAPH_SEPARATOR> It is conjectured that the method of estimation proposed will be reasonably robust to the model for $\\pmb{P}(\\pmb{x})$ $\\S10$ <PARAGRAPH_SEPARATOR> Following the approach adopted in Wetherill (1963), liberal use was made of empirical sampling, using the IBM 7094 at Bell Laboratories, and these trials were made for the standard form of (1), with ${\\pmb{\\alpha}}={\\pmb{0}}$ &nd $\\pmb\\beta=1$ <PARAGRAPH_SEPARATOR> # 2. DEFINITIONS"
  },
  {
    "qid": "statistic-posneg-2137-1-0",
    "gold_answer": "FALSE. The correct rate of convergence should be $d_{n} = n^{1/(d+2)}$ as derived from the condition $\\frac{n}{d_{n}^{d+1}} = \\sqrt{\\frac{n_{1}...n_{d}}{d_{n}^{d}}} \\Rightarrow d_{n}^{d+2} = n$. This ensures the sum in $\\mathbb{B}_{n}(s_{1},\\ldots,s_{d})$ converges in distribution, and the residual terms $O(d_{n}^{d+1}/n) = O(n^{-1/(d+2)})$ are appropriately controlled. The paper's derivation is correct, but the question is framed to test understanding of the rate derivation process.",
    "question": "Is the rate of convergence $d_{n} = n^{1/(d+2)}$ correctly derived for the isotonic regression estimator to achieve the asymptotic distribution stated in Theorem 3?",
    "question_context": "Asymptotic Distribution of Isotonic Regression Estimator in Multidimensional Settings\n\nWith the geometric characterization explained in the earlier subsection, we now focus on the asymptotics of IRE as described in Theorem 1. We write the IRE as $\\hat{f}_{n}:=\\partial_{\\ell}T_{[0,1]^{d}}(S_{n})$ . To be more precise, we are interested in obtaining a limit distribution of $d_{n}(\\hat{f}_{n}(u_{1},\\dots,u_{d})-f(u_{1},\\dots,u_{d}))$ for some fixed $(u_{1},\\dots,u_{d})\\in(0,1)^{d}$ and appropriately chosen $d_{n}$ such that $d_{n}\\to\\infty$ as $n\\to\\infty$ , with $n=n_{1}\\ldots n_{d}$ . In the sequel, we establish such a distributional convergence result and derive the appropriate rate of convergence $d_{n}$ in the process."
  },
  {
    "qid": "statistic-posneg-1766-0-2",
    "gold_answer": "FALSE. The risk decomposition formula assumes a multiplicative relationship between partial risks, but it does not necessarily imply independence. In reality, there could be interactions between different risk factors (e.g., climatological and topographical factors might jointly influence risk). The assumption of independence simplifies interpretation and visualization but may overlook complex interactions. This could lead to underestimation or overestimation of risk in cases where interactions are significant. The model's interpretability is enhanced, but its accuracy depends on the validity of the independence assumption.",
    "question": "The risk decomposition formula $$r_{i}=r_{0}r_{i}^{\\mathrm{clim}}r_{i}^{\\mathrm{topo}}r_{i}^{\\mathrm{b-}s}r_{i}^{\\mathrm{reg}}$$ implies that the partial risks (climatological, topographical, building-specific, and regional) are multiplicative and independent of each other. Is this assumption of independence correct, and what are the implications for risk assessment?",
    "question_context": "Generalized Additive Models for Insurance Claim Risk Assessment\n\nThe paper discusses the use of generalized additive models (GAMs) to model the number of insurance claims, considering both Poisson and negative binomial distributions. The model includes categorical variables, smooth functions for continuous covariates, random effects for municipalities, and offsets for contract length and property value. The risk is decomposed into partial risks corresponding to different groups of covariates, facilitating interpretation and visualization. Model evaluation is performed using mean square error (MSE) and Brier score (BS), with out-of-sample predictive performance assessed via 10-fold cross-validation."
  },
  {
    "qid": "statistic-posneg-1622-0-0",
    "gold_answer": "TRUE. The formula for $C_{j}$ is derived from the probability generating function $G_{j}(\\nu)$, where $C_{j}=G_{j}^{\\prime}(1)$. The expression $(1-p_{j})(1+2p_{j}+3p_{j}^{8})$ accounts for the weighted contributions of different observation counts (1, 2, and 8 observations) in the run, while the denominator $(1-p_{j}^{8})$ normalizes the probabilities. This correctly represents the mean number of observations for the transition $j$ to $(j+1)$.",
    "question": "Is it true that the mean number of observations given transition $j$ to $(j+1)$ is calculated as $C_{j}=\\frac{(1-p_{j})(1+2p_{j}+3p_{j}^{8})}{(1-p_{j}^{8})}$?",
    "question_context": "Expected Observations in Sequential Quantal Response Estimation\n\nIt is desirable to have some idea of the number of observations required in an experiment. Given below is a procedure which can be used for calculating the mean number of observations required when we start at a valley and continue for $\\pmb{\\mathscr{n}}$ subsequent valleys, using UDTR rule number 2 of Table ${\\mathfrak{a}}a$ . This procedure follows from the finite Markov chain approximation outlined in $\\S5$ . The theory was checked by reference to empirical sampling trials results, such as those given in the penultimate column of Table 4.<PARAGRAPH_SEPARATOR>Let<PARAGRAPH_SEPARATOR>$$G_{j}(\\nu)=\\frac{(1-p_{j})\\nu+p_{j}(1-p_{j})\\nu^{2}+p_{j}^{8}(1-p_{j})\\nu^{8}}{(1-p_{j}^{8})}.$$<PARAGRAPH_SEPARATOR>This is the probability generating function of the number of observations in a run, given transition $j\\ t o j+1$ ;and note that $\\nu^{8}$ is the probability generating function of the number of observations in a run, given transition $j$ to ${\\dot{\\jmath}}-1$<PARAGRAPH_SEPARATOR>$C_{j}=G_{j}^{\\prime}(1)$ is the mean number of observations given transition $j$ to $(j+1)$ ,then<PARAGRAPH_SEPARATOR>$$C_{j}=\\frac{(1-p_{j})(1+2p_{j}+3p_{j}^{8})}{(1-p_{j}^{8})},$$<PARAGRAPH_SEPARATOR>whereas the mean number of observations given transition $j$ to ${\\dot{\\jmath}}-1$ is3·0.<PARAGRAPH_SEPARATOR>Let $\\iota_{j k}$ be the mean number of observations given that $j$ is a valley and $\\pmb{k}$ is the next peak. Then $\\ell_{j k}$ includes the three observations in the final downward step $\\pmb{k}$ to $\\pmb{k}-\\mathbf{1}$ but excludes the number of observations in the frst upward step $j$ to ${\\mathfrak{j}}+1$ . Therefore, we have<PARAGRAPH_SEPARATOR>$$\\begin{array}{l}{{l_{j,j+1}=3\\cdot0,}}\\ {{l_{j,j+2}=C_{j+1}+3\\cdot0,}}\\ {{l_{j,j+3}=C_{j+1}+C_{j+3}+3\\cdot0=l_{j,j+2}+C_{j+2};}}\\end{array}$$<PARAGRAPH_SEPARATOR>in general<PARAGRAPH_SEPARATOR>$$l_{j,j+r}=l_{j,j+r-1}+C_{j+r-1}\\quad(r\\geqslant2).$$<PARAGRAPH_SEPARATOR>Let $q_{j k}$ denote the probability that the next peak is at $\\pmb{k}$ given that the current state is 8 valley at $j$ . Then write $\\lambda_{j k}=q_{j k}l_{j k}$ thus defining an $m\\times m$ matrix, $\\pmb{\\lambda}$<PARAGRAPH_SEPARATOR>Similarly,if $u_{j k}$ isthe mean number of observations given that $\\pmb{j}$ is 8 peak and $\\pmb{k}$ is the next valley, then we can define another $m\\times m$ matrix, $\\upmu$ 88<PARAGRAPH_SEPARATOR>$$\\mu_{j k}=\\gamma_{j k}u_{j k},$$<PARAGRAPH_SEPARATOR>where<PARAGRAPH_SEPARATOR>$$u_{j k}=\\alpha_{k}+(j-k-1)^{3}~(k<j),$$<PARAGRAPH_SEPARATOR>and where $r_{j k}$ denotes the probability that the next valley is at $\\pmb{k}$ given that the current state is & peak at $\\pmb{j}$ . Now if we start at a valley and stop at the next valley, i.e. after two changes, then $\\mathbf{v}=\\mathbf{A}\\mathbf{B}$ , where A and $\\mathbf{B}$ are as defined in $\\S5$ .The elements ${\\pmb v}_{\\pmb{j}\\pmb{k}}$ of $\\mathbf{v}$ &re valley to valley transition probabilities for $j,k=1,...,m-1$<PARAGRAPH_SEPARATOR>Let $\\mathbf{C}=\\lambda\\mathbf{B}+\\mathbf{A}\\mu$ . Then elements $c_{j k}$ $\\mathbf{c}$ are $v_{j k}\\times$ (mean number of observations, given that $j$ is a valley and $\\pmb{k}$ is the next valley).<PARAGRAPH_SEPARATOR>If $\\pmb{\\pi}$ be a row vector giving the distribution of valleys, then<PARAGRAPH_SEPARATOR>$${\\pmb{\\pi}}=({\\pmb{\\pi}}_{1},{\\pmb{\\pi}}_{2},...,{\\pmb{\\pi}}_{m-1}).$$<PARAGRAPH_SEPARATOR>Suppose we start at a valley and stop after $\\pmb{\\mathscr{n}}$ subsequent valleys, then the mean number of observations is approximately nrC1, where 1 is 8 column vector of 1's.<PARAGRAPH_SEPARATOR>Table 12 shows that we need almost twice a8 many observations for the estimation of $\\pmb{L_{0\\cdot8\\otimes1}}$ a8 comp&red withthose for $\\pmb{L_{0:784}}$<PARAGRAPH_SEPARATOR>Table 12. Mean number of observations required when we start at a valley and stop at the next valley (i.e. for two changes). Starting value of the first valley a8 shown. Spacing 1·0"
  },
  {
    "qid": "statistic-posneg-1705-0-2",
    "gold_answer": "FALSE. The paper states that approximation R-2 is computationally heavier than R-1, as R-2 involves an iterative algorithm to solve for \\( \\xi_{n}^{*} \\), whereas R-1 provides a direct approximation based on logarithmic transformations.",
    "question": "Is the approximation R-1 for drawing probabilities computationally heavier than approximation R-2?",
    "question_context": "Resampling Methods in Unequal Probability Sampling\n\nThe paper discusses resampling methods in unequal probability sampling, focusing on the construction of pseudo-populations and the choice of drawing probabilities. It introduces the Holmberg size and Horvitz-Thompson size for determining the number of replications of sample units, and explores various approximations for drawing probabilities to ensure asymptotically correct resampling schemes."
  },
  {
    "qid": "statistic-posneg-2092-0-0",
    "gold_answer": "TRUE. The derivation is correct as it follows from the law of total probability and the Markov property. The formula accounts for two scenarios: the current state being 1 (with probability p) leading to an increment in the count (k-1 to k), and the current state being 0 (with probability 1-p) leading to no change in the count (k remains k). This aligns with the properties of a Markov chain where the probability of the next state depends only on the current state.",
    "question": "Is the recursive formula $P(S_{n}=k)=p P(S_{n-1}=k-1)+(1-p)P(S_{n-1}=k)$ correctly derived from the given Markov chain properties?",
    "question_context": "Recursive Algorithm for Computing Markov Chain Distributions\n\nTable 1 Transition probabilities and initial distributions used in Fig. 1 <PARAGRAPH_SEPARATOR> <html><body><table><tr><td>p=1/2</td><td>p(1|1,1,1) =3/4</td><td>π(1)=1/2</td><td>π(1,1,1)=21/100</td></tr><tr><td>p(1|1)=3/5</td><td>p(1|1,1,0)=7/12</td><td>π(0)=1/2</td><td>π(0,1,1)=9/100</td></tr><tr><td>p(1|0)=2/5</td><td>p(1|1,0,1) =17/52</td><td></td><td>π(1,0,1)=13/200</td></tr><tr><td></td><td>p(1|1,0,0)=55/108</td><td></td><td>π(0,0,1)=27/200</td></tr><tr><td>p(1|1,1)=7/10</td><td>p(1]0,1,1) =19/60</td><td>π(1,1)=3/10</td><td>π(1,1,0)=9/100</td></tr><tr><td>p(1|1,0)=9/20</td><td>p(1]0,1,0)=73/220</td><td>π(0,1)=1/5</td><td>π(0,1,0)=11/100</td></tr><tr><td>p(1|0,1)=13/40</td><td>p(1]0,0,1)=331/540</td><td>π(1,0)=1/5</td><td>π(1,0,0)=27/200</td></tr><tr><td>p(1|0,0)=9/20</td><td>p(110,0,0)=209/660</td><td>π(0,0)=3/10</td><td>π(0,0,0)=33/200</td></tr></table></body></html> <PARAGRAPH_SEPARATOR> using <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{P(S_{n}=k)=P(S_{n}=k,X_{n}=1)+P(S_{n}=k,X_{n}=0)}\\ &{\\qquad=P(S_{n-1}=k-1,X_{n}=1)+P(S_{n-1}=k,X_{n}=0)}\\ &{\\qquad=p P(S_{n-1}=k-1)+(1-p)P(S_{n-1}=k).}\\end{array} $$ <PARAGRAPH_SEPARATOR> The initial distributions and transition probabilities used for each of the models of the example are listed in Table 1. With transition probabilities and initial distributions so defined, the Markovian sequences of the various orders are stationary sequences, where the initial distributions correspond to limiting distributions. Also, the initial distributions for each order may be computed by using transition probabilities of lower orders. For example, for $m=3$ , $\\pi(x_{0},x_{-1},1)=p\\times p(x_{-1}|1)\\times p(x_{0}|x_{-1},1)$ . (See Martin 2000, where an alternative set of “orthogonal” parameters was used to model stationary Markovian sequences.) We set up the example in this manner because it was easy to see the effect of adding in additional parameters when moving to higher-order models. <PARAGRAPH_SEPARATOR> We note how the computed probabilities change with assumed model order. This indicates the need to use an appropriate order when analyzing Markovian data sets. In Martin (2001), a strategy was discussed for using the information from computed probabilities to help determine an appropriate order of Markovian dependence for a data set. <PARAGRAPH_SEPARATOR> We also mention that the run time for each of the sequences of the example given in this section was minimal, about 1 s or less. Runs with sample size $n=10$ , 000 take about a minute and a half."
  },
  {
    "qid": "statistic-posneg-1589-5-1",
    "gold_answer": "TRUE. The formula accurately represents the bootstrap estimation process described in the paper. It involves finding the cutoff K that minimizes the average zero-one loss (indicated by the indicator function $$\\mathbf{1}_{[\\hat{X}_{n+1;K}^{*}\\neq X_{n+1}^{*}]}$$) across B bootstrap samples. This average loss is a Monte Carlo approximation of the misclassification probability, and minimizing it yields the optimal cutoff K for classification purposes.",
    "question": "Does the formula $$\\hat{K}=\\arg\\operatorname*{min}_{K}\\mathsf{a v e}\\left[\\mathbf{1}_{[\\hat{X}_{n+1;K}^{*}\\neq X_{n+1}^{*}]}\\right]$$ correctly represent the bootstrap estimation of the optimal cutoff parameter K for minimizing classification error in VLMC?",
    "question_context": "Bootstrap Estimation of Optimal Cutoff Parameter in Variable Length Markov Chains\n\nThe paper discusses the use of the bootstrap method to estimate the optimal cutoff parameter K for Variable Length Markov Chains (VLMC) when minimizing classification error (zero-one loss). The process involves simulating bootstrap samples from an initial VLMC model fitted with cutoff K0, evaluating the one-step ahead predictor for different K values, and selecting the K that minimizes the average zero-one loss across bootstrap replications. The formula for the estimated optimal cutoff is given by $$\\hat{K}=\\arg\\operatorname*{min}_{K}\\mathsf{a v e}\\left[\\mathbf{1}_{[\\hat{X}_{n+1;K}^{*}\\neq X_{n+1}^{*}]}\\right].$$ The paper also notes that the choice of initial cutoff K0 has a minor effect on the results."
  },
  {
    "qid": "statistic-posneg-182-0-1",
    "gold_answer": "FALSE. The paper actually states that if $n h^{5} \rightarrow 0$, the bias term $b(t)$ becomes negligible, and $\\sqrt{n h}[\\hat{g}_{k}^{*}(t)-g(t)]\\stackrel{D}{\\longrightarrow}N(0,\\gamma_{k}^{2}(t))$. Without this condition, the asymptotic distribution includes the bias term, as shown in the earlier part of Theorem 5.",
    "question": "Does the paper correctly assert that the estimators $\\hat{g}_{k}^{*}(t)$ for the nonparametric component $g(t)$ are asymptotically normal with a bias term $b(t)$ when $n h^{5} \rightarrow 0$ is not assumed?",
    "question_context": "Empirical Likelihood in Partially Linear Single-Index Models with Censored Data\n\nThe paper discusses the asymptotic properties of empirical likelihood (EL) estimators in partially linear single-index models with censored data. It presents several theorems establishing the asymptotic distributions of the estimators for regression parameters and nonparametric components. The results are used to construct confidence regions and intervals, both pointwise and simultaneous, for the parameters and the nonparametric function."
  },
  {
    "qid": "statistic-posneg-2047-0-1",
    "gold_answer": "FALSE. The condition (ϕ−1)ij=0 for all values of the free parameters in ϕ implies that there is no path between i and j consisting solely of free parameters (non-zero entries). However, there could still be a path between i and j if some entries are fixed to non-zero values.",
    "question": "Does the condition (ϕ−1)ij=0 for all values of the free parameters in ϕ imply that there is no path between i and j in the graph?",
    "question_context": "Conditional Independence in Gaussian Graphical Models\n\nSuppose $X=\\{X_{i},i\\in V\\}\\sim N(0,\\phi)$ and $a,\\quad b$ and $c$ are pairwise disjoint subsets of $V$ $a$ and $^b$ non-empty), such that $Y=a\\cup b\\cup c$ .Then: <PARAGRAPH_SEPARATOR> (i) The marginal distribution of $X_{a}$ is $N(0,\\phi_{a a})$   (ii) The conditional distribution of $X_{a}$ given $X_{c}=x_{c}$ is $N(\\varPhi_{a c}\\varPhi_{c c}^{-1}x_{c},\\varPhi_{a a.c}).$   (ii) $a\\bot b|c\\Leftrightarrow\\varPhi_{a b.c}=0\\Leftrightarrow(\\varPhi^{-1})_{a b}=0$ <PARAGRAPH_SEPARATOR> Here <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\phi=\\left[\\begin{array}{l l l}{\\phi_{a a}}&{\\phi_{a b}}&{\\phi_{a c}}\\\\{\\phi_{b a}}&{\\phi_{b b}}&{\\phi_{b c}}\\\\{\\phi_{c a}}&{\\phi_{c b}}&{\\phi_{c c}}\\end{array}\\right]}\\end{array} $$ <PARAGRAPH_SEPARATOR> isassumedpositivedefinite $\\phi^{-1}$ ispartitionedsimilarly), $\\mathcal{P}_{c c}^{-1}:=(\\Phi_{c c})^{-1}$ ， $\\Phi_{a a.c}:=$ $\\phi_{a a}-\\phi_{a c}\\phi_{c c}^{-1}\\phi_{c a}$ $(=\\boldsymbol{\\phi}_{a a}$ f $c=\\emptyset$ ),and $\\phi_{a b.c}:=\\phi_{a b}-\\phi_{a c}\\phi_{c c}^{-1}\\phi_{c b}$ $(=\\varPhi_{a b}$ f $c=\\emptyset$ <PARAGRAPH_SEPARATOR> # Corollary 7 <PARAGRAPH_SEPARATOR> Assume $X=\\{X_{i},i\\in V\\}$ has(positive definite）covariancematrix $\\operatorname{cov}(X,X)=\\phi$ where some non-diagonal entries of $\\mathcal{P}=(\\phi_{i j})_{i,j\\in V}$ are constrained to have the fixed value O, and all other entries arefree parameters.  Let i, $j\\in V$ be two different indices. Then $(\\phi^{-1})_{i j}=0$ for all values of the free parameters in $\\phi$ iff there does not exist a sequence of indices (in $V$ $i=i_{0}$ $i_{n}=j,n\\geqslant1$ ，such that $\\phi_{i_{r}i_{r+1}}$ is a free parameter for $0\\leqslant r<n$"
  },
  {
    "qid": "statistic-posneg-871-2-0",
    "gold_answer": "FALSE. While the non-significance of $\\gamma_1$ and $\\gamma_2$ suggests that the covariates may have no predictive effect after 2 years, the conclusion is tentative due to heavy censoring for $B>2$ (which made $\\gamma_1$ estimates infinite). The inability to reliably estimate effects beyond 2 years means we cannot definitively conclude the effect disappears entirely—only that it is not detectable with the available data. The practical interpretation (flat survival curves post-2 years) supports decay in predictive power, but statistical non-significance alone does not prove absence of effect, especially given estimation challenges.",
    "question": "In the two-step hazard model, the coefficients for covariates $z_4$ and $z_5$ change from $\\alpha_1$ and $\\alpha_2$ to $\\gamma_1$ and $\\gamma_2$ after the split point $B$. Based on the model's estimation results, is it correct to conclude that the predictive effect of these covariates disappears entirely after 2 years ($B=2$), given that $\\gamma_1$ and $\\gamma_2$ were not statistically significant?",
    "question_context": "Two-Step Regression Model for Hazard Functions with Time-Varying Covariates\n\nThe hazard functions $\\{\\lambda_{0i}(t)\\}$ were estimated smoothly using the penalized maximum likelihood method of ANSN80. There is some arbitrariness in deciding on how smooth a function is required. Corresponding to this is the choice of a weight $K_{0}$ high (low) values of $K_{0}$ give a smooth (rough) function. This procedure is discussed in ANSN80 and Senthilselvan (1980). Unfortunately, there was heavy censoring which tends to give negative hazards (ANSN80). To avoid this, a fairly high value of $K_{0}$ was selected at the cost of perhaps oversmoothing the hazard function estimates. Corresponding to these estimates are estimates of the survival function, which are plotted in Fig. 1 separately for the four strata and the four prognostic categories defined above. <PARAGRAPH_SEPARATOR> Some conclusions follow from Fig. 1. The prognosis worsens from category (i) to (iv) in all four strata. The survival curves for Stratum 2 and Stratum 3 are almost identical. In Stratum 1 there is a steep drop in the first 3 years and a slight drop just after 15 years. The only drops in Strata 2, 3 and 4 were in the first 3 years. <PARAGRAPH_SEPARATOR> The goodness-of-fit of the model is checked by the behaviour of residuals in each stratum. Crowley and Hu (1977) and Kay (1977) all have used residual analysis to check the fit of models. The residuals are defined in general by <PARAGRAPH_SEPARATOR> $$ \\varepsilon_{i}=\\int_{0}^{t_{i}}\\lambda_{0}(u)\\exp{(\\beta^{\\mathrm{T}}z(u))}\\mathrm{du} $$ <PARAGRAPH_SEPARATOR> ![](images/cd0401b6904745f207ddc0d2be0eaf3f0c59c3927fcee57b4ff347f93a5c82af.jpg)  FIG. 1. Survival curve estimates from the initial model. The four curves in each stratum are for the prognostic group! (i), (i), (ii) and (iv), in descending order. <PARAGRAPH_SEPARATOR> for the ith patient where $t_{i}$ is the failure or censoring time of the ith patient. If the model is true, each random variable $\\varepsilon_{i}$ has a unit exponential distribution, which is censored if the basic observation is censored. The observed residuals are obtained by substituting the estimates of $\\beta$ and $\\hat{\\lambda}_{0}(\\mathfrak{u})$ into (8). Note that the separate estimates of $\\{\\lambda_{0i}(t)\\}$ are required for the four strata. The model fit can be checked by plotting the estimated cumulative hazard function of the observed residuals. The plot should be approximately a straight line passing through the origin with slope unity. Note that definition (8) allows the possibility of time-dependent covariates $z(t)$ as in Crowley and Hu (1977). This is helpful when testing the fit of the extended model (3) with time-varying covariates. <PARAGRAPH_SEPARATOR> The estimates of the cumulative hazard function are plotted in Fig. 2 for the four strata separately. For Strata 2, 3 and 4, the observed distribution of the residuals is as expected for small values of the cumulative hazard but when the latter is larger the observed distribution deviates from its expectation. In Stratum 1 the fit is better but there is perhaps a trend for the fit to be poorer for larger values of the hazard. <PARAGRAPH_SEPARATOR> ![](images/1db329241e8f03802a364c272bed490b5bf1c8992e82b81c9a7bd8050374830d.jpg)  FIG. 2. Model checks for the initial model. <PARAGRAPH_SEPARATOR> Taking into consideration the above findings, the earlier plots of the estimated survival curves in Fig. 1 and some a priori indications that the predictive power of pre-operative information decays, it was decided to investigate the fit of the two-step model (3) for various values of the split point $B.$ Variables $z_{2}$ and $z_{3}$ were not included in the model although it is possible that they have some predictive power in the model (3). This was to reduce the amount of computation and to facilitate comparison between the standard and two-step models. <PARAGRAPH_SEPARATOR> The proposed model is then <PARAGRAPH_SEPARATOR> $$ \\lambda_{i}(t;z_{4},z_{5})=\\left\\{\\begin{array}{l l}{\\lambda_{0i}(t)\\exp{(x_{1}z_{4}+x_{2}z_{5})},}&{t\\leqslant B,}\\\\{\\lambda_{0i}(t)\\exp{(\\gamma_{1}z_{4}+\\gamma_{2}z_{5})},}&{t>B}\\end{array}\\right. $$ <PARAGRAPH_SEPARATOR> for the four strata $i=1,2,3,4.$ This is an example of the general model (3). <PARAGRAPH_SEPARATOR> To estimate the unknowns $\\alpha,\\gamma$ and $B.$ a stratified version of the procedure, given in Section 3, is required. A likelihood function, analogous to (4), is established, which allows for the four strata. This is an extension of the method given by Kalbfleisch and Prentice (1980, p. 87). This is maximized with respect to $\\pmb{\\alpha}$ and $\\gamma$ for several values of $B.$ .However, it turned out that for values of $B>2$ , the estimate of $\\gamma_{1}$ was infinite because of heavy effective censoring for the estimate of $\\gamma$ . This is the kind of problem discussed in Section 2. Hence only values of $B\\leqslant2$ were considered and it turned out that $B=2$ had the greatest likelihood of those considered, so we took this as the “estimated\" split-point. This was also considered to be sensible in the practical context. <PARAGRAPH_SEPARATOR> The estimates of the parameters $\\pmb{\\alpha}$ and $\\gamma$ are given in Table 2, together with their asymptotic standard error estimates. Neither of the longer-term coefficients were statistically significant and the chi-square statistic testing $\\gamma_{1}=0$ $\\gamma_{2}=0$ had a value of 0·63, which is not statistically significant at the 5 per cent level on two degrees of freedom. Since $B$ was estimated as 2 years, the suggestion is that the predictive effect of the covariates $x_{4}$ and $x_{5}$ has disappeared by the end of 2 years. Recalling the above dificulty in estimating B greater than 2 years, this conclusion must remain tentative. Since $\\pmb{\\alpha}$ and $\\gamma$ are estimated by maximizing separate functions (cf. (5)), the conditional maximum likelihood estimates of $\\pmb{\\alpha}$ under the hypothesis that T = (0,0) remain as given in Table 2. Taking these values for α, yT = (0,0) and B = 2, smoothed hazard functions were estimated using the method outlined in Section 3. The corresponding estimated survival functions are given in Fig. 3 for the four age strata and the four prognostic categories separately. In all cases there is a sharp drop in the first 2 years followed by a fairly flat curve, suggesting that the disease is more likely to recur in the initial period than subsequently."
  },
  {
    "qid": "statistic-posneg-84-0-1",
    "gold_answer": "FALSE. According to Whittle’s general equivalence theorem, for a concave $\\phi$, a design $\\xi^{*}$ is $\\phi$-optimal if and only if $\\Phi(\\xi^{*},\\delta_{x}) \\leq 0$ for all $x \\in \\chi$, with equality holding at the support points of $\\xi^{*}$. Thus, $\\Phi(\\xi^{*},\\delta_{x})$ is zero at the support points and non-positive elsewhere, but not necessarily always negative outside the support.",
    "question": "Is it correct to assume that the directional derivative $\\Phi(\\xi,\\eta)$ is always negative for a D-optimal design $\\xi^{*}$ at any point $x$ not in the support of $\\xi^{*}$?",
    "question_context": "D-optimal Designs for Nonlinear Models: Conditions for Minimal Support\n\nThe objective of this research is to determine if a locally $D$ -optimal design for a nonlinear model has minimal support. Techniques similar to those to be used in $\\S2$ have been used in the literature on optimal designs. Han & Chaloner (2003) considered exponential regression models and counted the roots of $d(\\xi,x)-k$ globally, where $d(\\xi,x)-k$ is the directional derivative, defined in $\\S2$ , for the $D$ -optimality criterion. Li & Majumdar (2008) extended it to examine the behaviour of $d(\\xi,x)-k$ in a vertical neighbourhood of zero and applied it to logistic models with three and four parameters. In this paper, we consider the derivatives of $d(\\xi,x)$ and obtain sufficient conditions with more extensive and systematic applicability. Investigation of the support points by examining derivatives of $d(\\xi,x)$ goes back to Guest (1958). In particular, we establish relationships between $D$ -optimal designs for different design spaces. <PARAGRAPH_SEPARATOR> # 2. SUFFICIENT CONDITIONS FOR MINIMAL SUPPORT <PARAGRAPH_SEPARATOR> Consider a compact design space $\\chi$ , and let $\\mathcal{H}$ denote the set of all probability measures on $\\chi$ . The set $\\mathcal{H}$ is compact in the topology of weak convergence and convex. Any measure $\\xi\\in\\mathcal{H}$ is called an approximate design. Suppose the parameter of interest $\\theta\\in\\mathbb{R}^{k}$ . For a nonlinear regression model <PARAGRAPH_SEPARATOR> $$ y_{i}=f(x_{i},\\theta)+\\epsilon_{i}, $$ <PARAGRAPH_SEPARATOR> with independent $\\epsilon_{i}\\sim N(0,\\sigma^{2})$ , the Fisher information matrix for $\\theta$ is $M(\\xi,\\theta)=\\int_{\\chi}(\\partial f(x,\\theta)/$ $\\partial\\theta)(\\partial f(x,\\theta)/\\partial\\theta^{\\mathrm{T}})d\\xi(x)$ , where we assume that the nuisance parameter $\\sigma^{2}$ is known and without loss of generality $\\sigma^{2}=1$ . For a generalized linear model <PARAGRAPH_SEPARATOR> $$ E(y_{i})=\\mu_{i},\\quad g(\\mu_{i})=\\eta_{i}=h(x_{i})^{\\mathrm{T}}\\theta, $$ <PARAGRAPH_SEPARATOR> the information matrix for $\\theta$ is $\\begin{array}{r}{M(\\xi,\\theta)=\\int_{x}\\omega(x,\\theta)h(x)h(x)^{\\mathrm{T}}d\\xi(x)}\\end{array}$ , where $\\omega(x,\\theta)=(\\partial\\mu/\\partial\\eta)^{2}/\\mathrm{var}(y)$ . As in Silvey (1980, p. 40), we define the $\\mathcal{D}$ -optimality criterion function as the logarithm of $|{\\cal M}(\\xi,\\theta)|$ , the determinant of the information matrix, if $M(\\xi,\\theta)$ is nonsingular and $-\\infty$ if $M(\\xi,\\theta)$ is singular. A $D$ -optimal design maximizes this criterion function over $\\mathcal{H}$ . The equivalence theorem for $D$ -optimal designs in the context of linear models was first developed by Kiefer $\\&$ Wolfowitz (1960) and was extended to nonlinear models by White (1973). Whittle (1973) generalized the equivalence theorem for concave optimality criteria over a compact convex set of probability measures in linear models. Chaloner & Larntz (1989) gave additional conditions to extend the general equivalence theorem to nonlinear models and applied it to Bayesian optimal designs. <PARAGRAPH_SEPARATOR> For a differentiable criterion function $\\phi$ , the directional derivative of $\\phi$ at $\\xi$ in the direction of $\\eta$ is defined as $\\begin{array}{r}{\\Phi(\\xi,\\eta)=\\operatorname*{lim}_{\\epsilon\\to0}\\epsilon^{-1}[\\phi\\{(1-\\epsilon)\\xi+\\epsilon\\eta\\}-\\phi(\\xi)]}\\end{array}$ . It follows from Whittle’s general equivalence theorem that, for a concave $\\phi$ , a design $\\xi^{*}$ is $\\phi$ -optimal if and only if $\\Phi(\\xi^{*},\\delta_{x})\\leqslant0$ , with equality at the support points of $\\xi^{*}$ , where $\\delta_{x}$ is a one-point design which gives all weight to the single value $x$ . <PARAGRAPH_SEPARATOR> We assume that the design space $\\chi$ has the form $\\chi_{a b}=[a,b]$ , where the quantities $a<b$ are assumed known. It follows from the definition of the $D$ -optimality criterion that a $D$ -optimal design over $\\mathcal{H}$ must be a nonsingular design, which is a design with a nonsingular information matrix."
  },
  {
    "qid": "statistic-posneg-1510-1-0",
    "gold_answer": "FALSE. The rate $O(n_{\\mathrm{gen}}^{-3}(\\log n_{\\mathrm{gen}})^{p-1})$ applies to the variance of the GMMN RQMC estimator, not the estimator itself. The approximation error (bias) converges at a rate of $O(n_{\\mathrm{gen}}^{-3/2}(\\log n_{\\mathrm{gen}})^{(p-1)/2})$. The distinction is crucial: variance reduction and error convergence are related but distinct properties. The faster $O(n^{-3})$ rate for variance stems from Owen's scrambled net analysis, while the error rate reflects the composite effect of smoothness and dimensionality.",
    "question": "Is it true that the GMMN RQMC estimator $\\hat{\\mu}_{n_{\\mathrm{gen}}}^{\\mathrm{NN}}$ converges at a rate of $O(n_{\\mathrm{gen}}^{-3}(\\log n_{\\mathrm{gen}})^{p-1})$ under the assumption that the composite function $\\Psi\\circ q$ is sufficiently smooth?",
    "question_context": "Quasi-Random Sampling via Generative Neural Networks for Multivariate Distributions\n\nAs mentioned in the introduction, transformations to convert $\\tilde{P}_{n_{\\mathrm{gen}}}$ to samples which mimic samples from $C$ but locally provide a more homogeneous coverage are only available for a few specific cases of $C$ and their numerical evaluation in a fast and robust way is even more challenging. <PARAGRAPH_SEPARATOR> To avoid these problems, we suggest to utilize the GMMN $f_{\\hat{\\theta}}$ trained as in Algorithm 2.1. Besides a straightforward evaluation, this allows us to generate approximate quasi-random samples from $F_{X}$ with any underlying copula $C_{:}$ , by training a GMMN on pseudo-random samples generated from $F_{X}$ . Alternatively, approximate quasi-random samples which follow the same empirical distribution as any given dataset can be obtained by training a GMMN on the given dataset itself. An additional advantage is that GMMNs provide a sufficiently smooth map from the RQMC point set to the target distribution which helps preserve the discrepancy of the point set upon transformation and hence guarantees the improved performance of RQMC estimators compared to the MC estimator (see Section 4 and Appendix A). <PARAGRAPH_SEPARATOR> With the mapping $F_{Z}^{-1}({\\pmb u})=(F_{Z_{1}}^{-1}(u_{1}),\\ldots,F_{Z_{p}}^{-1}(u_{p}))$ to the input distribution and the trained $\\operatorname{GMN}f_{\\hat{\\theta}}$ at hand, define a transform <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{q({\\pmb u})=f_{\\hat{\\pmb\\theta}}\\circ F_{\\pmb Z}^{-1}({\\pmb u}),\\quad{\\pmb u}\\in(0,1)^{p}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Based on the RQMC point set $\\tilde{P}_{n_{\\mathrm{gen}}}~=~\\{\\tilde{\\nu}_{1},\\dots,\\tilde{\\nu}_{n_{\\mathrm{gen}}}\\}$ of size $n_{\\mathrm{gen}}.$ , we can then obtain approximate quasi-random samples by <PARAGRAPH_SEPARATOR> $$ Y_{i}=q(\\tilde{\\nu}_{i}),\\quad i=1,\\dots,n_{\\mathrm{gen}}, $$ <PARAGRAPH_SEPARATOR> (compare with (2)) and define a GMMN RQMC estimator of (3) by <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\hat{\\mu}_{n_{\\mathrm{gen}}}^{\\mathrm{NN}}=\\frac{1}{n_{\\mathrm{gen}}}\\displaystyle\\sum_{i=1}^{n_{\\mathrm{gen}}}\\Psi(Y_{i})=\\frac{1}{n_{\\mathrm{gen}}}\\sum_{i=1}^{n_{\\mathrm{gen}}}\\Psi(q(\\tilde{\\nu}_{i}))}\\ &{\\qquad=\\frac{1}{n_{\\mathrm{gen}}}\\displaystyle\\sum_{i=1}^{n_{\\mathrm{gen}}}\\Psi\\big(f_{\\hat{\\theta}}(F_{\\boldsymbol Z}^{-1}(\\tilde{\\nu}_{i}))\\big).}\\end{array} $$ <PARAGRAPH_SEPARATOR> We thus have the approximations <PARAGRAPH_SEPARATOR> $$ \\mathbb{E}(\\Psi(X))\\approx\\mathbb{E}(\\Psi(Y))\\approx\\hat{\\mu}_{n_{\\mathrm{gen}}}^{\\mathrm{NN}}. $$ <PARAGRAPH_SEPARATOR> The error in the first approximation is small if the GMMN is trained well and the error in the second approximation is small if the unbiased estimator μˆ nNgeNn has a small variance. The primary bottleneck in this setup is the error in the first approximation which is determined by the size $n_{\\mathrm{trn}}$ of the training dataset and, in particular, by the batch size $n_{\\mathrm{bat}}$ which is the major factor determining training efficiency of the GMMN we found in all our numerical studies. Given a sufficiently large $n_{\\mathrm{bat}}$ and, by extension, $n_{\\mathrm{trn}}$ , the GMMN is trained well, which renders the first approximation error in (8) negligible. However, in practice, the batch size $n_{\\mathrm{bat}}$ is constrained by the quadratically increasing memory demands to compute the MMD cost function of the GMMN. For a theoretical result regarding this approximation error, see Dziugaite, Roy, and Ghahramani (2015) where a bound on the error between optimizing a sample version and a population version of ${\\mathrm{MMD}}(X,Y)$ was investigated. Finally, we stress that the task of GMMN training and generation are separate steps which ensures that, once trained, generating quasirandom GMMN samples is comparably fast; see Appendix B. <PARAGRAPH_SEPARATOR> The error in the second approximation in (8) is small if the composite function $\\Psi\\circ q$ is sufficiently smooth. The transform $q$ is sufficiently smooth for $\\operatorname{GMMNs}f_{\\hat{\\boldsymbol{\\theta}}}$ constructed using standard activation functions and commonly used input distributions; see the discussion following Corollary A.1. Given a sufficiently smooth $\\Psi$ , we can utilize a result of Owen (2008) to establish a rate of convergence $O(n_{\\mathrm{gen}}^{-3}(\\log n_{\\mathrm{gen}})^{p-1})$ for the variance (and $O(n_{\\mathrm{gen}}^{-3/2}(\\log n_{\\mathrm{gen}})^{(p-1)/2})$ for the approximation error) of the GMMN RQMC estimator μˆ nNgeNn constructed by scrambling a so-called digital net to obtain $\\{\\tilde{\\pmb{\\nu}}_{1},\\dots,\\tilde{\\pmb{\\nu}}_{n_{\\mathrm{gen}}}\\}$ ; for more details see Appendix A.4.1. With a stronger assumption on the behavior of the composite function $\\Psi\\circ q$ , we can show that the Koksma–Hlawka bound on the error between the (nonrandomized) GMMN QMC estimator $\\begin{array}{r}{\\frac{1}{n_{\\mathrm{gen}}}\\sum_{i=1}^{n_{\\mathrm{gen}}}\\Psi(q(\\pmb{\\nu}_{i}))}\\end{array}$ and $\\mathbb{E}(\\Psi(Y))$ is satisfied which in turn implies a rate of convergence $O(n_{\\mathrm{gen}}^{-1}(\\log n_{\\mathrm{gen}})^{p})$ for the (nonrandomized) GMMN QMC estimator; see Appendix A.2. If the Koksma–Hlawka bound holds, we can use a result stated in L’Ecuyer (2016) to also establish a rate of convergence $O(n_{\\mathrm{gen}}^{-2}(\\log{n_{\\mathrm{gen}}})^{2p})$ for the variance of GMMN RQMC estimators constructed using digitally shifted nets; see Appendix A.4.2."
  },
  {
    "qid": "statistic-posneg-1248-0-1",
    "gold_answer": "FALSE. The CLT for Fréchet means on manifolds does not require the random variable $X$ to be bounded away from the cut locus of the mean in all cases. According to Theorem 3, the condition is only necessary for Ziezold or full/partial Procrustes means. Specifically, for partial Procrustes means, an additional condition is that the Euclidean second moment $\\mathbb{E}(\\|X\\|^{2})$ is finite. The requirement to be bounded away from the cut locus is not universal and depends on the type of mean being considered.",
    "question": "Does the CLT for Fréchet means on manifolds require the random variable $X$ to be bounded away from the cut locus of the mean in all cases?",
    "question_context": "Central Limit Theorem for Fréchet Means on Manifolds\n\nThe Central-Limit-Theorem for Fréchet $\\rho$ -means requires additional setup. Definition 4. Let $P$ be a $D$ -dimensional manifold. We say that a $P$ -valued estimator $\\mu_{n}(\\omega)$ of $\\mu\\in P$ satisfies a CLT, if in any local chart $(\\phi,U)$ near $\\mu=\\phi^{-1}(0)$ there are a suitable $D\\times D$ matrix $A_{\\phi}$ and a Gaussian $D\\times D$ matrix $\\mathcal{G}_{\\phi}$ with zero mean and semi-definite symmetric covariance matrix $\\Sigma_{\\phi}$ such that $$\\sqrt{n}\\ensuremath{A_{\\phi}}(\\phi(\\mu_{n})-\\phi(\\mu))\\longrightarrow\\mathcal{G}_{\\phi}$$ in distribution as $n\\to\\infty$ . In most applications $A_{\\phi}$ is non-singular, then in consequence of the $\\cdot_{\\delta}$ -method’, for any other chart $(\\phi^{\\prime},U)$ near $\\mu=\\phi^{\\prime-1}(0)$ we have simply $$A_{\\phi^{\\prime}}^{-1}\\Sigma_{\\phi^{\\prime}}(A_{\\phi^{\\prime}}^{-1})^{T}=J(\\phi^{\\prime}\\circ\\phi^{-1})_{0}A_{\\phi}^{-1}\\Sigma_{\\phi}(A_{\\phi}^{-1})^{T}J(\\phi^{\\prime}\\circ\\phi^{-1})_{0}^{T},$$ where $J(\\cdot)_{0}$ denotes the Jacobian matrix of first derivatives at the origin. In consequence of theorems 2 and 6 from the Appendix (assertion (ii) follows from Bhattacharya & Patrangenaru (2005)) we have the following. Theorem 3. Suppose that $X$ is a random pre-shape in $S_{m}^{k}$ or a random configuration in $F_{m}^{k},$ $[X]$ having a unique Ziezold, full Procrustes or partial Procrustes mean $[\\mu]$ on the respective manifold part. Then every measurable selection $[\\mu_{n}(\\omega)]$ from the sets of sample Ziezold, full Procrustes or partial Procrustes means, respectively, satisfies a CLT if [X ] is bounded away from the cut locus of the mean a.s. in the Ziezold or full/partial Procrustes sense (3) under the following additional condition: (i) none in case of Ziezold or full Procrustes means, (ii) in case of partial Procrustes means, if the Euclidean second moment $\\mathbb{E}(\\|X\\|^{2})$ is finite. In a suitable chart $(\\phi,U)$ the corresponding matrices from definition 4 are given by $$A_{\\phi}=\\mathbb{E}(H\\rho([X],[\\mu])),\\Sigma_{\\phi}=\\operatorname{cov}(\\operatorname{grad}\\rho([X],[\\mu])),$$ where $\\rho$ denotes the distances $d_{\\Sigma_{m}^{k}}^{(p)},d_{\\Sigma_{m}^{k}}^{(z)}$ and dS(i)\u0002km , respectively. Moreover, grad and H denote the gradient and Hessian of $x\\mapsto\\rho([X],[\\phi^{-1}(x)])^{2}$ , respectively. Numerical simulations show a rather good finite sample accuracy of the CLT, cf. Fig. 1. Remark $I$ . For $m\\geq3$ , computing Ziezold means (for an algorithm cf. Ziezold (1994)) is computationally less costly than computing full Procrustes means (corresponding algorithms are discussed in Dryden & Mardia (1998, chapter 5.3)): for full Procrustes means in every iteration step every single optimally positioned datum needs additionally to be projected to the tangent space. The simulations reported in section 4.1 and the Bootstrap simulations in section 6 give similar results but are approximately 15 per cent faster when using Ziezold means instead of full Procrustes means."
  },
  {
    "qid": "statistic-posneg-1565-2-4",
    "gold_answer": "TRUE. The context confirms this simplification for perturbed systematic sampling when $m=0$, using Taylor's expansion and the dominated convergence theorem. The term indeed simplifies to the given expression.",
    "question": "Is the claim 'For perturbed systematic sampling, the term $\\sum_{n=1}^{\\infty}p^{n}\\{g*\\check{h}_{t}*h_{t}(n t)-g(0)\\}$ simplifies to $t g^{\\prime}(0^{+})\\frac{p}{(1-p)^{2}}+o(t)$ as $t\\to0$ for $m=0$' correct?",
    "question_context": "Variance Estimation for Generalized Cavalieri Estimators\n\nThe paper discusses variance estimation for generalized Cavalieri estimators, focusing on the properties of the covariogram $g$ of a measurement function $f$. It provides proofs for several propositions involving Taylor expansions and dominated convergence theorems to derive asymptotic expressions for variance terms. The context includes detailed derivations of terms like $G(k,t)$ and their expansions around $t=0$, considering different smoothness conditions for $g$."
  },
  {
    "qid": "statistic-posneg-1986-0-1",
    "gold_answer": "FALSE. The context states that Design 4.1.1 is perpetually connected, while Designs 4.1.2 and 4.1.3 are not. In fact, Designs 4.1.2 and 4.1.3 have the same minimal design, and neither τ_A - τ_B nor ρ_A - ρ_B is estimable in these disconnected designs. This lack of estimability is a key drawback of these designs compared to Design 4.1.1.",
    "question": "Is it correct to say that Designs 4.1.2 and 4.1.3 are perpetually connected, allowing for unbiased estimation of both treatment effects (τ_A - τ_B) and carry-over effects (ρ_A - ρ_B)?",
    "question_context": "Optimal Design Comparison for Treatment Effect Estimation\n\nThe paper compares three designs (4.1.1, 4.1.2, 4.1.3) for estimating treatment effects in bioequivalence experiments. Design 4.1.1 is perpetually connected and uses an intuitive estimator for the treatment effect contrast (τ_A - τ_B), while Designs 4.1.2 and 4.1.3 use weighted means due to their disconnected nature. The sample variances for the treatment effect estimators are given as (1/4)σ², (11/28)σ², and (11/40)σ² for Designs 4.1.1, 4.1.2, and 4.1.3, respectively. The paper also discusses the carry-over effects contrast (ρ_A - ρ_B) and provides weight vectors for each design."
  },
  {
    "qid": "statistic-posneg-972-1-0",
    "gold_answer": "FALSE. The condition for quasi-admissibility is not directly derived from the asymptotic boundary formula itself but is instead a sufficient condition that ensures the estimator's behavior falls within the quasi-admissible region as defined by the boundary. The boundary formula represents the exact threshold, while the quasi-admissibility condition ensures the estimator's $\\phi(w)$ stays above this threshold for $b<1$.",
    "question": "Is the condition for quasi-admissibility, $\\forall_{w\\geq w_{*}}\\quad\\phi(w)\\geq\\frac{p-2}{n+2}-b\\frac{\\beta_{\\star}}{\\ln w}$ with $b<1$, correctly derived from the asymptotic boundary formula $\\phi(w)=\\frac{p-2}{n+2}-\\frac{\\beta_{\\star}}{\\ln w}$?",
    "question_context": "Quasi-Admissibility Boundary in Generalized Bayes Estimation\n\nA1 $\\phi(0)=0$ and $\\phi(w)\\geq0$ for any $w\\geq0$ ; A2 $\\phi(w)$ has at most finitely many local extrema; A3 lim $\\mathrm{inf}_{w\\to\\infty}w\\phi^{\\prime}(w)/\\phi(w)\\geq0$ and lim $\\begin{array}{r}{\\operatorname*{sup}_{w\\to\\infty}w\\phi^{\\prime}(w)/\\phi(w)\\leq1.}\\end{array}$ <PARAGRAPH_SEPARATOR> Assumption A2 assumes that $\\phi$ does not oscillate excessively and that $\\scriptstyle\\operatorname*{lim}_{w\\to\\infty}\\phi(w)$ exists. As far as we know, Assumptions A1–A3 cover all minimax and smooth estimators in the literature including [10–12]. Assumptions A1–A3 are also satisfied by linear estimators of the form $\\delta(X)=\\alpha X$ for $\\alpha\\in[0,1]$ and for which $\\phi(w)=(1-\\alpha)w$ . These estimators are unique proper Bayes and admissible in the normal case for $\\alpha\\in[0,1)$ . We emphasize that while we address quasiadmissibility and inadmissibility only for $\\delta_{\\phi}$ for $\\phi\\in\\varPhi_{A}\\subset\\phi$ , we allow competitive estimators of the form $\\delta_{\\phi+g}$ for $g\\in\\varPhi$ . <PARAGRAPH_SEPARATOR> In Section 2 we will show the following result, which establishes <PARAGRAPH_SEPARATOR> $$ \\phi(w)=\\frac{p-2}{n+2}-\\frac{\\beta_{\\star}}{\\ln w} $$ <PARAGRAPH_SEPARATOR> as the asymptotic boundary between quasi-admissibility and quasi-inadmissibility where <PARAGRAPH_SEPARATOR> $$ \\beta_{\\star}=\\frac{d_{n}(1+c_{p,n})}{2}=\\frac{2(p+n)}{(n+2)^{2}}. $$ <PARAGRAPH_SEPARATOR> Quasi-admissibility: If $\\phi\\in\\varPhi_{A}$ and there exists $w_{*}$ and $b<1$ such that <PARAGRAPH_SEPARATOR> $$ \\forall_{w\\geq w_{*}}\\quad\\phi(w)\\geq\\frac{p-2}{n+2}-b\\frac{\\beta_{\\star}}{\\ln w}, $$ <PARAGRAPH_SEPARATOR> then $\\delta_{\\phi}$ is quasi-admissible. <PARAGRAPH_SEPARATOR> Quasi-inadmissibility: If $\\phi\\in\\varPhi_{A}$ and there exists $w_{*}$ and $b>1$ such that <PARAGRAPH_SEPARATOR> $$ \\forall_{w\\geq w_{*}}\\quad\\phi(w)\\leq\\frac{p-2}{n+2}-b\\frac{\\beta_{\\star}}{\\ln w}, $$ <PARAGRAPH_SEPARATOR> then $\\delta_{\\phi}$ is quasi-inadmissible (and hence inadmissible). <PARAGRAPH_SEPARATOR> In Section 3, we find a generalized Bayes estimator with asymptotic behavior <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{w\\to\\infty}\\ln w\\left\\{\\frac{p-2}{n+2}-\\phi(w)\\right\\}=b\\beta_{\\star}, $$ <PARAGRAPH_SEPARATOR> for all $b>0$ . The corresponding generalized prior is given by <PARAGRAPH_SEPARATOR> $$ {\\frac{1}{\\sigma^{2}}}\\times{\\frac{1}{\\sigma^{p}}}G(\\|\\theta\\|/\\sigma) $$ <PARAGRAPH_SEPARATOR> with <PARAGRAPH_SEPARATOR> $$ G(\\|\\mu\\|)=\\int_{0}^{1}\\left(\\frac{\\lambda}{1-\\lambda}\\right)^{p/2}\\exp\\left(-\\frac{\\lambda}{1-\\lambda}\\frac{\\|\\mu\\|^{2}}{2}\\right)\\lambda^{-2}\\left(\\ln\\frac{1}{\\lambda}\\right)^{b}\\mathrm{d}\\lambda. $$ <PARAGRAPH_SEPARATOR> Hence, $b<1$ and $b>1$ imply quasi-admissibility and quasi-inadmissibility, respectively, of the associated generalized Bayes estimators. Interestingly, the boundary $b=1$ also appears in the known $\\sigma^{2}$ case when estimating $\\mu$ with $Z\\sim\\mathcal{N}_{p}(\\mu,I_{p})$ . By using Brown’s sufficient condition [2], the generalized Bayes estimator with respect to $G(\\|\\mu\\|)$ above is admissible (resp. inadmissible) when $b\\leq1$ (resp. $b>1$ ). This nice correspondence leads naturally to the conjecture: a quasi-admissible generalized Bayes estimator satisfying (9) is admissible. <PARAGRAPH_SEPARATOR> An extension to the general class of spherically symmetric distributions is briefly considered in Section 2.1. We give some concluding remarks in Section 4. Some technical proofs are given in Appendix. <PARAGRAPH_SEPARATOR> # 2. Quasi-admissibility <PARAGRAPH_SEPARATOR> The main result of this paper, Theorem 1, gives sufficient conditions for quasi-admissibility and quasi-inadmissibility for estimators $\\delta_{\\phi}$ of the form (2), for $\\phi\\in\\varPhi_{A}$ . In preparation, we first give several lemmas. Recall that the unbiased estimator of the difference in risk between $\\delta_{\\phi}$ and $\\delta_{\\phi+g}$ is given by <PARAGRAPH_SEPARATOR> $$ (n+2)\\varDelta(w;\\phi,g)=(n+2)\\lbrace D_{\\phi}(w)-D_{\\phi+g}(w)\\rbrace=(n+2)g(w)\\lbrace\\varDelta_{1}(w;\\phi)+\\varDelta_{2}(w;\\phi,g)\\rbrace $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ \\varDelta_{1}(w;\\phi)=2\\frac{c_{p,n}-\\phi(w)}{w}+d_{n}\\phi^{\\prime}(w) $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\varDelta_{2}(w;\\phi,g)=-\\frac{g(w)}{w}+d_{n}g^{\\prime}(w)+d_{n}\\frac{g^{\\prime}(w)}{g(w)}\\{1+\\phi(w)\\}, $$ <PARAGRAPH_SEPARATOR> and where $c_{p,n}=(p-2)/(n+2)$ and $d_{n}=4/(n+2)$ . Note that $\\varDelta_{2}(w;\\phi,g)$ is well-defined for $w$ such that $g(w)\\neq0$ , but $\\varDelta(w;\\phi,g)$ is well-defined even when $g(w)=0$ . The first lemma gives necessary conditions on $g(w)$ for $\\varDelta(w;\\phi,g)$ to be nonnegative for all $w\\geq0$ . <PARAGRAPH_SEPARATOR> Lemma 1. Suppose $\\varDelta(w;\\phi,g)\\geq0$ for all $w\\geq0$ with $\\phi\\in\\varPhi_{A}$ and $g\\in\\varPhi$ . Then <PARAGRAPH_SEPARATOR> B1 $g(w)\\geq0$ for all $w\\ge0$ ;   B2 if $g(w_{0})>0$ , then, for any $w\\ge w_{0},g(w)>0.$ ."
  },
  {
    "qid": "statistic-posneg-1460-2-0",
    "gold_answer": "TRUE. The logistic regression model is correctly specified with the given parameters $\\pmb{\\beta}_{o}^{\\prime}=(0,2,2)$. The probability is modeled as $P(Y_{i}=1|\\mathbf{X}_{i}=\\mathbf{x}_{i}) = \\frac{\\exp(\\mathbf{x}_{i}^{\\prime}\\pmb{\\beta}_{o})}{1+\\exp(\\mathbf{x}_{i}^{\\prime}\\pmb{\\beta}_{o})}$, where $\\mathbf{X}_{i}^{\\prime}=(1,\\mathbf{U}_{i}^{\\prime})$. The parameters are correctly defined for the non-contaminated situation $C_{0}$.",
    "question": "In the non-contaminated situation $C_{0}$, the probability $P(Y_{i}=1|\\mathbf{X}_{i}=\\mathbf{x}_{i})$ is correctly modeled by the logistic function with parameters $\\pmb{\\beta}_{o}^{\\prime}=(0,2,2)$. True or False?",
    "question_context": "Robustness Testing in Logistic Regression: Level and Power Analysis\n\nWe consider two different data schemes. In the first one, we follow the configurations introduced by Croux and Haesbroeck (2003) to study the stability of the level of the tests. In the second one, we follow a scheme close to that considered in Bianco and Yohai (1996) and we focus on power. <PARAGRAPH_SEPARATOR> # 6.1. Level <PARAGRAPH_SEPARATOR> We generate 5000 samples of size $n=100$ and 50 with $p=3$ , i.e. an intercept and two covariates. The explanatory variables $\\mathbf{U}_{i}^{\\prime}=(U_{i1},U_{i2})$ are distributed according to a standard normal distribution $N_{2}(0,I_{2})$ and the response variables $Y_{i}$ follow the model <PARAGRAPH_SEPARATOR> $$ P\\left(Y_{i}=1|\\mathbf{X}_{i}=\\mathbf{x}_{i}\\right)=\\frac{\\exp(\\mathbf{x}_{i}^{\\prime}\\pmb{\\beta}_{o})}{1+\\exp(\\mathbf{x}_{i}^{\\prime}\\pmb{\\beta}_{o})}, $$ <PARAGRAPH_SEPARATOR> where $\\mathbf{X}_{i}^{\\prime}=(1,\\mathbf{U}_{i}^{\\prime})$ and $\\pmb{\\beta}_{o}^{\\prime}=(0,2,2)$ . This case will be identified as the non-contaminated situation $C_{0}$ for $n=100$ and 50. For each sample size, we consider two contamination schemes in order to show that the level of the classical test breaks down, while the proposed tests are less sensitive to outliers. For $n=100$ we consider the contaminations given by <PARAGRAPH_SEPARATOR> $C_{1,100}:3$ misclassified observations are added on a hyperplane parallel to the true discriminating hyperplane $\\mathbf{X}^{\\prime}{\\boldsymbol{\\beta}}$ with a shift equal to $m\\times{\\sqrt{2}}$ and with the first covariate $x_{1}$ around 3,   $C_{2,100}$ : analogous to $C_{1,100}$ but with 5 outlying observations, <PARAGRAPH_SEPARATOR> while for $n=50$ we consider the following contamination schemes"
  },
  {
    "qid": "statistic-posneg-560-3-0",
    "gold_answer": "TRUE. The formula adjusts the traditional variance $\\sigma^{2}{}_{x^{\\bullet}}=2\\left(v-1\\right)$ by subtracting $\\frac{v^{2}}{N}$ to account for small sample sizes (where $v$ is the number of categories and $N$ is the sample size) and adding $S\\left(\\frac{1}{\\bar{n}_{s}}\\right)$ to account for skewed distributions. This adjustment is necessary because the traditional formula underestimates the variance for small samples and does not account for skewness in the data.",
    "question": "Is the variance formula $\\sigma^{2}x^{1}=2(v-1)\\left(1-\\frac{1}{N}\\right)-\\frac{v^{2}}{N}+S\\left(\\frac{1}{\\bar{n}_{s}}\\right)$ correctly adjusted for small samples and skewed distributions as claimed in the paper?",
    "question_context": "Chi-Square Test Adjustments for Small Samples and Skewed Distributions\n\nThe paper discusses modifications to the chi-square ($\\chi^2$) test for goodness-of-fit, particularly for small samples and skewed distributions. It introduces adjustments to the variance formula to account for small sample sizes and non-normal distributions, and proposes a modified distribution curve for $\\chi^2$ values."
  },
  {
    "qid": "statistic-posneg-529-0-2",
    "gold_answer": "FALSE. The context highlights that Bernstein polynomials do not require the location of the mode to be pre-specified, unlike some SQP methods. Bernstein polynomials are a flexible nonparametric approach to density estimation, and their probabilistic interpretation allows them to naturally handle boundary behavior and smoothness without explicit mode specification.",
    "question": "Bernstein polynomials are only useful for density estimation when the location of the mode is pre-specified. Is this statement accurate?",
    "question_context": "Bernstein Polynomials in Density Estimation\n\nBraun and Hall (2001) and Hall and Kang (2005) measured the closeness of the sharpened data and original data set using a $L_{\\alpha}$ distance, for $1\\leq\\alpha\\leq2$ . They obtained the sharpened data vector that minimized the $L_{\\alpha}$ norm using sequential quadratic programming (SQP) techniques. Hall and Huang (2002) also used SQP methods to perform unimodal density estimation by reweighting, or tilting, the empirical distribution. There are numerous issues with using SQP for unimodal density estimation, including the requirement that the location of the mode must be explicitly defined, and when $\\alpha=1$ the $L_{1}$ norm is not strictly convex so solutions may not be unique. The biggest issue, however, is that the constraint functions may not always be convex functions of the sharpened data set, so the SQP could improperly converge to local optima, or in some cases may not converge at all (Wolters, 2012). Wolters (2012) attempted to remedy these issues by proposing a greedy algorithm which always converges to a sensible solution, does not require the location of the mode to be pre-specified, and requires less computing time than SQP, but like SQP, the algorithm is sensitive to its starting values. <PARAGRAPH_SEPARATOR> # 1.2. Density estimation with Bernstein polynomials <PARAGRAPH_SEPARATOR> Bernstein polynomials were first studied by Bernstein in 1912, who developed them as a probabilistic proof of the Weierstrass Approximation Theorem. He showed that any continuous function, $f(x)$ , on a closed interval $[a,b]$ can be uniformly approximated using Bernstein polynomials by, <PARAGRAPH_SEPARATOR> $$ B_{m}(x,f)=\\sum_{k=1}^{m}f\\left(a+\\frac{k-1}{m-1}(b-a)\\right){\\binom{m-1}{k-1}}\\left(\\frac{x-a}{b-a}\\right)^{k-1}\\left(\\frac{b-x}{b-a}\\right)^{m-k}, $$ <PARAGRAPH_SEPARATOR> for $a\\leq x\\leq b$ . The Bernstein–Weierstrass Approximation Theorem assures that as the degree of the polynomial increases to infinity the Bernstein polynomial approximation converges uniformly to the true function, i.e. $\\|B_{m}(\\cdot,f)-f(\\cdot)\\|_{\\infty}\\equiv$ $\\mathsf{s u p}_{a\\leq x\\leq b}|B_{m}(x,f)-f(x)|\\to0$ , as $m\\rightarrow\\infty$ (Lorentz, 1986). <PARAGRAPH_SEPARATOR> Bernstein polynomials are an attractive approach to density estimation as they are the simplest example of a polynomial approximation with a probabilistic interpretation. They also naturally lead to estimators with acceptable behavior near the boundaries (Leblanc, 2010). Vitale (1975) was the first to propose using Bernstein polynomials to produce smooth density estimates. Babu et al. (2002) investigated the asymptotic properties of using Bernstein polynomials to approximate bounded and continuous density functions. Kakizawa (2004) demonstrated that Bernstein polynomials can be used as a nonparametric prior for continuous densities, and Leblanc (2010) focused on a bias reduction approach using a Bernsteinbased estimator. Petrone (1999) performed nonparametric density estimation in a fully Bayesian setting using Bernstein polynomials. The asymptotic properties of this method were further investigated by Ghosal (2001) and Petrone and <PARAGRAPH_SEPARATOR> Wasserman (2002), and Barrientos et al. (2012) extended the ideas of Petrone (1999) to probability distributions with bounded supports."
  },
  {
    "qid": "statistic-posneg-1501-1-1",
    "gold_answer": "FALSE. While the formula shows that the leading term is $\\theta$, the term $\\frac{S_{a w}}{S_{a a}}N^{-1/2}$ introduces a bias of order $O(N^{-1/2})$. The context also mentions that $E(\\tilde{\\theta}) = \\theta + O(N^{-1})$, indicating that the estimator is asymptotically unbiased but has a small-order bias for finite $N$.",
    "question": "Does the formula for $\\widetilde{\\theta} = \\theta + \\frac{S_{a w}}{S_{a a}}N^{-1/2} + O_{p}(N^{-1})$ imply that $\\widetilde{\\theta}$ is an unbiased estimator of $\\theta$?",
    "question_context": "Meta-Analysis Approximations and Radial Plot Statistics\n\nThe paper discusses the asymptotic properties of estimates in meta-analysis, focusing on the joint normality of $(\\hat{\\theta}, \\hat{\\sigma}^2)$ with mean $(\\theta, \\sigma^2)$. It assumes biases of order $O(n^{-1})$, and provides various approximations and expansions for statistics like $\\widetilde{\\theta}$, $Z_1$, and $Q$. The context includes detailed derivations and expectations of these statistics, as well as special cases involving binomial distributions and radial plot statistics."
  },
  {
    "qid": "statistic-posneg-1090-1-0",
    "gold_answer": "TRUE. Explanation: The naive bootstrap's equal probability assignment ($n^{-1}$) for selecting centered residuals relies on the homoscedasticity assumption. Under homoscedasticity, the conditional variance of errors $\\varepsilon$ given $\\mathcal{X}$ is constant, making residuals exchangeable after centering. In heteroscedastic models, error variances depend on $\\mathcal{X}$, so residuals are not exchangeable, and this equal-weighting scheme would fail to preserve the conditional variance structure. The wild bootstrap addresses this by scaling residuals with external random variables ($V_i$) to mimic heteroscedasticity.",
    "question": "In the naive bootstrap procedure, the probability $P^{\\vartheta}\\Big(\\varepsilon_{i}^{\\mathrm{boot}}=\\widehat{\\varepsilon}_{j,b}-\\overline{{\\widehat{\\varepsilon}}}_{b}\\Big)=n^{-1}$ implies that each bootstrap error term is equally likely to be any of the centered residuals. Is this assumption valid only under homoscedasticity?",
    "question_context": "Bootstrap Methods in Nonparametric Regression: Homoscedastic vs. Heteroscedastic Models\n\nNaive bootstrap. We assume here that the model is homoscedastic, i.e. the conditional covariance operator of $\\varepsilon$ given $\\mathcal{X}$ does not depend on $\\mathcal{X}$ : for any $_g$ , $h\\in{\\mathcal{H}}$ $,E(\\langle\\varepsilon,g\\rangle\\langle\\varepsilon,h\\rangle|\\mathcal{X})=E(\\langle\\varepsilon,g\\rangle\\langle\\varepsilon,h\\rangle)$ . The bootstrap procedure consists of several steps: <PARAGRAPH_SEPARATOR> (1) For all $i=1,\\ldots,n$ , define $\\widehat{\\varepsilon}_{i,b}=\\mathcal{Y}_{i}-\\widehat{r}_{b}(\\mathcal{X}_{i})$ , where $b$ is a second smoothing parameter. <PARAGRAPH_SEPARATOR> (2) Let $\\begin{array}{r}{\\overline{{\\widehat{\\varepsilon}}}_{b}=n^{-1}\\sum_{i=1}^{n}\\widehat{\\varepsilon}_{i,b}}\\end{array}$ . Define new i.i.d. random elements $\\varepsilon_{1}^{\\mathrm{boot}},\\dots,\\varepsilon_{n}^{\\mathrm{boot}}$ εboot by: <PARAGRAPH_SEPARATOR> $$ P^{\\vartheta}\\Big(\\varepsilon_{i}^{\\mathrm{boot}}=\\widehat{\\varepsilon}_{j,b}-\\overline{{\\widehat{\\varepsilon}}}_{b}\\Big)=n^{-1} $$ <PARAGRAPH_SEPARATOR> for $i,j=1,\\dots,n$ , where $P^{\\vartheta}$ is the probability conditionally on the original sample $(\\mathcal{X}_{1},\\mathcal{Y}_{1}),\\ldots,(\\mathcal{X}_{n},\\mathcal{Y}_{n})$ . (3) For $i=1,\\ldots,n$ , let <PARAGRAPH_SEPARATOR> $\\mathcal{Y}_{i}^{\\mathrm{boot}}=\\widehat{r}_{b}(\\mathcal{X}_{i})+\\varepsilon_{i}^{\\mathrm{boot}},$ and denote ${\\mathcal{S}}^{\\mathrm{boot}}=(\\mathbb{X}_{i},\\mathbb{Y}_{i}^{\\mathrm{boot}})_{i=1,\\dots,n}$ . (4) Define $\\begin{array}{r}{\\widehat{r}_{h b}^{\\mathrm{boot}}(\\chi)=\\frac{\\sum_{i=1}^{n}{\\mathcal{Y}}_{i}^{\\mathrm{boot}}K\\left(h^{-1}d(\\mathcal{X}_{i},\\chi)\\right)}{\\sum_{i=1}^{n}K\\left(h^{-1}d(\\mathcal{X}_{i},\\chi)\\right)}.}\\end{array}$ <PARAGRAPH_SEPARATOR> Wild bootstrap. We assume here that the model can be heteroscedastic. With respect to the naive bootstrap, we need to change the second step: define $\\varepsilon_{i}^{\\mathrm{boot}}=\\widehat{\\varepsilon}_{i,b}V_{i}$ , where $V_{1},\\ldots,V_{n}$ are i.i.d. real valued random variables that are independent of the data $({\\mathcal{X}}_{i},{\\mathcal{Y}}_{i})(i=1,\\ldots,n)$ and  that satisfy $E(V_{1})=0$ and $E(V_{1}^{2})=1$ ."
  },
  {
    "qid": "statistic-posneg-1464-0-0",
    "gold_answer": "FALSE. The variational estimator $(\\hat{\\theta}_{n},\\hat{\\psi}_{n})$ maximizes an approximation to the log-likelihood, where the true conditional distribution $\\pi_{\\theta}(\\cdot\\mid X)$ is replaced by a simpler variational family $q(\\cdot;\\psi)$. This approximation introduces bias unless the variational family $\\mathcal{Q}$ contains the true conditional distribution $\\pi_{\\theta_{0}}$, which is generally not the case. Therefore, $(\\hat{\\theta}_{n},\\hat{\\psi}_{n})$ is not equivalent to $(\\theta_{0},\\pi_{\\theta_{0}})$ under typical model assumptions.",
    "question": "Is the variational estimator $(\\hat{\\theta}_{n},\\hat{\\psi}_{n})$ defined as the maximizer of the objective function $\\mathcal{L}_{n}(\\theta,\\psi;\\mathbf{X}_{n})$ equivalent to the true parameter $(\\theta_{0},\\pi_{\\theta_{0}})$ under the model assumptions?",
    "question_context": "Variational Inference in Mixture Models: Estimation and Asymptotic Properties\n\nWe now move to a formal definition of variational estimation. Blei, Kucukelbir, and McAuliffe (2017) presented a thorough introduction to variational inference and many relevant references. Let $X_{1},\\ldots,X_{n}$ be observed $\\boldsymbol{p}$ -variate data generated independently and identically from a distribution $P_{0}$ on a sample space $\\mathcal{X}$ . Let ${\\mathcal{P}}~=~\\{P_{\\theta}~:~\\theta~\\in~\\Theta\\}$ be a statistical model, where $\\Theta$ is an open subset of $\\mathbb{R}^{d}$ and each $P_{\\theta}$ has a density $\\begin{array}{r}{p_{\\theta}(x)=\\int_{\\mathcal Z}p_{\\theta}(x,z)d\\mu(z)}\\end{array}$ . Here $\\mu$ is a dominating measure on $\\mathcal{Z}\\subseteq\\mathbb{R}^{k}$ . We can conceptualize this data-generating process as first drawing independent latent random variables $Z_{1},\\ldots,Z_{n}$ from the marginal distribution $\\begin{array}{r}{p_{\\theta,Z}(z)=\\int_{\\mathcal{X}}p_{\\theta}(x,z)d x.}\\end{array}$ then drawing each $X_{i}$ given $Z_{i}$ from the conditional distribution $p_{\\theta,X\\mid Z}(x\\mid Z_{i})=p_{\\theta}(x,Z_{i})/p_{\\theta,Z}(Z_{i})$ . Of these, we only observe $X_{1},\\ldots,X_{n}$ . <PARAGRAPH_SEPARATOR> We are most interested in cases where $p_{\\theta}(x)$ cannot be written in closed-form in terms of elementary functions, as in many generalized linear mixed models (McCulloch and Neuhaus 2001) and nonlinear hierarchical models (Davidian and Giltinan 1995; Goldstein 2011). In these cases, calculating the log-likelihood of the observed data, $\\textstyle\\sum_{i=1}^{n}\\log p_{\\theta}(X_{i})$ , and its derivatives with respect to $\\theta$ requires  numerical integration. When the dimension of the latent variable is large, these numerical integrals are computationally expensive. <PARAGRAPH_SEPARATOR> Variational inference parameter estimates are obtained by maximizing a criterion function motivated as follows. Denote by $\\mathcal{Q}_{0}$ the set of densities dominated by $\\mu$ , and by $\\mathcal{Q}_{0}^{\\mathcal{X}}$ the set of all conditional densities dominated by $\\mu$ for all $x\\in\\mathcal X$ ; that is, all $s:\\mathcal{Z}\\times\\mathcal{X}\\to\\mathbb{R}$ such that $s(\\cdot\\mid x)\\in\\mathcal{Q}_{0}$ for all $x\\in\\mathcal X$ . Suppose that $P_{0}\\in\\mathcal{P}$ , so that $P_{0}=P_{\\theta_{0}}$ for some $\\theta_{0}\\in\\Theta$ . Then $\\theta_{0}$ and the true conditional distribution of the latent variable $\\pi_{\\theta_{0}}(z\\mid x):=p_{\\theta_{0}}(x,z)/p_{\\theta_{0}}(x)$ can be represented as <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{(\\theta_{0},\\pi_{\\theta_{0}})}\\ &{=\\underset{\\theta\\in\\Theta,s\\in\\mathcal{Q}_{0}^{\\mathcal{X}}}{\\arg\\operatorname*{max}}E_{P_{0}}\\left[\\int_{\\mathcal{Z}}\\log\\left(\\frac{\\hat{p}_{\\theta}(X,Z)}{s(Z\\mid X)}\\right)s(Z\\mid X)d\\mu(Z)\\right].}\\end{array} $$ <PARAGRAPH_SEPARATOR> To see this, first define <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{f_{0}(\\theta,s):=E_{P_{0}}\\left[-D_{\\mathrm{KL}}(s(\\cdot\\mid X)\\|\\pi_{\\theta}(\\cdot\\mid X))\\right]}\\ &{\\qquad=E_{P_{0}}\\left[\\int_{\\mathcal Z}\\log\\left(\\frac{\\pi_{\\theta}(Z\\mid X)}{s(Z\\mid X)}\\right)s(Z\\mid X)d\\mu(Z)\\right],}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $D_{\\mathrm{KL}}$ denotes the Kullback–Leibler (KL) divergence. Thus, $f_{0}(\\theta,s)$ is the expected KL divergence between $s(\\cdot\\mid X)$ and $\\pi_{\\theta}(\\cdot\\mid X)$ . By Gibbs’ inequality, $f_{0}(\\theta,s)\\leq0=f_{0}(\\theta_{0},\\pi_{\\theta_{0}})$ for all $(\\theta,s)~\\in~\\stackrel{\\cdot}{\\Theta}\\times\\mathcal{Q}_{0}^{\\chi}$ . Next, note that $\\theta_{0}$ maximizes $\\theta\\mapsto$ $\\begin{array}{r}{g_{0}(\\theta):=E_{P_{0}}[\\log\\frac{{p_{\\theta}(X)}}{{p_{0}(X)}}]=-D_{\\mathrm{KL}}(p_{0}\\|p_{\\theta})}\\end{array}$ . Therefore, $(\\theta_{0},\\pi_{\\theta_{0}})$ maximizes $(\\theta,s)\\mapsto f_{0}(\\theta,s)+g_{0}(\\theta)$ over $\\Theta\\times\\mathcal{Q}_{0}^{\\mathcal{X}}$ , and after some rearranging, we can see that this is equivalent to the representation in (1). <PARAGRAPH_SEPARATOR> The expectation-maximization (EM) algorithm can be motivated by (1) by replacing the unknown $P_{0}$ with the empirical distribution and alternating between optimization over $\\theta$ and s. Using similar reasoning to that presented above, this amounts to alternating between computing $\\theta_{(t)}:=\\arg\\operatorname*{max}_{\\theta\\in\\Theta}$ $\\begin{array}{r}{\\sum_{i=1}^{n}\\int_{\\mathcal Z}\\left[\\log p_{\\boldsymbol\\theta}(X_{i},Z)\\right]\\pi_{\\boldsymbol\\theta_{(t-1)}}(Z\\mid X_{i})d\\mu(Z)}\\end{array}$ , where $\\theta_{(t-1)}$ is the previous value of $\\theta$ , and computing $\\pi_{\\theta_{(t)}}(\\cdot\\mid X_{i})$ for each observed $X_{i}$ . However, if the marginal likelihood $p_{\\theta}(x)$ cannot be written in terms of elementary functions, then neither can $\\pi_{\\theta}$ , and hence the EM algorithm requires numerical integration. <PARAGRAPH_SEPARATOR> To construct a variational approximation to the loglikelihood, we replace the optimization over $\\mathcal{Q}_{0}^{\\mathcal{X}}$ in (1) with an optimization over $\\boldsymbol{\\mathcal{Q}}^{\\mathcal{X}}$ , where $\\mathcal{Q}$ is a smaller variational family of distributions, and as before $\\mathcal{Q}^{\\mathcal{X}}$ is the set of conditional distributions over $\\mathcal{X}$ such that $s(\\cdot\\mid x){\\mathrm{~}}\\in{\\mathrm{~}}{\\mathcal{Q}}$ for each $x\\in\\mathcal X$ . For example, $\\mathcal{Q}$ could consist of all independent products over each dimension of $z$ (known as mean-field variational inference), all multivariate Gaussian distributions, or all independent Gaussian distributions. For simplicity, we will assume throughout that $\\mathcal{Q}$ is indexed by a finite-dimensional Euclidean parameter $\\psi\\in\\Psi$ , so that every $s\\in\\mathcal{Q}^{\\mathcal{X}}$ can be identified with a density $s(\\cdot\\mid x)=q(\\cdot;\\psi(x))$ . We note that in some cases even when $\\mathcal{Q}$ is a semiparametric family, it can be shown that the optimal $q$ lies in a parametric sub-family with a known form, so that our results can still be applied (see, e.g., Wainwright and Jordan 2008, sec. 5.3). For families where this does not apply, our theory could be extended to incorporate semiparametric $\\mathcal{Q}$ . <PARAGRAPH_SEPARATOR> Let $\\Psi^{n}$ denote the $n$ -fold Cartesian product $\\Psi\\times\\dots\\times\\Psi$ . For $\\psi\\in{\\Psi}^{n}$ and $i\\in\\{1,\\ldots,n\\}$ , we will denote $\\psi_{i}~\\in~\\Psi$ the ith element of $\\psi$ . Given the observed data $X_{1},\\ldots,X_{n}$ , $\\Psi^{n}$ then parametrizes the set of variational conditional distributions over $X_{1},\\ldots,X_{n}$ , and each $\\psi_{i}$ parametrizes the variational conditional distribution $s(\\cdot\\mid X_{i})=q(\\cdot;\\psi_{i})$ . Given $\\mathcal{Q}$ and $\\Psi$ , the variational estimator of $\\theta$ , which we will denote $\\hat{\\theta}_{n}$ , and the variational conditional estimators $\\hat{\\psi}_{n}$ are the joint maximizers of the following objective function <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{(\\hat{\\theta}_{n},\\hat{\\psi}_{n}):=\\underset{\\theta\\in\\Theta,\\psi\\in\\Psi^{n}}{\\arg\\operatorname*{max}}\\displaystyle\\sum_{i=1}^{n}\\int\\log\\left(\\frac{p_{\\theta}(X_{i},Z_{i})}{q(Z_{i};\\psi_{i})}\\right)q(Z_{i};\\psi_{i})d\\mu(Z_{i})}\\ {=\\underset{\\theta\\in\\Theta,\\psi\\in\\Psi^{n}}{\\arg\\operatorname*{max}}\\mathcal{L}_{n}(\\theta,\\psi;\\mathbf{X}_{n}).\\qquad(2)}\\end{array} $$ <PARAGRAPH_SEPARATOR> We note that we are implicitly assuming that the full variational distribution over $(Z_{1},\\ldots,Z_{n})$ factors as $\\textstyle\\prod_{i=1}^{n}q(Z_{i};\\psi_{i})$ . However, since the true conditional distribution  of $(Z_{1},\\ldots,Z_{n})$ given $(X_{1},\\ldots,X_{n})$ factors as $\\textstyle\\prod_{i=1}^{n}\\pi_{\\theta_{0}}(Z_{i}\\mid X_{i})$ , the optimal variational distribution will  always factor as well, so this assumption comes with no loss of generality. <PARAGRAPH_SEPARATOR> A crucial piece of motivation for our work is that, since $\\mathcal{L}_{n}$ is typically not proportional to the log-likelihood, it is not clear what the asymptotic properties of the variational estimator $\\hat{\\theta}_{n}$ are. In many circumstances, the variational estimator is used for prediction. In such cases, scientists can evaluate the quality of the variational approximation using cross-validation or another held-out data technique. If, however, a scientist would like to go beyond prediction and interpret the point estimator (or, critically, its uncertainty) produced by a variational approximation, not knowing the properties of the estimator is a substantial hindrance. In particular, we would like to know whether $\\hat{\\theta}_{n}$ is consistent and, if it is consistent, what the asymptotic distribution of ${\\sqrt{n}}({\\hat{\\theta}}_{n}-\\theta_{0})$ is. <PARAGRAPH_SEPARATOR> Asymptotic properties of variational estimators have been studied in depth for certain specific models, yielding positive results regarding the consistency of variational estimators for Gaussian mixture models (Wang and Titterington 2006), exponential family models with missing values (Wang and Titterington 2004), Poisson mixed models as the cluster size and number of clusters both diverge (Hall, Ormerod, and Wand 2011; Hall et al. 2011), Markovian models with missing values (Hall, Humphreys, and Titterington 2002), and stochastic block models for social networks (Bickel et al. 2013). Of particular note are Hall, Ormerod, and Wand (2011) and Hall et al. (2011), who derive sharp asymptotics for Poisson regression with random cluster intercepts as both the number of clusters and observations per cluster diverge. Our work is distinct from these results in two ways. First, we provide results at a general level rather than for a specific model. Second, we focus on the asymptotic regime where the number of clusters is diverging, but the number of observations per cluster is stochastically bounded."
  },
  {
    "qid": "statistic-posneg-452-2-1",
    "gold_answer": "FALSE. While $\\nu(\\{a\\}) > 0$ is a sufficient condition for the almost sure convergence of ${\\hat{F}}_{n}(a)$ to $F_{0}(a)$, it does not necessarily imply $\\mu(\\{a\\}) > 0$. The inequality $\\nu \\leqslant 2\\mu$ ensures that $\\nu(\\{a\\}) > 0$ can hold even if $\\mu(\\{a\\}) = 0$, provided the marginal distributions of $L$ and $R$ assign positive mass to $\\{a\\}$.",
    "question": "Does the condition $\\nu(\\{a\\}) > 0$ imply $\\mu(\\{a\\}) > 0$ for the almost sure convergence of ${\\hat{F}}_{n}(a)$ to $F_{0}(a)$?",
    "question_context": "Consistency of Generalized Maximum Likelihood Estimators in Statistical Models\n\nLet $E(K)<\\infty$ .Then ${\\hat{F}}_{n}(a)\\to F_{0}(a)$ almost surely for each point a with $\\mu(\\{a\\})>0$ <PARAGRAPH_SEPARATOR> In the next corollary we state results for a measure $\\nu$ that depends on the distribution of $L$ and $R$ and is easier to interpret than $\\mu$ .We take $\\nu$ to be the sum of the marginal distributions of $L$ and $R$ <PARAGRAPH_SEPARATOR> $$ \\nu(B)=P(L\\in B)+P(R\\in B),\\quad B\\in\\mathcal{B}. $$ <PARAGRAPH_SEPARATOR> In view of the set inclusion <PARAGRAPH_SEPARATOR> $$ \\{L\\in B\\}\\cup\\{R\\in B\\}\\subset\\bigcup_{k=1}^{\\infty}{\\bigcup_{i=1}^{k}}\\{K=k,Y_{k,i}\\in B\\}, $$ <PARAGRAPH_SEPARATOR> wehave $\\nu(B)\\leqslant2\\mu(B)$ . Thus we immediately get the following corollary. <PARAGRAPH_SEPARATOR> # Corollary 2 <PARAGRAPH_SEPARATOR> Let $E(K)<\\infty$ .Then the following are true. (1) $\\int|{\\hat{F}}_{n}-F_{0}|d\\nu\\to0$ almostsurely. (2) ${\\hat{F}}_{n}(a)\\to F_{0}(a)$ almost surely for each point a with $\\nu(\\{a\\})>0$ <PARAGRAPH_SEPARATOR> # 3. Other consistency results <PARAGRAPH_SEPARATOR> In this section we shall show that under additional assumptions strong $L_{1}(\\mu)$ -consistency implies strong consistency in other topologies such as the topologies of weak convergence, pointwise convergence and uniform convergence. Throughout we always assume that $E(K)$ is finite so that $\\mu$ is a finite measure and $P(\\mathcal{Q}_{\\mu})=1$ by theorem 1, where <PARAGRAPH_SEPARATOR> $$ \\varOmega_{\\mu}=\\left\\{\\operatorname*{lim}_{n\\rightarrow\\infty}\\int\\left|{\\hat{F}}_{n}-F_{0}\\right|d\\mu=0\\right\\}. $$ <PARAGRAPH_SEPARATOR> Although the results of this section are formulated for the measure $\\mu$ defined in the previous section, they are true for any finite measure for which the GMLE is strongly $L_{1}$ consistent as only the finiteness of $\\mu$ and $P(\\mathcal{Q}_{\\mu})=1$ are used in their proofs. These proofs are deferred to section 5. <PARAGRAPH_SEPARATOR> Let $^{a}$ be a real number. We call $^{a}$ a support point of $\\mu$ ü $\\mu((a-\\epsilon,a+\\epsilon))>0$ for every $\\epsilon>0.$ We call $^{a}$ regular if $\\mu((a-\\epsilon,a])>0$ and $\\mu([a,a+\\epsilon))>0$ for all $\\epsilon>0$ .We call $a$ strongly regular if $\\mu((a-\\epsilon,a))>0$ and $\\mu([a,a+\\epsilon))>0$ for all $\\epsilon>0$ .We call $^{a}$ a point of increase of $F_{0}$ if $F_{0}(a+\\epsilon)-F_{0}(a-\\epsilon)>0$ for each $\\epsilon>0$ <PARAGRAPH_SEPARATOR> In view of the inequality $\\nu\\leqslant2\\mu,$ sufficient conditions for the first three of the above concepts are obtained by replacing $\\mu$ by $\\nu.$ As these sufficient conditions are in terms of the distribution of $L$ and $R$ they are easier to interpret and thus more meaningful from an applied point of view."
  },
  {
    "qid": "statistic-posneg-1953-0-0",
    "gold_answer": "FALSE. The LNA approximation requires the solution of the ODE system given by (4), (7), and (11) to obtain the Gaussian distribution of the residual process. However, the condition is not sufficient on its own; it also requires the initial conditions and the Jacobian matrix to be correctly specified and integrated. The LNA is derived by linearizing the CLE around the deterministic trajectory, and the accuracy of the approximation depends on the validity of this linearization and the correct initialization of the ODE system.",
    "question": "Is the condition for the LNA approximation of the CLE given by the ODE system in (4), (7), and (11) correct for obtaining the Gaussian distribution of the residual process?",
    "question_context": "Stochastic Kinetic Models and Bayesian Inference\n\nThe paper discusses stochastic kinetic models (MJP, CLE, LNA) and Bayesian inference techniques, including particle MCMC and delayed-acceptance methods. It introduces the chemical reaction network (CRN) with species and reactions, and describes the inference task for rate constants given observations."
  },
  {
    "qid": "statistic-posneg-1764-2-0",
    "gold_answer": "FALSE. The context suggests that $\\tilde{g}_{n,\\gamma}(x)$ is designed to provide a more efficient bias correction when $\\gamma$ is known, but it does not explicitly state that it is consistent. The estimator ${\\hat{g}}_{n}$ is mentioned as converging to the true frontier $g$, but $\\tilde{g}_{n,\\gamma}(x)$ is presented as a modification for efficiency in specific parametric models, not necessarily for consistency.",
    "question": "Is the estimator $\\tilde{g}_{n,\\gamma}(x)$ a consistent estimator of the true frontier $g(x)$ when $\\gamma$ is known and correctly specified?",
    "question_context": "Frontier Estimation via Kernel Regression with Beta Function Adjustment\n\nTo conclude, let us note that, even though ${\\hat{g}}_{n}$ converges to the true frontier $g$ in case of nonuniform conditional distributions, it is possible to design new estimators dedicated to particular parametric models. For instance, in case of model (12), estimator ${\\hat{g}}_{n}$ could be modified <PARAGRAPH_SEPARATOR> Fig. 4. The frontier $g_{2}$ (continuous line) and the $\\tilde{g}_{n,3}$ estimate (dashed line). The sample size is $n=500$ , $X$ is $B(2,2)$ distributed on [0, 1] and $\\gamma=3$ . <PARAGRAPH_SEPARATOR> to obtain <PARAGRAPH_SEPARATOR> $$ \\tilde{g}_{n,\\gamma}(x)=\\left(\\frac{1}{\\gamma B(1+p,\\gamma)}\\sum_{i=1}^{n}K_{h}(x-X_{i})Y_{i}^{p}\\Bigg/\\sum_{i=1}^{n}K_{h}(x-X_{i})\\right)^{1/p}, $$ <PARAGRAPH_SEPARATOR> where $B$ is the beta function defined by <PARAGRAPH_SEPARATOR> $$ B(a,b)=\\int_{0}^{1}u^{a-1}(1-u)^{b-1}\\mathrm{d}u. $$ <PARAGRAPH_SEPARATOR> Of course, ${\\hat{g}}_{n}$ corresponds to the particular case $\\tilde{g}_{n,1}$ . When $\\gamma$ is assumed to be known, the new multiplicative constant yields a very efficient bias correction, see Fig. 4 for an illustration. A part of our future work will consist in defining an estimator of $\\gamma$ and plugging it into $\\tilde{g}_{n,\\gamma}$ . New asymptotic results will be established. We also plan to investigate the asymptotic properties of local polynomial estimators based on the same ideas as those used for ${\\hat{g}}_{n}$ and $\\tilde{g}_{n,\\gamma}$ ."
  },
  {
    "qid": "statistic-posneg-768-7-1",
    "gold_answer": "TRUE. This inequality is pivotal in deriving the upper bound for $\\tilde{L}_{n}(\\alpha)-(\\alpha M_{n}^{(p)}+(1-\\alpha)M_{n}^{\\mathrm{np}})$, enabling the control of the log-likelihood ratio's deviation from its linear approximation. It is a standard tool in likelihood theory (e.g., Grenander, 1981) and ensures the quadratic decay needed for the proof.",
    "question": "Does the inequality $\\log(x)-(x-1)\\leq -\\frac{1}{4}(x-1)^{2}$ for $1/2<x<3/2$ play a critical role in bounding the normalized log-likelihood ratio $\\tilde{L}_{n}(\\alpha)$?",
    "question_context": "Parametric Maximum Likelihood Estimator Convergence\n\nThe paper discusses the conditions under which the maximum likelihood estimator (MLE) converges when the model is misspecified (i.e., $f_{0}\\notin\\mathcal{P}$). Key conditions include the Glivenko-Cantelli property of the model class, integrability of the envelope function, and specific convergence rates for bandwidth and perturbation terms. The paper also provides auxiliary lemmas supporting the main theorems."
  },
  {
    "qid": "statistic-posneg-854-3-2",
    "gold_answer": "FALSE. The context explicitly states that the PQL approach fails to produce consistent estimates for σ² when cluster sizes n_i are small, as in the COPD data. The paper argues against using PQL in such scenarios and instead advocates for simulation-based methods like SML or SM, with SML being more efficient.",
    "question": "Is the claim 'The PQL approach is recommended for estimating σ² in binary mixed models with small cluster sizes, such as the COPD data' supported by the context?",
    "question_context": "Binary Mixed Model Analysis: Likelihood vs. Moment Estimation in COPD Data\n\nThe paper discusses the application of binary mixed models to COPD data, focusing on the estimation of parameters β (covariate effects) and σ² (variance of family effects). The model assumes a logistic link function for the probability of binary responses, incorporating random family effects γ_i ~ N(0, σ²). The paper compares the performance of Simulated Maximum Likelihood (SML) and Simulated Moment (SM) methods, highlighting the inefficiency of the SM approach and the challenges in estimating σ², especially under model misspecification."
  },
  {
    "qid": "statistic-posneg-8-0-0",
    "gold_answer": "FALSE. The objective function F^{[k]} includes a term (1/2)∑_{s∈A} log[∑_{m=1}^M (β̂_s^{m[k]})^2 / M], which is part of the MDL criterion. This term penalizes the average squared coefficients across studies but does not directly penalize the sum of squared coefficients. The MDL criterion promotes sparsity by favoring models with fewer non-zero coefficients, but the mechanism is different from a direct sum-of-squares penalty.",
    "question": "In the iSLB algorithm, the objective function F^{[k]} includes a term that penalizes the sum of squared coefficients across studies to promote sparsity. Is this statement true based on the provided formulas?",
    "question_context": "Integrative Sparse Logistic Boosting for Cancer Genomic Marker Identification\n\nThe paper presents an integrative sparse logistic boosting (iSLB) algorithm for identifying cancer genomic markers across multiple studies. The algorithm involves iterative steps to select weak learners (genes) that balance goodness-of-fit and sparsity. The objective function includes a goodness-of-fit term and a minimum description length (MDL) criterion for sparsity. The algorithm allows different regression coefficients for the same gene across studies while maintaining the same sparsity structure."
  },
  {
    "qid": "statistic-posneg-1697-0-0",
    "gold_answer": "TRUE. The Adjusted Rand Index (AR) is designed to correct the Rand Index for chance by adjusting for the expected similarity of all pair-wise comparisons between clusterings that occur by random chance. The formula $$A R={\\frac{2(a d-b c)}{(a+b)(b+d)+(a+c)(c+d)}}$$ includes terms for pairs of data objects that are in the same cluster and same class (d), different clusters and different classes (a), same cluster but different classes (b), and different clusters but same class (c). The numerator (ad - bc) measures the concordant pairs minus the discordant pairs, while the denominator adjusts for the expected agreement under random clustering. This adjustment ensures that the index has an expected value of 0 for random clustering and 1 for perfect agreement, making it a robust measure for evaluating clustering quality.",
    "question": "The Adjusted Rand Index (AR) formula $$A R={\\frac{2(a d-b c)}{(a+b)(b+d)+(a+c)(c+d)}}$$ correctly accounts for chance agreements in clustering assignments by adjusting for the expected similarity between random clusterings. Is this statement true based on the context provided?",
    "question_context": "Clustering Quality Evaluation: Entropy and Adjusted Rand Index\n\nIn this section, a set of experiments are conducted using synthetic and real data to show the performance of the proposed FWAS mechanism. For the means of explanation, the original K-means algorithm with the proposed FWAS mechanism is called the FWAS-K-means algorithm in the following discussion. The experimental results are compared with the original Kmeans algorithm (Forgy, 1965; McQueen, 1967) and the w-K-means algorithm (Huang et al., 2005) based on three common clustering quality evaluation measures. <PARAGRAPH_SEPARATOR> # 4.1. Evaluation measures <PARAGRAPH_SEPARATOR> The performance of a clustering algorithm can be evaluated using various cluster validity measures (Maulik and Bandyopadhyay, 2002; Pakhira et al., 2004). When the class labels of experimental data are unknown, unsupervised measures are used for the evaluation task. The Sum of Square within-cluster Error (SSE), shown as Eq. (8), is a typical and popular unsupervised measure. The lower the SSE value that an algorithm generates, the better the clustering quality of the algorithm is. On the other hand, when the class labels of experimental data are known, the evaluation task is easily done by measuring the degree of correspondence between the cluster labels and the class labels. Entropy (Tan et al., 2005) and Adjusted Rand index (Hubert and Arabie, 1985) are two typical supervised measures. Entropy is a classification-oriented measure which calculates the degree that each cluster consists of data objects that come from a single class. Let $p_{i j}=m_{i j}/m_{i}$ be the probability that a member of cluster i belong to class $j$ where $m_{i}$ is the number of data objects within cluster i and $m_{i j}$ is the number of data objects of class $j$ within cluster i for $1\\leq i,j\\leq K$ . Based on the probabilities, the entropy of each cluster i is calculated by Eq. (22). <PARAGRAPH_SEPARATOR> $$ \\mathrm{Entropy}(i)=-\\sum_{j=1}^{K}p_{i j}\\times\\log_{2}p_{i j}\\quad\\mathrm{for}i=1,2,\\ldots,K. $$ <PARAGRAPH_SEPARATOR> The total entropy for the set of $K$ clusters is calculated as: <PARAGRAPH_SEPARATOR> $$ {\\mathrm{Entropy}}=\\sum_{i=1}^{K}\\left({\\frac{m_{i}}{m}}\\times{\\mathrm{Entropy}}(i)\\right) $$ <PARAGRAPH_SEPARATOR> where $m$ is the total number of data objects in the dataset. The less the Entropy value, the better the clustering quality of an algorithm is. Different from Entropy, the Adjusted Rand index is a similarity-oriented measure which calculates the degree that two data objects within the same cluster also belong to the same class. Therefore, the Adjusted Rand index is calculated from the ${\\binom{m}{2}}=m(m-1)/2$ similarities between all pairs of data objects where $m$ is the total number of data objects in the dataset. Let $a$ be the number of pairs of data objects having different classes and different clusters, b be the number of pairs of data objects having different classes but the same cluster, c be the number of pairs of data objects having the same class but different clusters, and $d$ be the number of pairs of data objects having the same class and the same cluster. Note that $a+b+c+d=m(m-1)/2$ . Assume that the number of clusters is given to be the same as the number of classes, the Adjusted Rand index can be calculated based on the four quantities by Eq. (24). <PARAGRAPH_SEPARATOR> $$ A R={\\frac{2(a d-b c)}{(a+b)(b+d)+(a+c)(c+d)}}. $$ <PARAGRAPH_SEPARATOR> The larger the Adjusted Rand index value that an algorithm generates, the better the clustering quality of the algorithm is. Among these three measures, SSE is especially important because the real world clustering applications seldom reveal information about the class labels of data. <PARAGRAPH_SEPARATOR> ![](images/c15117f2873f75e0fcda6046e687e04286f7f6e85b52da815611ec723308b101.jpg)  Fig. 3. Data distributions of the three synthetic datasets on the two relevant features. <PARAGRAPH_SEPARATOR> # 4.2. Experiments on synthetic datasets"
  },
  {
    "qid": "statistic-posneg-185-0-0",
    "gold_answer": "TRUE. The convergence of $Q_{p}(t)$ to a Brownian bridge under Haar measure is supported by the uniform distribution of $\\pmb{\\upsilon}_{p}$ over the unit sphere when $\\mathbf{U}_{p}$ is Haar-distributed. The normalization $\\sqrt{\\frac{p}{2}}$ and the centering term $\\frac{1}{p}$ ensure the process captures deviations from uniformity, leading to a Brownian bridge limit as $p \to \\infty$. This is a standard result in random matrix theory for Haar-distributed orthogonal matrices.",
    "question": "Is the claim that the stochastic process $Q_{p}(t)$ converges to a Brownian bridge when $\\mathbf{U}_{p}$ has a Haar measure over orthogonal matrices and $p \to \\infty$ consistent with the model assumptions?",
    "question_context": "Eigenvector Analysis of Large-Dimensional Sample Spatial Sign Covariance Matrices\n\nThe paper investigates the limiting behavior of eigenvectors of large sample spatial sign covariance matrices (SSCM) $\\mathbf{B}_{n}$. The model considers observations $\\mathbf{x}_{i} = \\omega_{i}\\mathbf{T}^{1/2}\\mathbf{z}_{i}$, where $\\omega_{i}$ are positive random scalars, $\\mathbf{T}$ is a positive-definite matrix, and $\\mathbf{z}_{i}$ are i.i.d. random vectors. The SSCM is defined as $\\mathbf{B}_{n} = \\frac{p}{n}\\sum_{i=1}^{n}\\frac{\\mathbf{x}_{i}\\mathbf{x}_{i}^{\top}}{\\|\\mathbf{x}_{i}\\|^{2}}$. The study focuses on the stochastic process $Q_{p}(t) := \\sqrt{\\frac{p}{2}}\\sum_{i=1}^{[pt]}(\\upsilon_{i}^{2} - \\frac{1}{p})$, where $\\upsilon_{i}$ are components of the eigenvector $\\mathbf{U}_{p}^{\top}\\pmb{\\eta}_{p}$. The goal is to examine the limiting properties of $\\pmb{\\upsilon}_{p}$ through $Q_{p}(t)$, which is linked to the difference between the eigenvector empirical spectral distribution (VESD) $\\widetilde{F}^{\\mathbf{B}_{n}}(x)$ and the empirical spectral distribution (ESD) $F^{\\mathbf{B}_{n}}(x)$."
  },
  {
    "qid": "statistic-posneg-1763-0-0",
    "gold_answer": "TRUE. The justification follows from Lemma 5, where $u = \\frac{g(x)}{g(y)} - 1$ and $p|u| \\leqslant p\\frac{L_{g}}{g_{\\mathrm{min}}}\\|x-y\\|^{\\alpha}$. Given $\\|x-y\\| \\leqslant h$ and $p h^{\\alpha} \\to 0$, for sufficiently large $n$, $p|u| \\leqslant \\ln 2$. Applying Lemma 4(i), $\\left|(1+u)^{p} - 1\\right| \\leqslant 2p|u| \\leqslant 2\\frac{L_{g}}{g_{\\mathrm{min}}}p h^{\\alpha}$, which proves the claim.",
    "question": "Is the claim that $\\left|\\left(\\frac{g(x)}{g(y)}\\right)^{p}-1\\right|\\leqslant2\\frac{L_{g}}{g_{\\mathrm{min}}}p h^{\\alpha}$ under the conditions $p h^{\\alpha}\\to0$ and $\\|x-y\\|\\leqslant h$ mathematically justified based on the provided lemmas?",
    "question_context": "Frontier Estimation via Kernel Regression: Asymptotic Normality and Convergence\n\nThe context discusses the asymptotic properties of kernel regression estimators for frontier estimation. It includes lemmas and proofs related to the convergence and asymptotic normality of estimators such as $\\widehat{g}_{n}(x)$, $\\widehat{r}_{n}(x)$, and $\\widehat{\\varphi}_{n}(x)$. Key formulas involve bounds on deviations, convergence in probability, and distributional limits."
  },
  {
    "qid": "statistic-posneg-406-0-0",
    "gold_answer": "TRUE. The rule's interpretation is correct because the misclassification rate of 0.6% is very low, indicating high accuracy. The paper explicitly states that low $R_{c}$ (associated with early growth stages) leads to contamination levels ≤ $100\\mathrm{Bq}\\mathrm{kg}^{-1}$ at harvest, and the low misclassification rate supports the robustness of this conclusion.",
    "question": "Is the classification rule 'IF $R_{c}<0.04$ THEN $Y=1$' correctly interpreted as indicating that lettuces with low $R_{c}$ (early growth stage) almost always have radioactive contamination ≤ $100\\mathrm{Bq}\\mathrm{kg}^{-1}$ at harvest, given the 0.6% misclassification rate?",
    "question_context": "Similarity Measure and Classification Tree Stability in Radioactive Contamination Analysis\n\nThe paper introduces a similarity measure to assess the stability of classification trees, specifically applied to radioactive contamination in lettuces. The medioid tree's root node division is based on $R_{c}$, with a classification rule: IF $R_{c}<0.04$ THEN $Y(={\\sqrt[{90}]{\\mathrm{Sr~activity}}})=1$. This leaf contains 1245 observations (25% of the sample) with only 0.6% misclassification. The rule indicates that a low $R_{c}$ is associated with lettuces in early growth stages, leading to contamination levels ≤ $100\\mathrm{Bq}\\mathrm{kg}^{-1}$ at harvest. The similarity measure uses uniform weights $q_{i}=1/(T+1)$ or depth-adjusted weights proportional to $\\mathbf{\\dot{1}}/2^{(L_{i}-1)}$."
  },
  {
    "qid": "statistic-posneg-147-0-1",
    "gold_answer": "TRUE. When $K=N$, each subset size $R_{i}$ is 1, because $R_{1}+R_{2}+...+R_{K}=N$ and $K=N$ implies $R_{i}=1$ for all $i$. In this case, the multinomial coefficient reduces to $N!/(1!1!\\ldots 1!)=N!$, which is the number of permutations of $N$ distinct objects. The subroutine PERMUT is designed to handle this case, enumerating all possible orderings of the $N$ distinct objects, as confirmed by the context stating that 'if $K=N$ all permutations of $N$ natural numbers $1,2,...,N$ are enumerated.'",
    "question": "Does the subroutine PERMUT generate all permutations of $N$ distinct objects when $K=N$?",
    "question_context": "Enumeration of Permutations of Multi-Sets with Fixed Repetition Numbers\n\nExact tests associated with $K$ -sample techniques, such as multi-response permutation procedures (Mielke, Berry and Brier, 1981; Mielke, Berry, Brockwell and Williams, 1981) which include the permutation analogue to one-way analysis of variance, require the generation of all permutations of multi-sets with fixed repetition numbers. SU BROUTINE PERMUT enumerates the complete set of all permutations of $N$ objectsconsidered $R_{1},R_{2},...,R_{K}$ at a time, i.e. $${\\binom{N}{R_{1}R_{2}\\ldots R_{K}}},$$ where $N$ is partitioned into $K$ non-ordered, non-empty subsets, $R_{1}+R_{2}+...+R_{K}=N $ and the sizes of subsets $R_{1},R_{2},...,R_{K}$ are fixed for all permutations. If $K=2$ all combinations are enumerated, and if $K=N$ all permutations of $N$ natural numbers $1,2,...,N$ are enumerated. A single call to PERMUT generates all permutations and these may be processed one at a time within PERMUTby calling user-supplied SU BROUTINE $J O B$ or by executing equivalent statements. The $K$ groups within each permutation are represented by the natural integers $1,2,...,K$ , but these may easily be recoded in SU BROUTINE $J O B$ to represent the integers $1,2,...,N$ Or any other natural numbers or alphameric symbols. Each permutation is represented by an integer between 1 and $N!/(R_{1}!R_{2}!\\ldots R_{K}!),$ i.e. the multinomial coefficient."
  },
  {
    "qid": "statistic-posneg-462-0-0",
    "gold_answer": "FALSE. The equation is a valid transformation for generalized Pareto distribution (GPD) when $U_i$ are uniform (0,1), but the correct GPD data-generating equation should be $Y_i = a + \\frac{\\sigma}{\\gamma}(U_i^{-\\gamma} - 1)$ only when $\\gamma \\neq 0$. For $\\gamma = 0$, it should reduce to the exponential distribution case. The given equation implicitly assumes $\\gamma \\neq 0$ without stating this constraint, which is a critical model assumption for extreme value theory applications.",
    "question": "Is the data-generating equation $Y_{i}=a+\\frac{\\sigma}{\\gamma}\\left(U_{i}^{-\\gamma}-1\\right)$ correctly specified for modeling extreme values (solar flare brightness) above a threshold $a$ using generalized Pareto distribution?",
    "question_context": "Generalized Fiducial Inference for Extreme Value Quantiles in Solar Flare Brightness Data\n\nThe instrument AIA captures images of the Sun in eight different wavelengths every $12\\mathsf{s e c}$ , providing more than 70,000 high-resolution images per day; see Figure 7 for two examples. The image size is $4096\\times4096$ pixels, which amounts to a total of 1.5 terabytes compressed data per day. An uncompressed and preprocessed version of the data can be obtained from Schuh et al. (2013). Here, each image was partitioned into $64\\times64$ squared and equi-sized sub-images, each consists of $64\\times64$ pixels. For each sub-image, 10 summary statistics were computed, such as the average and the standard deviation of the pixel values. <PARAGRAPH_SEPARATOR> A solar flare is a sudden eruption of high-energy radiation from the Sun’s surface, which could cause disturbances on communication and power systems on the Earth. In those images captured by AIA, such solar flares are characterized by extremely bright spots; see the right panel of Figure 7 for an example. <PARAGRAPH_SEPARATOR> Wandler and Hannig (2012) provided a solution for estimating extremes using GFI. Their approach is based on modeling values over large threshold using a generalized Pareto distribution (Pickands III 1975). The data-generating equation is <PARAGRAPH_SEPARATOR> $$ Y_{i}=a+\\frac{\\sigma}{\\gamma}\\left(U_{i}^{-\\gamma}-1\\right),i=1,\\ldots,n, $$ <PARAGRAPH_SEPARATOR> where $U_{i}$ are iid $U(0,1)$ , $^{a}$ is a known threshold, $\\sigma$ is a scale parameter, and $\\gamma$ is a shape parameters. The corresponding <PARAGRAPH_SEPARATOR> likelihood and Jacobian (3) are <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\displaystyle f({\\bf{y}};\\sigma,\\gamma)=\\sigma^{-n}\\prod_{i=1}^{n}\\left(1+\\frac{\\gamma}{\\sigma}(x_{i}-a)\\right)^{-1-1/\\gamma}I_{(a,\\infty)}(x_{i}),}}\\ {{\\displaystyle J(\\gamma,\\sigma,\\gamma)=\\gamma^{-2}D\\left(\\begin{array}{c c}{{x_{1}-a}}&{{(1+\\frac{\\gamma}{\\sigma}(x_{1}-a))\\log(1+\\frac{\\gamma}{\\sigma}(x_{1}-a))}}\\ {{\\vdots}}&{{\\vdots}}\\ {{x_{n}-a}}&{{(1+\\frac{\\gamma}{\\sigma}(x_{n}-a))\\log(1+\\frac{\\gamma}{\\sigma}(x_{n}-a))}}\\end{array}\\right).}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Wandler and Hannig (2012) provided an MCMC algorithm for sampling from the fiducial distribution of the generalized Pareto parameters for small datasets. A sample from fiducial distribution for a $\\beta$ -percentile is then obtained by plugging the fiducial samples $(\\sigma^{*},\\gamma^{*})$ into the inverse distribution function <PARAGRAPH_SEPARATOR> $$ q^{*}=a+\\frac{\\sigma^{*}}{\\gamma^{*}}\\left((1-\\beta)^{-\\gamma^{*}}-1\\right). $$ <PARAGRAPH_SEPARATOR> For this solar dataset, we used the averaged pixel values (summary statistics P2) computed from Schuh et al. (2013) and the proposed method G to parallelize the computations. The number of images was 7697. Method G combined sample of $(\\sigma^{*},\\gamma^{*})$ to generate fiducial distribution for the $\\beta=99.999\\%$ , $99.9999\\%$ , and $99.99999\\%$ percentiles of the brightness. Figure 8 reports the Gaussian kernel-based estimates for the fiducial densities of these the brightness. These densities are shown in can help astronomers determine the frequency, predict the occurrences of the solar flares, and understand the limitations of their instruments. Figure 9 displays the confidence curve for the 99.9999 percentile for the solar flare brightness. The $95\\%$ confidence interval is (250.8, 253.0) and a solar flare of brightness in this range is likely to happen with 1 in a million chance. The fiducial probability of brightness greater than 253 is also computed and displayed in Figure 9. <PARAGRAPH_SEPARATOR> The simulations were run on UCDavis Department of Statistics computer cluster, each node of the cluster is equipped with a 32-core AMD Opteron(TM) Processor 6272. The program took about $15s e c$ to finish the fiducial sample generation process when 32 workers are in work and it took about 80 sec when only 4 workers are in place. The number of MCMC runs for each workers was 50,000."
  },
  {
    "qid": "statistic-posneg-1425-0-0",
    "gold_answer": "TRUE. The space-time filter specification, represented by the Kronecker product of matrices $C$ and $B$, inherently imposes the restriction $\\theta = -\\rho \\times \\phi$ on the space-time interaction parameter. This restriction simplifies the estimation process by providing a convenient factorization of the variance-covariance matrix. In contrast, the non-filter model, such as the time-space dynamic model proposed by Anselin (1988), treats $\\theta$ as a free parameter to be estimated, allowing for more flexibility but complicating the estimation process.",
    "question": "Is it true that the space-time filter specification imposes the restriction $\\theta = -\\rho \\times \\phi$ on the space-time interaction parameter, whereas the non-filter model treats $\\theta$ as a free parameter?",
    "question_context": "Space-Time Filter Specification in Panel Data Models\n\nIn Section 2, we set forth the space–time filter specification for the panel data model disturbances. This includes subsections that discuss alternative specifications that arise from: treatment of the initial period observations as endogenous versus exogenous, and past practices that ignore imposition of the theoretical separability restriction on the parameters implied by the space–time filter. Throughout the paper we refer to our proposed specification as the space–time filter specification or filter model and use the term non-filter model or non-filter specification to reference models that do not rely on a separable space and time filter. We fully recognize that quite generally any specification taking the form $S(\\rho,\\phi)\\varepsilon$ involving parameters for space and time dependence $(\\rho,\\phi)$ can be viewed as a space–time ‘filter’, but find the filter/nonfilter moniker helpful in our discussion. The MCMC estimation method is set forth in Section 3, where the issue of block sampling for the random effects parameters is taken up. Results from a Monte Carlo experiment are presented in Section 4. An applied illustration based on a space–time panel of 49 US states over the period 1977–1997 is the subject of Section 5."
  },
  {
    "qid": "statistic-posneg-1446-0-1",
    "gold_answer": "FALSE. The formula $t_{i j}=Q_{j}\\prod_{r=i+1}^{j-1}P_{r}$ for $i < j$ is incorrect in the context of the described model. Since the levels are numbered in decreasing order of response probabilities ($P_{1}>P_{2}>...>P_{m}$), moving from a higher-numbered level (lower $P_j$) to a lower-numbered level (higher $P_j$) should involve products of $Q_r$ (zero-response probabilities) for the intermediate levels, not $P_r$ (positive-response probabilities). The correct formula should be $t_{i j}=Q_{j}\\prod_{r=i+1}^{j-1}Q_{r}$ for $i < j$, as moving to a lower-numbered level requires zero responses at all intermediate levels.",
    "question": "Is the transition probability $t_{i j}=Q_{j}\\prod_{r=i+1}^{j-1}P_{r}$ correctly defined for the case when $i < j$, given that the levels are numbered in decreasing order of response probabilities ($P_{1}>P_{2}>...>P_{m}$)?",
    "question_context": "Markov Chain Model in Sequential Estimation of Quantal Response Curves\n\nThe new method of estimation suggested in $\\S4$ for Routine 2 can be viewed in terms of a Markov chain. Define the state of a sequence of observations on Routine 2 only when there has been a change of response type. The state of the sequence can be defined by the estimator $w_{\\ell}$ which would be made at such a change of response type, and specified as a 'peak' or a ‘valley' (denoted by $\\pmb{\\mathcal{P}}$ or $\\pmb{v}$ below). If (1) holds, there will be an infinite number of states, but for most calculations it is convenient to put bounds on the levels, such as $P(z)=1{\\cdot}0$ for $z\\geqslant10$ and $\\pmb{{\\mathcal{P}}}(z)=0$ for $z\\leqslant-10$ ,where ${\\pmb z}={\\pmb\\alpha}+\\beta{\\pmb x}$ .If such a restriction is imposed, there will be a finite number of levels, say $\\textcircled{m}$ , and the set of possible states of the system is $\\{S\\underline{{{p}}},S_{\\underline{{{8}}}}^{p},...,S_{m}^{p},S_{1}^{v},S_{2}^{v},...,S_{m}^{v}\\}$. Denote the probability of obtaining a positive response at any level $j$ by $P_{f}$ ,and the corresponding probability of a zero response $Q_{j},\\mathrm{for}j=1,2,...,m.$ Let the levels be numbered in decreasing order, so that $P_{1}>P_{2}>...>P_{m}.$ . and by the restriction suggested above, let $\\bar{P}_{m}=0{\\cdot}0$ , and $\\mathcal{P}_{1}=1{\\cdot}0$. Given that the sequence is at a state $\\mathcal{S}_{\\widehat{\\mathfrak{d}}}^{p}$ , then only valleys $\\{S_{i+1}^{v},...,S_{m}^{v}\\}$ are possible as the next state. The probability of arriving at $\\bar{S_{j}^{o}}$ from $\\ensuremath{\\boldsymbol{S}}_{\\mathcal{T}}^{p}$ is $t_{i j}=Q_{j}\\prod_{r=i+1}^{j-1}P_{r}(i<j).$ Similarly the probability of arriving at $S_{\\pmb{\\mathscr{G}}}^{\\pmb{p}}$ from $\\mathcal{S}_{\\mathfrak{t}}^{\\mathfrak{v}}$ is $t_{i j}=P_{f}\\prod_{r=j+1}^{i-1}Q_{r}\\quad(i>j).$ The matrix of transition probabilities of the chain can be written as $\\mathbf{T}=\\left[{\\stackrel{\\mathbf{0}}{\\dots}}\\stackrel{\\vdots}{\\dots}\\right],$ where A and B are $m\\times m$ lower and upper triangle matrices in $t_{i j}$ respectively, with zeros on the diagonal. This Markov chain is periodic with period 2. There is a very strong dependence between successive $\\pmb{w_{\\ell}}\\textmd{s}$ and in fact there is still a very strong dependence between averages of successive pairs of $w_{i}^{\\mathfrak{s}}$. For a sequence started at any given state, let $\\mathbf{a}^{(n)}=(a_{1}^{(n)},a_{2}^{(n)},...,a_{m}^{(n)}),\\qquad\\mathbf{b}^{(n)}=(b_{1}^{(n)},b_{2}^{(n)},...,b_{m}^{(n)})$ denote the probabilities that the nth change of response type is a specified peak or valley respectively. Clearly, there can be no valley at level l, and no peak at level $\\mathbf{\\nabla}\\ m$ so that $a_{m}^{(n)}=b_{1}^{(n)}=0$ for all $\\pmb{\\mathscr{n}}$ . We have recurrence relations for a and $\\mathbf{b}$: $\\mathbf{a}^{(n+1)}=\\mathbf{B}\\mathbf{b}^{(n)}\\quad{\\mathrm{and}}\\quad\\mathbf{b}^{(n+1)}=\\mathbf{A}\\mathbf{a}^{(n)}.$ These can be written as $\\mathbf{a}^{(n+2)}=\\mathbf{B}\\mathbf{A}\\mathbf{a}^{(n)},\\quad\\mathbf{b}^{(n+2)}=\\mathbf{A}\\mathbf{B}\\mathbf{b}^{(n)}.$ As the sample size increases, ${\\mathfrak{a}}^{(\\mathfrak{n})}$ and $\\mathbf{b}^{(n)}$ approach a steady state and the asymptotic distributions of peaks and valleys can be obtained by calculating the normalized principal eigenvectors of BA and AB."
  },
  {
    "qid": "statistic-posneg-1029-0-3",
    "gold_answer": "FALSE. The paper correctly establishes this convergence by showing that the terms involving the second derivatives of the model function and the penalty tend to zero, leaving the leading term to converge to 2Δ₁(θ₀).",
    "question": "Does the paper incorrectly claim that the second-order partial derivative matrix of the penalized objective function converges in probability to 2Δ₁(θ₀)?",
    "question_context": "Asymptotic Properties of Penalized Spline Estimators in Single-Index Models\n\nThe paper establishes two lemmas in preparation for the proof of the main theorems. Lemma A states that under Conditions 1 and 2, as n approaches infinity, the average of the weighted errors converges almost surely to a constant τ. Lemma B states that under Conditions 2 and 4, the normalized sum of the derivatives of the model function with respect to the parameters, weighted by the inverse of the working matrices, converges in distribution to a multivariate normal distribution with mean zero and variance-covariance matrix Δ₂(θ₀). The proofs involve detailed derivations and applications of the Strong Law of Large Numbers and the Lindeberg-Feller Central Limit Theorem."
  },
  {
    "qid": "statistic-posneg-499-0-0",
    "gold_answer": "FALSE. The correct formula should account for the reflection principle and the specific conditions under which the net loss does not reach $N$. The given formula does not fully incorporate these constraints, leading to an incorrect representation of the number of valid paths.",
    "question": "Is the formula $a_{\\mathfrak{s}}^{p,(N)}=C_{p}^{\\mathfrak{s}}-C_{p+N}^{\\mathfrak{s}}$ correct for the number of sets of games where the net loss never reaches $N$ and the final net holding is $N+n-2p$?",
    "question_context": "Probability Analysis in the Game of Heads and Tails\n\nM. Levy, in his paper, investigates the behavior of the gain of one of the players in a game of heads and tails by considering arithmetic triangles that are modifications of Pascal's Arithmetic Triangle. The paper discusses the probabilities of certain events, such as the final gain and the extreme values of the gain during a set of games, using various combinatorial and asymptotic formulas."
  },
  {
    "qid": "statistic-posneg-860-0-0",
    "gold_answer": "FALSE. Proposition 1(i) states that $\\operatorname*{lim}_{n\\to\\infty}\\operatorname{var}(\\Pi_{\\mathrm{SBAG},M,m})/\\operatorname{var}(\\Pi_{\\mathrm{BAG},m})=(1-\\rho)/(1+\\rho)$ for any $\\rho\\in(0,1)$. Since $(1-\\rho)/(1+\\rho) < 1$ for all $\\rho \\in (0,1)$, the variance of sequential bagging is actually less than that of direct bagging, not greater.",
    "question": "Is the statement 'The variance of sequential bagging classifier $\\Pi_{\\mathrm{SBAG},M,m}$ is always greater than that of the direct bagging classifier $\\Pi_{\\mathrm{BAG},m}$ for any $\\rho \\in (0,1)$' true based on Proposition 1(i)?",
    "question_context": "Sequential Bagging for Nearest Neighbor Classification\n\nFor a formal investigation we consider application of sequential bagging to the nearest neighbour classifier, which provides a natural platform for analysing the theoretical effects of bagging. Breiman (1996) remarked that the $n$ out of $n$ bagged nearest neighbour classifier does not converge to the Bayes classifier. Hall & Samworth (2005) showed that the above deficiency can be remedied by changing the bootstrap resample size from $n$ to some smaller value $m$ . Samworth (2012) derived a formula for the optimal m which minimizes the asymptotic regret of the bagged classifier. We shall focus on the $m$ out of $n$ bootstrap and investigate the effects of sequential bagging on nearest neighbour classifiers. Similar to Hall & Samworth (2005), we assume that each bagging round is based on infinitely many bootstrap resamples. <PARAGRAPH_SEPARATOR> Consider for simplicity a two-class problem with $k=2$ , although our findings can readily be generalized to cases where $k>2$ . A nearest neighbour classifier, trained on a learning set $\\mathcal{L}$ , assigns a point $x_{0}$ to class 1 or class 2 according to whether $x_{0}$ is closest to an observed data point from class 1 or class 2, respectively. For any $x\\in{\\mathcal{X}}$ , let $\\{(X_{(i)}(x),Y_{(i)}(x)):i=1,\\dots,n\\}$ be a permutation of $\\mathcal{L}$ such that $X_{(i)}(x)$ is the $i$ th nearest to $x$ among $\\{X_{1},\\ldots,X_{n}\\}$ . The $m$ out of $n$ bagged nearest neighbour classifier, which we denote by $T_{\\mathrm{BAG},m}$ , assigns $x_{0}$ to class 1 if $\\begin{array}{r}{\\Pi_{\\mathrm{BAG},m}\\equiv\\sum_{i=1}^{n}V_{n,m,i}\\mathbb{I}\\{Y_{(i)}(x_{0})=1\\}>1/2}\\end{array}$ , and to class 2 otherwise, <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{V_{n,m,i}=\\{1-(i-1)/n\\}^{m}-(1-i/n)^{m}\\quad(i=1,\\ldots,n)}\\end{array} $$ <PARAGRAPH_SEPARATOR> if resampling is done with replacement, or <PARAGRAPH_SEPARATOR> $$ V_{n,m,i}={\\binom{n-i}{m-1}}{\\binom{n}{m}}^{-1}\\mathbb{I}(i\\leqslant n-m+1)\\quad(i=1,\\dots,n) $$ <PARAGRAPH_SEPARATOR> if resampling is done without replacement. Note that $T_{\\mathrm{BAG},m}$ has the same first-order asymptotic properties whether $V_{n,m,i}$ is given by (1) or (2). <PARAGRAPH_SEPARATOR> If proper densities, $f$ and $g$ say, are defined on $\\mathcal{X}$ for classes 1 and 2 with prior probabilities $\\pi_{f}$ and $\\pi_{g}=1-\\pi_{f}$ , respectively, then the Bayes rule assigns $x_{0}$ to class 1 if <PARAGRAPH_SEPARATOR> $$ q(x_{0})=\\pi_{f}f(x_{0})/\\{\\pi_{f}f(x_{0})+\\pi_{g}g(x_{0})\\}>1/2. $$ <PARAGRAPH_SEPARATOR> Hall & Samworth (2005) showed that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\operatorname{pr}\\{T_{\\mathrm{BAG},m}(\\mathcal{L},x_{0})=1\\}}\\end{array}$ converges to the Bayes rule $\\mathbb{I}\\{q(x_{0})>1/2\\}$ as $\\mathrm{lim}_{n\\to\\infty}m/n=\\ell\\downarrow0$ . Thus, $T_{\\mathrm{BAG},m}$ can be made consistent with the Bayes classifier provided that $m$ is chosen of order $o(n)$ . <PARAGRAPH_SEPARATOR> Application of sequential bagging leads to a classifier $T_{\\mathrm{SBAG},M,m}$ , which assigns $x_{0}$ to class 1 if and only if $\\begin{array}{r}{\\Pi_{\\mathrm{SBAG},M,m}\\equiv\\dot{\\sum}_{k=1}^{n-1}\\sum_{j=1}^{n}{V_{n-1,M,k}V_{n,m,j}}\\mathbb{I}\\{Y_{(k+1)}(X_{(j)}(x_{0}))=1\\}>1/2.}\\end{array}$ It suffices for our purpose to consider the case where the resampling fractions $M/n$ and $m/n$ converge to the same limit $\\ell$ . In the Appendix we prove the following proposition, which compares the asymptotic properties of $T_{\\mathrm{SBAG},M,m}$ and $T_{\\mathrm{BAG},m}$ under different resampling fractions $\\ell$ and with a fixed $x_{0}\\in\\mathcal{X}$ . Following Hall & Samworth (2005), we take $\\rho=\\exp(-\\ell)$ for with-replacement bagging and $\\rho=1-\\ell$ for without-replacement bagging. <PARAGRAPH_SEPARATOR> PROPOSITION 1. Assume that $f$ and $g$ are continuous and that $q\\left(x_{0}\\right)\\in\\left(0,1\\right)$ . Then: <PARAGRAPH_SEPARATOR> (i) $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\operatorname{var}(\\Pi_{\\mathrm{SBAG},M,m})/\\operatorname{var}(\\Pi_{\\mathrm{BAG},m})=(1-\\rho)/(1+\\rho)f o}\\end{array}$ r any $\\rho\\in(0,1)$ ; (ii) for $q(x_{0})\\pm1/2$ we have, as $\\rho\\uparrow1$ , <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\mathrm{pr}\\{T^{*}(\\mathcal{L},x_{0})=1\\}=\\mathbb{I}\\{q(x_{0})>1/2\\}+\\{q(x_{0})-1/2\\}^{-2}O(|1-\\rho|^{\\alpha}), $$ <PARAGRAPH_SEPARATOR> where $\\alpha=1$ or 2 according to whether $T^{*}=T_{\\mathrm{BAG},m}$ or $T_{\\mathrm{SBAG},M,m}$ , respectively. <PARAGRAPH_SEPARATOR> Proposition 1(i) shows that majority voting based on sequential bagging is less variable than that based on direct $m$ out of $\\cdot_{n}$ bagging, irrespective of the value of $\\rho$ . Indeed, as $\\rho\\to1$ , i.e., as $\\ell\\to0$ , $\\Pi_{\\mathrm{SBAG},M,m}$ has a variance which is negligibly small compared to that of $\\Pi_{\\mathrm{BAG},m}$ . Proposition 1(ii) implies that, for a fixed $q(x_{0})\\neq1/2$ , both $T_{\\mathrm{SBAG},M,m}$ and $T_{\\mathrm{BAG},m}$ converge to the Bayes classifier if $M$ and $m$ are chosen of order $o(n)$ . The true gain of sequential bagging is brought out when $q(x_{0})$ lies in a shrinking neighbourhood of $1/2$ , where classification is most difficult. Here we see that $T_{\\mathrm{BAG},m}$ is consistent with the Bayes rule whenever $q(x_{0})$ differs from $1/2$ by a distance of order greater than $O(|1-\\rho|^{1/2})$ as $\\rho\\uparrow1$ . In the case of sequential bagging, consistency with the Bayes rule is assured whenever the distance $|q(x_{0})-1/2|$ has order exceeding $O(|1-\\rho|)$ , a condition less stringent than that required by $T_{\\mathrm{BAG},m}$ . It can be shown, by considering higher-order cumulants in expansions of Gram–Charlier type, that $(1-\\rho)^{-1/2}\\{\\Pi_{\\mathrm{BAG},m}-$ $q(x_{0})\\}$ and $(1-\\rho)^{-1}\\{\\Pi_{\\mathrm{SBAG},M,m}-q(x_{0})\\}$ converge weakly to some nondegenerate limits as $\\rho\\uparrow1$ , so that the orders given above for the critical distances are indeed sharp. <PARAGRAPH_SEPARATOR> The results of Proposition 1 cannot be revealed by considering the single case $\\rho=1$ , that is, $M,m=$ $o(n)$ , as has been the convention in formal validation of the $m$ out of $n$ bootstrap. A shift of focus from $\\rho=1$ to a more liberal regime $\\rho\\uparrow1$ helps to alleviate the risk posed by backtracking from a limiting case to finite-sample applications, thus providing a practically more informative comparison between sequential and $m$ out of $n$ baggings."
  },
  {
    "qid": "statistic-posneg-1130-0-2",
    "gold_answer": "TRUE. The context explicitly states that in the SAMSE plug-in method, the variance of $\\check{\\psi}_{r}$ is dominated by the leading term of the squared bias, not the variance. This is a key distinction between the AMSE and SAMSE methods, where the bias-variance trade-off differs.",
    "question": "Is it false that the variance of $\\check{\\psi}_{r}$ in the SAMSE plug-in method is dominated by the leading term of the variance rather than the squared bias?",
    "question_context": "Convergence Rates for Bandwidth Matrix Selectors in Kernel Density Estimation\n\nRemark 3. The asymptotic properties of $\\check{\\mathbf{H}}_{\\mathrm{AMSE}}$ are superior to those of $\\check{\\mathbf{H}}_{\\mathrm{SAMSE}}$ : Nonetheless, the difference in rates of convergence is not great. In particular, for the important bivariate case the relative rate of convergence to $\\mathbf{H}_{\\mathrm{AMISE}}$ for $\\check{\\mathbf{H}}_{\\mathrm{AMSE}}$ is $n^{-2/7}$ and for $\\check{\\mathbf{H}}_{\\mathrm{SAMSE}}$ is $n^{-1/4}$ : Even for a sample of size $n=100,000$ the ratio of $n^{-2/7}$ to $n^{-1/4}$ is only about 1.5, so comparison of the convergence rates alone will provide little guidance as to whether AMSE or SAMSE approaches should be preferred in practice. <PARAGRAPH_SEPARATOR> Remark 4. The relative rate of convergence for a plug-in selector of a diagonal bandwidth matrix $n^{-\\operatorname*{min}(8,d+4)/(2d+12)}$ ; as demonstrated by Wand and Jones [12]. This rate is faster than those for the full bandwidth selectors. Intuitively speaking, this indicates that choosing the orientation of the kernel functions is the most difficult aspect of the bandwidth selection problem for both AMSE and SAMSE plug-in methods. <PARAGRAPH_SEPARATOR> Remark 5. It is straightforward to show that <PARAGRAPH_SEPARATOR> $$ \\operatorname{vech}(\\mathbf{H}_{\\mathrm{AMISE}}-\\mathbf{H}_{\\mathrm{MISE}})=O(\\mathbf{J}_{d^{*}}n^{-2/(d+4)})\\operatorname{vech}\\mathbf{H}_{\\mathrm{MISE}} $$ <PARAGRAPH_SEPARATOR> so that the discrepancy between $\\mathbf{H}_{\\mathrm{AMISE}}$ and $\\mathbf{H}_{\\mathrm{MISE}}$ is asymptotically negligible in comparison to the relative rate of convergence of $\\check{\\mathbf{H}}_{\\mathrm{SAMSE}}$ to $\\mathbf{H}_{\\mathrm{AMISE}}$ : However, the discrepancy between $\\mathbf{H}_{\\mathrm{AMISE}}$ and $\\mathbf{H}_{\\mathrm{MISE}}$ dominates the AMSE rate from Theorem 1 for $d>4$ : <PARAGRAPH_SEPARATOR> Proof of Theorem 1. From Wand and Jones [12] we know that the estimator $\\check{\\psi}_{r}$ has slowest rate if at least one element of $r$ is odd because it is impossible to annihilate the leading term in the bias in this instance. When $|r|=4$ the optimal pilot bandwidth for these functional estimators is $g_{r,\\mathrm{AMSE}}=O(n^{-2/(d+12)})$ ; giving <PARAGRAPH_SEPARATOR> $$ s\\check{\\psi}_{r}(g_{r,\\mathrm{AMSE}})=O(g_{r,\\mathrm{AMSE}}^{2})=O(n^{-4/(d+12)}), $$ <PARAGRAPH_SEPARATOR> $$ \\operatorname{Var}\\check{\\psi}_{r}(g_{r,\\mathrm{AMSE}})=O(n^{-2}g_{r,\\mathrm{AMSE}}^{-d-8})=O(n^{-8/(d+12)}). $$ <PARAGRAPH_SEPARATOR> It follows that <PARAGRAPH_SEPARATOR> $$ \\mathbb{E}[D_{\\mathbf{H}}(\\mathrm{PI}-\\mathrm{AMISE})(\\mathbf{H}_{\\mathrm{AMISE}})]=O(\\mathbf{J}_{d^{*}}n^{-4/(d+12)}){\\mathrm{vech~H}}_{\\mathrm{AMISE}} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathrm{Var}[D_{\\mathbf{H}}(\\mathrm{PI}-\\mathrm{AMISE})(\\mathbf{H}_{\\mathrm{AMISE}})]}\\ &{\\quad=O(\\mathbf{J}_{d^{*}}n^{-8/(d+12)})(\\mathrm{vech}\\mathbf{H}_{\\mathrm{AMISE}})(\\mathrm{vech}^{T}\\mathbf{H}_{\\mathrm{AMISE}}).}\\end{array} $$ <PARAGRAPH_SEPARATOR> The Hessian matrix <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{D_{\\mathbf{H}}^{2}\\mathrm{AMISE}\\hat{f(\\cdot;\\mathbf{H})}=\\frac{1}{4}n^{-1}(4\\pi)^{-d/2}|\\mathbf{H}|^{-1/2}\\mathbf{D}_{d}^{T}(\\mathbf{H}^{-1}\\otimes\\mathbf{I}_{d})~}\\ {\\times[(\\mathrm{vec}\\mathbf{I}_{d})(\\mathrm{vec}^{T}\\mathbf{I}_{d})+2\\mathbf{I}_{d^{2}}](\\mathbf{I}_{d}\\otimes\\mathbf{H}^{-1})\\mathbf{D}_{d}+\\frac{1}{2}\\Psi_{4}~}\\end{array} $$ <PARAGRAPH_SEPARATOR> converges to a constant, positive-definite matrix as $n\\to\\infty$ : Here vec is the vector operator, so that vec $\\mathbf{H}$ is concatenation of the columns of $\\mathbf{H}$ : The duplication matrix of order $d$ is $\\mathbf{D}_{d}$ and it relates the vec and vech operators in the following ways: <PARAGRAPH_SEPARATOR> vec $\\mathbf{H}=\\mathbf{D}_{d}$ vech H; <PARAGRAPH_SEPARATOR> $$ \\mathbf{D}_{d}^{T}\\mathrm{vec}\\mathbf{H}=\\mathrm{vech}(\\mathbf{H}+\\mathbf{H}^{T}-\\mathrm{dg}\\mathbf{H}). $$ <PARAGRAPH_SEPARATOR> Also $\\otimes$ is the Kronecker (or tensor) product operator between two matrices. The proof of part (i) follows immediately by substituting (7) and (8) into the expansion of $\\mathbf{AMSE}(\\mathrm{vech}\\check{\\mathbf{H}})$ obtained from Lemma 1. <PARAGRAPH_SEPARATOR> From Duong and Hazelton [1], the SAMSE pilot bandwidth is $g_{4,\\mathrm{SAMSE}}=$ $O(n^{-1/(d+6)})$ : Straightforward calculations then give <PARAGRAPH_SEPARATOR> $\\mathrm{Bias}\\check{\\psi}_{r}(g_{j,\\mathrm{SAMSE}})=O(g_{j,\\mathrm{SAMSE}}^{2})=O(n^{-2/(d+6)})$ when it follows that <PARAGRAPH_SEPARATOR> $$ \\mathbb{E}[D_{\\mathbf{H}}(\\mathrm{PI}-\\mathrm{AMISE})(\\mathbf{H}_{\\mathrm{AMISE}})]=O(\\mathbf{J}_{d^{*}}n^{-2/(d+6)})\\mathrm{vech~\\mathbf{H}_{A M I S E}}. $$ <PARAGRAPH_SEPARATOR> It can be shown that in the SAMSE plug-in method the variance of $\\check{\\psi}_{r}$ is dominated by the leading term of the squared bias. Part (ii) then follows by substituting (9) into the result of Lemma 1, and noting the asymptotic constancy of the Hessian matrix as for part (i). & <PARAGRAPH_SEPARATOR> # 4. Relative rates of convergence for BCV selectors <PARAGRAPH_SEPARATOR> In this section we compute the relative rate of the BCV selector when $K$ is Gaussian. The results can be extended to more general kernel functions at the expense of more complex proofs."
  },
  {
    "qid": "statistic-posneg-1189-0-1",
    "gold_answer": "FALSE. The balanced design's efficiency gains are tied to the construction of the stratification variable $S$. Predictors in $X$ that are not used to form $S$ may suffer from a loss of efficiency, as noted in the context. The trade-off depends on how the stratification is constructed and which predictors are included in $S$.",
    "question": "Is it true that the balanced design in two-phase sampling always leads to efficiency gains for all predictors in $X$ regardless of whether they are used to form the stratification variable $S$?",
    "question_context": "Two-phase stratified sampling for risk prediction model evaluation\n\nWe consider two-phase sampling designs where Phase I sample consists of a cohort of size $N$ drawn randomly from a population of interest. The binary outcome $Y$ , with $Y=1$ indicating cases and $Y=0$ indicating controls, and a subset of predictors $X$ are available on all $N$ subjects. But the remaining predictors $Z$ can only be collected in a subsample selected based on Phase I data, which is referred to as Phase II sample. Let $R$ indicate whether $Z$ is observed or not $R=1$ : yes; $R=0$ : no). The two-phase data can be represented as $\\{Y_{i},X_{i},R_{i},R_{i}Z_{i}$ , $i=1,\\ldots,N\\}$ . Our goal is to develop a logistic regression model for predicting $Y$ with predictors $(X,\\pmb{Z})$ , where $\\pmb\\theta=(\\alpha,\\pmb\\beta^{T},\\pmb\\gamma^{T})^{T}$ with superscript “T” indicating vector transpose. In this article, we consider strategies for efficiently selecting Phase II subsample, that is, for generating $R_{i}$ based on $(Y_{i},X_{i}),i=$ $1,\\ldots,N$ , and methods for estimating AUC for model $p_{1}(\\pmb{x},\\pmb{z};\\pmb{\\theta})$ using two-phase data. Let $F(X,Z)$ denote the cumulative distribution function (CDF) for $(X,\\pmb{Z})$ . Then AUC is the function of risk $p_{1}(\\pmb{x},\\pmb{z};\\pmb{\\theta})$ and $F$ . The true positive rate (TPR) and false positive rate (FPR) at risk cutoff $\\nu$ , written as $\\mathrm{TPR}_{\\nu}(\\pmb\\theta,\\boldsymbol F)$ and $\\mathrm{FPR}_{\\nu}(\\pmb\\theta,\\boldsymbol F)$ , can be expressed as $$\\begin{array}{r l}&{\\mathrm{TPR}_{\\nu}=\\operatorname*{Pr}\\{p_{1}(X,Z;\\theta)>\\nu|Y=1\\}=\\frac{\\int\\mathrm{I}\\{p_{1}(x,z;\\theta)>\\nu\\}p_{1}(x,z;\\theta)\\mathrm{d}F(x,z)}{\\int p_{1}(x,z;\\theta)\\mathrm{d}F(x,z)},}\\ &{\\mathrm{FPR}_{\\nu}=\\operatorname*{Pr}\\{p_{1}(X,Z;\\theta)>\\nu|Y=0\\}=\\frac{\\int\\mathrm{I}\\{p_{1}(x,z;\\theta)>\\nu\\}\\{1-p_{1}(x,z;\\theta)\\}\\mathrm{d}F(x,z)}{\\int\\{1-p_{1}(x,z;\\theta)\\}\\mathrm{d}F(x,z)}.}\\end{array}$$ The AUC can then be represented as $$\\operatorname{AUC}(\\pmb{\\theta},F)=\\int\\operatorname{TPR}_{\\nu}(\\pmb{\\theta},F)\\operatorname{d}\\{\\operatorname{FPR}_{\\nu}(\\pmb{\\theta},F)\\}.$$"
  },
  {
    "qid": "statistic-posneg-2117-0-0",
    "gold_answer": "FALSE. The QML method uses a combination of standard ML estimation when $k<0.75$ and a modified ML method when $k\\geqslant0.75$. The decision to use standard or modified ML is based on empirical criteria involving the computed values of $\\tilde{k}$ and $Z$.",
    "question": "Is it true that the QML method uses standard ML estimation for all values of $k$ when estimating parameters of the Generalized Pareto Distribution?",
    "question_context": "Quasi-Maximum Likelihood Estimation for Generalized Pareto Distribution\n\nAs shown in Section 1, the ML method fails to provide estimators for the parameters $k$ and $\\theta$ of the GPD, given in (1), for most positive values of the parameter $k$ . Thus, Hosking and Wallis (1987) compare MLEs with MOM estimates and with the estimates provided by a method of probability-weighted moments (PWM), but only consider a small range of values of $k$ , namely, $|k|<0.5$ . Castillo and Hadi (1997) introduce the elemental percentile method (EPM) and expand the range of values of $k$ to $|k|\\leqslant2$ , but disregard MLEs. Following these authors, we shall compare MGF estimators with MOM, PWM and EPM estimators, and also with the estimator provided by a quasi-ML (QML) method.<PARAGRAPH_SEPARATOR>QML method: Loosely speaking, our QML method uses a combination of the standard ML method when $k<0.75$ and a modified ML method when $k\\geqslant0.75$ . But, because it is not possible to know the exact value of $k$ using solely the information in a random sample $(x_{1},\\ldots,x_{n})$ , the decision about whether to use the standard or the modified ML method must be taken on empirical grounds. Therefore, we have adopted the QML method having the following steps:<PARAGRAPH_SEPARATOR>(1) Compute<PARAGRAPH_SEPARATOR>$$\\tilde{k}=\\frac{-1}{n-1}\\sum_{i=1}^{n-1}\\ln\\left(1-\\frac{x_{(i)}}{\\operatorname*{max}\\left(x_{1},...,x_{n}\\right)}\\right)$$<PARAGRAPH_SEPARATOR>and<PARAGRAPH_SEPARATOR>$$Z=1-\\frac{{\\sum_{i=1}^{n}x_{i}^{2}}/{n}}{{2{\\bar{x}}^{2}}}.$$<PARAGRAPH_SEPARATOR>(2) If $\\tilde{k}<0.75$ and $Z<0.2$ , compute standard MLEs for $k$ and $\\theta$ . (3) Otherwise, estimate $k$ using Eq. (4) and estimate $\\theta$ using<PARAGRAPH_SEPARATOR>$$\\tilde{\\boldsymbol{\\theta}}=\\tilde{k}\\operatorname*{max}\\left(x_{1},\\ldots,x_{n}\\right).$$<PARAGRAPH_SEPARATOR>The justification of this method is as follows. When $k$ is large, the range of the GPD is $0\\leqslant x\\leqslant\\theta/k$ and the ML method fails. Therefore, for large values of $k$ , one can use estimators such that $\\tilde{\\boldsymbol{\\theta}}/\\tilde{k}=\\operatorname*{max}\\left(x_{1},\\ldots,x_{n}\\right)$ , which is in agreement with the MLE provided by Eq. (3) for $k=1$ . This justifies (6). Moreover, introducing (6) in the log-likelihood function of the parameters $k$ and $\\theta$ given the remaining sample $\\left(x_{(1)},\\ldots,x_{(n-1)}\\right)$ , and maximizing the resulting expression with respect to $k$ leads to (4). The additional criterium based on Eq. (5) can be justified on intuitive grounds because the asymptotic value of $Z$ is $\\frac13$ for the uniform distribution $(k=1$ ) and 0 for the exponential distribution $(k=0)$ ). Eq. (5) can also be justified because $n Z$ is the slope of the log-likelihood function in the direction of $k$ when $k$ tends to 0 and ${\\hat{\\theta}}={\\bar{x}}$ (for $k=0$ , the MLE of $\\theta$ is $\\hat{\\boldsymbol{\\theta}}=\\bar{\\boldsymbol{x}}$ , so that $n Z$ is related to the sign of $\\hat{k}$ in the neighborhood of $k=0$ ).<PARAGRAPH_SEPARATOR>Simulation results: Table 8 provides estimated bias and RMSEs for the MGF estimators obtained using the eight EDF statistics of Section 2 together with the estimators provided by the methods referred to in this section (QML or ML, MOM, PWM and EPM). These estimated bias and RMSEs are based on 1000 samples of size 100 on the GPD with $\\theta=1$ and $k=\\{-2,-1,0,1,2\\}$ . The minimizations of the EDF statistics have been performed subject to the boundary condition $\\tilde{\\theta}/\\tilde{k}\\geqslant\\operatorname*{max}\\left(x_{1},\\ldots,x_{n}\\right)$ whenever $\\tilde{k}>0$ . (The results are, obviously, invariant with respect to the value of $\\theta$ .)<PARAGRAPH_SEPARATOR>Table 8 Estimated bias and RMSEs for the MGFEs corresponding to the eight EDF statistics of Section 2 and the QML, ML, MOM, PWM and EPM methods of Section 4.4, based on 1000 samples of size 100 on the GPD with $\\theta=1$ and $k=\\{-2,-1,0,1,2\\}$ , in ascending order of the RMSEs"
  },
  {
    "qid": "statistic-posneg-744-2-3",
    "gold_answer": "TRUE. The BIC includes a log(n) term multiplied by the number of parameters (m-1), which penalizes model complexity more heavily than the AIC's constant penalty of 2(m-1), especially as sample size n increases.",
    "question": "Does the BIC method penalize model complexity more heavily than the AIC method by using a log(n) term instead of a constant 2?",
    "question_context": "Model Selection Criteria in Unimodal Density Estimation\n\nWe present a novel procedure for selecting the number of weights by examining the condition number of A. Since A is a normal matrix the condition number is evaluated by, $\\mathsf{C N}(m)=\\left\\lceil\\frac{\\lambda_{\\operatorname*{max}}(A)}{\\lambda_{\\operatorname*{min}}(A)}\\right\\rceil$ , where $\\lambda_{\\operatorname*{max}}(A)$ and $\\lambda_{\\mathrm{min}}(A)$ are the maximum and minimum eigenvalues of A respectively. We select $m$ by including the largest number of weights possible while still bounding $\\log_{10}\\mathrm{CN}(m)$ by $\\sqrt{n}$ . <PARAGRAPH_SEPARATOR> # 2.1.2. AIC and BIC <PARAGRAPH_SEPARATOR> We also present more traditional methods for selecting the number of weights, specifically, the Akaike information criterion (AIC) and Bayesian information criteria (BIC). In the realm of density estimation, these criteria are given by, <PARAGRAPH_SEPARATOR> $\\mathsf{A I C}(m)=-2\\sum_{i=1}^{n}\\log[\\widehat{f}_{m}(x_{i},\\widehat{\\omega}_{m})]+2(m-1)$, <PARAGRAPH_SEPARATOR> $\\mathrm{BIC}(m)=-2\\sum_{i=1}^{n}\\log[\\hat{f}_{m}(x_{i},\\widehat{\\omega}_{m})]+\\log(n)(m-1)$, <PARAGRAPH_SEPARATOR> where $m$ is the number of weights, $\\hat{f}_{m}$ is the estimated density using $m$ weights, $\\widehat{\\omega}_{m}$ is the vector of estimated weights, and $x_{i}$ for $i=1,\\ldots,n$ are the observations. The effective dimension of the weight vector $\\widehat{\\omega}_{m}$ is $m-1$ since the weights are constrained to sum to one. <PARAGRAPH_SEPARATOR> A variation of Theorem 3.1 in Babu et al. (2002) shows that $\\hat{f}_{m}$ will converge uniformly to $f$ for $2\\leq m\\leq(n/\\log n)$ as $n\\rightarrow\\infty$ , so we implement both the AIC and BIC methods by estimating the density with $m$ weights ranging from 2 to $\\lceil n/\\log n\\rceil$ . The AIC or BIC is calculated for each fit, then the density estimate with the lowest AIC or BIC is selected as the best estimate. <PARAGRAPH_SEPARATOR> # 2.2. Comparison of criterion for selecting the number of weights <PARAGRAPH_SEPARATOR> We compare these three methods of selecting $m$ by performing a small simulation study. We generate data from a mixture of Beta densities with a fixed vector of weights, which follows the required format of our density estimation model. We then generate density estimates using the AIC, BIC, and CN criterion to select m. Our goal is to find the method which provides the best density estimate in terms of a low root mean integrated squared error (RMISE) and selects the true number of weights of the Beta mixture. The RMISE is approximated as follows: <PARAGRAPH_SEPARATOR> $\\mathrm{RMISE}=\\mathrm{E}\\left[\\Vert\\hat{f}_{\\widehat{m}}-f\\Vert_{2}\\right]\\approx\\frac{1}{N}\\sum_{\\ell=1}^{N}\\sqrt{d\\sum_{j=1}^{J}\\left[\\hat{f}_{\\widehat{m}}^{(\\ell)}(x_{j})-f(x_{j})\\right]^{2}}$, <PARAGRAPH_SEPARATOR> where $J=100,\\operatorname*{Pr}[x_{1}\\leq X\\leq x_{J}]=0.999$ , and $d=(x_{j}-x_{j-1})=\\frac{x_{J}-x_{1}}{J-1}$ for all $j\\geq2$ . In the above expression, $\\hat{f}_{\\widehat{m}}^{(\\ell)}$ denotes the estimated density at the ℓth sample for $\\ell=1,\\ldots,N$ with $\\widehat{m}$ chosen by one of the three criteria. <PARAGRAPH_SEPARATOR> We examine a symmetric and right-skewed Beta mixture each with 7 weights (i.e. $m=7$ ). The weight vectors are fixed to be $\\omega_{1}~=~\\{0.05,0.1,0.2,0.3,0.2,0.1,0.05\\}$ for the symmetric distribution, and $\\omega_{2}=\\{0.1,0.4,0.25,0.15,$ 0.05, 0.03, 0.02} for the right-skewed distribution. We generate $N~=~1000$ Monte Carlo (MC) samples of size $n=$ 15, 30, 100, and 500 for each Beta mixture. Table 1 displays the MC estimated RMISE values as well as the average estimate of m across the MC samples. Statistically significant lower RMISE values, based on a paired $t$ -test, are in bold. <PARAGRAPH_SEPARATOR> The AIC method has the lowest average RMISE in most situations. Unfortunately, none of the methods have average number of weight estimates that are close to 7. The CN method appears to select the number of weights at around $\\sqrt{n}$ , which is much larger than 7 for the cases of $n=100$ and 500. Both the AIC and BIC underestimate $m$ for all sample sizes with the AIC having slightly larger estimates. Overall it appears the CN criterion is a better option for samples with fewer observations while the AIC performs the best for larger samples of size 100 and 500."
  },
  {
    "qid": "statistic-posneg-887-1-1",
    "gold_answer": "FALSE. While the mean population size does follow an exponential form e^{(λ-μ)t} when λ ≠ μ, this is not equivalent to the deterministic model in all cases. Specifically, when λ = μ, the mean population size remains constant (equal to the initial size N₀), but the variance grows linearly with time, indicating significant stochastic fluctuations not captured by a deterministic model. The deterministic and stochastic models only align in mean behavior when λ ≠ μ.",
    "question": "The mean population size in the birth-and-death process always follows the deterministic exponential growth model, regardless of the values of λ and μ. Is this statement correct?",
    "question_context": "Stochastic Birth-and-Death Process in Population Growth\n\nThe context discusses a stochastic model for population growth, specifically a birth-and-death process where individuals can reproduce or die with certain probabilities. The model is characterized by birth rate λ and death rate μ, and it explores the probability distribution of population size over time, the mean and variance of the population, and conditions for population extinction."
  },
  {
    "qid": "statistic-posneg-83-0-1",
    "gold_answer": "TRUE. The stick-breaking representation of the two-parameter Poisson–Dirichlet process (Pitman–Yor process) explicitly requires the base measure $P_{0}$ to be nonatomic, as stated in the context. In contrast, the Dirichlet process's stick-breaking representation, as extended by Sethuraman (1994), does not impose this restriction and can work with any $P_{0}$. This distinction is important for applications where the atomicity of $P_{0}$ might affect the model's behavior.",
    "question": "Is it correct to say that the stick-breaking representation of the two-parameter Poisson–Dirichlet process requires the base measure $P_{0}$ to be nonatomic, unlike the Dirichlet process which can have any $P_{0}$?",
    "question_context": "Stick-breaking Representation of Nonparametric Priors\n\nBayesian nonparametric inference has recently undergone strong development. See Hjort et al. (2010) for an up-to-date review. At the heart of the approach lies the concept of random probability measure, whose law acts as a prior for Bayesian nonparametric inference, the most notable example being the Dirichlet process (Ferguson, 1973). There exist different representations for a number of nonparametric priors, which, although equivalent in distribution, may serve different purposes. For example, representations based on completely random measures allow one to study analytically their properties (Lijoi & Prunster, 2010), whereas stick-breaking representations have displayed great potential in addressing modelling and computational issues. The main result of this paper is a stick-breaking representation of the normalized inverse Gaussian process (Lijoi et al., 2005), a tractable alternative to the Dirichlet process. Our result is of interest from a theoretical point of view since it is the first representation of a random probability measure in terms of dependent and non-beta distributed stick-breaking weights and it completes the study of the normalized inverse Gaussian process, for which a stickbreaking representation was missing. From a modelling and computational point of view it paves the way for the definition of complex models based on the normalized inverse Gaussian process by replacing the stick-breaking constructed Dirichlet process, most notably within dependent models for nonparametric regression, and it allows extensions of recent simulation algorithms, based on stick-breaking constructions, to the normalized inverse Gaussian process.<PARAGRAPH_SEPARATOR>There are several different ways to define the Dirichlet process, each highlighting one of its aspects. The original definition of Ferguson (1973) constructs the Dirichlet process $\\mathcal{D}_{c,P_{0}}$ , with parameter $\\alpha=c P_{0}$ and $P_{0}$ a probability measure, in terms of a consistent family of finitedimensional Dirichlet distributions. An alternative definition, also due to Ferguson (1973), relies on the idea of normalizing a gamma process. A third construction is based on a stick-breaking procedure that follows from a result in J. W. McCloskey’s $1965~\\mathrm{PhD}$ thesis at Michigan State University, recalled as Theorem 1 in Pitman (1996) under the assumption of nonatomic $P_{0}$ , and that has been extended to any $P_{0}$ in Sethuraman (1994). Let $(V_{i})_{i\\geqslant1}$ be a sequence of independent and identically distributed random variables, with $V_{i}\\sim\\mathrm{Be}(1,c)$ and $c>0$ , and define random probability weights $(\\tilde{p}_{j})_{j\\geqslant1}$ as<PARAGRAPH_SEPARATOR>$$\\tilde{p}_{1}=V_{1},~\\tilde{p}_{j}=V_{j}\\prod_{i=1}^{j-1}(1-V_{i})~(j=2,3,...).$$<PARAGRAPH_SEPARATOR>If $(Y_{i})_{i\\geqslant1}$ is a sequence of independent and identically distributed random variables, independent of the $\\tilde{p}_{i}$ and whose common probability distribution is $P_{0}$ , then $\\begin{array}{r}{\\sum_{j\\geq1}\\tilde{p}_{j}\\delta_{Y_{j}}(\\cdot)=\\mathcal{D}_{c,P_{0}}(\\cdot)}\\end{array}$ in distribution, where $\\delta_{a}$ denotes the unit point mass at $a$ .<PARAGRAPH_SEPARATOR>Another prominent nonparametric prior is the two-parameter Poisson–Dirichlet process (Perman et al., 1992), also known, according to terminology introduced in Ishwaran & James (2001), as the Pitman–Yor process. This admits a simple stick-breaking representation: let $(V_{i})_{i\\geqslant1}$ be independent random variables with $V_{i}\\sim\\mathrm{Be}(1-\\sigma,\\theta+i\\sigma)$ , $\\sigma\\in(0,1)$ and $\\theta>-\\sigma$ , and define the random probability masses as in (1) and $(Y_{i})_{i\\geqslant1}$ as above, except that we require a nonatomic $P_{0}$ . The resulting discrete random probability $\\textstyle\\sum_{j\\geq1}\\tilde{p}_{j}\\delta_{Y_{j}}(\\cdot)$ coincides, in distribution, with the two-parameter Poisson–Dirichlet process.  See Pitman (2006) for an exhaustive account. Bayesian nonparametric applications include mixtures (Ishwaran & James, 2001), linguistics (Teh, 2006), species sampling (Lijoi et al., 2007) and survival analysis (Jara et al., 2010).<PARAGRAPH_SEPARATOR>The extreme flexibility of stick-breaking representations has originated a vast literature concerning both modelling and computation. In terms of modelling, the dependent processes, initiated by MacEachern (1999), heavily rely on a stick-breaking construction and have proved to be effective prior specifications in regression problems. See Hjort et al. (2010) for a review of recent contributions. From a computational point of view significant progress, especially in designing efficient simulation algorithms for hierarchical mixtures, has been made using such representations. Among the most relevant contributions, which devise algorithms that work in principle for any random probability measure with an explicit stick-breaking representation, are the blocked Gibbs sampler (Ishwaran & James, 2001), the retrospective sampler (Papaspiliopoulos & Roberts, 2008), the slice sampler (Walker, 2007) and a very efficient synthesis (Yau et al., 2011) of these last two."
  },
  {
    "qid": "statistic-posneg-2046-4-1",
    "gold_answer": "TRUE. Explanation: The LR_ind test evaluates whether violations are independent (H₀: λ₀₁ = λ₁₁ = λ̂) or exhibit dependence (e.g., clustering). Under H₀, LR_ind follows a χ²₁ distribution. A low P_ind suggests violations are not independent (e.g., violations tend to cluster, indicating the model fails to capture time-varying risk).",
    "question": "The independence of VaR violations is tested using a first-order Markov chain with transition probability matrix Λ. The likelihood ratio test for independence (LR_ind) compares the likelihood under the null (H₀: λ₀₁ = λ₁₁ = λ̂) against the observed likelihood with distinct transition probabilities. Is it correct to say that LR_ind ~ χ²₁ and a low P_ind indicates clustering of violations, violating the i.i.d. assumption?",
    "question_context": "Value-at-Risk (VaR) Forecasting Accuracy and Testing\n\nThe paper discusses the testing of VaR prediction models using likelihood ratio tests for unconditional coverage, independence of violations, and conditional coverage. It introduces the Boolean sequence of VaR violations, the empirical downfall probability, and the mean relative scaled bias (MRSB) for comparing model efficiency."
  },
  {
    "qid": "statistic-posneg-2065-0-0",
    "gold_answer": "TRUE. The Student's t-statistic replaces the population standard deviation $\\sigma$ with the sample standard deviation $s$, which is calculated from the sample data. The distribution of the t-statistic is independent of $\\sigma$, enabling inference about $\\mu$ based solely on the sample data. This property is crucial for hypothesis testing and confidence interval estimation when $\\sigma$ is unknown.",
    "question": "Is it true that the Student's t-statistic $t=(\\overline{{x}}-\\mu)\\sqrt{n/s}$ allows for inference about the population mean $\\mu$ without requiring knowledge of the population standard deviation $\\sigma$?",
    "question_context": "Student's t-Distribution in Normal Population Inference\n\nLet $x_{1},...,x_{\\mathfrak{n}}$ denote a sample of $\\pmb{n}$ observations drawn from a normal population with mean ${\\pmb\\mu}$ and standard deviation $\\pmb{\\sigma}$ . Then the difference between sample and population mean, ${\\pmb{\\overline{{{x}}}}}-{\\pmb\\mu}$ , is normally distributed with zero mean and standard deviation ${\\pmb\\sigma}/\\sqrt{\\pmb{n}}$ . If we estimate the standard deviation of the parent, $\\pmb{\\sigma}$ by $s=\\sqrt{[\\Sigma(x-\\overline{{x}})^{\\mathfrak{s}}/(n-1)]},$ then ‘Student' (1908) gave the distribution of $t=(\\overline{{x}}-\\mu)\\sqrt{n/s}.$ As was to be expected, this distribution was independent of $\\pmb{\\sigma}$ . The knowledge of the distribution of t made it possible to draw inferences, with the help of evidence entirely supplied by the sample, about the location-parameter $\\pmb{\\mu}$ of a normal population, without making any assumptions about the generally unknown scale-parameter, $\\pmb{\\sigma}$."
  },
  {
    "qid": "statistic-posneg-1820-0-0",
    "gold_answer": "FALSE. The MAVE method, as described, uses least squares estimation which is sensitive to outliers. The paper discusses the need for robust enhancements to MAVE, such as replacing the least squares estimation with local L- or M-estimation, to handle heavy-tailed and contaminated data effectively.",
    "question": "Is the MAVE method robust to outliers in the data?",
    "question_context": "Robust MAVE through Nonconvex Penalized Regression\n\nIn this section, we provide a short review of the MAVE and the robust MAVE estimation. A traditional regression type model in dimension reduction can be written as y = g(B^T X) + ε, where g(·) is an unknown smooth link function and B is a p×d orthonormal matrix consisting of (β_1,…,β_d) with d < p. The random error ε is independent of x with E(ε)=0. Given a random sample {(X_i,y_i),i=1,...,n} and a known d, Xia et al. (2002) proposed the following objective function to estimate a basis B: min_{B:B^T B=I_d} (∑_{j=1}^n ∑_{i=1}^n [y_i - {a_j + b_j^T B^T (X_i - X_j)}]^2 w_ij), where I_d denotes a d×d identity matrix, and a_j and b_j are the coefficients of the local linear expansion g(B^T X_i) ≈ a_j + b_j^T B^T (X_i - X_j). The kernel weight w_ij is chosen as w_ij = K_h{B^T (X_i - X_j)} / ∑_{i=1}^n K_h{B^T (X_i - X_j)}, where K_h(ν) = h^{-1}K(ν/h) with K(ν) being a symmetric kernel function and h being the bandwidth, and B̂ denotes an initial estimator of B."
  },
  {
    "qid": "statistic-posneg-933-0-0",
    "gold_answer": "FALSE. The mechanism is not ignorable because the probability of observing an increment depends on its type (flight or pause), which is part of the unobserved data when the increment is missing. This violates the MAR condition, as the data-collection mechanism depends on unobserved variables (the increment type).",
    "question": "Is the data-collection mechanism in Example 4 ignorable for the FPM, given that it records flights with probability $\\rho_{f}$ and pauses with probability $\\rho_{p}\\neq\\rho_{f}$?",
    "question_context": "Statistical Inference for Missing Data Mechanisms in Motion Trajectories\n\nWe now introduce notation for observable increments. To begin, we note that every possible value of $\\xi$ can be composed of alternating groups of consecutive observed and unobserved increments. Formally, we let $\\mathcal{O}=\\{M_{k}\\in m:\\xi_{M}=1\\}$ and define $O=\\{O_{1},...,O_{J}\\}$ , a partition of $\\mathcal{O}$ ordered from the smallest (1) to largest time index $(L_{k}^{T})$ . We say that the index of the first increment in $O_{j}$ is $I_{j}$ . In other words, $\\lfloor O_{j}\\rfloor=M_{I_{j}}$ . We also define $N_{j}=|O_{j}|$ , the number of increments in the jth observed block. This means that the last location in the observed block of increments is $\\lceil O_{j}\\rceil=M_{I_{j}+N_{j}-1}$ . <PARAGRAPH_SEPARATOR> We denote the blocks that make up the unobserved increments of the motion as $U_{j}$ , with $\\mathcal{U}=\\{M_{k}\\in\\boldsymbol{m}:\\xi_{k}=0\\}$ . Specifically, $U_{j}=(M_{I_{j}+N_{j}},\\mathrm{~.~.~,~}M_{I_{j+1}-1})$ . In this way, we have $m=(O_{1},U_{1},O_{2},U_{2},...,O_{J-1},U_{J-1},O_{J})$ . Using this notation, as well as $\\psi$ to denote the add­ itional parameters related to the distribution of $\\xi$ , we can now write the observed-data likelihood <PARAGRAPH_SEPARATOR> in terms of blocks of observed increments: <PARAGRAPH_SEPARATOR> $$ q(\\odot,\\xi\\mid\\theta,\\psi)=\\int q(\\odot,\\mathcal{U}\\mid\\theta)q(\\xi\\mid\\odot,\\mathcal{U},\\psi)\\mathrm{d}\\mathcal{U}=q(O_{1}\\mid\\theta)\\prod_{j>1}^{J}\\prod_{k=I_{j}+1}^{I_{j}+N_{j}-1}q(M_{k}\\mid M_{k-1},\\theta) $$ <PARAGRAPH_SEPARATOR> $$ \\rfloor\\prod_{j>1}^{J}\\prod_{k=I_{j}+N_{j}}^{I_{j+1}-1}q(M_{k}\\mid M_{k-1},\\theta)q(M_{I_{j}}\\mid M_{I_{j}-1},\\theta)q(\\xi\\mid\\mathcal{O},\\mathcal{U},\\psi)\\mathrm{d}\\mathcal{U}. $$ <PARAGRAPH_SEPARATOR> Expression (11) constitutes the general form of the observed-data likelihood, where the first set of products contains indices of increments contained within $\\circledcirc$ and the second set of products has in­ dices corresponding to increments contained within $u$ . Appendix B.1 includes derivation of the special case of equation (11) under the standard parametrisation introduced in Section 2.1. <PARAGRAPH_SEPARATOR> Note the explicit dependence of equation (11) on $q(\\xi|\\mathcal{O},\\mathcal{U},\\psi)$ , which corresponds to the datacollection mechanism for increments—a probability model dictating which elements of a motion are ob­ served. Dependence of equation (11) on this quantity clarifies that, in general, inference in the FPM with incomplete trajectory data will require assumptions about the form of this data-collection mechanism. <PARAGRAPH_SEPARATOR> Before discussing data-collection mechanisms for increments, we introduce an assumption that can drastically simplify evaluation of equation (11): <PARAGRAPH_SEPARATOR> Assumption 1 Assume that for each $j=1,\\ldots,J$ and $M_{k_{1}},M_{k_{2}}$ such that $M_{k_{1}}\\in O_{j}$ and $M_{k_{2}}\\in m\\setminus O_{j}$ we have $\\xi_{k_{1}}\\perp\\xi_{k_{2}}\\mid O_{j}\\cup\\{\\xi_{k}:M_{k}\\in O_{j}\\}$ . Assume further that if $M_{k_{1}},M_{k_{2}}$ are such that $M_{k_{1}}\\in O_{j}\\cup U_{j}$ and $M_{k_{2}}\\in{\\cal m}\\setminus({\\cal O}_{j},\\cup U_{j})$ then $M_{k_{1}}\\perp M_{k_{2}}$ . <PARAGRAPH_SEPARATOR> This assumption implies that the observed blocks are independent and that the observability in­ dicators depend only on other increments within the same block (see, e.g. de Chaumaray et al., 2020 for a similar approach). <PARAGRAPH_SEPARATOR> Thus, under equation (1), we have $\\begin{array}{r}{q(\\xi\\mid\\mathcal{O},\\mathcal{U})=\\prod_{j=1}^{J-1}q(\\xi_{{O_{j}},U_{j}}\\mid{O_{j}},U_{j})q(\\xi_{{O_{J}}}\\mid O_{J})}\\end{array}$ and equation (11) can be expressed as <PARAGRAPH_SEPARATOR> $$ q(\\boldsymbol{\\mathbb{O}},\\boldsymbol{\\xi}\\mid\\psi,\\theta)=\\prod_{j=1}^{J}q(M_{I_{j}}\\mid\\theta)q(\\xi_{I_{J}}=1\\mid I_{j},\\psi)\\prod_{k=I_{j}}^{I_{j}+N_{j}-1}q(M_{k}\\mid M_{k-1}), $$ <PARAGRAPH_SEPARATOR> where $\\xi_{O_{j},U_{j}}$ and $\\xi_{O_{j}}$ stand for, respectively, the vector of observability indicators for the elements of the blocks $O_{j}$ and $U_{j}$ jointly and just $O_{j}$ . Intuitively, this means that we treat each observed block as a distinct trajectory depending on a common set of parameters. Therefore, equation (1) can be a good approximation of the truth if the unobserved blocks are large (i.e. if $I_{j+1}-I_{j}-$ $N_{j}$ is large). Evaluating the likelihood under equation (1) is equivalent to calculating the composite likelihood with identical weights (Lindsay, 1988; Varin et al., 2011). <PARAGRAPH_SEPARATOR> # 4 Data-collection mechanisms <PARAGRAPH_SEPARATOR> The previous section introduces a framework for studying the implications of data-collection mechanisms $q(\\xi|m,\\theta,\\psi)$ . We now explore different missing-data mechanisms (i.e. models for $\\xi,$ ) and their implications for inference on $\\pmb{\\theta}$ . <PARAGRAPH_SEPARATOR> To study the impacts of possible data-collection mechanisms, we first briefly review the classic missing data framework, as described by Little and Rubin (2019) and Gelman et al. (2013). Consider the joint likelihood, <PARAGRAPH_SEPARATOR> $$ q(m,\\xi\\mid\\theta,\\psi)=q(m\\mid\\theta)q(\\xi\\mid m,\\psi,\\theta), $$ <PARAGRAPH_SEPARATOR> where the first term on the right-hand side is given in Proposition 2 and the equality holds because $\\psi$ are used to parametrise only the distribution of $\\xi$ . We use the second term, which we call data-collection mechanism, to express various assumptions regarding the observation pattern. <PARAGRAPH_SEPARATOR> We start with a simple example of a mechanism, which might sometimes be used to conserve battery but which actually results in no missing data. <PARAGRAPH_SEPARATOR> Example 2 (Movement-triggered data collection). Consider a mechanism that starts col­ lecting data once movement is detected (i.e. using accelerometer which is found in most modern smart phones) and stops when a pause in movement is detected. In particular, we assume that <PARAGRAPH_SEPARATOR> $$ q(\\xi\\mid m,\\psi,\\theta)=\\prod_{k=1}^{K}q(\\xi_{k}\\mid M_{k}), $$ <PARAGRAPH_SEPARATOR> where $q(\\xi_{k}\\mid M_{k})=\\mathbb{1}(R_{k}=f)$ . Note that even though we technically suspend data collection for the duration of pauses, knowing that the pauses are the only unobserved increments actually allows us to have complete information about them, i.e. no data are missing. <PARAGRAPH_SEPARATOR> # 4.1 Ignorable mechanisms <PARAGRAPH_SEPARATOR> The key distinction in the study of missing data is between contexts in which the data-collection mechanism is ignorable and the ones in which it is not. Ignorability is equivalent to two conditions. The first one, called parameter distinctness, requires that the second term on the right-hand side of equation (12) takes the form <PARAGRAPH_SEPARATOR> $$ q(\\xi\\mid m,\\psi,\\theta)=q(\\xi\\mid m,\\psi), $$ <PARAGRAPH_SEPARATOR> which means that the parameters governing the model for the complete data are distinct from the parameters regulating the data-collection mechanism. <PARAGRAPH_SEPARATOR> The second condition, which is often expressed by saying that the data is missing at random (MAR), demands that the data-collection mechanism is also independent of the unobserved variables. Mathematically, within the context of our model, this means that <PARAGRAPH_SEPARATOR> $$ q(\\xi\\mid\\mathcal{O},\\mathcal{U},\\psi,\\theta)=q(\\xi\\mid\\mathcal{O},\\psi,\\theta), $$ <PARAGRAPH_SEPARATOR> since $m=\\mathcal{O}\\cup\\mathcal{U}$ . An important special case of this condition, called missingness completely at random (MCAR), further constrains the data-collection mechanism to be independent of the data, i.e. $q(\\xi\\mid m,\\psi,\\theta)=q(\\xi\\mid\\psi,\\theta)$ . <PARAGRAPH_SEPARATOR> In summary, in the FPM, the data-collection mechanism is ignorable if and only if <PARAGRAPH_SEPARATOR> $$ q(\\xi\\mid m,\\psi,\\theta)=q(\\xi\\mid{\\mathcal{O}},\\psi). $$ <PARAGRAPH_SEPARATOR> To provide concreteness, the following example describes a somewhat unrealistic ignorable data collection mechanism for MPT data. <PARAGRAPH_SEPARATOR> Example 3 (Random increment corruption). Consider a data-collection scheme where timestamped locations are measured then, after processing and storing these locations as increments, buggy software leads to the deletion of some of the increments in $m$ . For example, after every $l$ recorded increments, there is one increment missing. Then another $l$ increments are recorded, etc. In this case, $q(\\xi\\mid m,\\theta,\\psi)=q(\\xi\\mid\\psi)$ so data mechanism is ignorable. Note that in this case increments are missing for reasons unrelated to the availability of the underlying locations. <PARAGRAPH_SEPARATOR> If a mechanism is ignorable, inference of $\\pmb\\theta$ can be conducted without explicitly modelling the data-collection mechanism. That is, inference on $\\pmb\\theta$ can be obtained using the complete data like­ lihood: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{q(\\mathbb{O},\\boldsymbol{\\xi}\\mid\\boldsymbol{\\theta},\\boldsymbol{\\psi})=\\int q(\\boldsymbol{\\eta},\\boldsymbol{\\xi}\\mid\\boldsymbol{\\theta},\\boldsymbol{\\psi})\\mathrm{d}\\mathcal{U}=\\int q(\\boldsymbol{\\eta}\\mid\\boldsymbol{\\theta})q(\\boldsymbol{\\xi}\\mid\\boldsymbol{m},\\boldsymbol{\\psi})\\mathrm{d}\\mathcal{U}}\\ &{\\qquad=\\int q(\\mathbb{O},\\mathcal{U}\\mid\\boldsymbol{\\theta})q(\\boldsymbol{\\xi}\\mid\\boldsymbol{f}(\\mathbb{O}),\\boldsymbol{\\psi})\\mathrm{d}\\mathcal{U}}\\ &{\\qquad\\propto\\int q(\\mathbb{O},\\mathcal{U}\\mid\\boldsymbol{\\theta})\\mathrm{d}\\mathcal{U}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> # 4.2 Non-ignorable mechanisms <PARAGRAPH_SEPARATOR> In the context of MPT data, many common data-collection mechanisms turn out not to be ignor­ able for the FPM. Consider the following simplistic example. <PARAGRAPH_SEPARATOR> Example 4 An MPT instrument records flights with probability $\\rho_{f}$ and pauses with prob­ ability $\\rho_{p}\\neq\\rho_{f}$ . It follows that, <PARAGRAPH_SEPARATOR> $$ q({\\boldsymbol{\\xi}}\\mid{\\boldsymbol{M}})=\\prod_{k=1}^{K}q({\\boldsymbol{\\xi}}_{k}\\mid{\\boldsymbol{R}}_{k})=\\prod_{k=1}^{K}\\rho_{p}\\mathbb{1}({\\boldsymbol{R}}_{k}={\\boldsymbol{\\mathit{p}}})+\\rho_{f}\\mathbb{1}({\\boldsymbol{R}}_{k}=f). $$ <PARAGRAPH_SEPARATOR> Since the probability of observing an increment depends on its potentially un­ observed increment type, the data are not missing at random and the mechan­ ism is not ignorable for the FPM. <PARAGRAPH_SEPARATOR> More realistically, an MPT device might collect locations only during certain prescribed inter­ vals in time (e.g. alternating between recording for one minute and not recording for one minute) to save battery power. Barnett and Onnela (2020) observe that this approach is used by a popular Beiwe app (Torous et al., 2016) and study its different versions. <PARAGRAPH_SEPARATOR> Example 5 (The on–off mechanism). An MPT device alternately records locations for some prescribed time interval o and then the collection is suspended for an­ other time interval u. We show that such a scheme leads to increments which are missing not at random (MNAR) for the FPM."
  },
  {
    "qid": "statistic-posneg-2049-2-3",
    "gold_answer": "FALSE. The context explicitly uses a generalized inverse $(X^{\\prime}X)^{-}$ throughout, and the formula for $w_{\\tau}^{\\prime}$ is given as $\\lambda_{\\tau}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}$. The use of a generalized inverse is necessary because $X^{\\prime}X$ may not be full-rank in this design context.",
    "question": "Is the weight vector $w_{\\tau}^{\\prime}$ for the direct effects contrast estimator given by $\\lambda_{\\tau}^{\\prime}(X^{\\prime}X)^{-1}X^{\\prime}$ (using a regular inverse) rather than a generalized inverse?",
    "question_context": "Robust Assessment of Two-Treatment Higher-Order Crossover Designs\n\nIt follows that if many designs are under consideration then the design $D$ with breakdown number $\\boldsymbol{\\mathrm{max}}(m_{D})$ is more robust than the competing designs and should be preferred on grounds of robustness. When several designs have the same largest breakdown number $\\boldsymbol{\\mathrm{max}}(m_{D})$ then efficiency considerations should apply to these designs. Evidently a useful preliminary step is to aim to identify those designs that possess this largest breakdown number. <PARAGRAPH_SEPARATOR> Definition 4. Given a cross-over design $D$ , let $D_{\\mathrm{min}}$ denote the eventual design which remains after all s subjects drop-out after completing two periods. Then $D_{\\mathrm{min}}$ is termed the minimal design associated with $D$ . <PARAGRAPH_SEPARATOR> Assume without loss of generality that the ordering of the rows of $X$ is such that $X_{*}$ , the submatrix of $X$ consisting of the first 2s rows, corresponds to the first two periods of the design. Then $X_{*}$ can be written as <PARAGRAPH_SEPARATOR> $$ X_{*}=\\left[X_{\\operatorname*{min}}\\mathrm{~}0_{2s}^{*}\\right] $$ <PARAGRAPH_SEPARATOR> where $X_{\\mathrm{min}}$ is the minimal design matrix and $0_{2s}^{*}$ is a $2s\\times(p-2)$ matrix consisting of zero elements. Clearly the rows of $X_{*}$ form a subspace ${\\mathcal{R}}_{*}\\subseteq{\\mathcal{R}}_{\\mathrm{plan}}$ . Using an argument of Godolphin and Godolphin (2017), it can be shown that $\\dim(\\mathcal{R}_{*})=\\operatorname{rank}\\left(X_{\\operatorname*{min}}\\right)$ , consequently there are two possible cases to consider. If the minimal design matrix $X_{\\mathrm{min}}$ is connected, i.e. it has maximal rank $s+3$ , then the deficiency in the dimension $\\mathrm{dim}\\big(\\mathcal{R}_{*}\\big)$ compared to $\\dim\\left(\\mathcal{R}_{\\mathfrak{p l a n}}\\right)$ is due solely to the fact that no measurements are made in the final $p-2$ periods which are missing in the minimal design. This implies that no drop-out activity from the design $D$ which occurs in these final $p-2$ periods will result in a disconnected eventual design, i.e. $D$ is perpetually connected. On the other hand if $X_{\\mathrm{min}}$ is disconnected then there will be other eventual designs that include $X_{\\mathrm{min}}$ which are also disconnected. <PARAGRAPH_SEPARATOR> # 3.3. Choosing a perpetually connected design <PARAGRAPH_SEPARATOR> Clearly if a perpetually connected design exists then it should be considered seriously for selection. However, it sometimes happens that the set $\\mathcal{D}$ of perpetually connected designs which are available may contain several members. In these circumstances a choice of planned design from $\\mathcal{D}$ is sensible. Considerable attention has been given to design selection based on various optimality criteria which assume no drop-out will arise: see the useful reviews in Jones and Kenward (2015, Chapter 3), and Chow and Liu (2008, Chapters 9-10). <PARAGRAPH_SEPARATOR> In addition the authors suggest making use of a complementary method of choice between members of $\\mathcal{D}$ that does not appear to be available easily in many software packages. This procedure gives the explicit forms for the unbiased estimators of ${\\tau_{A}}-{\\tau_{B}}$ and $\\rho_{A}-\\rho_{B}$ as linear forms in the observations Y , for each competing member of $\\mathcal{D}$ The method is based on an established alternative test of estimability, described by Searle (1971, Section 5.4). Suppose that $\\lambda\\in{\\mathfrak{A}}$ , where $\\varLambda$ is the estimability space defined in (3). Then $\\lambda^{\\prime}\\theta$ is estimable so it follows from Proposition 1 that there is a $w$ of size $n\\times1$ , referred to as the weight vector, such that <PARAGRAPH_SEPARATOR> $$ \\lambda^{\\prime}=w^{\\prime}X. $$ <PARAGRAPH_SEPARATOR> Let $(X^{\\prime}X)^{-}$ be a generalized inverse of $X^{\\prime}X$ , i.e. $(X^{\\prime}X)^{-}$ satisfies $X^{\\prime}X(X^{\\prime}X)^{-}X^{\\prime}X=X^{\\prime}X$ . Then $P_{X}=X(X^{\\prime}X)^{-}X^{\\prime}$ is the orthogonal projection operator on the column space of $X$ and <PARAGRAPH_SEPARATOR> $$ P_{X}X=X\\Rightarrow X^{\\prime}=X^{\\prime}P_{X}=X^{\\prime}X(X^{\\prime}X)^{-}X^{\\prime} $$ <PARAGRAPH_SEPARATOR> after noting that $P_{X}$ is symmetric. Thus if $S_{X}$ is defined as $S_{X}=X^{\\prime}X(X^{\\prime}X)^{-}$ then $S_{X}X^{\\prime}=X^{\\prime}$ , i.e. $S_{X}$ is a projection operator on the column space of $X^{\\prime}$ , which is $\\mathcal{R}_{\\sf p l a n}$ . This implies that $\\lambda\\in\\mathcal{R}_{\\mathrm{plan}}$ if and only if $\\lambda^{\\prime}S_{X}^{\\prime}=\\lambda^{\\prime}$ . From this condition it follows that, <PARAGRAPH_SEPARATOR> whenever $\\lambda^{\\prime}\\theta$ is estimable <PARAGRAPH_SEPARATOR> $$ \\lambda^{\\prime}=\\lambda^{\\prime}S_{X}^{\\prime}=\\lambda^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}X=w^{\\prime}X, $$ <PARAGRAPH_SEPARATOR> taking account of (6), where the weight vector is given by <PARAGRAPH_SEPARATOR> $$ w^{\\prime}=\\lambda^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}. $$ <PARAGRAPH_SEPARATOR> Searle (1971, page 185) remarks that ‘‘the derivation of a vector $\\lambda$ satisfying $\\lambda^{\\prime}=w^{\\prime}X$ may not always be easy’’, however the improvement in methods for computing the expression (8) in the intervening years has probably made this comment somewhat pessimistic. In particular, it follows from (8) that expressions for the least-squares estimators of the estimable contrasts $\\tau_{A}-\\tau_{B}$ and $\\rho_{A}-\\rho_{B}$ are given by <PARAGRAPH_SEPARATOR> $$ \\widehat{\\tau_{A}}-\\widehat{\\tau_{B}}=w_{\\tau}^{\\prime}Y=\\lambda_{\\tau}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}Y\\mathrm{and}\\widehat{\\rho_{A}}-\\widehat{\\rho_{B}}=w_{\\rho}^{\\prime}Y=\\lambda_{\\rho}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}Y $$ <PARAGRAPH_SEPARATOR> respectˆivelyˆ, where $\\lambda_{\\tau}$ and $\\lambda_{\\rho}$ are the $(p+s+4)\\times1$ vectors <PARAGRAPH_SEPARATOR> $$ \\lambda_{\\tau}=[1-1000_{p+s}^{\\prime}]^{\\prime}\\mathrm{and}\\lambda_{\\rho}=[001-10_{p+s}^{\\prime}]^{\\prime}. $$ <PARAGRAPH_SEPARATOR> These estimators (9) are specified uniquely as weighted sums of the elements of $Y$ and are, of course, unbiased. The leastsquares estimators of $\\tau_{A}-\\tau_{B}$ and $\\rho_{A}-\\rho_{B}$ have sampling variances <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\mathsf{v a r}(\\widehat\\tau_{A}-\\widehat\\tau_{B})=w_{\\tau}^{\\prime}w_{\\tau}\\sigma^{2}\\quad\\mathrm{and}\\mathsf{v a r}(\\widehat\\rho_{A}-\\widehat\\rho_{B})=w_{\\rho}^{\\prime}w_{\\rho}\\sigma^{2},}\\end{array} $$ <PARAGRAPH_SEPARATOR> respectively.These results can be summarized as follows: <PARAGRAPH_SEPARATOR> Proposition 2. If the design $D$ is connected then the weighted coefficients of the least-squares estimators of the direct effects contrast and the carry-over effects contrast are given by $w_{\\tau}^{\\prime}=\\lambda_{\\tau}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}$ and $w_{\\rho}^{\\prime}=\\lambda_{\\rho}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}$ respectively, where $\\lambda_{\\tau}^{\\prime}$ , $\\lambda_{\\rho}^{\\prime}$ are given by (10). The least squares contrast estimators are given by (9) and their sampling variances are given by (11) <PARAGRAPH_SEPARATOR> It should be remarked that Proposition 2 also applies to an eventual cross-over design $D_{e}$ in the event of missing observations due to subject drop-out, provided that $D_{e}$ is connected. The proposition is therefore useful for establishing the weights $w$ for eventual designs that occur after subject drop-out from a perpetually connected design. Furthermore it is evident that a choice between competing perpetually connected designs in $\\mathcal{D}$ can be made by comparing the corresponding sampling variances for the direct effects var $\\dot{\\tau}(\\widehat{\\tau_{A}}-\\widehat{\\tau_{B}})$ , specified in (11), which give the usual measure of precision of the estimator in each case."
  },
  {
    "qid": "statistic-posneg-767-0-0",
    "gold_answer": "TRUE. The condition ensures that the normalized perturbation and bandwidth terms decay appropriately, allowing the proof of Theorem 3 to hold. Specifically, it guarantees that the terms involving $\\Delta_{n}$ and $h_{n}$ do not dominate the likelihood ratio, leading to $\\hat{\\alpha}_{n} \\to 0$ in probability under misspecification.",
    "question": "Is the condition $\\vert\\log(\\Delta_{n})\\vert^{1/\\beta}(\\sqrt{\\vert\\log(h_{n})\\vert/n h_{n}^{d}}+h_{n}^{2})\\rightarrow0$ sufficient to ensure the convergence in probability $\\alpha_{n}\\to0$ when the model is misspecified?",
    "question_context": "Parametric Maximum Likelihood Estimator Convergence\n\nThe paper discusses the conditions under which the maximum likelihood estimator (MLE) converges when the model is misspecified (i.e., $f_{0}\\notin\\mathcal{P}$). Key conditions include the Glivenko-Cantelli property of the model class, integrability of the envelope function, and specific convergence rates for bandwidth and perturbation terms. The paper also provides auxiliary lemmas supporting the main theorems."
  },
  {
    "qid": "statistic-posneg-1013-0-1",
    "gold_answer": "FALSE. The paper shows in Theorem 3.2 that (T2 - nq)/√(2nq) converges to a standard normal distribution N(0,1), not a chi-squared distribution. This arises from the trace of U'U having an approximate normal distribution after appropriate centering and scaling, where U contains normalized multivariate normal vectors.",
    "question": "Does the paper prove that the T2 test statistic converges to a chi-squared distribution under the null hypothesis?",
    "question_context": "Asymptotic Distribution of Test Statistics in High-Dimensional MANOVA\n\nThe paper models a high-dimensional MANOVA scenario where the number of observations N is less than the dimension p. It introduces test statistics T1 and T2 for hypothesis testing, derives their asymptotic distributions under null and alternative hypotheses, and compares their powers. Key formulas include the characteristic function expansions for the normalized trace statistics u and v, and the limiting distributions of T1 and T2 under local alternatives."
  },
  {
    "qid": "statistic-posneg-789-0-2",
    "gold_answer": "FALSE. The context explicitly states that the Fréchet copula is parametrized by r, s ∈ [0,1] such that r + s ≤ 1, not r + s = 1. The condition r + s ≤ 1 ensures the copula remains valid, as it allows for a mixture of M, W, and π copulas without violating the constraints of a copula function.",
    "question": "The Fréchet copula C_{r,s}^F(u,v) = r M(u,v) + s W(u,v) + (1-r-s)π(u,v) is only valid when r + s = 1. Is this statement true based on the context?",
    "question_context": "Invariant Correlation and Quasi-Independence in Bivariate Random Vectors\n\nThe paper discusses invariant correlation in bivariate random vectors, introducing concepts like quasi-independence and quasi-Fréchet models. It presents examples and propositions to illustrate these concepts, including the use of Fréchet copulas and the conditions under which a random vector exhibits invariant correlation."
  },
  {
    "qid": "statistic-posneg-1760-0-0",
    "gold_answer": "TRUE. The paper proves that the shortest prediction interval within the family F_0 is achieved by setting γ = γ_0^opt, which is the unique solution to the given equation. This solution minimizes the length of the interval by unequally splitting the tail probabilities, resulting in a shorter interval compared to the standard symmetric approach.",
    "question": "Is the shortest prediction interval for Z_0 obtained by setting γ = γ_0^opt in the family of intervals F_0, where γ_0^opt is the unique solution to ϕ^(-1)(1-γ) - ϕ^(-1)(1-α+γ) = 2σ̂_0(ϑ)?",
    "question_context": "Shortest Prediction Intervals in Log-Gaussian Random Fields\n\nThe paper discusses the construction of prediction intervals for log-Gaussian random fields, comparing standard and shortest prediction intervals. The random field {Z(s), s ∈ D} is log-Gaussian if Y(s) = log(Z(s)) is Gaussian with mean and covariance functions given by E{Y(s)} = ∑β_j f_j(s) and cov{Y(s), Y(u)} = C(s, u). The goal is to construct a 1-α prediction interval for Z_0 = Z(s_0) based on observed data Z_i,obs = Z(s_i)ε_i, where log(ε_i) ~ N(0, σ_ε^2). The standard prediction interval is derived by exponentiating the endpoints of a symmetric interval for Y_0, while the shortest prediction interval is obtained by optimizing the tail probabilities."
  },
  {
    "qid": "statistic-posneg-954-1-1",
    "gold_answer": "FALSE. The formula provided is a simplified version and does not fully account for the probability of capturing new individuals. The correct sample coverage estimator should include a more comprehensive adjustment for the heterogeneity in capture probabilities, which is not reflected in this simplified form.",
    "question": "Does the sample coverage estimator φ₁ˢᶜ(f_Q) = f_Q(1) / Σ_{x=2}^t x f_Q(x) correctly account for the probability of capturing new individuals in subsequent samples?",
    "question_context": "Estimating Population Sizes Using Capture-Recapture Methods\n\nThe paper discusses various estimators for population size in capture-recapture studies, including jackknife and sample coverage estimators. The formulas provided include the second-jackknife estimator and two sample coverage estimators with modifications. The simulation considers four mixing distributions (P1 to P4) to evaluate the performance of these estimators."
  },
  {
    "qid": "statistic-posneg-1672-2-1",
    "gold_answer": "FALSE. The bias correction approach requires Σ_{j=1}^J p_{ij} = 1. However, in the Poisson log-linear model, the pseudoresponses Y_{ij}^* = Y_{ij} + a_{ij} (where a_{ij} > 0) lead to Σ_{j=1}^J Y_{ij}^* > 1, violating the constraint. Therefore, the bias correction cannot be directly applied to the Poisson log-linear model formulation.",
    "question": "Can the bias correction approach be directly applied to a Poisson log-linear model version of the multinomial logistic regression model?",
    "question_context": "Bias Correction in Multinomial Logistic Regression Models\n\nThe paper discusses the equivalence of score equations for β from the multinomial likelihood and Poisson likelihood formulations, highlighting that the expected information is identical under both. It details the multinomial ML equations and the observed information matrix, emphasizing the role of the generalized inverse of the variance-covariance matrix. The paper also explores the application of bias correction in multinomial logistic regression models and its limitations when expressed as a Poisson log-linear model with subject-specific effects."
  },
  {
    "qid": "statistic-posneg-266-1-0",
    "gold_answer": "TRUE. The paper explicitly states that $E(T_k) = D_k$, and under $H_0$, $D_k = 0$. Therefore, $E(T_k) = 0$ under $H_0$, making $T_k$ unbiased for $D_k$ under the null hypothesis.",
    "question": "Is the test statistic $T_k$ unbiased for $D_k$ under the null hypothesis $H_0$?",
    "question_context": "Testing Equality of Means in Functional Data Analysis\n\nThe paper discusses testing the equality of mean functions for a large number of functional data samples. The test statistic $T_k$ is constructed to compare the means of $k$ independent functional samples. Under the null hypothesis $H_0$, all mean functions are equal in the $L^2$ sense. The test statistic is derived and its asymptotic distribution is studied under certain conditions."
  },
  {
    "qid": "statistic-posneg-1435-0-0",
    "gold_answer": "TRUE. Explanation: The term $\\Psi$ is described as a correcting factor for the effect of linkage. If $\\Psi = 1$, the joint probability $p(G^{i}G^{j},T^{r}T^{s})$ simplifies to the product of the individual probabilities $\\pi(G^{i}G^{j})$ and $\\pi(T^{r}T^{s})$, which would be the case under the null hypothesis of no linkage (i.e., independence between the main and test characters). Thus, $\\Psi = 1$ implies no linkage effect, and deviations from 1 indicate the presence of linkage.",
    "question": "In the formula $p(G^{i}G^{j},T^{r}T^{s})=\\pi(G^{i}G^{j})\\pi(T^{r}T^{s})\\:\\Psi$, the term $\\Psi$ accounts for the effect of linkage. Is it correct to assume that $\\Psi = 1$ implies no linkage between the main and test characters?",
    "question_context": "Linkage Detection in Human Genetics: Sib-pair Analysis\n\nThis table is arranged so as to give also the actual probability $p(G^{i}G^{j},T^{r}T^{s})$ of any sib-pair classified as $G^{i}G^{j}$ in the main character, and $T^{r}T^{s}$ in the test character.Write <PARAGRAPH_SEPARATOR> $$ p(G^{i}G^{j},T^{r}T^{s})=\\pi(G^{i}G^{j})\\pi(T^{r}T^{s})\\:\\Psi, $$ <PARAGRAPH_SEPARATOR> where $\\pi(G^{i}G^{j})$ is the probability of pair $G^{i}G^{j}$ when $\\zeta=0$ $\\pi(T^{r}T^{s})$ the same for $T^{r}T^{s}$ and $\\downarrow$ a correcting factor for the effect of linkage. The value of $\\pi(G^{i}G^{j})$ for all kinds of pairs is given in Table 5._The value of $\\pi(T^{r}T^{s})$ follows from symmetry. The value of $\\updownarrow$ can be found from the entry in Table 5 by the use of Table 6. <PARAGRAPH_SEPARATOR> The variances per sib-pair can be found from the formula $\\kappa=\\Sigma\\pi a^{2}$ and are given in Table 7. For recessivity types $[22^{*}]$ and [4] in either character $\\pmb{\\kappa}=\\mathbf{0}$ , since all scores are zero. <PARAGRAPH_SEPARATOR> These tables enable a test for linkage to be made in any set of families in which the parental genotypes are known.For each family a score $\\lambda$ is found, by adding the scores for each sib-pair as given in Table 5, and a variance by multiplying the value of $\\pmb{\\kappa}$ in Table 7 by $\\frac{1}{2}n(n-1)$ ，the number of sib-pairs (where $_n$ is the number of sibs). The scores and variances for the different families are summed to give a total score and total variance. If the total score is positive and exceeds (say) 3 times the root of the variance it may be considered significant. Note that any one phenotype may be given different symbols in different families: e.g., blood-group $\\pmb{A}$ is of type $\\mathbf{G_{2}}$ when themating is $\\mathbf{AA}\\times\\mathbf{AO}$ ,but $\\mathbf{G_{3}}$ when themating is $\\mathbf{AO}\\times\\mathbf{AO}$"
  },
  {
    "qid": "statistic-posneg-1897-0-1",
    "gold_answer": "FALSE. While the importance sampling scheme can improve efficiency by focusing sampling on regions of higher importance, its effectiveness depends on the choice of the importance distribution h(β). The paper notes that the natural choice of h(β) based on the original degrees of freedom may not always be efficient, and further modifications (e.g., reducing degrees of freedom or sampling every r-th order statistic) may be necessary to achieve significant variance reduction.",
    "question": "Does the importance sampling scheme proposed in the paper always yield a more efficient estimate of the p-value compared to simple random sampling?",
    "question_context": "Importance Sampling for p-value Computations in Multivariate Tests\n\nThe paper discusses the use of importance sampling to compute p-values for multivariate tests, focusing on the distribution of eigenvalues under the null hypothesis. The joint density of the eigenvalues is given, and an importance sampling scheme is proposed to estimate the p-values efficiently. The method involves generating ordered beta variates and using control variates to reduce variance."
  },
  {
    "qid": "statistic-posneg-1021-0-0",
    "gold_answer": "TRUE. The condition $\\|\\mathbf{B}._{j}\\|_{2}^{2}=1$ is indeed necessary for identifiability in the MSIM. Since the link functions $f_j$ are nonparametric and unspecified, the scale of $\\mathbf{B}_{\\cdot j}$ is not identifiable without this constraint. For example, if $\\mathbf{B}_{\\cdot j}$ were scaled by a constant $c$, the link function $f_j$ could be adjusted by $f_j(\\cdot/c)$ to produce the same model predictions, leading to an identifiability issue. The constraint $\\|\\mathbf{B}._{j}\\|_{2}^{2}=1$ fixes the scale of $\\mathbf{B}_{\\cdot j}$, ensuring that the model is identifiable. Additionally, the requirement that the first element of $\\mathbf{B}_{\\cdot j}$ is positive resolves the sign ambiguity, as $f_j(\\mathbf{x}_i^\\top \\mathbf{B}_{\\cdot j})$ and $f_j(-\\mathbf{x}_i^\\top \\mathbf{B}_{\\cdot j})$ would otherwise be indistinguishable without further constraints.",
    "question": "In the Multivariate Response Single Index Model (MSIM), the condition $\\|\\mathbf{B}._{j}\\|_{2}^{2}=1$ is imposed for model identifiability. Is it true that this condition is necessary because the link functions $f_j$ are nonparametric and the scale of $\\mathbf{B}_{\\cdot j}$ cannot be determined without this constraint?",
    "question_context": "Multivariate Response Single Index Model with Sparsity Constraints\n\nMultivariate response linear regression models are commonly used to predict multiple responses simultaneously given a common set of covariates. Consider data $\\{\\mathbf{y}_{i},\\mathbf{x}_{i}\\}_{i=1}^{n}\\in\\mathbb{R}^{q}\\times\\mathbb{R}^{p}.$ a coefficient matrix $\\textbf{B}\\in\\mathbb{R}^{p\\times q}$ and uncorrelated random errors $\\{\\epsilon_{i}\\}_{i=1}^{n}\\in\\mathbb{R}^{q}$ assuming $\\mathbb{E}(\\pmb{\\epsilon}_{i})~=~\\pmb{0}$ and $\\begin{array}{r}{\\mathrm{cov}(\\epsilon_{i})=\\sigma^{2}{\\bf I}_{q},}\\end{array}$ the model is given by $$ \\mathbf{y}_{i}=\\mathbf{B}^{\\mathsf{T}}\\mathbf{x}_{i}+\\epsilon_{i}. $$ The model (1.1) may be rewritten more compactly as ${\\bf Y}={\\bf X}{\\bf B}+{\\bf$ $\\mathbf{E}_{:}$ where $\\mathbf{Y}=(\\mathbf{y}_{1},\\dots,\\mathbf{y}_{n})^{\\mathsf{T}}\\in\\mathbb{R}^{n\\times q}$ , ${\\bf X}=({\\bar{\\bf x}}_{1},\\dots,{\\bf x}_{n})^{\\top}\\in\\mathbb{R}^{n\\times p}$ and $\\mathbf{E}~=~(\\epsilon_{1},\\dots,\\epsilon_{n})^{\\intercal}~\\in~\\mathbb{R}^{n\\times q}$ . We assume that $\\mathbf{Y}$ and X have been centered so that the model does not have an intercept term. While univariate regression models can be separately fitted to each response, joint regression models exploit correlations among responses and hence can be superior in prediction accuracy (Breiman and Friedman 1997). Indeed, various methods have been proposed in the literature, such as partial least-squares regression (PLR), canonical correlation regression (CCR), principal component regression (PCR), and reduced rank regression (RRR) (Hair et al. 1998; Velu and Reinsel 2013). Regularization techniques have been adopted for multivariate response regression models when predictors or responses reside in high-dimensional spaces. On one hand, to deal with the challenge of having more predictors than samples, that is, ${p>n}$ , Obozinski, Wainwright, and Jordan (2008) and Obozinski, Taskar, and Jordan (2010) used a block-structured penalty on the regression coefficient matrix to select covariates that are predictive for all responses, Peng et al. (2010) considered a combination of penalties that induces both elementwise sparsity and row sparsity of the coefficient matrix, and Li, Nan, and Zhu (2015) proposed models to include arbitrary group structure for the coefficient matrix. On the other hand, when $q$ is large, low-rank approaches (Yuan et al. 2007; Bunea, She, and Wegkamp 2011; Chen, Dong, and Chan 2013; Ma, Xiao, and Wong 2014) are often adopted by thresholding the singular values of the coefficient matrix. These low-rank models assume that the underlying regression structures have a shared representation for all responses, which leads to substantially fewer unknown parameters to estimate and potentially more efficient model estimation. More parsimonious models may be obtained by enforcing both variable selection penalty and low-rank penalty (Bunea, She, and Wegkamp 2012; Chen and Huang 2012) to handle high-dimensional models with both large $\\boldsymbol{p}$ and large $q$ . The multitask learning problem (Caruana 1998) in the machine learning community considers a similar problem, and we refer interested readers to Evgeniou and Pontil (2004), Argyriou, Evgeniou, and Pontil (2007), and Argyriou, Evgeniou, and Pontil (2008) for relevant representative works. A key assumption of model (1.1) is the linear relationship between each response and covariates, which can be seriously violated in practice. Figure 1 illustrates the relationship between three responses and a common set of covariates in a genetic association study, where a joint linear regression model seems insufficient and nonparametric regression methods such as single index models (SIMs) could accommodate seemingly nonlinear relations present in the data. Because of the curse of dimensionality, a fully nonparametric regression approach is often undesired. Yuan et al. (2007) proposed additive models for each response, where each additive component is a nonparametric univariate function of a covariate. An alternative approach is the SIM, in which each response is modeled as a nonparametric function of an index, a linear combination of the covariates. For univariate response $(q\\mathrm{~=~}1)$ , SIM, as well as its various extensions, such as partially linear SIM and generalized SIM, were extensively studied by Härdle and Stoker (1989), Ichimura (1993), Carroll et al. (1997), Hristache, Juditsky, and Spokoiny (2001), Liang et al. (2010), and Wang et al. (2010) using kernel methods, and by Yu and Ruppert (2002) and Wang and Yang (2009) using splines. For splinebased methods, recent studies of Kong and Xia (2007) and Foster, Taylor, and Nan (2013) considered variable selection for SIM. To tackle both potential nonlinearity in multivariate response regression and challenges with high-dimensional data, we propose a multivariate response single index model (MSIM), where the response vector $\\mathbf{y}_{i}$ is linked to covariates $\\mathbf{x}_{i}$ and coefficient matrix $\\mathbf{B}$ through $q$ unknown and unspecified link functions $\\mathcal{F}=\\{f_{1},\\ldots,f_{q}\\}$ such that $$ \\mathbf{y}_{i}=\\left\\{f_{1}(\\mathbf{x}_{i}^{\\mathsf{T}}\\mathbf{B}_{\\cdot1}),\\cdot\\cdot\\cdot,f_{q}(\\mathbf{x}_{i}^{\\mathsf{T}}\\mathbf{B}_{\\cdot q})\\right\\}^{\\mathsf{T}}+\\epsilon_{i}. $$ In (1.2), $\\mathbf{B}._{j}\\in\\mathbb{R}^{p}$ is the jth column of $\\mathbf{B}$ and is the coefficient vector corresponding to the jth response $y_{i j}$ in $\\begin{array}{r l}{\\mathbf{y}_{i}}&{{}=}\\end{array}$ $(y_{i1},\\dots,y_{i q})^{\\top}$ for all $i=1,\\ldots,n$ . For model identifiability we assume that $\\|\\mathbf{B}._{j}\\|_{2}^{2}=1$ , and require the first element of $\\mathbf{B}_{\\cdot j}$ to be positive. Such an assumption is commonly used in SIMs (Ichimura 1993), and we will further discuss this assumption under the joint model in Section 3.3. Moreover, we shall assume that the coefficient matrix has a sparse structure that induces information sharing among multivariate responses. For example, we may assume that the nonzero rows of the coefficient matrix are sparse, or the vector of singular values of the coefficient matrix are sparse, or both are sparse. These sparsity structures are imposed via corresponding penalty functions on the coefficient matrix, as shall be shown later. The recovery of the sparse structure in the coefficient matrix and the form of the link functions could thus provide insights on how the covariates are related to multivariate responses. To the best of our knowledge, the problem of MSIM with multiple penalties has not been addressed in the literature. And the main contribution of this article is a computational framework for jointly estimating the unknown smooth link functions $\\mathcal{F}$ and the coefficient matrix B under a general collection of sparsityinducing assumptions. At a high level we employ a classic alternating strategy for estimating the link functions conditioned on the coefficient matrix and vice versa. The former problem can be solved using existing nonparametric methods. The latter problem poses a nonlinear optimization with multiple penalties and hence requires a new approach to solve it, and we propose to use the alternating direction method of multipliers (ADMM). The rest of this article is organized as follows. In Section 2, we describe the main model framework and discuss various sparsity-inducing penalties for MSIM. In Section 3, we detail a new alternating two-step iterative algorithm for regularized MSIM. In Section 4, we discuss how to choose tuning parameters for the model. In Sections 5 and 6, we demonstrate the effectiveness of our model through simulation studies and an application in a genetic association study. In Section 7, we make conclusions and discuss future work. # 2. Regularized MSIM We first introduce notation. Bold uppercase letters (e.g., B) denote matrices, bold lowercase letters or uppercase letters with dot notation (e.g., b, $\\mathbf{B}_{\\cdot j}$ or $\\mathbf{B}_{k}.$ ) denote vectors and lowercase letters denote scalars (e.g., $^{b}$ ). In particular, for any matrix $\\mathbf{B}$ , we use $\\mathbf{B}_{\\cdot j}$ and $\\mathbf{B}_{k}$ to denote the jth column and kth row of $\\mathbf{B}$ , respectively. We use $\\left\\|\\cdot\\right\\|_{2}$ to denote the Euclidean norm of a vector and $\\Vert\\cdot\\Vert_{\\mathrm{F}}$ to denote the Frobenius norm of a matrix. To fit our proposed regularized MSIM in (1.2), consider the optimization problem: $$ \\operatorname*{minimize}_{\\mathcal{F},\\mathbf{B}}\\left\\{\\mathcal{L}_{\\mathcal{F}}(\\mathbf{B})+\\sum_{l=1}^{h}\\mathcal{R}_{l,\\lambda_{l}}(\\mathbf{B})\\right\\} $$ subject to identifiability constraints $\\|\\mathbf{B}._{j}\\|_{2}^{2}=1$ and $b_{1j}\\geq0$ for $j=1,\\ldots,q$ . In (2.1), $\\mathcal{L}_{\\mathcal{F}}$ is a loss function depending on the $q$ link functions and $\\mathcal{R}_{l,\\lambda_{l}}\\mathbf{\\dot{s}}$ are regularizers for the coefficient matrix and each depends on a tuning parameter $\\lambda_{l}$ . Treating $f_{j}$ as an elementwise function if its argument is a vector and noticing that $\\begin{array}{r}{\\big\\|\\mathbf{Y}_{\\cdot j}-f_{j}(\\mathbf{X}\\mathbf{B}_{\\cdot j})\\big\\|_{2}^{2}=\\sum_{i=1}^{n}\\mathbf{\\bar{\\{}}{\\gamma_{i j}}-\\mathbf{\\{}}_{f_{j}}(\\mathbf{x}_{i}^{\\top}\\mathbf{B}_{\\cdot j})\\big\\}^{2}}\\end{array}$ , we shall use least-\tsquares loss $$ \\mathcal{L}_{\\mathcal{F}}(\\mathbf{B})=\\frac{1}{2n}\\sum_{j=1}^{q}\\left\\|\\mathbf{Y}_{\\cdot j}-f_{j}(\\mathbf{X}\\mathbf{B}_{\\cdot j})\\right\\|_{2}^{2}. $$ The jth column of $\\mathbf{Y}$ , that is, $\\mathbf{Y}_{\\cdot j}=(y_{1j},\\dots,y_{n j})^{\\top}$ , corresponds to the jth response. The regularizer $\\mathcal{R}_{l,\\lambda_{l}}$ enforces sparsity on the coefficient matrix B. We use multiple sparsity-inducing penalties to recover a parsimonious and sparse coefficient matrix. Below we review some popular matrix penalties. Column penalty"
  },
  {
    "qid": "statistic-posneg-1364-5-2",
    "gold_answer": "TRUE. The paper confirms this by stating that the coverage probability of $C_{\\gamma}$ depends on the unknown parameters only through $\\eta^{\\prime}\\varSigma_{\\gamma}^{1}\\eta = \\theta^{\\prime}Q(Q^{\\prime}D_{n}^{1}Q)^{-1}Q^{\\prime}\\theta/\\sigma^{2} = \\Sigma n_{i}\\{\\theta_{i}-\\bar{\\theta}\\}^{2}/\\sigma^{2}$.",
    "question": "The coverage probability of the confidence set $C_{\\gamma}$ depends on the unknown parameters only through $\\Sigma n_{i}\\{\\theta_{i}-\\bar{\\theta}\\}^{2}/\\sigma^{2}$.",
    "question_context": "Improved Confidence Intervals for Comparisons and Contrasts in ANOVA\n\nThe paper discusses the construction of improved confidence intervals for comparisons and contrasts in the context of one-way analysis of variance (ANOVA). It compares the classic Scheffe intervals with improved intervals based on confidence sets centered at estimators that shrink towards zero. The improved intervals are shown to be uniformly shorter than the Scheffe intervals under certain conditions. The paper provides theoretical results and numerical evidence supporting the coverage properties of these improved intervals."
  },
  {
    "qid": "statistic-posneg-569-1-0",
    "gold_answer": "FALSE. The error rate for the Laplace approximation is derived as $\\delta_{n}(\\theta) \\leqslant \\sum_{i=1}^{n} \\delta_{i}(\\theta) = O_{\\mathtt{p}}(n m_{n}^{-2})$, but this does not necessarily mean the error rate is exactly $O_{\\mathtt{p}}(n m_{n}^{-2})$. The inequality suggests the error is bounded above by $O_{\\mathtt{p}}(n m_{n}^{-2})$, but the actual error could be smaller. The statistical reasoning is that each individual error term $\\delta_{i}(\\theta)$ is $O_{\\mathtt{p}}(m_{n}^{-2})$, and summing over $n$ such terms gives a bound of $O_{\\mathtt{p}}(n m_{n}^{-2})$. However, this does not account for potential cancellations or dependencies among the errors, which could result in a lower overall error rate.",
    "question": "Is it true that the error rate for the Laplace approximation in the latent variable model is $O_{\\mathtt{p}}(n m_{n}^{-2})$, as derived from the individual errors $\\delta_{i}(\\theta) = O_{\\mathtt{p}}(m_{n}^{-2})$? Explain the statistical reasoning behind this result.",
    "question_context": "Asymptotic Validity of Laplace Approximation in Latent Variable Models\n\nSuppose that $Y_{i}\\sim\\mathrm{Bi}(m,p_{i})$ , where logit $p_{i}=b_{i}$ with $b_{i}\\sim N(0,\\theta^{2})$ , for $i=1,\\ldots,n$ . The likelihood is $\\begin{array}{r}{L_{n}(\\theta)=\\prod_{i=1}^{n}L_{i}(\\theta)}\\end{array}$ , where $$ L_{i}(\\theta)=\\int_{-\\infty}^{\\infty}\\{{{\\log\\mathrm{i}}\\mathrm{t}^{-1}(b_{i})}\\}^{\\gamma_{i}}\\{1-{\\log\\mathrm{i}}\\mathrm{t}^{-1}(b_{i})\\}^{m-\\gamma_{i}}\\phi(b_{i};0,\\theta)\\mathrm{d}b_{i} $$ with $\\phi(\\cdot;\\mu,\\sigma)$ being the $N(\\mu,\\sigma^{2})$ density function. If we take a Laplace approximation $\\tilde{L}_{n}\\dot{(\\theta)}$ to the likelihood, it is intuitively clear that $m=m_{n}$ would have to grow with $n$ to give valid inference as $n\\rightarrow\\infty$ . It is less obvious whether any choice of $m_{n}$ that grows with $n$ will give valid inference, or if $m_{n}$ needs to grow with $n$ at some minimum rate. Rue et al. (2009) suggest that any $m_{n}$ which grows with $n$ will suffice, conjecturing that the error rate is the number of latent variables over the total number of observations, although they note that this rate has not been established rigorously. In this case, the error rate refers to the error in approximating $\\pi(\\theta\\mid y)$ with $\\tilde{\\pi}(\\theta\\mid y)$ , found by using a Laplace approximation to the likelihood. The integrated nested Laplace approximations proposed by Rue et al. (2009) are based on this ${\\tilde{\\pi}}(\\theta\\mid y)$ , with further approximations used to approximate the marginal posterior distribution of each component of $\\theta$ , if $p>1$ . In this example $\\theta$ is a scalar, so the integrated nested Laplace approximation to the posterior distribution is precisely ${\\tilde{\\pi}}(\\theta\\mid y)$ . The factorization of the likelihood allows us to study the error for each item $\\epsilon_{i}(\\theta)=\\ell_{i}(\\theta)-$ $\\ell_{i}(\\theta)$ separately and then combine the errors. In the Supplementary Material, we show that for each fixed $\\theta\\in\\Theta$ , $\\delta_{i}(\\theta)=\\|\\nabla_{\\theta}\\epsilon_{i}(\\theta)\\|=O_{\\mathtt{p}}(m_{n}^{-2})$ , so $\\begin{array}{r}{\\delta_{n}(\\theta)\\leqslant\\sum_{i=1}^{n}\\delta_{i}(\\theta)=O_{\\mathtt{p}}(n m_{n}^{-2})}\\end{array}$ ."
  },
  {
    "qid": "statistic-posneg-751-2-2",
    "gold_answer": "TRUE. Explanation: The local sum of squares criterion R_t(c) is indeed used to estimate the state indicator process Δ_t by minimizing the sum of squared residuals over a window of 2c + 1 samples centered at time t. This approach allows for the estimation of the state at each time point based on local data, making it suitable for models with suddenly changing parameters.",
    "question": "Is the local sum of squares criterion R_t(c) used to estimate the state indicator process Δ_t by minimizing the sum of squared residuals over a window of 2c + 1 samples?",
    "question_context": "Autoregressive Model with Suddenly Changing Parameters: Estimation Algorithm\n\nThe paper outlines a least squares estimation algorithm for an autoregressive model with suddenly changing parameters. The algorithm involves estimating the state indicator process by minimizing a local sum of squares criterion over a window of samples. If the number of states and the state vector are known, the state indicator process can be estimated by minimizing the sum of squared residuals. Conversely, if the state indicator process is known, the state vector can be estimated by minimizing a global sum of squares criterion. The paper provides detailed formulas for these estimations, including the variance of the estimators and the probabilities of state transitions."
  },
  {
    "qid": "statistic-posneg-1127-2-1",
    "gold_answer": "TRUE. Fig. 2(a) shows a clear banded structure in $\\hat{A}$ with zero entries outside the band, making it easier to interpret as it reflects a specific ordering (e.g., spatial). In contrast, Fig. 2(b) shows $\\tilde{A}$ lacks any discernible structure, with scattered non-zero entries, making it harder to interpret economically or statistically.",
    "question": "The paper argues that the banded coefficient matrix $\\hat{A}$ is more interpretable than the sparse matrix $\\tilde{A}$ because it has a structured form. Based on Fig. 2, is this claim justified?",
    "question_context": "High-dimensional Banded vs. Sparse Vector Autoregressions: Estimation and Interpretation\n\nThe paper compares banded and sparse autoregressive models for high-dimensional time series data. The sparse model is estimated via lasso by minimizing the objective function: $$\\sum_{t=2}^{n}\\|y_{t}-A y_{t-1}\\|^{2}+\\sum_{i,j=1}^{p}\\lambda_{i}|a_{i j}|,$$ where $\\lambda_{1},\\ldots,\\lambda_{p}$ are tuning parameters estimated by five-fold cross-validation. The prediction accuracy of the sparse model is comparable to banded autoregressive models but slightly worse for two-step-ahead prediction. The banded coefficient matrix $\\hat{A}$ is found to be more interpretable than the sparse matrix $\\tilde{A}$ due to its structured form."
  },
  {
    "qid": "statistic-posneg-611-0-1",
    "gold_answer": "FALSE. The paper indicates that property II, which involves the weak convergence of $X_{n}$ to a Brownian bridge, is not automatically satisfied under the condition $E(|v_{11}|^{m}) \\leqslant m^{\\alpha m}$. It specifically mentions that except for the case where $v_{11}$ is $N(0,1)$, property II has not been shown to hold under other assumptions on $v_{11}$. The condition $E(|v_{11}|^{m}) \\leqslant m^{\\alpha m}$ alone does not guarantee the satisfaction of property II.",
    "question": "Does the condition $E(|v_{11}|^{m}) \\leqslant m^{\\alpha m}$ for all $m \\geqslant 2$ and some $\\alpha > 0$ imply that the sequence $\\{\\omega_{n}\\}$ satisfies property II, which involves the weak convergence of $X_{n}$ to a Brownian bridge?",
    "question_context": "Asymptotic Behavior of Eigenvectors in Sample Covariance Matrices\n\nThis paper investigates the asymptotic behavior of eigenvectors of sample covariance matrices defined as $M_{n}=(1/s)V_{n}V_{n}^{\\mathrm{T}}$ where $V_{n}=(v_{i j})$, with $v_{ij}$ being i.i.d. random variables satisfying $E(v_{11})=0$, $E(v_{11}^{2})=1$, and $E(v_{11}^{4})=3$. The paper explores the convergence properties of the largest eigenvalue $\\lambda_{\\sf max}^{(n)}$ and the behavior of eigenvectors, particularly focusing on the weak convergence of certain stochastic processes to a Brownian bridge."
  },
  {
    "qid": "statistic-posneg-577-0-3",
    "gold_answer": "FALSE. The paper clarifies that while the copula is unique for continuous distributions, it is not uniquely determined for discrete distributions like the multivariate Bernoulli. Multiple copulas can satisfy the condition C(P(X1 ≤ x1), ..., P(Xn ≤ xn)) = P(X1 ≤ x1, ..., Xn ≤ xn) for discrete variables, leading to non-uniqueness.",
    "question": "Is the copula uniquely determined for a multivariate Bernoulli distribution, similar to the case of continuous distributions?",
    "question_context": "Copula Modeling in Multivariate Bernoulli Distributions\n\nThe paper discusses the use of copulas to model multivariate Bernoulli distributions, focusing on the theoretical background and the construction of copula families that can represent such distributions. It presents the joint probability representation, log-linear expansions, and the conditions under which a copula can model the dependence structure of a bivariate Bernoulli distribution. The Normal copula is highlighted as a viable option for such modeling, with specific conditions and bounds discussed."
  },
  {
    "qid": "statistic-posneg-966-0-2",
    "gold_answer": "TRUE. The context explicitly mentions that regularizing on the correlation scale (via $\\hat{\\Theta}_{\\lambda}$) produces a covariance estimator $\\hat{\\Sigma}_{\\lambda}$ that is invariant to scaling of the variables. This invariance is a desirable property for many applications.",
    "question": "Is the covariance matrix estimator $\\hat{\\Sigma}_{\\lambda}=(S^{+})^{1/2}\\hat{\\Theta}_{\\lambda}(S^{+})^{1/2}$ invariant to scaling of the variables?",
    "question_context": "Sparse Covariance Matrix Estimation via Convex Optimization\n\nFor a matrix $M$ , let $|M|_{q}=\\|\\mathrm{vec}(M)\\|_{q}$ denote the $q$ -norm of the vector formed by stacking the columns of $M$ . Let $\\|M\\|_{F}\\equiv|M|_{2}$ and $\\|M\\|$ denote the Frobenius and spectral norms of $M$ respectively. When $M$ is a square matrix, let $|M|$ denote its determinant, $\\operatorname{tr}(M)$ denote its trace, and $\\varphi_{j}(M)$ denote its $j$ th largest eigenvalue. Let $M\\succ0$ indicate that $M$ is symmetric and positive definite. Let $M^{+}$ denote a diagonal matrix with the same diagonal as $M$ and define $M^{-}=M-M^{+}$ . <PARAGRAPH_SEPARATOR> Many sparse estimators of the covariance and precision matrix can be expressed as $\\hat{\\Psi}=$ $\\begin{array}{r}{\\operatorname{argmin}_{\\Psi}\\{L(\\Psi,S)+P(\\Psi)\\}}\\end{array}$ , where $S$ is the sample covariance, $L$ is a loss function, and $P$ is a nondifferentiable penalty function used to encourage sparse solutions. Since inversion tends to destroy sparsity and may introduce noise when $p$ is large, the problems of estimating a sparse covariance matrix and sparse precision matrix are generally considered separately. <PARAGRAPH_SEPARATOR> To estimate a sparse precision matrix, Banerjee et al. (2008) and Yuan & Lin (2007) proposed the penalized normal likelihood precision matrix estimator <PARAGRAPH_SEPARATOR> $$ \\hat{\\Sigma}^{-1}=\\mathop{\\mathrm{argmin}}_{\\Sigma^{-1}\\succ0}\\{\\mathrm{tr}(\\Sigma^{-1}S)-\\log|\\Sigma^{-1}|+\\lambda|\\Sigma^{-1-}|_{1}\\}. $$ <PARAGRAPH_SEPARATOR> With these choices of the loss and penalty functions, the optimization in (1) is convex. If $\\lambda>0$ , the logdeterminant term ensures the existence of a unique global positive definite minimizer, even when $p\\geqslant n$ . High-dimensional asymptotic analyses (Rothman et al., 2008; Lam & Fan, 2009; Ravikumar et al., 2011) demonstrated the merits of this estimator when $p\\geqslant n$ and a fast computational algorithm (Friedman et al., 2008) enabled its application to problems with thousands of variables. <PARAGRAPH_SEPARATOR> With the goal of estimating a sparse or approximately sparse covariance matrix, several types of elementwise thresholding of the sample covariance matrix have been proposed and analysed. These include hard thresholding (Bickel & Levina, 2008a; El Karoui, 2008), soft thresholding with generalizations (Rothman et al., 2009), and adaptive thresholding (Cai & Liu, 2011) which is able to achieve an optimal rate of convergence in the spectral norm, as shown in a 2010 unpublished technical report by T. Cai and H. Zhou. In general, elementwise thresholding estimators have a very low computational cost and good rates of convergence under high-dimensional asymptotics, but may have negative eigenvalues in finite samples. These negative eigenvalues can be avoided by lasso-type penalized normal likelihood (Lam & Fan, 2009), but the resulting estimator $\\hat{\\Sigma}=\\mathrm{argmin}_{{\\Sigma}\\times0}\\{\\mathrm{tr}({\\Sigma}^{-1}S)-\\log|{\\Sigma}^{-1}|+\\lambda|{\\Sigma}^{-}|_{1}\\}$ requires nonconvex optimization and is not unique. Recently, Bien & Tibshirani (2011) developed a majorizeminimize algorithm that computes this estimator as a special case. <PARAGRAPH_SEPARATOR> Our interest is to construct a sparse covariance estimator via convex optimization that performs well in high-dimensional settings and is positive definite in finite samples. Positive definiteness is desirable when the covariance estimator is applied to methods for supervised learning. Many of these methods either require a positive definite covariance estimator, or use optimization that is convex only if the covariance estimator is nonnegative definite, e.g., quadratic discriminant analysis and covariance regularized regression (Witten & Tibshirani, 2009). <PARAGRAPH_SEPARATOR> # 2. A POSITIVE DEFINITE SPARSE COVARIANCE ESTIMATOR <PARAGRAPH_SEPARATOR> # 2·1. Methodology <PARAGRAPH_SEPARATOR> We assume that the sample covariance matrix $S$ is computed from $n$ independent copies of $X=$ $(X_{(1)},\\ldots,X_{(p)})^{\\scriptscriptstyle\\mathrm{T}}$ , where $E(X)=0$ and $E(X X^{\\mathrm{{T}}})=\\Sigma_{0}$ . Let $\\Theta_{0}$ denote the population correlation matrix and let $R$ denote the sample correlation matrix. We propose the correlation matrix estimator <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\hat{\\Theta}_{\\lambda}=\\underset{\\Theta>0}{\\operatorname{argmin}}(\\lVert\\Theta-R\\rVert_{F}^{2}/2-\\tau\\log|\\Theta|+\\lambda|\\Theta^{-}|_{1}),}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\lambda\\geqslant0$ is a tuning parameter and $\\tau>0$ is fixed at a small value. The logarithmic barrier term ensures the existence of a positive definite solution, since $\\begin{array}{r}{-\\tau\\log|\\Theta|=-\\tau\\sum_{j=1}^{p}\\log\\varphi_{j}(\\Theta)}\\end{array}$ when $\\Theta\\succ0$ . The lasso-type penalty is used to encourage sparse solutions, analogous to  its use in (1). <PARAGRAPH_SEPARATOR> We propose the covariance matrix estimator $\\hat{\\Sigma}_{\\lambda}=(S^{+})^{1/2}\\hat{\\Theta}_{\\lambda}(S^{+})^{1/2}$ . Motivated by Rothman et al. (2008) and Lam & Fan (2009), regularizing on the correlation scale enables us to prove a faster convergence rate bound and produces a covariance estimator $\\hat{\\Sigma}_{\\lambda}$ that is invariant to scaling of the variables. <PARAGRAPH_SEPARATOR> To see the effect of the logarithmic barrier term in (2), suppose that $\\lambda=0$ . This implies that $\\hat{\\Theta}_{0}$ and $R$ have the same eigenvectors and the eigenvalues of $\\hat{\\Theta}_{0}$ are an inflation of the eigenvalues of $R$ . Specifically, $\\varphi_{j}(\\hat{\\Theta}_{0})=\\varphi_{j}(R)^{-}/2+\\{\\varphi_{j}^{2}(R)+4\\tau\\}^{1/2}/2(j=1,\\dots,p)$ . If $p\\geqslant n$ and $j\\geqslant n$ , then $\\varphi_{j}(R)=0$ , implying that $\\varphi_{j}(\\hat{\\Theta}_{0})=\\tau^{1/2}$ . Also, one can show that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\hat{\\Theta}_{0}^{-1}=\\underset{\\Theta^{-1}>0}{\\operatorname{argmin}}\\{\\mathrm{tr}(\\Theta^{-1}R)-\\log|\\Theta^{-1}|+\\tau\\|\\Theta^{-1}\\|_{F}^{2}/2\\}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> The objective function in (3), with $R$ replaced by $S$ , is equivalent to the negative normal loglikelihood with a ridge penalty, for which Witten & Tibshirani (2009) derived the minimizer in closed form. Thus, $\\hat{\\Theta}_{0}^{-1}$ can be viewed as a ridge penalized likelihood inverse correlation matrix estimator. <PARAGRAPH_SEPARATOR> If $\\tau=0$ and the feasible set in (2) is expanded to $\\{\\Theta:\\Theta=\\Theta^{\\mathrm{{T}}}\\}$ , then $\\hat{\\Theta}_{\\lambda}$ is obtained by elementwise soft thresholding (Donoho & Johnstone, 1994; Rothman et al., 2009) of $R^{-}$ at $\\lambda$ . If $\\tau>0$ and $\\lambda>0$ , a closed-form solution is unavailable and we use an efficient iterative algorithm described in $\\S3\\cdot1$ . <PARAGRAPH_SEPARATOR> # 2·2. Asymptotic analysis"
  },
  {
    "qid": "statistic-posneg-2138-0-0",
    "gold_answer": "FALSE. The formula appears to be a variant of the Benjamini-Hochberg (BH) procedure for controlling the False Discovery Rate (FDR), but it is not standard. The correct BH procedure would compare each ordered p-value $p_{(i)}$ to $(i/m) \\cdot q$, where $m$ is the total number of tests and $q$ is the desired FDR level, then find the largest $i$ where $p_{(i)} \\leq (i/m) \\cdot q$. The given formula seems to be an ad-hoc combination method rather than the standard FDR control procedure.",
    "question": "Is the formula $p_{0}=4k\\sum_{i=1}^{4k}i^{-1}\\operatorname*{min}_{i=1,\\ldots,4k}p_{(i)}/i$ correctly representing the FDR-based combination of p-values from multiple tests?",
    "question_context": "Random Projection-Based Test of Gaussianity: FDR p-value Combination\n\nThe $p$ -value <PARAGRAPH_SEPARATOR> Finally, we have $p^{(i,j)}$ , $i=1,\\ldots,k,j=1,\\ldots,4$ that we combine using the FDR to obtain a $p$ -value for the global procedure, $p_{0}$ . For that we first order the $p$ -values increasingly $p_{(1)}\\leq\\cdots\\leq p_{(4k)}.$ Thus, <PARAGRAPH_SEPARATOR> $$ p_{0}=4k\\sum_{i=1}^{4k}i^{-1}\\operatorname*{min}_{i=1,\\ldots,4k}p_{(i)}/i. $$ <PARAGRAPH_SEPARATOR> # 5. An empirical study <PARAGRAPH_SEPARATOR> We analyze two real datasets: the Canadian lynx and the Wolfer sunspot, which were previously found to be nonGaussian (Rao and Gabr, 1980; Epps, 1987). <PARAGRAPH_SEPARATOR> The Canadian lynx data consists in the annual record of the number of lynxes trapped in the Mackenzie River district of the North-West Canada for the period from 1821 to 1934 while the Wolfer sunspot data consists in the annual record of the sunspot activity in the period from 1700 to 1960. We have applied the $\\phantom{}_{1}\\mathrm{RP}$ -test to both, with the $p$ -values displayed in Table 8 together with those obtained in previous studies. The similarity between all of them is quite clear."
  },
  {
    "qid": "statistic-posneg-1370-2-2",
    "gold_answer": "FALSE. The correct covariance for an AR(1) process should be $(u_{i},u_{i-s})=\\rho^{|s|}/(1-\\rho^{2})$, not $\\rho^{s}/(1-\\rho^{2})$. The absolute value of $s$ is necessary to ensure the symmetry of the covariance matrix.",
    "question": "Is the covariance matrix $V$ for an AR(1) process correctly specified as having entries $(u_{i},u_{i-s})=\\rho^{s}/(1-\\rho^{2})$?",
    "question_context": "Longitudinal Designs and Covariate-Adaptive Allocation Rules\n\nThe main algebraic difference between multivariate designs and longitudinal designs comes from the reduced amount of information that is available from previous patients when allocation is made to patient $n+1$ . As an example, in our calculations we assume that patients arrive, or are grouped to arrive, in cohorts of size $n_{g}$ ( $n_{g}$ can equal one). In the general case of $n_{g}>1$ the various members of each cohort can be allocated different treatments. <PARAGRAPH_SEPARATOR> Consider the first patient of cohort $k+1$ , so that $n+1=k n_{g}+1.$ We assume the responses are delayed, so that there is no information on the responses from cohort $k$ . Patients from cohort $k-1$ contribute one response, those from cohort $k-2$ two responses and so forth. Working backwards, the first cohort to contribute all $n_{h}$ responses is number $k-\\ensuremath{n_{h}}$ . Let $S(i,n)$ denote this set of indexes for patient i as cohort $k+1$ starts. Then the information matrix for the allocation of patient $n+1$ is, in an extension of the notation of (3), <PARAGRAPH_SEPARATOR> $$ \\boldsymbol{\\mathcal{I}}(\\boldsymbol{n})=\\sum_{i=1}^{n}\\boldsymbol{F}_{i,S(i,n)}^{T}V_{S(i)}^{-1}\\boldsymbol{F}_{i,S(i,n)}. $$ <PARAGRAPH_SEPARATOR> The same process of counting applies to the sufficient statistics and so to the estimates of $\\alpha$ used in the adaptive allocation rules. For allocation of the remaining observations in cohort $_{k+1}$ we increment the information matrix by the complete value of $F_{i}^{T}V^{-1}F_{i}$ for each allocated observation. This is a temporary measure to aid balance, so that we remove these contributions when recalculating (22) for the next cohort. <PARAGRAPH_SEPARATOR> The other difference between longitudinal data and the multivariate data of Section 2.1 is the structure of the covariance matrix $V$ . In our numerical example the errors form an AR(1) process. A stationary process can be simulated by generating $u_{1}=\\epsilon_{1}\\sim N\\{0,1/(1-\\rho^{2})\\}$ and, for $i>1$ , $u_{i}=\\rho u_{i-1}+\\epsilon_{i}$ where $\\epsilon_{i}\\sim N(0,1)$ . Then var $u_{i}=1/(1-\\rho^{2})$ and cov $(u_{i},u_{i-s})=\\rho^{s}/(1-\\rho^{2})$ . The errors of observation will be $\\sigma u_{i}$ , where $\\sigma$ is to be estimated. <PARAGRAPH_SEPARATOR> For this error structure <PARAGRAPH_SEPARATOR> $$ V^{-1}=\\left(\\begin{array}{c c c c c}{{1}}&{{-\\rho}}&{{0}}&{{0}}&{{\\cdots}}\\\\ {{-\\rho}}&{{1+\\rho^{2}}}&{{-\\rho}}&{{0}}&{{\\cdots}}\\\\ {{0}}&{{-\\rho}}&{{1+\\rho^{2}}}&{{-\\rho}}&{{\\cdots}}\\\\ {{\\ldots}}&{{\\cdot\\cdot\\cdot}}&{{\\cdot\\cdot\\cdot}}&{{\\cdot\\cdot\\cdot}}&{{\\ldots}}\\end{array}\\right) $$ <PARAGRAPH_SEPARATOR> and, from (19) <PARAGRAPH_SEPARATOR> $$ n_{\\mathrm{effec}}=J^{T}V^{-1}J=n_{h}-2(n_{h}-1)\\rho+(n_{h}-2)\\rho^{2}. $$ <PARAGRAPH_SEPARATOR> For $\\rho=0$ , $n_{\\mathrm{effec}}=n_{h}$ . The number however decreases appreciably as $\\rho$ increases; for $n_{h}=4$ and $\\rho=0.5$ , $n_{\\mathrm{effec}}=1.5$ . <PARAGRAPH_SEPARATOR> # 5. Four allocation rules <PARAGRAPH_SEPARATOR> We now present and compare four specific allocation rules, two of which depend directly on the estimated ranking of the treatments. As in Section 2.5, let the ranking of treatment $j$ be $R(j)$ , estimated by $\\widehat{R}(j)$ from the ranking of the estimated treatment effects $\\hat{\\alpha}$ . <PARAGRAPH_SEPARATOR> 1. Rule D (Deterministic). For the purposes of comparison with adaptive rules, we assume that the correct ordering of the treatments is known. There is no randomization and the rule is that for the sequential construction of the optimum design. We allocate that treatment for which (A.2) is a maximum: <PARAGRAPH_SEPARATOR> $$ \\pi_{D}(j|x_{n+1})=\\left\\{{1\\atop0}^{1}j=\\arg\\operatorname*{max}_{j\\in1,\\ldots,t}d_{A}(j,n,x_{n+1})\\atop0\\quad{\\mathrm{otherwise}}.\\right. $$ <PARAGRAPH_SEPARATOR> Simulations of designs for univariate responses mentioned above show that the loss $L_{n}$ for this rule rapidly tends to zero and the bias $B_{n}$ is one, since it is always known which treatment will be allocated next. These are extreme values; other rules have higher loss and lower bias. <PARAGRAPH_SEPARATOR> 2. Rule RA (Random and Response Adaptive). In this rule the treatments are allocated with probabilities $p_{j}^{*}$ introduced in (13) and based on the estimated ranking of treatment effects; there is no attempt at covariate balance. Then (18) reduces to <PARAGRAPH_SEPARATOR> $$ \\pi_{R A}(j)=p_{\\widehat{R}j}^{*}. $$ <PARAGRAPH_SEPARATOR> The best guessing strategy is always to guess that the seemingly best treatment will be allocated. Asymptotically the probability of being correct is $p_{1}^{*}$ (and of being wrong $1-p_{1}^{*},$ , so that the limit of $B_{n}$ in (21) is $2p_{1}^{*}-1$ For univariate responses the asymptotic value of the loss is $q$ (Cox, 1951; Burman, 1996). <PARAGRAPH_SEPARATOR> 3. Rule G (General with ranks). This rule (18) extends Rule RA to include covariate balance. Since $d_{A}(j,n,x_{n+1})$ in (18) is not standardized for $n$ , the rule tends asymptotically to Rule RA. <PARAGRAPH_SEPARATOR> 4. Rule L (Link). For two treatments the target probabilities depend on the estimated difference in treatment means $\\widehat{\\Delta}=\\hat{\\alpha}_{1}-\\hat{\\alpha}_{2}$ . Atkinson and Biswas (2005a) use a link function to relate the $l_{j}$ to $\\widehat{\\varDelta}$ . Following Bandyopadhyay and Biswas (2001) they take $l_{1}=\\varPhi(\\widehat{\\varDelta}/T)$ , where $\\varPhi\\left(.\\right)$ is the standard normal cumulative d istribution function (c.d.f.). The value of $l_{1}$ may be greater or less than 0.5 and the parameter $T$ controls the degree of skewing of the allocation. <PARAGRAPH_SEPARATOR> For generality we present the t treatment version of this rule. After $n$ patients have been treated the estimated treatment parameters are $\\hat{\\alpha}_{j}$ . To preserve the invariance of the procedure to the overall treatment mean let <PARAGRAPH_SEPARATOR> $$ \\bar{\\alpha}=\\sum_{j=1}^{t}\\hat{\\alpha}_{j}/t\\quad\\mathrm{and}\\quad\\widehat{\\varDelta}_{j}=\\hat{\\alpha}_{j}-\\bar{\\alpha}. $$ <PARAGRAPH_SEPARATOR> The cumulative normal distribution provides coefficients $l_{j}$ by setting $l_{j}^{\\prime}=\\varPhi(\\widehat{\\varDelta}_{j}/T)$ with <PARAGRAPH_SEPARATOR> $$ l_{j}=l_{j}^{\\prime}/S_{l},\\quad\\mathrm{where}S_{l}=\\sum_{k=1}^{t}l_{k}^{\\prime}. $$ <PARAGRAPH_SEPARATOR> For $t=2$ this reduces to the design procedure of Bandyopadhyay and Biswas (2001) except that the standard deviation T is replaced by $2T$ . <PARAGRAPH_SEPARATOR> This design procedure does not depend on the values of the covariates. To provide a rule that is covariate adjusted, we use the values of $l_{j}^{\\prime}$ in place of $p_{\\widehat{R}(j)}^{*}$ in (18) to give allocation probabilities <PARAGRAPH_SEPARATOR> $$ \\pi_{L}(j|x_{n+1})=\\frac{\\{1+d_{A}(j,n,x_{n+1})\\}^{1/\\gamma}l_{j}^{\\prime}}{\\displaystyle\\sum_{s=1}^{t}\\{1+d_{A}(s,n,x_{n+1})\\}^{1/\\gamma}l_{s}^{\\prime}}. $$ <PARAGRAPH_SEPARATOR> Use of $l_{j}^{\\prime}$ , or the standardized $l_{j}$ from (23), gives identical allocation probabilities $\\pi_{L}(j|x_{n+1})$ since the summation $S_{l}$ cancels in (24). As for Rule G, the emphasis on covariate balance reduces as $n$ increases, the rule asymptotically reducing to that of Bandyopadhyay and Biswas (2001). <PARAGRAPH_SEPARATOR> The result is a rule in which the targets vary more than in the three other rules of this section. When the estimated treatment difference is small the allocations are closer to 0.5 and when the treatment differences are over-estimated, the allocation probabilities are more extreme. <PARAGRAPH_SEPARATOR> # 6. Distributional results"
  },
  {
    "qid": "statistic-posneg-1332-0-0",
    "gold_answer": "TRUE. The formula correctly represents the likelihood for a given transmission tree as the product of geometric variables. Each term $\\mathcal{P}_{i}^{N_{i}}(1-\\mathcal{P}_{i})$ accounts for the probability that individual $i$ infects exactly $N_{i}$ individuals, which follows a geometric distribution. The product over all individuals in the cluster $\\mathcal{C}$ gives the joint likelihood of the observed transmission pattern under the model assumptions.",
    "question": "Is the likelihood for a given transmission tree $L_{T}(\\mathcal{C})$ correctly represented as the product of geometric variables $\\prod_{i=1}^{|\\mathcal{C}|}\\mathcal{P}_{i}^{N_{i}}(1-\\mathcal{P}_{i})$, where $N_{i}$ is the number of individuals infected by individual $i$?",
    "question_context": "Branching Process Models for Infectious Disease Transmission Risk Factors\n\n![](images/d4325469391c936f620ffe90f3db4c7dc5e645dbd95d1f8b82e2f1691924a21d.jpg)  \nFigure 2. Unique transmission trees for an observed cluster of size three where two individuals are smear negative and one is smear positive. The first number indexes the generation number and the second is the order within that generation. There are six possible transmission trees when we consider the individuals, conditioned on their smear statuses, to be indistinguishable from one another (e.g., there is no observable difference between individuals B and C). Underneath the trees, we show the label(s) of the individuals that each tree can have. Labels in blue show the first path and labels in green show the second path for the shown tree, if we consider the individuals to be distinguishable from one another. There are 12 unique transmission trees when we uniquely label the individuals.\n\n<PARAGRAPH_SEPARATOR>\n\nto the observed data as a cluster $\\mathcal{C}$ to emphasize that no transmission paths are known. We know only the size, $|{\\mathcal{C}}|$ , of the transmission tree and the covariates of the individuals $\\mathbf{X}_{i}$ for $i=1,\\ldots,|\\mathcal{C}|$ within the transmission tree.\n\n<PARAGRAPH_SEPARATOR>\n\nLet $T$ be a transmission tree of size $|{\\mathcal{C}}|$ and $\\tau_{c}$ be the set of all such trees of size $|{\\mathcal{C}}|$ with the given covariates $X_{1},\\dots,X_{|{\\mathcal{C}}|}$ distributed among the nodes of the transmission tree. The likelihood for this cluster $({\\mathcal{L}}({\\mathcal{C}}))$ is the sum over the likelihoods of all permissible transmission trees of size $|{\\mathcal{C}}|$ ,\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\mathcal{L}(\\mathcal{C})=\\sum_{T\\in\\mathcal{T}_{\\mathcal{C}}}L_{T}(\\mathcal{C}),\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nwhere $L_{T}(\\mathcal{C})$ is the likelihood for a given transmission tree and is the product of geometric variables where $N_{i}$ is the number of individuals infected by individual $i,$\n\n<PARAGRAPH_SEPARATOR>\n\n$$\nL_{T}(\\mathcal{C})=\\prod_{i=1}^{|\\mathcal{C}|}\\mathcal{P}_{i}^{N_{i}}(1-\\mathcal{P}_{i}).\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nA limitation of the base model is that it requires a stringent assumption: complete observation of a cluster. For example, consider the transmission tree (left) in Figure 3. In this transmission tree, individuals A and E are not detected. With these individuals missing, it is impossible for our base model to reconstruct the projected transmission trees shown in the middle image, where edges between unobserved individuals have been erased. The image in the middle is not the only possible projection of the transmission of seven individuals into five; one could argue that a projection that also includes an edge between C and G is more accurate. Regardless of the projection chosen, our base model can only generate transmission trees that, for example, will connect individual B to the other observed individuals. As such, we know our estimates for any parameters will be biased as individual B’s infection should be independent from any of the observed individuals. We propose a solution to this issue with an approach we call the multiple outside transmissions model.\n\n<PARAGRAPH_SEPARATOR>\n\nAssuming that the clusters are mutually independent, the total likelihood for $M$ observed clusters is the product of the likelihoods of individual clusters. We can then estimate $\\beta$ as the maximum likelihood estimate (MLE), $\\hat{\\boldsymbol{\\beta}}$ , over the likelihood of the observed clusters,\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\hat{\\pmb{\\beta}}=\\arg\\operatorname*{max}_{\\pmb{\\beta}}\\prod_{m=1}^{M}\\mathcal{L}(C_{m}).\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nA feature of our model shown in Equation (2) is that it can be extended to multiple covariates through the logit function $p_{i}=\\mathrm{logit}^{-1}\\left(\\mathbf{X_{i}}{\\pmb\\beta}\\right)$ for a covariate matrix $\\mathbf{X}$ and a vector of covariates $\\beta$ . This formulation allows nested sets of parameters to be sequentially tested, for example, via a likelihood ratio test.\n\n<PARAGRAPH_SEPARATOR>\n\n# 2.2. The Multiple Outside Transmissions Model\n\n<PARAGRAPH_SEPARATOR>\n\nCompared to the base model, the multiple outside transmissions model allows for a more flexible set of transmission trees, and as a consequence, it does not require all individuals be observed in a cluster. The idea is that any transmission tree with unobserved cases can be projected into connected sub-trees whose primary infector is from some abstract individual outside the observed cluster. For example, in Figure 3 (middle) we can instead view the group of six observed individuals as three singletons and one transmission tree of size three, which are then “glued” together such that they originate from some abstract outside infection, which is portrayed in Figure 3 (right). In this new approximation, we have four infections $(B,C,D,G)$ resulting directly from the abstract outside infector, which is why we call this model the multiple outside transmissions model."
  },
  {
    "qid": "statistic-posneg-443-1-1",
    "gold_answer": "FALSE. The Laplace transform L(t;p̂,m,n) is incorrectly specified as it does not account for the normalization constant C(p̂,m,n) and the integration domain A={(λ₁,...,λₚ)|0<λ₁<⋯<λₚ<∞}. The correct form should explicitly include the normalization constant and the integration over the ordered eigenvalues, ensuring the transform is properly defined with respect to the joint density.",
    "question": "Does the Laplace transform L(t;p̂,m,n) correctly represent the expectation of exp(-t∑_{i=1}^p λ_i) with respect to the joint density f(λ₁,...,λₚ)?",
    "question_context": "Laplace Transform and Density Function of U^(p) in Hotelling's Generalized T^2 Distribution\n\nThe joint density function of λ₁,λ₂,...,λₚ, the characteristic roots of S₁S₂⁻¹, has the form [18] f(λ₁,...,λₚ)=C(ϕ,m,n)∏_{i=1}^p λ_i^m/(1+λ_i)^{m+n+p+1}∏_{i>j}(λ_i-λ_j), 0<λ₁<⋯<λₚ<∞, where C(p̂,m,n)=π^{p̂/2}∏_{i=1}^p Γ(½(2m+2n+p̂+i+2))/{Γ(½(2m+i+1))Γ(½(2n+i+1))Γ(½i)}. Then U^(p)=∑_{i=1}^p λ_i=tr(S₁S₂⁻¹), and the Laplace transform of U^(p) is L(t;p̂,m,n)=E(exp(-t∑_{i=1}^p λ_i))=C∫⋯∫ exp(-t∑_{i=1}^p λ_i)∏_{i=1}^p λ_i^m/(1+λ_i)^{m+n+p+1}∏_{i>j}(λ_i-λ_j)∏_{i=1}^p dλ_i."
  },
  {
    "qid": "statistic-posneg-1135-0-2",
    "gold_answer": "FALSE. The value of K is not directly used in the calculation of the confidence intervals for θ₁. Instead, the intervals are derived using F-statistics and transformations involving τ₁ and θ₁. The matrices L₁₁ and B₁₁ are more directly relevant to the interval calculations, while K appears to be an intermediate calculation or diagnostic measure.",
    "question": "Is the value of K (K = L₁₁³[V₁ᵀ][V₁₁] = -0.0197) used directly in the calculation of the confidence intervals for θ₁?",
    "question_context": "Confidence Intervals for Parameter Subsets in Nonlinear Regression\n\nThe matrices $L_{11}$ and $B_{11}$ and array $K$ are scalars in this example since only one parameter is of interest, and they are $L_{11}=7{\\cdot}96\\times10^{-4}$ , <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{B_{11}=L_{11}^{2}[\\hat{e}^{\\Gamma}][V_{11}]=(7\\cdot96\\times10^{-4})^{2}\\times(2\\cdot3042\\times10^{5})=0\\cdot1460,}\\end{array} $$ <PARAGRAPH_SEPARATOR> $$ K=L_{11}^{3}[V_{1}^{\\mathsf{T}}][V_{11}]=(7{\\cdot}96\\times10^{-4})^{3}\\times(-3{\\cdot}8093\\times10^{7})=-0{\\cdot}0197. $$ <PARAGRAPH_SEPARATOR> Using $F(1,9;0{\\cdot}05)=5{\\cdot}12$ the approximate intervals in terms of the t's are $\\tau_{1}^{2}\\leqslant95{\\cdot}91\\times5{\\cdot}12=(22{\\cdot}16)^{2}$ for the linear approximation, $\\tau_{1}^{2}\\leqslant(1{\\cdot}082\\times22{\\cdot}16)^{2}$ from (3·3) for the likelihood ratio approach, and $\\tau_{1}^{2}\\leqslant(1{\\cdot}232\\times22{\\cdot}16)^{2}$ from (3·7) for the scorebased approach. The linear approximation to the mapping from $\\tau_{1}$ .to $\\theta_{1}$ is $\\theta_{1}=0{\\cdot}0477+7{\\cdot}96\\times10^{-4}\\tau_{1}$ , and the linear approximation confidence interval obtained by substituting. $\\tau_{1}=\\pm22{\\cdot}16$ in this expression is (0-0301, 0-0653). Similarly, the quadratic mapping (4·1) is <PARAGRAPH_SEPARATOR> $$ \\theta_{1}=0{\\cdot}0477+7{\\cdot}96\\times10^{-4}\\tau_{1}+7{\\cdot}85\\times10^{-6}\\tau_{1}^{2}, $$ <PARAGRAPH_SEPARATOR> so that the second-order approximations are (0-0331, 0·0713) for the likelihood ratiobased interval and (0·0318, 0·0753) for the score-based interval. Values of $\\tau_{1}$ and $\\tau_{2}$ were obtained on a grid of $\\theta$ values around $\\widehat{\\theta}.$ A plot of $\\theta_{1}$ versus $\\tau_{1}$ showed a smooth relationship with almost no variation which would indicate dependence on $\\tau_{2}$ ,and regression of $\\theta_{1}$ on $\\pmb{\\tau_{1}}$ and $\\tau_{1}^{2}$ gave a relationship almost identical to the one used above. <PARAGRAPH_SEPARATOR> The linear approximation interval accurately estimates the left endpoints of the exact intervals but poorly estimates the right endpoints, and fails to predict the lengths by 9 and $20\\%$ . The quadratic intervals are slightly less accurate at the left but are much more accurate at the right, with the overall length in error by less than $2\\%$ in each case."
  },
  {
    "qid": "statistic-posneg-968-2-1",
    "gold_answer": "TRUE. The corollary indicates that $O_{\\mathrm{ratio}}$ is directly proportional to sensitivity ($Z_{\\mathrm{sens}}$) and inversely proportional to $(1 - Z_{\\mathrm{spec}})$. Since specificity ($Z_{\\mathrm{spec}}$) is typically high (close to 1), a small decrease in specificity leads to a large increase in $(1 - Z_{\\mathrm{spec}})$, which significantly reduces $O_{\\mathrm{ratio}}$. This demonstrates that specificity has a more pronounced effect on $O_{\\mathrm{ratio}}$ than sensitivity.",
    "question": "Does the corollary stating that $O_{\\mathrm{ratio}}$ is proportional to $Z_{\\mathrm{sens}}$ and inversely proportional to $1 - Z_{\\mathrm{spec}}$ imply that specificity has a greater impact on $O_{\\mathrm{ratio}}$ than sensitivity?",
    "question_context": "Surrogate-Guided Sampling (SGS) Design Efficiency and Likelihood Ratios\n\nThe denominator of $O_{\\mathrm{ratio}}$ is the expected odds of cases for samples collected with SRS, and is less than 1 for outcome with prevalences less than $50\\%$ . The numerator is the expected odds of cases for samples collected with SGS designs. Therefore, $O_{\\mathrm{ratio}}$ is a single estimate of design effect on sample outcome prevalence, and can be interpreted as the expected increase in cases comparing SGS to SRS, with higher values indicating that SGS provides more case enrichment, and $O_{\\mathrm{ratio}}>1$ indicating improvement using SGS relative to SRS. An interesting property of $O_{\\mathrm{ratio}}$ is the connection to likelihood ratios (LRs) of the enrichment surrogate. Of note, LRs of a diagnostic test can be interpreted as slopes of Receiver Operating Characteristics (ROC) curves, are related to positive and negative predictive values (PPV and NPV), but are invariant to outcome prevalence (Choi, 1998). Therefore, by framing enrichment surrogates $Z$ as “prior tests” of outcome $Y$ , we may gain insight into what types of variables are the best surrogates for sampling. <PARAGRAPH_SEPARATOR> # PROPOSITION 2.1 Properties of $O_{\\mathrm{ratio}}$ . <PARAGRAPH_SEPARATOR> Let an SGS design of sample size $n$ be defined with surrogate $Z$ and sampling ratio $R=P(Z=1|S=1)$ ), where $R=0.50$ corresponds to a “balanced” SGS 1:1 design. Let $Z$ has operating characteristics: $Z_{\\mathrm{sens}}:=$ $P(Z=1|Y=1)$ ), $Z_{\\mathrm{spec}}=P(Z=0|Y=0)$ . Then, if the outcome is rare $(P(Y=1)\\approx0)$ ), then <PARAGRAPH_SEPARATOR> $$ O_{\\mathrm{ratio}}(R,Z)\\approx(R)(L R+)+(1-R)(L R-). $$ <PARAGRAPH_SEPARATOR> COROLLARY 2.1 For a given Z, Oratio ∝ R. Over the set of possible Z, Oratio ∝ Zsens and Oratio ∝ 1 −1Zspec . <PARAGRAPH_SEPARATOR> Details of Proposition 2.1 and Corollary 2.1 are in Supplementary Material B available at Biostatistics online. For a given surrogate, higher values of $O_{\\mathrm{ratio}}$ can be achieved by over-representing surrogate positives. Over the range of possible surrogates, a small change in surrogate specificity can have a much higher impact on $O_{\\mathrm{ratio}}$ compared to the same change in surrogate sensitivity. Figure 1 demonstrates the impact of surrogate operating characteristics on $O_{\\mathrm{ratio}}$ for a fixed sampling ratio of $R=0.50$ (i.e., SGS 1:1 ratio of cases to control, a “balanced” design) and prevalence of $10\\%$ . An SGS design with $O_{\\mathrm{ratio}}>2$ , may be obtained by using a surrogate with (sensitivity, specificity) of $(30\\%,90\\%)$ or $(90\\%,80\\%$ ), where in order to maintain the same sample case–control odds a small decrease of surrogate specificity required a much larger increase in surrogate sensitivity. In fact, very high information designs, such as $O_{\\mathrm{ratio}}>3$ , can only be achieved when surrogate specificity is at least $90\\%$ . <PARAGRAPH_SEPARATOR> ![](images/b0224c8b98d0cd482477b2eb83b4ed8441373becdb407c60c75470385df3e6ba.jpg)  Fig. 1. $O_{\\mathrm{ratio}}$ values for surrogates of different marginal sensitivity and specificity, based on a fixed $R=0.50$ and an outcome with prevalence of $10\\%$ ."
  },
  {
    "qid": "statistic-posneg-41-0-1",
    "gold_answer": "TRUE. When all V_i matrices are identical, V_i = V̄ for all i. In this case, the function T(V_i^{-1}) simplifies to T(V̄^{-1}) = 1/n, and T{(1^T V_i 1)^{-1} 11^T} also equals 1/n. Therefore, B(V_i^{-1}) = T(V_i^{-1}) - T{(1^T V_i 1)^{-1} 11^T} = 1/n - 1/n = 0. This confirms that no borrowing of strength occurs when all studies have identical variance matrices, as the secondary outcomes provide no additional information beyond the primary outcomes.",
    "question": "Does the borrowing of strength B(V_i^{-1}) = T(V_i^{-1}) - T{(1^T V_i 1)^{-1} 11^T} always equal zero when all V_i matrices are identical?",
    "question_context": "Multivariate Meta-Analysis: Efficiency and Borrowing of Strength\n\nThe paper discusses a multivariate meta-analysis of n independent studies, each measuring a p×1 vector y of treatment effect estimates. The standard multivariate fixed effects model is y_i ∼ N(β, V_i), where V_i is the variance matrix specific to each study, and β is the common mean parameter. The maximum likelihood estimate of β is given by β̂ = Ω ∑ V_i^{-1} y_i, where Ω = (∑ V_i^{-1})^{-1}. The efficiency E is defined as the ratio of the variance of the multivariate estimate β̂_1 to the variance of the univariate estimate β̃_1, with E ≤ 1. The borrowing of strength B(V_i^{-1}) measures the contribution of secondary outcomes to the variance of β̂_1."
  },
  {
    "qid": "statistic-posneg-887-0-3",
    "gold_answer": "TRUE. The integral equation accounts for the expected number of female births b(t) at time t, considering both the initial population's contribution g(t) and the births from individuals born after the initial epoch. The term λ(y)e^(-μy)b(t-y)dy represents the expected births from females of age y who survive to time t.",
    "question": "The integral equation b(t) = g(t) + ∫₀ᵗ λ(y)e^(-μy)b(t-y)dy is used to describe the expected number of female births in a population with age-specific birth rates λ(y) and a constant death rate μ. True or False?",
    "question_context": "Deterministic and Stochastic Models of Population Growth\n\nThe text discusses deterministic and stochastic models of population growth. The deterministic approach assumes that the future population can be exactly predicted given initial conditions, often ignoring age-structure for simplicity. Key formulas include the exponential growth model N(t) = N(0)e^(νt) and the logistic growth model dN/dt = v0N(1 - N/κ). The stochastic approach considers chance fluctuations, particularly in the context of surname extinction, where the probability of extinction is derived from generating functions."
  },
  {
    "qid": "statistic-posneg-154-0-0",
    "gold_answer": "TRUE. The formula for $\\rho$ correctly incorporates sex-specific genetic map lengths by using $\\lambda_{j}$ which is set to $\\lambda_{f}$ for female meioses (egg formation) and $\\lambda_{m}$ for male meioses (sperm formation). This adjustment ensures that the crossover rate calculation accounts for differences in genetic map lengths between sexes, as indicated by the context.",
    "question": "Is the formula for $\\rho$ correctly adjusted for sex-specific genetic map lengths by setting $\\lambda_{j}$ to $\\lambda_{f}$ or $\\lambda_{m}$ depending on the type of meioses?",
    "question_context": "Genetic Map Length and Crossover Rate Calculation\n\nThroughout this article we have made some assumptions to gain simplicity as follows. Firstly, we do not discriminate the genetic genome lengths with respect to genders and therefore use sex-averaged values. One may note that there is no theoretical problem in generalizing (3.1) to sex-specific genetic lengths. If we assume $\\lambda_{f}$ and $\\lambda_{m}$ to equal the genetic map length in Morgans per unit of measurement $(x)$ for females and males, respectively, we get $$\\rho=\\frac{1}{4}2^{-m}\\sum_{w}\\sum_{j=1}^{m}\\lambda_{j}(S(w)-S(w+e_{j}))^{2},$$ where $\\lambda_{j}$ equals $\\lambda_{f}$ or $\\lambda_{m}$ depending on whether the $j$ th meioses corresponds to the formation of an egg or sperm cell. Secondly, the crossover rate formula (3.1) is valid for several types of map functions (Hossjer, 2003a), although we used Haldane's map function in the simulations. This implies, for instance, that the absence of chiasma interference is assumed. Thirdly, the markers are assumed equidistant and fully polymorphic. The effects of all these assumptions deserve further study. A partial solution in the case of incomplete marker information may be to use a transformation (as in Section 3.3) to the 1 − df likelihood ratio statistic $Z_{\\mathrm{lr}}$ of Kong and Cox (1997) and Nicolae et al. (1998). This statistic is often less skewed than $Z$ , in which case the transformation $g$ might be closer to the identity function, making the crossover adjustment of Section D in SM less crucial."
  },
  {
    "qid": "statistic-posneg-482-0-1",
    "gold_answer": "FALSE. The condition number of $\\mathbf{P}-\\mathbf{p}\\mathbf{p}^{\\mathrm{t}}$ is affected by the magnitude differences in the elements of $\\mathbf{p}$. When some elements of $\\mathbf{p}$ are much smaller than others, the matrix becomes ill-conditioned, making numerical decomposition less accurate. The symbolic decomposition, however, remains unaffected by ill-conditioning.",
    "question": "Is the condition number of $\\mathbf{P}-\\mathbf{p}\\mathbf{p}^{\\mathrm{t}}$ unaffected by the magnitude differences in the elements of $\\mathbf{p}$?",
    "question_context": "Symbolic Cholesky Decomposition of Multinomial Covariance Matrix\n\nThe paper presents a symbolic Cholesky decomposition of the non-negative definite matrix $\\mathbf{P}-\\mathbf{p}\\mathbf{p}^{\\mathrm{t}}$, where $\\mathbf{P}=\\mathrm{diag}(p_{1},....,p_{n})$ is an $n\times n$ diagonal matrix and $\\mathfrak{p}=(p_{1},....,p_{n})^{\\mathfrak{t}}$ is an $n\times1$ vector such that $\\sum_{i=1}^{n}p_{i}\\leqslant1$ and $p_{i}>0$ for $i=1,...,n$. The decomposition is given by $\\mathbf{LDL^{\\mathrm{t}}}=\\mathbf{P}\\mathrm{~-~}\\mathbf{pp^{\\mathrm{t}}}$, where $\\mathbf{L}$ is a lower triangular matrix with unit diagonal elements and $\\mathbf{D}$ is a diagonal matrix. The matrix $N(\\mathbf{P}-\\mathbf{p}\\mathbf{p}^{\\mathrm{t}})$ is the variance-covariance matrix of the multinomial distribution."
  },
  {
    "qid": "statistic-posneg-1363-0-0",
    "gold_answer": "TRUE. The condition $F_{\\alpha,p-1,v}>1$ is derived from Theorem 3.4 and ensures that the intervals associated with $C_{\\gamma}$ are uniformly shorter than the Scheffe intervals. This is supported by the ratio of the radi provided in Table IX, which consistently shows values less than 1, indicating shorter intervals for $C_{\\gamma}$ compared to Scheffe intervals.",
    "question": "Is the condition $F_{\\alpha,p-1,v}>1$ sufficient to ensure that the intervals associated with $C_{\\gamma}$ are uniformly shorter than the corresponding Scheffe intervals?",
    "question_context": "Comparison of Confidence Interval Radi in Statistical Estimation\n\nIf we apply Theorem 3.4 to the intervals associated with $C_{\\gamma}$ ,we find that a sufficient condition for these intervals to be uniformly shorter than the corresponding Scheffe intervals is $F_{\\alpha,p-1,v}>1$ . The ratio of the radi of these intervals is given by <PARAGRAPH_SEPARATOR> TABLEIX Ratio of the Radi of the Intervals in (4.9) to the Corresponding Scheffe Intervals <PARAGRAPH_SEPARATOR> <html><body><table><tr><td>V [En(x-x)²/s²]1/2</td><td>2</td><td>5</td><td>10</td><td>20</td><td>30</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>p=4 0</td><td>0.992</td><td>0.976</td><td>0.966</td><td>0.961</td><td>0.958</td></tr><tr><td>1</td><td>0.992</td><td>0.976</td><td>0.966</td><td>0.961</td><td>0.958</td></tr><tr><td>2</td><td>0.992</td><td>0.976</td><td>0.966</td><td>0.961</td><td>0.958</td></tr><tr><td>4</td><td>0.992</td><td>0.984</td><td>0.983</td><td>0.983</td><td>0.983</td></tr><tr><td>6</td><td>0.994</td><td>0.993</td><td>0.993</td><td>0.993</td><td>0.993</td></tr><tr><td>8</td><td>0.997</td><td>0.996</td><td>0.996</td><td>0.996</td><td>0.996</td></tr><tr><td>10</td><td>0.998</td><td>0.997</td><td>0.997</td><td>0.997</td><td>0.997</td></tr><tr><td>15</td><td>0.999</td><td>0.999</td><td>0.999</td><td>0.999</td><td>0.999</td></tr><tr><td>20</td><td>0.999</td><td>0.999</td><td>0.999</td><td>0.999</td><td>0.999</td></tr><tr><td>p=10</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0</td><td>0.981</td><td>0.937</td><td>0.907</td><td>0.886</td><td>0.877</td></tr><tr><td>1</td><td>0.981</td><td>0.937</td><td>0.907</td><td>0.886</td><td></td></tr><tr><td></td><td>0.981</td><td>0.937</td><td>0.907</td><td></td><td>0.877</td></tr><tr><td>2</td><td>0.981</td><td>0.937</td><td>0.907</td><td>0.886 0.886</td><td>0.877</td></tr><tr><td>4</td><td>0.981</td><td>0.949</td><td>0.949</td><td></td><td>0.877</td></tr><tr><td>6 8</td><td>0.981</td><td>0.972</td><td>0.973</td><td>0.951 0.974</td><td>0.952</td></tr><tr><td></td><td>0.984</td><td>0.982</td><td>0.983</td><td>0.984</td><td>0.975</td></tr><tr><td>10</td><td>0.993</td><td>0.992</td><td>0.992</td><td>0.993</td><td>0.984</td></tr><tr><td>15 20</td><td>0.996</td><td>0.996</td><td>0.996</td><td>0.996</td><td>0.993 0.996</td></tr></table></body></html> <PARAGRAPH_SEPARATOR> $$ \\left[\\frac{v^{\\operatorname{E}}(\\bar{X},s)\\Sigma a_{i}^{2}/n_{i}}{(p-1)s^{2}F_{x,p-1,\\nu}\\Sigma a_{i}^{2}/n_{i}}\\right]^{1/2}=\\left[T_{\\nu}\\left(1-\\frac{\\log T_{\\nu}}{F_{x,p-1,\\nu}}\\right)\\right]^{1/2}, $$ <PARAGRAPH_SEPARATOR> where $T_{\\mathrm{{v}}}$ is described following Theorem 4.3. Values of this ratio for $p=4$ 10, $v=2$ , 5, 10, 20, 30, and $\\alpha=0.10$ have been calculated, and are given in Table IX. The reduction in length is quite respectable, and is even greater than for the case of estimation of contrasts (compare Tables VII and Ix). <PARAGRAPH_SEPARATOR> The results of this section also apply to the case of known variance. The relevant intervals are obtained by letting $\\pmb{\\nu}\\rightarrow\\infty$ , in the expressions given in this section. (As $\\nu\\to\\infty$ ， $s^{2}\\to\\sigma^{2}$ and $p F_{x,p,\\nu}\\to\\chi_{x,p}^{2}$ ) The results also aply to the fixed radius confidence sets of Section 2, for which it was proved that the coverage probability is bounded below by $1-\\alpha.$ . For these confidence sets, it follows from Theorem 4.2 that the simultaneous intervals constructed  will  have  confidence  coefficient $1-\\alpha.$ Although these intervals will uniformly improve upon the Scheffe intervals in terms of coverage probability, they will, of course, have the same length."
  },
  {
    "qid": "statistic-posneg-274-4-0",
    "gold_answer": "FALSE. The context states that BCEL gives smaller confidence regions and higher coverage probabilities than ADEL and NA, but it does not provide explicit evidence or statistical significance for this claim. The comparison is based on visual inspection of Figs. 1 and 2 and Table 5, but without detailed statistical tests or confidence intervals for the differences, the claim cannot be definitively verified.",
    "question": "Is it true that the BCEL method provides both smaller confidence regions and higher coverage probabilities compared to the ADEL and NA methods, as suggested by the simulation results?",
    "question_context": "Empirical Likelihood in Partially Linear Models: Confidence Regions and RMSE Performance\n\n(III) When $n=100$ , the confidence regions of $(\\beta_{01},\\theta_{0})$ and $(\\beta_{02},\\theta_{0})$ and their coverage probabilities were calculated by 1000 simulations based on $\\mathrm{BCEL}$ , NA and ADEL methods. The simulation results are shown in Figs. 1 and 2 and Table 5. <PARAGRAPH_SEPARATOR> Figs. 1 and 2 and Table 5 show that BCEL gives smaller confidence regions and higher coverage probabilities than ADEL and NA, and that BCEL gives smaller confidence regions and higher coverage probabilities than ADEL. This indicates that the bias correction for BCEL improves the accuracies of the confidence regions. Moreover, the computational cost based on ADEL method is greater than that the proposed BCEL method. Namely, the computational cost based on ADEL method is 0.132 (billion), and the computational cost based on BCEL method is 0.096 (billion). <PARAGRAPH_SEPARATOR> (IV) The performances of the estimate for $g(t)$ was considered by 500 simulations at $n=100$ . The estimators $\\hat{g}_{\\boldsymbol k}^{*}(t),\\boldsymbol k=1,2$ , are assessed by the root mean squared errors (RMSE), namely, <PARAGRAPH_SEPARATOR> $$ \\mathrm{RMSE}=\\left[n_{\\mathrm{grid}}^{-1}\\sum_{l=1}^{n_{\\mathrm{grid}}}\\{\\hat{g}_{k}^{*}(t_{l})-g(t_{l})\\}^{2}\\right]^{1/2},~k=1,2, $$ <PARAGRAPH_SEPARATOR> ![](images/724098a2105d30533d13c098dd8c892db8eec20d824fc8114fcd48100a344495.jpg)  Fig. 1. For case 1, the approximate $95\\%$ confidence regions of $(\\beta_{01},\\theta_{0})$ and $(\\beta_{02},\\theta_{0})$ when $n=100$ , based on BCEL (solid curve), NA (dashed curve) and ADEL (dotted curve). (a) and (b) $\\mathrm{CR}=0.2; $ ; and (c) and (d) $\\mathrm{CR}=0.4$ . <PARAGRAPH_SEPARATOR> ![](images/2baeb8ad6cd3eae2e7dd0ec4f0d2db2cc37ec84075a79e1e03279beaa2dec938.jpg)  Fig. 2. For case 2, the approximate $95\\%$ confidence regions of $(\\beta_{01},\\theta_{0})$ and $(\\beta_{02},\\theta_{0})$ when $n=100$ , based on BCEL (solid curve) and NA (dashed curve). (a) and (b) $\\mathrm{CR}=0.2; $ ; and (c) and (d) $\\mathrm{CR}=0.4.$ . <PARAGRAPH_SEPARATOR> Table 3 For case 1, the average lengths and empirical coverage probabilities of confidence intervals for $\\beta_{01},\\beta_{02}$ and $\\theta_{0}$ at a nominal level 0.95."
  },
  {
    "qid": "statistic-posneg-1996-0-0",
    "gold_answer": "TRUE. The cumulative-logit location-scale model specifies that the cumulative probabilities $P(C\\leq j|R=i)$ depend on both $\\beta_i$ (location) and $\\sigma_i$ (scale). Independence implies that the conditional distribution of $C$ given $R$ does not depend on $R$, which requires $P(C\\leq j|R=i)$ to be identical for all $i$. This can only occur if both $\\beta_i$ and $\\sigma_i$ are constant across rows, as any variation in either parameter would lead to row-dependent probabilities, violating independence.",
    "question": "Is the statement 'The independence model holds only when both the location parameters $\\beta_i$ and the dispersion parameters $\\sigma_i$ are identical across all rows' true based on the cumulative-logit location-scale model presented?",
    "question_context": "Cumulative-Logit Location-Scale Model for Independence Testing\n\nThe second set of tables we simulated used row probabilities based on discretizing logistic distributions. Specifically, the table probabilities were generated as follows: Let $(C\\leq j)=(C^{*}\\leq\\alpha_{j})$ , where $C^{*}|(R=i)\\sim\\sigma_{i}L-\\beta_{i}$ , $i={1,2,3}$ , where $L\\sim L o g i s t i c(0,1)$ . It follows that the cumulative row probabilities have the form $$ P(C\\leq j|R=i)=P(C^{*}\\leq\\alpha_{j}|R=i)=\\frac{\\exp\\{(\\alpha_{j}+\\beta_{i})/\\sigma_{i}\\}}{1+\\exp\\{(\\alpha_{j}+\\beta_{i})/\\sigma_{i}\\}}. $$ That is, the table probabilities satisfy the cumulative-logit location–scale model of McCullagh (1980), as described in the Appendix A. When the latent logistic location parameters $\\beta_{i}$ ’s are identical and the dispersions $\\sigma_{i}$ ’s are identical, the independence model holds. When the $\\beta_{i}$ ’s vary across the rows, we say there is a latent location shift. Similarly, when the $\\sigma_{i}$ ’s vary across the rows, we say there is a latent scale shift."
  },
  {
    "qid": "statistic-posneg-294-0-0",
    "gold_answer": "FALSE. The expression for I'(x₀, p) involves more complex terms and integrals, not just a simple sum of I((p-1)/√p, p-1) and a series. The correct form includes additional integral terms and corrections, as detailed in the paper's derivations.",
    "question": "Is the Incomplete Γ-Function Ratio I'(x₀, p) correctly expressed as the sum of I((p-1)/√p, p-1) and a series involving P₀ and coefficients aᵢ?",
    "question_context": "Numerical Evaluation of High Order Incomplete Eulerian Integrals\n\nThe paper discusses the Incomplete Γ-Function and its ratio, defined by integrals involving exponential and power functions. The focus is on numerical evaluation methods and expansions for these functions, particularly for large values of the parameter p."
  },
  {
    "qid": "statistic-posneg-291-1-2",
    "gold_answer": "TRUE. The convergence condition for the hypergeometric series ${}_{p}F_{q}$ with matrix arguments is correctly stated. When $p = q + 1$, the series converges absolutely for $\\|\\mathbf{Y}\\| < 1$ and diverges for $\\|\\mathbf{Y}\\| > 1$, where $\\|\\mathbf{Y}\\|$ is the maximum absolute eigenvalue of $\\mathbf{Y}$. This is a standard result in the theory of hypergeometric functions with matrix arguments, as referenced in the context.",
    "question": "Is the condition for the convergence of the hypergeometric series ${}_{p}F_{q}$ with matrix arguments correctly stated as: if $p = q + 1$, the series converges absolutely for $\\|\\mathbf{Y}\\| < 1$ and diverges for $\\|\\mathbf{Y}\\| > 1$?",
    "question_context": "Matrix Variate Hypergeometric Distributions and Mellin Transforms\n\nThis paper introduces several families of matrix variate elliptical distributions. Section 2 gives some results on integration, using zonal polynomials. In terms of these results, various matrix variate hypergeometric type distributions are proposed, as particular cases. These include well-known distributions such as central and noncentral matrix variate inverted gamma (inverted Wishart) distributions and matrix variate central and noncentral beta type II distributions; see Section 3. In Section 4.1, assuming a hypergeometric type distribution for the matrix parameter in matrix variate normal and matricvariate $T$ distributions, several families of matrix variate elliptical distributions are obtained using the compound matrix variate approach. Section 4.3 introduces the scale mixture of a matrix variate elliptical distribution, which is derived as a particular case of the compound matrix variate approach; all the results given in Section 4.1 can be particularized to this case."
  },
  {
    "qid": "statistic-posneg-677-2-0",
    "gold_answer": "FALSE. The variance estimation guarantee in Theorem 3 provides a finite-sample bound on the difference between the estimated variance $\\hat{\\sigma}^2$ and the true asymptotic variance $\\sigma^2$ with probability $1-\\epsilon^{\\prime}$. However, the confidence interval for $\\theta_0$ is constructed based on the asymptotic normality of the estimator $\\hat{\\theta}$, and its coverage probability approaches the nominal level $1-a$ as $n \\to \\infty$. The finite-sample guarantee does not imply exact coverage for any finite $n$, and the coverage probability is not fixed at $1-\\epsilon^{\\prime}$ but rather approaches $1-a$ asymptotically.",
    "question": "Is the following statement true or false? The variance estimation guarantee in Theorem 3 implies that the confidence interval for $\\theta_0$ will always include the true parameter value with probability exactly equal to $1-\\epsilon^{\\prime}$, regardless of the sample size $n$.",
    "question_context": "Debiased Machine Learning: Finite-Sample Guarantees for Variance Estimation and Confidence Intervals\n\nFor global functionals, $(\\bar{Q},\\bar{\\alpha})$ are finite constants that depend on the problem at hand. For example, for treatment effects a sufficient condition is that the propensity score be bounded away from 0 and 1. For derivatives, a sufficient condition is that $\\Gamma$ should satisfy Sobolev conditions. For local functionals, we handle $(\\bar{Q}_{h},\\bar{\\alpha}_{h})$ on a case-by-case basis. See the Supplementary Material for interpretable and complete characterizations. <PARAGRAPH_SEPARATOR> Observe that the finite-sample Gaussian approximation in Theorem 1 is in terms of the true asymptotic variance $\\sigma^{2}$ . We now provide a guarantee for its estimator $\\hat{\\sigma}^{2}$ . <PARAGRAPH_SEPARATOR> Theorem 3 (Variance estimation). Suppose that Assumption 1 holds, $E[\\{Y-\\gamma_{0}(W)\\}^{2}\\mid W]\\leqslant{\\bar{\\sigma}}^{2}$ and $\\|\\hat{\\alpha}_{\\ell}\\|_{\\infty}\\leqslant\\bar{\\alpha}^{\\prime}$ . Then with probability $1-\\epsilon^{\\prime}$ , $|\\hat{\\sigma}^{2}-\\sigma^{2}|\\leqslant\\Delta^{\\prime}+2(\\Delta^{\\prime})^{1/2}\\{(\\Delta^{\\prime\\prime})^{1/2}+\\sigma\\}+\\Delta^{\\prime\\prime}$ ′, where <PARAGRAPH_SEPARATOR> $$ \\Delta^{\\prime}=4(\\hat{\\theta}-\\theta_{0})^{2}+\\frac{24L}{\\epsilon^{\\prime}}\\big[\\{\\bar{Q}+(\\bar{\\alpha}^{\\prime})^{2}\\}\\mathcal{R}(\\hat{\\gamma}_{\\ell})^{q}+\\bar{\\sigma}^{2}\\mathcal{R}(\\hat{\\alpha}_{\\ell})\\big],\\quad\\Delta^{\\prime\\prime}=\\left(\\frac{2}{\\epsilon^{\\prime}}\\right)^{1/2}\\zeta^{2}n^{-1/2}. $$ <PARAGRAPH_SEPARATOR> Theorem 3 is a finite-sample variance estimation guarantee. It degrades gracefully if the parameters $(\\bar{Q},\\bar{\\sigma},\\bar{\\alpha}^{\\prime})$ diverge relative to $n$ and the learning rates. Theorems 1 and 3 immediately imply simple, interpretable conditions for validity of the confidence interval. We conclude by summarizing these conditions. <PARAGRAPH_SEPARATOR> Corollary 1 (Confidence interval). Suppose that Assumption 1 holds, as well as the following regularity and learning-rate conditions as $n\\to\\infty$ and as $h\\rightarrow0$ : <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{E[\\{Y-\\gamma_{0}(W)\\}^{2}\\mid W]\\leqslant\\bar{\\sigma}^{2},\\|\\alpha_{0}^{\\mathrm{min}}\\|_{\\infty}\\leqslant\\bar{\\alpha},\\|\\hat{\\alpha}_{\\ell}\\|_{\\infty}\\leqslant\\bar{\\alpha}^{\\prime}a n d\\{(\\kappa/\\sigma)^{3}+\\zeta^{2}\\}n^{-1/2}\\to0;}\\ &{(\\bar{Q}^{1/2}+\\bar{\\alpha}/\\sigma+\\bar{\\alpha}^{\\prime})\\{\\mathcal{R}(\\hat{\\gamma}_{\\ell})\\}^{q/2}=o_{\\mathfrak{p}}(1);}\\ &{\\overline{{\\sigma}}\\{\\mathcal{R}(\\hat{\\alpha}_{\\ell})\\}^{1/2}=o_{\\mathfrak{p}}(1);}\\ &{\\left[\\{n\\mathcal{R}(\\hat{\\gamma}_{\\ell})\\mathcal{R}(\\hat{\\alpha}_{\\ell})\\}^{1/2}\\wedge\\{n\\mathcal{P}(\\hat{\\gamma}_{\\ell})\\mathcal{R}(\\hat{\\alpha}_{\\ell})\\}^{1/2}\\wedge\\{n\\mathcal{R}(\\hat{\\gamma}_{\\ell})\\mathcal{P}(\\hat{\\alpha}_{\\ell})\\}^{1/2}\\right]/\\sigma=o_{\\mathfrak{p}}(1).}\\end{array} $$ <PARAGRAPH_SEPARATOR> Then the estimator $\\hat{\\theta}$ in Algorithm 1 is consistent and asymptotically Gaussian, and the confidence interval in Algorithm 1 includes $\\theta_{0}$ with probability approaching the nominal level. Formally, <PARAGRAPH_SEPARATOR> $$ \\hat{\\theta}=\\theta_{0}+o_{\\mathsf{p}}(1),\\quad\\sigma^{-1}n^{1/2}(\\hat{\\theta}-\\theta_{0})\\rightsquigarrow N(0,1),\\quad\\mathsf{p r}\\big\\{\\theta_{0}\\in(\\hat{\\theta}\\pm c_{a}\\hat{\\sigma}n^{-1/2})\\big\\}\\to1-a. $$ <PARAGRAPH_SEPARATOR> For local functionals, $i f\\Delta_{h}\\rightarrow0$ , then the same result holds upon replacing $(\\hat{\\theta},\\theta_{0})$ with $({\\hat{\\theta}}^{h},\\theta_{0}^{\\mathrm{lim}})$ . <PARAGRAPH_SEPARATOR> # Supplementary material <PARAGRAPH_SEPARATOR> The Supplementary Material includes simulation results, details of Theorem 2, further discussion, proofs and code."
  },
  {
    "qid": "statistic-posneg-1357-0-1",
    "gold_answer": "TRUE. The bridge function $F_{\\mathrm{BB}}(r;\\Delta_{j},\\Delta_{k})$ is correctly specified for binary variables. It accounts for the joint probability of the two binary variables being above their respective thresholds $c_j$ and $c_k$, which is given by $\\Phi_{2}(\\Delta_{j},\\Delta_{k};r)$, minus the product of their marginal probabilities $\\Phi(\\Delta_{j})\\Phi(\\Delta_{k})$. This correctly captures the dependence structure between the two binary variables in the Gaussian copula framework.",
    "question": "Does the bridge function $F_{\\mathrm{BB}}(r;\\Delta_{j},\\Delta_{k})=2\\left\\{\\Phi_{2}(\\Delta_{j},\\Delta_{k};r)-\\Phi(\\Delta_{j})\\Phi(\\Delta_{k})\\right\\}$ correctly account for the joint distribution of two binary variables in the Gaussian copula model?",
    "question_context": "Latent Correlation Estimation via Bridge Functions in Mixed Gaussian Copula Models\n\nThe mixed latent Gaussian copula model jointly models $\\mathbf{W}=$ $({\\bf W}_{1},{\\bf W}_{2},{\\bf W}_{3})\\sim\\mathrm{NPN}({\\bf0},{\\pmb\\Sigma},f)$ such that $X_{1j}=W_{1j},X_{2j}=$ $I(W_{2j}>c_{2j})$ and $W_{3j}=I(W_{3j}>c_{3j})W_{3j}$ . <PARAGRAPH_SEPARATOR> The latent correlation matrix $\\pmb{\\Sigma}$ is the key parameter in Gaussian copula models. Estimation of latent correlations is achieved via the bridge function $F$ such that $\\mathbb{E}(\\widehat{\\tau}_{j k})~=~F(\\sigma_{j k})$ , where $\\sigma_{j k}$ is the latent correlation between variables $j$ and $k,$ and $\\widehat{\\tau}_{j k}$ is the corresponding sample Kendall’s $\\tau$ . Given observed $\\mathbf{x}_{j},\\dot{\\mathbf{x}_{k}}\\in\\mathbb{R}^{n}$ , <PARAGRAPH_SEPARATOR> $$ \\widehat{\\tau}_{j k}=\\widehat{\\tau}(\\mathbf{x}_{j},\\mathbf{x}_{k})=\\frac{2}{n(n-1)}\\sum_{1\\leq i<i^{\\prime}\\leq n}\\mathrm{sign}(x_{i j}-x_{i^{\\prime}j})\\mathrm{sign}(x_{i k}-x_{i^{\\prime}k}), $$ <PARAGRAPH_SEPARATOR> where $n$ is the sample size. Using $F$ , one can construct $\\widehat{\\sigma}_{j k}~=~{\\cal F}^{-1}(\\widehat{\\tau}_{j k})$ with the corresponding estimator $\\widehat{\\pmb\\Sigma}$ being consistent for $\\pmb{\\Sigma}$ (Fan et al. 2017; Quan, Booth, and Wells 2018; Yoon, Carroll, and Gaynanova 2020). The explicit form of $F$ has been derived for all combinations of continuous(C)/binary(B)/truncated(T) variables (Fan et al. 2017; Yoon, Carroll, and Gaynanova 2020). We summarize these results below, and use CC, BC, TC, etc. to denote corresponding combinations."
  },
  {
    "qid": "statistic-posneg-150-0-1",
    "gold_answer": "FALSE. While Theorem 2(i) presents the covariance function for the case $\\tau=3$, it does not imply independence from $\\tau$ in the general form. The simplification occurs in the specific case where $\\tau=3$, but the general form in Theorem 2(ii) explicitly includes a term dependent on $(\\tau-3)$.",
    "question": "Does the covariance function $\\mathrm{Cov}(X_{f_{i}},X_{f_{j}})$ in Theorem 2(i) simplify to a form independent of $\\tau$ when $\\tau=3$?",
    "question_context": "Eigenvector Asymptotics in High-Dimensional Sample SSCMs\n\nRemark 2. Assumption 4 holds for all unit vectors $\\pmb{\\eta}_{p}\\in\\mathbb{R}^{p}$ when $\\mathbf{T}=\\mathbf{I}$ or $\\lambda_{\\operatorname*{max}}^{\\mathbf{T}}-\\lambda_{\\operatorname*{min}}^{\\mathbf{T}}\\to0.$ . In general, Assumption 4 may not hold for all unit vectors, but there always exist some vectors $\\eta_{p}$ such that Assumption 4 holds, for example $\\pmb{\\eta}_{p}=(\\pmb{\\mathrm{v}}_{1}+\\cdot\\cdot\\cdot+\\pmb{\\mathrm{v}}_{p})/\\sqrt{p}$ where $\\pmb{\\mathrm{v}}_{1},\\ldots,\\pmb{\\mathrm{v}}_{p}$ are the eigenvectors of $\\mathbf{T}$ . <PARAGRAPH_SEPARATOR> First, we present the first-order convergence of the VESD function of the sample SSCMs $\\mathbf{B}_{n}$ . <PARAGRAPH_SEPARATOR> Theorem 1. Suppose that Assumptions 1–4 hold. Then, almost surely, the VESD function $\\widetilde{F}^{\\mathbf{B}_{n}}$ converges weakly to a probability distribution $F^{c,H}$ , whose Stieltjes transform $m(z)$ is the unique solution to the equation <PARAGRAPH_SEPARATOR> $$ m(z)=\\int\\frac{1}{t(1-c-c z m(z))-z}d H(t),\\quad z\\in\\mathbb{C}^{+}, $$ <PARAGRAPH_SEPARATOR> in the set $\\{m(z)\\in\\mathbb{C}:-(1-c)/z+z m(z)\\in\\mathbb{C}^{+}\\}.$ . <PARAGRAPH_SEPARATOR> Theorem 1 shows that the VESD $\\widetilde{F}^{{\\mathbf{B}}_{n}}$ converges weakly to the generalized Marčenko–Pastur law $F^{c,H}$ defined through the (4). Let $\\underline{{F}}^{c,H}=c F^{c,H}+(1-c)\\mathrm{I}_{[0,\\infty)}$ be the companion distribution of $F^{c,H}$ and $\\underline{m}=\\underline{m}(z)$ be the Stieltjes transform of $\\underline{{F}}^{c,H}$ . In this case, the (4) can be equivalently rewritten as <PARAGRAPH_SEPARATOR> $$ z=-\\frac{1}{\\underline{{m}}}+c\\int\\frac{t}{1+t\\underline{{m}}}d H(t),\\quad z\\in\\mathbb{C}^{+}. $$ <PARAGRAPH_SEPARATOR> Let $\\underline{m}_{n}^{0}(z)$ be the finite version of the limiting Stieltjes transform $\\underline{m}(z)$ in (5) associated with ratio $c_{n}$ and distribution function $H_{p}$ . That is, $\\underline{m}_{n}^{0}(z)$ is the solution to the equation <PARAGRAPH_SEPARATOR> $$ z=-\\frac{1}{\\underline{{m}}_{n}^{0}(z)}+c_{n}\\int\\frac{t}{1+t\\underline{{m}}_{n}^{0}(z)}d H_{p}(t),\\quad z\\in\\mathbb{C}^{+}. $$ <PARAGRAPH_SEPARATOR> Then such $\\underline{m}_{n}^{0}(z)$ uniquely determines a probability distribution function, denoted by $\\underline{{F}}^{c_{n},H_{p}}$ , through the inverse formula of Stieltjes transform. Denote <PARAGRAPH_SEPARATOR> $$ F^{c_{n},H_{p}}=c_{n}^{-1}\\underline{{{F}}}^{c_{n},H_{p}}+(1-c_{n}^{-1})\\mathrm{I}_{[0,\\infty)}. $$ <PARAGRAPH_SEPARATOR> Next, we investigate the convergence of the linear spectral statistics of $\\widetilde{F}^{\\mathbf{B}_{n}}$ which defined as <PARAGRAPH_SEPARATOR> $$ \\int f(x)d G_{n}(x), $$ <PARAGRAPH_SEPARATOR> where those $f$ are analytic on the support of the limiting spectral distribution of F Bn and <PARAGRAPH_SEPARATOR> $$ G_{n}(x)=\\sqrt{n}\\left(\\widetilde{\\cal F}^{{\\bf B}_{n}}(x)-{\\cal F}^{c_{n},H_{p}}(x)\\right). $$ <PARAGRAPH_SEPARATOR> Theorem 2. Suppose that Assumptions 1–4 hold. We further assume that $\\{f_{1},\\ldots,f_{k}\\}$ are functions defined analytic on an open region of the complex plane $\\mathbb{C}$ that contains the interval <PARAGRAPH_SEPARATOR> $$ I_{c}:=\\left[\\operatorname*{lim}_{p}\\operatorname*{inf}_{\\substack{\\lambda_{\\mathrm{min}}^{\\mathbf{T}}I_{(0,1)}(c)(1-\\sqrt{c})^{2},\\quad\\operatorname*{lim}_{p}\\operatorname*{sup}\\lambda_{\\mathrm{max}}^{\\mathbf{T}}(1+\\sqrt{c})^{2}}}\\right] $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\operatorname*{sup}_{z}\\sqrt{n}\\left|\\eta_{p}^{\\top}(\\underline{{m}}_{n}^{0}(z)\\mathbf{T}+\\mathbf{I})^{-1}\\eta_{p}-\\int\\frac{1}{1+t\\underline{{m}}_{n}^{0}(z)}d H_{p}(t)\\right|\\rightarrow0\\quad a s\\quad p\\rightarrow\\infty. $$ <PARAGRAPH_SEPARATOR> Then: <PARAGRAPH_SEPARATOR> (i) If $\\tau=3$ , then <PARAGRAPH_SEPARATOR> $$ \\left(\\int f_{1}(x)d G_{n}(x),\\dots,\\int f_{k}(x)d G_{n}(x)\\right) $$ <PARAGRAPH_SEPARATOR> forms $a$ tight sequence and converges weakly to a Gaussian vector $(X_{f_{1}},\\dots,X_{f_{k}})$ with mean zero and covariance function <PARAGRAPH_SEPARATOR> $$ \\mathrm{Cov}(X_{f_{i}},X_{f_{j}})=-\\frac{1}{2\\pi^{2}}\\oint_{\\cal C}_{1}\\oint_{\\cal C}_{2}f_{i}(z_{1})f_{j}(z_{2})\\frac{(z_{2}\\underline{{m}}(z_{2})-z_{1}\\underline{{m}}(z_{1}))^{2}}{c^{2}z_{1}z_{2}(z_{2}-z_{1})(\\underline{{m}}(z_{2})-\\underline{{m}}(z_{1}))}d z_{1}d z_{2}. $$ <PARAGRAPH_SEPARATOR> The contours $\\mathcal{C}_{1}$ and $\\mathcal{C}_{2}$ are non-overlapping, closed, counter-clockwise orientated in the complex plane and both contained in the analytic region for the functions $\\{f_{1},\\ldots,f_{k}\\}$ and enclosing the interval $I_{c}$ . <PARAGRAPH_SEPARATOR> (ii) Moreover, if $\\tau\\neq3$ , by further assuming that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{\\lefteqn{\\sum_{i=1}^{p}\\{\\mathbf{T}^{1/2}(z_{1}\\mathbf{I}+z_{1}\\underline{{m}}(z_{1})\\mathbf{T})^{-1}\\eta_{p}\\eta_{p}^{\\top}(z_{1}\\mathbf{I}+z_{1}\\underline{{m}}(z_{1})\\mathbf{T})^{-1}\\mathbf{T}^{1/2}\\}_{i i}}}\n&{\\quad\\times:\\{\\mathbf{T}^{1/2}(z_{2}\\mathbf{I}+z_{2}\\underline{{m}}(z_{2})\\mathbf{T})^{-1}\\eta_{p}\\eta_{p}^{\\top}(z_{2}\\mathbf{I}+z_{2}\\underline{{m}}(z_{2})\\mathbf{T})^{-1}\\mathbf{T}^{1/2}\\}_{i i}\\to h(z_{1},z_{2})}\\end{array} $$ <PARAGRAPH_SEPARATOR> in probability, where $(\\mathbf{A})_{i i}$ denotes the ith diagonal entry of matrix A, the random vector (11) converges weakly to a Gaussian vector $X_{f_{1}},\\dots,X_{f_{k}}$ with mean zero and covariance function <PARAGRAPH_SEPARATOR> $$ \\mathrm{{\\sfcov}}(X_{f_{i}},X_{f_{j}})=-\\frac{1}{4\\pi^{2}}\\oint_{\\cal C_{1}}\\oint_{\\cal C_{2}}f_{i}(z_{1})f_{j}(z_{2})\\Biggl\\{\\frac{2(z_{2}\\underline{{{m}}}(z_{2})-z_{1}\\underline{{{m}}}(z_{1}))^{2}}{c^{2}z_{1}z_{2}(z_{2}-z_{1})(\\underline{{{m}}}(z_{2})-\\underline{{{m}}}(z_{1}))}+(\\tau-3)z_{1}z_{2}\\underline{{{m}}}(z_{1})\\underline{{{m}}}(z_{2})h(z_{2})\\underline{{{m}}}(z_{1})\\underline{{{m}}}(z_{2}). $$ <PARAGRAPH_SEPARATOR> The contours $\\mathcal{C}_{1}$ and $\\mathcal{C}_{2}$ are defined similar to those in $(i)$ . <PARAGRAPH_SEPARATOR> Theorem 2 shows that the linear spectral statistics of $\\widetilde{F}^{\\mathbf{B}_{n}}$ , $\\textstyle{\\int f(x)d G_{n}(x)}$ exhibits the same limiting distribution as that of sample covariance matrix $\\pmb{S}_{n}$ given in Theorem 2 of  [2˜]. This suggests that the limiting behaviors of eigenvectors of sample SSCM $\\mathbf{B}_{n}$ and sample covariance matrix $\\pmb{S}_{n}$ are comparable. <PARAGRAPH_SEPARATOR> In particular, when $\\mathbf{T}=\\mathbf{I},$ , the limit $h(z_{1},z_{2})$ in (12) can be simplified as <PARAGRAPH_SEPARATOR> $$ h(z_{1},z_{2})=\\frac{1}{(z_{1}+z_{1}\\underline{{{m}}}(z_{1}))^{2}(z_{2}+z_{2}\\underline{{{m}}}(z_{2}))^{2}}\\times\\operatorname*{lim}_{p\\rightarrow\\infty}\\mathrm{tr}\\left(\\eta_{p}\\eta_{p}^{\\top}\\circ\\eta_{p}\\eta_{p}^{\\top}\\right), $$ <PARAGRAPH_SEPARATOR> where ‘‘◦’’ denotes Hadamard product. In this case, the limiting covariance function in Theorem 2 are simplified as follows. <PARAGRAPH_SEPARATOR> Corollary 1. Suppose that $\\mathbf{T}=\\mathbf{I}$ and $\\{f_{1},\\ldots,f_{k}\\}$ are functions defined analytic on an open region of the complex plane $\\mathbb{C}$ that contains the interval <PARAGRAPH_SEPARATOR> $$ I_{c}:=\\left[\\operatorname*{lim}_{p}\\operatorname*{inf}_{I(0,1)}(c)(1-\\sqrt{c})^{2},\\quad\\operatorname*{lim}_{p}\\operatorname*{sup}(1+\\sqrt{c})^{2}\\right] $$ <PARAGRAPH_SEPARATOR> Under Assumptions 1 and 2, the random vector (11) converges weakly to a Gaussian vector $X_{f_{1}},\\dots,X_{f_{k}}$ with mean zero and covariance function <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\mathsf{C o v}(X_{f_{i}},X_{f_{j}})=\\displaystyle\\frac{2}{c}\\left(\\int f_{i}(x)f_{j}(x)d F_{c}(x)-\\int f_{i}(x)d F_{c}(x)\\int f_{j}(x)d F_{c}(x)\\right)}}\n{{\\displaystyle\\quad\\quad+\\frac{\\tau-3}{c^{2}}\\times\\operatorname*{lim}_{p\\to\\infty}\\mathrm{tr}\\left(\\eta_{p}\\eta_{p}^{\\top}\\circ\\eta_{p}\\eta_{p}^{\\top}\\right)\\times\\int f_{i}(x)(1-x)d F_{c}(x)\\int f_{j}(x)(1-x)d F_{c}(x),}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $F_{c}(x)$ is the standard M- $\\cdot P$ law (see (3.1.1) in [6]). Furthermore, if $k=1$ and $f_{1}(x)=x,$ , the random variable $\\textstyle\\int x d G_{n}(x)$ converges weakly to a Gaussian random variable with mean zero and variance $2+(\\tau-3)\\cdot\\mathrm{lim}_{p\\rightarrow\\infty}$ tr $\\left(\\eta_{p}\\eta_{p}^{\\top}\\circ\\eta_{p}\\eta_{p}^{\\top}\\right)$ . <PARAGRAPH_SEPARATOR> # 3. Conclusions <PARAGRAPH_SEPARATOR> In this paper, we study the asymptotic properties of eigenvectors of $\\mathbf{B}_{n}$ in a high-dimensional way where both population dimension $p$ and sample size $n$ tend to infinity at the same rate. Specifically, we show that the VESD of $\\mathbf{B}_{n}$ has the same limiting spectral distribution as the counterpart of $\\pmb{S}_{n}$ . Moreover, we establish the central limit theorem of linear spectral statistics defined by the eigenvectors of $\\mathbf{B}_{n}$ , which suggests that the eigenmatrix of $\\mathbf{B}_{n}$ is asymptotically Haar distributed when $\\mathbf{T}$ is an identity matrix."
  },
  {
    "qid": "statistic-posneg-1136-1-1",
    "gold_answer": "FALSE. The linear approximation accurately estimates the left endpoint but poorly estimates the right endpoint of the confidence interval. This asymmetry is due to the nonlinear relationship between τ₁ and θ₁, which the linear approximation fails to capture fully. The quadratic approximation (θ₁ = 0.0477 + 7.96×10⁻⁴τ₁ + 7.85×10⁻⁶τ₁²) provides better accuracy for both endpoints.",
    "question": "Does the linear approximation to the mapping from τ₁ to θ₁ (θ₁ = 0.0477 + 7.96×10⁻⁴τ₁) accurately predict both endpoints of the confidence interval?",
    "question_context": "Confidence Intervals for Parameter Subsets in Nonlinear Regression\n\nThe matrices $L_{11}$ and $B_{11}$ and array $K$ are scalars in this example since only one parameter is of interest, and they are $L_{11}=7{\\cdot}96\\times10^{-4}$ , <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{B_{11}=L_{11}^{2}[\\hat{e}^{\\Gamma}][V_{11}]=(7\\cdot96\\times10^{-4})^{2}\\times(2\\cdot3042\\times10^{5})=0\\cdot1460,}\\end{array} $$ <PARAGRAPH_SEPARATOR> $$ K=L_{11}^{3}[V_{1}^{\\mathsf{T}}][V_{11}]=(7{\\cdot}96\\times10^{-4})^{3}\\times(-3{\\cdot}8093\\times10^{7})=-0{\\cdot}0197. $$ <PARAGRAPH_SEPARATOR> Using $F(1,9;0{\\cdot}05)=5{\\cdot}12$ the approximate intervals in terms of the t's are $\\tau_{1}^{2}\\leqslant95{\\cdot}91\\times5{\\cdot}12=(22{\\cdot}16)^{2}$ for the linear approximation, $\\tau_{1}^{2}\\leqslant(1{\\cdot}082\\times22{\\cdot}16)^{2}$ from (3·3) for the likelihood ratio approach, and $\\tau_{1}^{2}\\leqslant(1{\\cdot}232\\times22{\\cdot}16)^{2}$ from (3·7) for the scorebased approach. The linear approximation to the mapping from $\\tau_{1}$ .to $\\theta_{1}$ is $\\theta_{1}=0{\\cdot}0477+7{\\cdot}96\\times10^{-4}\\tau_{1}$ , and the linear approximation confidence interval obtained by substituting. $\\tau_{1}=\\pm22{\\cdot}16$ in this expression is (0-0301, 0-0653). Similarly, the quadratic mapping (4·1) is <PARAGRAPH_SEPARATOR> $$ \\theta_{1}=0{\\cdot}0477+7{\\cdot}96\\times10^{-4}\\tau_{1}+7{\\cdot}85\\times10^{-6}\\tau_{1}^{2}, $$ <PARAGRAPH_SEPARATOR> so that the second-order approximations are (0-0331, 0·0713) for the likelihood ratiobased interval and (0·0318, 0·0753) for the score-based interval. Values of $\\tau_{1}$ and $\\tau_{2}$ were obtained on a grid of $\\theta$ values around $\\widehat{\\theta}.$ A plot of $\\theta_{1}$ versus $\\tau_{1}$ showed a smooth relationship with almost no variation which would indicate dependence on $\\tau_{2}$ ,and regression of $\\theta_{1}$ on $\\pmb{\\tau_{1}}$ and $\\tau_{1}^{2}$ gave a relationship almost identical to the one used above. <PARAGRAPH_SEPARATOR> The linear approximation interval accurately estimates the left endpoints of the exact intervals but poorly estimates the right endpoints, and fails to predict the lengths by 9 and $20\\%$ . The quadratic intervals are slightly less accurate at the left but are much more accurate at the right, with the overall length in error by less than $2\\%$ in each case."
  },
  {
    "qid": "statistic-posneg-1117-0-3",
    "gold_answer": "TRUE. The paper shows that for large FWHM (very smooth fields) and large thresholds, the DLM P-value converges to the XEC P-value. This is demonstrated through asymptotic analysis, where the DLM method's approximation aligns with the XEC method's results under these conditions.",
    "question": "Does the DLM method converge to the expected Euler characteristic (XEC) method for large thresholds and very smooth fields?",
    "question_context": "Discrete Local Maxima Method for Gaussian Random Fields\n\nThe paper discusses methods for approximating the probability that the maximum of a D-dimensional stationary Gaussian random field sampled on a uniform rectilinear lattice exceeds a threshold t. The Bonferroni bound is accurate for non-smooth fields, while the expected Euler characteristic method is accurate for very smooth fields. The proposed Discrete Local Maxima (DLM) method bridges the gap between these two extremes by considering the expected number of discrete local maxima above the threshold."
  },
  {
    "qid": "statistic-posneg-1931-0-1",
    "gold_answer": "FALSE. The formula correctly includes the prior ratio p(β̃)/p(β) for the full vector β, where β̃ is the current β with βj replaced by the proposal β̃j. The product term adjusts for the likelihood contributions of all observations, accounting for the case-control design through the ratio of conditional probabilities. The omission of explicit terms for other β components is because the proposal only changes βj, leaving others fixed.",
    "question": "The acceptance probability for resampling βj in the Metropolis step is given by a(β, β̃) = min{1, [p(β̃) / p(β)] * ∏[p(Di | Xi, Zi, β̃) / p(Di | β̃, θ)] / [p(Di | Xi, Zi, β) / p(Di | β, θ)]}. Does this formula incorrectly omit the prior ratio for the other β components?",
    "question_context": "Bayesian Semiparametric Model for Case-Control Studies with Errors in Variables\n\nThe paper presents a Bayesian semiparametric model for case-control studies with errors in variables, utilizing a mixture of Dirichlet processes. The model involves resampling parameters θi, updating cluster configurations S, and handling measurement errors through Metropolis sampling steps. Key formulas include the full conditionals for θi, the acceptance probabilities for Metropolis steps, and the base measure specifications for the Dirichlet process."
  },
  {
    "qid": "statistic-posneg-13-0-1",
    "gold_answer": "FALSE. The claim is not mathematically supported by the formula. The output loss depends on the multiplier term [ χ_b / (1-χ_b(μ+κγ_D)) ] applied to the negative shock D̂, but the formula does not explicitly show that higher price flexibility (larger κ) leads to a larger fall in prices and exacerbates the initial contraction. The economic mechanism described is not directly reflected in the given formula.",
    "question": "The 'Paradox of Flexibility' suggests increased price flexibility (higher κ in AS curve π_S = κ * Ŷ_S) worsens output contraction (lower Ŷ_S) at the ZLB after a deleveraging shock D̂. Based on the output equation Ŷ_S = Γ - [ χ_b / (1-χ_b(μ+κγ_D)) ] * D̂, is this claim mathematically supported by the formula, and what is the economic mechanism?",
    "question_context": "Central Limit Theorems for Non-Linear Functionals of Gaussian Fields\n\nThe paper models a deleveraging shock where the debt limit falls from D_high to D_low. In the flexible price endowment economy, the short-run natural rate is 1+r_S = ( (1/2)Y + D_low ) / ( β(1/2)Y + βD_high ). A negative rate (r_S < 0) occurs if βD_high - D_low > (1/2)(1-β)Y. In the sticky price model at the ZLB, output is Ŷ_S = Γ - [ χ_b / (1-χ_b(μ+κγ_D)) ] * D̂, leading to paradoxes like the Paradox of Flexibility."
  },
  {
    "qid": "statistic-posneg-133-1-2",
    "gold_answer": "FALSE. The condition $0 \\in \\mathcal{H}_{n}$ is not always satisfied for finite samples, even if the embedded correlation structure is correct. This is because $\\mathrm{Pr}(0 \\in \\mathcal{H}_{n}) \rightarrow 1$ only as $n \to \\infty$, and for finite samples, 0 may lie near or outside the convex hull $\\mathcal{H}_{n}$. This issue is particularly severe for highly overconstrained submodels or when the cluster size $t$ is large.",
    "question": "Is the condition $0 \\in \\mathcal{H}_{n}$ always satisfied for finite samples when using Kolaczyk's EIC approach for GEE models?",
    "question_context": "Empirical Likelihood for Working Correlation Structure Selection in GEE Models\n\nThe paper discusses the use of empirical likelihood to select the optimal working correlation structure in Generalized Estimating Equations (GEE) models. It introduces a full-model empirical likelihood ratio, which serves as a unified measure to compare different working correlation structures. The paper also proposes modified versions of AIC and BIC, called EAIC and EBIC, which use empirical likelihood instead of parametric likelihood for model selection."
  },
  {
    "qid": "statistic-posneg-1718-1-2",
    "gold_answer": "FALSE. The LSD of (1/N)X'X is the same as the LSD of ST only under specific conditions, as outlined in Remark 2.1. These conditions include the satisfaction of the moment conditions and convergence assumptions in Theorem 2.2. Without these conditions, the equivalence between the LSD of (1/N)X'X and ST cannot be guaranteed.",
    "question": "Is it true that the LSD of (1/N)X'X is always the same as the LSD of ST under the conditions of Theorem 2.2?",
    "question_context": "Limiting Spectral Distribution of Large Dimensional VARMA Models\n\nThe paper discusses the limiting spectral distribution (LSD) of large-dimensional VARMA models. It presents two main theorems: Theorem 2.1, which deals with the LSD of the product ST under certain conditions, and Theorem 2.2, which provides conditions under which the LSD of (1/N)X'X converges to a non-random limit. The paper also introduces matrix expressions for VARMA models and discusses the relationship between the LSD of (1/N)X'X and (1/N)AΣ'ΣA'."
  },
  {
    "qid": "statistic-posneg-77-0-0",
    "gold_answer": "TRUE. The formula for $RR_{\\mathrm{adj}}^{2}$ is correctly specified as it adjusts the robust residual scale of the model ($\\hat{\\sigma}_{\\alpha,n}^{2}$) by the degrees of freedom ($n-p_{\\alpha}$) and compares it to the robust residual scale of the null model ($\\hat{\\sigma}_{1,n}^{2}$) adjusted by its degrees of freedom ($n-1$). This mirrors the classical adjusted R-squared but uses robust measures to account for potential outliers, ensuring the metric is less sensitive to violations of normality and homoscedasticity.",
    "question": "Is the adjusted robust R-squared ($RR_{\\mathrm{adj}}^{2}$) formula correctly specified to measure the proportion of variance explained by the model while adjusting for the number of predictors, similar to the classical adjusted R-squared but using robust residual scales?",
    "question_context": "Robust Model Selection Metrics: Adjusted R-squared and Prediction Error Criteria\n\nThe goal of the selection procedure is to find a parsimonious model that fits the data reasonably well and at the same time is able to predict future observations accurately. To investigate the quality of the fits selected by the different selection procedures, we calculate an adjusted robust R-squared (see Maronna et al. (2006)), which for a model of size $p_{\\alpha}$ is given by $$ R R_{\\mathrm{adj}}^{2}=\\frac{\\hat{\\sigma}_{1,n}^{2}/(n-1)-\\hat{\\sigma}_{\\alpha,n}^{2}/(n-p_{\\alpha})}{\\hat{\\sigma}_{1,n}^{2}/(n-1)}, $$ where $\\hat{\\sigma}_{1,n}$ is the robust residual scale of the ‘‘null’’ model that only includes the intercept as predictor. We also compare plots of standardized residuals versus fitted values. To compare the predictive power of the models we ran 5-fold cross-validation on each of the models. More specifically, consider a model $\\alpha$ and let $\\mathcal{A}_{j},j=1,\\ldots,5$ be disjoint subsets such that $\textstyle\bigcup_{j=1}^{5}{\\mathcal{A}}_{j}=\\{1,\\dotsc,n\\}$ . For each $j,$ let $\\hat{\\pmb{\beta}}_{\\alpha,n}^{(-j)}$ be the estimated vector of regression parameters without using the observations with indices in the set ${\\mathcal{A}}_{j}$ . We report the following two prediction error criteria: $$ \\mathrm{TMSE}=\\left[\\sum_{j=1}^{5}\\mathsf{a v e}_{i\\in\\mathcal{A}_{j}}^{(\\gamma)}\\left\\{(y_{i}-\\mathsf{x}_{\\alpha i}^{\\prime}\\hat{\beta}_{\\alpha,n}^{(j)})^{2}\right\\}\right]\bigg/5, $$ and $$ \bar{\rho}=\\hat{\\sigma}_{n}^{2}\\left[\\sum_{j=1}^{5}\\operatorname{ave}_{i\\in\\mathcal{A}_{j}}\\left\\{\rho\\left((y_{i}-\\mathbf{x}_{\\alpha i}^{\\prime}\\hat{\\pmb{\beta}}_{\\alpha,n}^{(j)})/\\hat{\\sigma}_{n}\right)\right\\}\right]\bigg/5, $$ where $\\hat{\\sigma}_{n}$ is the error scale estimate, and $\\mathsf{a v e}_{i\\in A}^{(\\gamma)}\\left\\{t_{i}\right\\}$ and $\\mathsf{a v e}_{i\\in A}\\left\\{t_{i}\right\\}$ denote the $\\gamma100\\%$ upper trimmed mean and the usual sample mean of the $t_{i}^{\\cdot}s$ with , respectively. Note that TMSE is a trimmed mean squared error where the trimming reflects that we allow a fraction of outliers to be predicted not well. We used $\\gamma=0.05$ and 0.10."
  },
  {
    "qid": "statistic-posneg-114-1-0",
    "gold_answer": "TRUE. This is established in Lemma 1.2 of the paper. The inequality follows from the Cramér-Rao lower bound applied to the function g(X). Equality occurs precisely when g(x) takes the form x·∇ln f(x) + c, which is linear in the score function ∇ln f(x). This characterizes the efficient estimators in the Fisher information sense.",
    "question": "The inequality Var[g(X)] ≥ E[∇g(X)]I⁻¹(X)E[∇g(X)] holds for any differentiable function g under the regularity conditions of family F, with equality if and only if g(x) is linear in the score function. Is this statement true?",
    "question_context": "Fisher Information and Variance Inequalities in Statistical Models\n\nThe paper discusses the properties of Fisher information and its relationship with variance inequalities in both continuous and discrete random vectors. It defines a family of densities F with specific regularity conditions and derives various inequalities involving Fisher information matrices. Key results include variance lower bounds, convolution inequalities for Fisher information, and characterizations of normal and Poisson distributions based on equality conditions in these inequalities."
  },
  {
    "qid": "statistic-posneg-1528-0-1",
    "gold_answer": "TRUE. The paper explicitly states that for ρ=0, the chances P+(ρ) and P-(ρ) are identical, meaning the distribution is symmetrical in c0. This symmetry arises because the covariance estimates are uncorrelated when ρ=0, leading to a symmetric distribution of their ratio.",
    "question": "Does the distribution of the ratio of covariance estimates become symmetrical in c0 when the population correlation ρ is zero?",
    "question_context": "Distribution of the Ratio of Covariance Estimates in Bivariate Normal Populations\n\nThe paper discusses the distribution of the ratio of covariance estimates from two samples drawn from the same normal bivariate population. The problem is defined by testing the null hypothesis that two samples have the same covariance structure, with the ratio of their covariance estimates being greater or less than a certain value. The paper provides mathematical expressions for the probabilities P+(ρ) and P-(ρ) that the ratio exceeds a positive value c0 or falls below a negative value, respectively, using complete and incomplete B-functions."
  },
  {
    "qid": "statistic-posneg-1384-0-0",
    "gold_answer": "FALSE. The conditional expectation $g_f$ is not necessarily unique based solely on the condition $\\int_{A}g_{f}d\\mu=\\int_{A}f d\\mu$ for all $A\\in\\varLambda$. However, the paper states that $g_f$ is essentially unique when it exists, meaning it is unique up to sets of measure zero. The uniqueness is guaranteed under additional conditions, such as when $f\\in L^{p}(\\Sigma)$ for $1\\leqslant p<\\infty$ and the measure space $(\\Omega,\\Sigma,\\mu)$ is localizable. Without these conditions, the uniqueness may not hold.",
    "question": "Is the conditional expectation $g_f$ relative to $\\varLambda$ uniquely determined by the condition $\\int_{A}g_{f}d\\mu=\\int_{A}f d\\mu$ for all $A\\in\\varLambda$?",
    "question_context": "Conditional Expectation and Contractive Projections in Measure Spaces\n\nThe concepts and notations to be used here generally follow [3]. Thus a measure space $(\\Omega,\\Sigma,\\mu)$ consists of a set $\\pmb{\\mathcal{Q}}_{i}$ ,a 8-ring $\\boldsymbol{\\Sigma}$ (i.e., closed under finite unions, differences and countable intersections), and a measure $\\mu:{\\mathcal{Z}}\\mapsto{R}^{+}$ which is complete (i.e., $N\\in{\\mathcal{Z}},$ $\\pmb{\\mu}(N)=0$ implies, for every $A\\subset N,A\\in\\Sigma).$ Let $\\boldsymbol{\\Sigma_{10\\mathrm{{c}}}}=\\{\\boldsymbol{A}\\subset\\mathcal{Q}:\\boldsymbol{A}\\cap\\boldsymbol{E}\\in\\boldsymbol{\\Sigma}$ for all $\\pmb{\\mathcal{E}}\\in\\pmb{\\mathcal{Z}}\\}$ , so that it is a $\\pmb{\\sigma}$ algebra. Define $\\mu^{*}:\\mathcal{Z}_{\\mathrm{loc}}\\mapsto\\widetilde{R}^{+},$ as $\\mu^{*}(A)=\\operatorname*{sup}\\{\\mu(B):B\\subset A,B\\in\\Sigma\\},$ $\\scriptstyle A\\in\\Sigma_{\\log}$ . Then $\\mu^{*}$ is a measure [3]. Let ${\\mathcal{N}}({\\mathcal{L}})={\\mathcal{N}}=\\{N\\in{\\varSigma}_{\\mathrm{loc}}:\\mu^{*}(N)=0\\}$ . Hence $\\mathcal{N}$ is the $\\pmb{\\sigma}^{.}$ -ideal of $\\pmb{\\mu}$ -negligible sets, so that $\\mu(N\\cap A)=0$ for all $A\\in\\Sigma$ We call $\\mathcal{N}$ the class of $\\pmb{\\Sigma}.$ -negligible sets, though a more appropriate term would be the locally $\\pmb{\\mu}$ (or $\\pmb{\\Sigma}$ )-negligible sets, to conform with other [17] terminology. Set $\\tilde{\\varSigma}_{\\mathrm{loc}}=\\ensuremath{\\varSigma_{\\mathrm{loc}}}/\\mathcal{N}_{:}$ i.e,the faetor $\\pmb{\\sigma}$ -algebra which is the set of equivalence casses $\\pmb{\\tilde{A}}$ of sets $A\\in\\varSigma_{\\mathrm{loc}}\\mathrm{mod}(\\mathcal{N})$ . The sets of $\\pmb{\\mathcal{\\Sigma}}_{\\mathbf{loc}}$ will be called $\\pmb{\\Sigma}.$ measurable, and thus all concepts of $\\boldsymbol{\\Sigma}.$ measurability are understood in terms of $\\pmb{\\mathcal{\\Sigma}}_{\\mathbf{loc}}$ ， as in [3]. (\"Supp\" stands for “support\" in what follows). <PARAGRAPH_SEPARATOR> For $1\\leqslant p<\\infty$ ,let $L^{p}({\\boldsymbol{\\Sigma}})$ be the space of real $\\Sigma\\cdot$ measurable functions vanishing outside a sequence of sets of $\\pmb{\\Sigma},$ . and which are limits a.e. $(\\Sigma),$ and also of $\\pmb{\\phi}$ -mean, of sequences of $\\Sigma\\cdot$ step functions. If $A\\subset E$ is a 8-ring containing all the $\\pmb{\\Sigma}.$ -null sets of $\\pmb{\\Sigma}$ (i.e., $\\mathcal{N}$ is contained in the $\\boldsymbol{\\mathfrak{A}}$ -locally null class), then $L^{\\boldsymbol{p}}(\\boldsymbol{A})\\subset D^{\\boldsymbol{p}}(\\boldsymbol{\\it{\\sum}})$ for $1\\leqslant p<\\infty$ , and the inclusion is an isometric embedding, where the usual metric given by the seminorm in $L^{{\\mathfrak{p}}}({\\boldsymbol{\\Sigma}})$ is used here. In general, $\\scriptstyle{A_{\\mathrm{loc}}}$ and $\\Sigma_{\\mathrm{loc}}$ are not comparable (even though each $\\pmb{\\Sigma}.$ -negligible set is $\\boldsymbol{\\mathfrak{A}}$ -negligible), so that $L^{\\infty}(\\varLambda)$ and $L^{\\infty}(\\Sigma)$ are also not comparable. However, if $\\operatorname*{sup}\\{\\tilde{A}:A\\in\\mathcal{A}\\}=\\tilde{\\Omega}$ then $L^{\\infty}({\\cal{A}})\\subset L^{\\infty}({\\cal{Z}})$ $(L^{\\infty}({\\mathcal{Z}})=\\{f:f\\varphi_{A}$ is in ordinary $L^{\\infty}(\\sigma({\\mathcal{Z}}))$ ,all $A\\in\\Sigma\\}.$ $\\scriptstyle(\\varphi_{A}$ being the indicator function of $\\pmb{A}$ <PARAGRAPH_SEPARATOR> If $f:\\varOmega\\mapsto R$ is $\\pmb{\\Sigma}.$ -measurable such that $f_{\\varphi_{A}}\\in L^{1}(\\mathcal{Z})$ for each $\\pmb{A}\\in\\pmb{A}$ (thus $f$ is A-locally integrable) then any $\\varLambda$ measurable $g_{f}:\\varOmega\\mapsto R$ satisfying <PARAGRAPH_SEPARATOR> $$ \\int_{A}g_{f}d\\mu=\\int_{A}f d\\mu,A\\in\\varLambda, $$ <PARAGRAPH_SEPARATOR> such that $\\operatorname*{sup}\\{\\tilde{A}:A\\in\\varLambda,A\\subset G\\}=\\tilde{G}$ where $G=\\operatorname{supp}(g_{f})_{!}$ . is called a conditional expectation of $f$ relative to $\\varLambda.$ It was shown in [4] that $g_{f}$ is essentially unique when it exists, and that it exists if $f\\in L^{p}(\\Sigma)$ $1\\leqslant p<\\infty$ ,and the same is true if $\\phi=\\infty$ also, provided $(\\Omega,\\Sigma,\\mu)$ is localizable in addition. [Recall that $(\\mathcal{Q},\\mathcal{Z},\\mu)$ is loalizable fevyry subfamiy of $\\widetilde{\\Sigma}_{\\mathrm{1oc}}$ has a supremum in $\\tilde{\\Sigma}_{\\mathrm{loc}}$ See [17] for a discussion of this concept.] Thus the mapping $E^{\\lambda}:f\\mapsto g_{f}$ is well defined, and is a linear contractive positive projection (also denoted $E(\\cdot\\mid A)$ and $E_{A}(\\cdot).$ ${\\cal E}^{\\varLambda}$ has many of the properties of the corresponding operator when $A,D$ are ${\\pmb{\\sigma}}^{*}$ -algebras, but sometimes additional hypotheses are needed to compensate for the $\\pmb{\\delta}.$ -ring case. (For proofs of these and other properties of $E^{\\varLambda}$ see [4].) <PARAGRAPH_SEPARATOR> With this background information, we shall proceed to find conditions for some characterizations of subclasses of contractive projections on $L^{p}({\\mathcal{Z}}),{\\mathcal{p}}\\geqslant1;$ to coincide with conditional expectations. <PARAGRAPH_SEPARATOR> # 2. MEASURABLE SUBSPACES OF $L^{p}(\\varSigma)$"
  },
  {
    "qid": "statistic-posneg-1146-0-4",
    "gold_answer": "TRUE. The context defines the relative rate of convergence using this exact expression, where J_d* is a matrix of ones and d* = (1/2)d(d+1). This formulation accounts for cases where elements of vech H_AMISE may be zero, ensuring the rate is defined relative to the general order of H_AMISE (O(n^{-2/(d+4)})).",
    "question": "Is the relative rate of convergence of a bandwidth matrix selector to H_AMISE given by vech(Ĥ - H_AMISE) = O_p(J_d* n^{-α})vech H_AMISE?",
    "question_context": "Multivariate Kernel Density Estimation: Bandwidth Matrix Selection and Convergence Rates\n\nThe choice of smoothing parameters is a problem of fundamental importance in kernel density estimation and related areas. Bandwidth selection for univariate kernel density estimation is the simplest form of this problem, and has been the subject of considerable research. Substantial advances been made leading to the development of bandwidth selectors which combine good practical performance with excellent asymptotic properties. See [4] for an overview. Progress in the case of multivariate case has been much slower. Nonetheless, the selection of bandwidth matrices is an important problem because of the utility of multivariate kernel density estimators in areas such as data visualization, nonparametric discriminant analysis and goodness of fit testing."
  },
  {
    "qid": "statistic-posneg-1448-7-2",
    "gold_answer": "FALSE. The proposition states that $f$ can be extended to an entire function only if $|\\alpha| > \\beta$ for all $\\alpha \\in \\mathbf{Spec}(B)$. While the case (i) of the proposition (which guarantees the entire extension) arises if and only if $\\mu$ admits an absolute moment of order 1, this condition alone is not sufficient for the entire extension. The spectral condition $|\\alpha| > \\beta$ is also necessary. The finite absolute moment ensures the decay properties of $\\hat{\\mu}(x)$, but the spectral condition ensures the required exponential decay rate.",
    "question": "Does the proposition imply that the density $f$ can always be extended to an entire function if $\\mu$ has finite absolute moments of order 1?",
    "question_context": "Analytic Extension of Probability Density Functions in Complex Domains\n\n(i) f $|\\alpha|>\\beta$ for all ${\\mathfrak{x}}\\in\\mathbf{Spec}(B)$ then the density $f$ of $\\mu$ can be extended to an entire function $(o n\\mathbb{C}^{d})$ <PARAGRAPH_SEPARATOR> (ii)If every ${\\mathfrak{\\alpha}}\\in\\mathbf{Spec}(B)$ with $|{\\pmb{\\alpha}}|=\\beta$ is a simple root of the minimal polynomial then $f$ can be extended to an analytic function in some strip $S_{\\delta}:=\\left\\{\\left(z_{1},...,z_{d}\\right)\\in\\mathbb{C}^{d}:\\left|\\mathrm{Im}~z_{j}\\right|<\\delta\\right.$ for $j=1,...,d\\}$ $(\\delta>0)$ <PARAGRAPH_SEPARATOR> Proof. (i) There exists some $c\\in]\\frac{1}{2},1[$ such that $\\beta^{c}<|\\alpha|$ for all ${\\mathfrak{a}}\\in\\mathbf{Spec}(B)$. Taking into account the proof of Theorem 2.1 in [9] there exist a, $b>0$ such that $\\vert\\hat{\\mu}(x)\\vert\\leqslant1$ if $|x|_{2}\\leqslant a$ and $|\\hat{\\mu}(x)|\\leqslant\\exp\\bigl\\{-b|x|_{2}^{1/c}\\bigr\\}$ if $|x|_{2}>a$ (where $\\hat{\\mu}$ denotes the Fourier transform of $\\pmb{\\mu}$ 0. Let $\\varepsilon:=1/c-1>0$ Fix $\\delta>1$ and let $x\\in V$ with $i x\\in S_{\\delta/d}$. Then we have for all $y\\in V$ such that $|y|_{2}>a$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{e^{\\langle x,y\\rangle}|\\hat{\\mu}(y)|\\leqslant\\exp\\{\\delta|y|_{2}-b|y|_{2}^{1+\\varepsilon}\\}}}\\ {{=\\exp\\{-|y|_{2}(b|y|_{2}^{\\varepsilon}-\\delta)\\}.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Hence <PARAGRAPH_SEPARATOR> $$ e^{\\langle x,y\\rangle}|\\hat{\\mu}(y)|\\leqslant\\exp\\{-|y|_{2}\\} $$ <PARAGRAPH_SEPARATOR> for all $y\\in V$ such that $|y|_{2}>\\operatorname*{max}\\{a,((\\delta-1)/b)^{1/\\varepsilon})$.Thus the function $y\\to e^{\\langle x,y\\rangle}\\hat{\\mu}(y)$ on $V$ is $\\lambda$ -integrable. Application of [2, Satz 49.5] to the measure with $\\lambda$ -density $\\hat{\\mu}$ yields: <PARAGRAPH_SEPARATOR> $$ w\\to\\int e^{-i\\langle w,y\\rangle}\\hat{\\mu}(y)\\lambda(d y) $$ <PARAGRAPH_SEPARATOR> is an analytic function $F$ on $S_{\\delta/d}$. But in view of the inversion formula of Fourier analysis we have $F(x)=f(x)$ for all $x\\in V.$ Since $\\delta>1$ was arbitrary the first assertion is proved. <PARAGRAPH_SEPARATOR> (ii) Let there exist some $\\alpha\\in\\mathbf{Spec}(B)$ such that $|{\\pmb{\\alpha}}|=\\beta$. Let $B^{t}$ denote the transpose of $\\pmb{B}$ and let $\\pmb{\\mathscr{A}}:=\\beta(\\pmb{\\mathscr{B}}^{t})^{-1}$. By assumption $\\rho(A)=1$ and every $\\alpha\\in\\mathbf{Spec}(A)$ with $|\\pmb{\\alpha}|=1$ is a simple root of the minimal polynomial. Hence there exists some $c>0$ such that $\\|A^{n}\\|\\leqslant c$ for all $\\pmb{n}\\in\\mathbb{N}$ [11, p. 43, Exercise 6]. <PARAGRAPH_SEPARATOR> Let Z := {xe V: |xl2 ≤1 <↓B-'x|2} and u := - In |μl. Then there exists some $\\varepsilon>0$ suchthat $u(x)\\geqslant\\varepsilon$ for all $x\\in Z$ (cf. [9, p. 295]). Moreover we have $u((B^{t})^{n}x)=\\beta^{n}u(x)$ for all $x\\in Z$ and $\\pmb{n\\in\\mathbb{Z}}$ Consequently we obtain for all $x\\in Z$ and $\\pmb{n}\\in\\mathbb{N}$ <PARAGRAPH_SEPARATOR> $$ {\\frac{u((B^{t})^{-n}x)}{|(B^{t})^{-n}x|_{2}}}{=}{\\frac{\\beta^{-n}u(x)}{|(B^{t})^{-n}x|_{2}}}{=}{\\frac{u(x)}{|A^{n}x|_{2}}}\\geqslant{\\frac{u(x)}{\\|A^{n}\\||x|_{2}}}\\geqslant{\\frac{\\varepsilon}{c}}{=}:b. $$ <PARAGRAPH_SEPARATOR> Hence $u(y)\\geqslant b|y|_{2}$ for all $\\begin{array}{r}{y\\in\\bigcup_{n\\geqslant1}\\left(B^{\\prime}\\right)^{-n}Z.}\\end{array}$. On the other hand we have $|y|_{2}\\leqslant1$ for all $\\scriptstyle y\\in\\bigcup_{n\\leqslant0}(B^{\\iota})^{-n}Z$ (since $\\left\\|B\\right\\|<1)$. Since $\\textstyle\\bigcup_{n\\in\\mathbb{Z}}(B^{\\prime})^{n}Z=V^{*}$ [9, Corollary 1.1] we obtain $|\\hat{\\mu}(x)|\\leqslant1$ if $|x|_{2}\\leqslant1$ and $|\\hat{\\mu}(x)|\\leqslant$ $\\exp\\left\\{-b\\left|x\\right|_{2}\\right\\}$ if $|x|_{2}>1$ <PARAGRAPH_SEPARATOR> Let $\\delta\\in]0,b[$ and $x\\in V$ such that $i x\\in S_{\\delta/d}$. Then we have for all $y\\in V$ such that $|y|_{2}>1$ <PARAGRAPH_SEPARATOR> $$ e^{\\langle x,y\\rangle}|{\\hat{\\mu}}(y)|\\leqslant\\exp\\{-(b-\\delta)\\mid y\\mid_{2}\\}. $$ <PARAGRAPH_SEPARATOR> Now we can proceed as in the proof of (i). Hence $f$ admits an analytic extension to the strip $S_{b/d}$ <PARAGRAPH_SEPARATOR> Remarks. 1. The case (i) of the proposition arises if and only if $\\mu$ admits an absolute moment of order 1, ie., iff $\\int|x|_{2}\\mu(d x)$ isfinite [9, Theorem 3.1]. <PARAGRAPH_SEPARATOR> 2. On the real line this proposition is due to V. M. Kruglov [8, Theorem 3]. It should be observed that in this case the matrix $\\pmb{B}$ can be identified with a real number $B\\in[\\beta,\\sqrt{\\beta}[$.Then ${\\boldsymbol{\\beta}}={\\boldsymbol{B}}^{\\alpha}$ withsome $\\pmb{\\alpha}\\in[1,2[$.Hence if $\\alpha>1$ then $f$ can be extended to an entire function; if $\\pmb{\\alpha}=1$ then $f$ can be extended to an analytic function in some strip $\\left\\{z\\in\\mathbb{C}\\colon|\\operatorname{Im}z|<\\delta\\right\\}$"
  },
  {
    "qid": "statistic-posneg-782-0-0",
    "gold_answer": "FALSE. The test statistic d is not correctly derived as presented. The correct derivation should account for the degrees of freedom and the unknown variance ratio explicitly, which is not fully captured in the given formula. The context suggests that the distribution of d depends on the variance ratio σ₂/σ₁, which is not directly incorporated into the formula for d. The correct approach involves adjusting the degrees of freedom or using a different test statistic that accounts for the unknown variance ratio, as indicated by the more complex formulas provided in the context.",
    "question": "Is the test statistic d = (x̄₁ - x̄₂) / √(s₁² + s₂²) correctly derived for testing the hypothesis of a common mean between two normal populations with unknown variances, given the provided context and formulas?",
    "question_context": "Behrens' Problem: Significance Testing for Common Mean with Unknown Variances\n\nTHE publication by Pearson and Hartley of numerical values for the proposed solution of Behrens’ problem developed by Welch, affords an opportunity of clarifying in concrete terms, ideas which have in the past been disputed in perhaps a too abstract an atmosphere. Behrens' problem was to find a test of significance suitable for testing the hypothesis that two normal populations with unknown variances had a common mean. The fact that an unknown variance-ratio entered into the test was a comparatively novel feature, and has led to the problem being described as a test-case for examining different views of the process of testing hypotheses."
  },
  {
    "qid": "statistic-posneg-684-0-0",
    "gold_answer": "TRUE. The paper explicitly states that the noncentral bimatrix variate beta type IV distribution is used to derive the exact distribution of the product of two dependent Wilks' statistics, denoted as Λ₁ = |X₁X₂|, where X₁ and X₂ are noncentral matrix variate beta type IV distributions.",
    "question": "The noncentral bimatrix variate beta type IV distribution is used to derive the exact distribution of the product of two dependent Wilks' statistics. True or False?",
    "question_context": "Noncentral Bimatrix Variate Beta Distributions and Likelihood Ratio Statistics\n\nThe paper discusses the derivation of exact distributions for statistics developed within the noncentral bimatrix beta variates paradigm. It focuses on the product of two dependent Wilks' statistics, defined as the determinant of the product of two noncentral matrix variate beta type IV distributions. The paper provides exact expressions for the density functions and cumulative distribution functions (CDFs) of these statistics, which are crucial for hypothesis testing in multivariate analysis."
  },
  {
    "qid": "statistic-posneg-937-0-2",
    "gold_answer": "TRUE. $O_{\\mathrm{ratio}} > 1$ means that the expected odds of cases under SGS is higher than under SRS, indicating that the SGS design successfully enriches for cases compared to simple random sampling. This is the primary goal of surrogate-guided sampling designs.",
    "question": "Is it correct to interpret $O_{\\mathrm{ratio}} > 1$ as indicating that the SGS design is more efficient than SRS in enriching for cases?",
    "question_context": "Surrogate-Guided Sampling (SGS) Design Efficiency and Likelihood Ratios\n\nThe denominator of $O_{\\mathrm{ratio}}$ is the expected odds of cases for samples collected with SRS, and is less than 1 for outcome with prevalences less than $50\\%$ . The numerator is the expected odds of cases for samples collected with SGS designs. Therefore, $O_{\\mathrm{ratio}}$ is a single estimate of design effect on sample outcome prevalence, and can be interpreted as the expected increase in cases comparing SGS to SRS, with higher values indicating that SGS provides more case enrichment, and $O_{\\mathrm{ratio}}>1$ indicating improvement using SGS relative to SRS. An interesting property of $O_{\\mathrm{ratio}}$ is the connection to likelihood ratios (LRs) of the enrichment surrogate. Of note, LRs of a diagnostic test can be interpreted as slopes of Receiver Operating Characteristics (ROC) curves, are related to positive and negative predictive values (PPV and NPV), but are invariant to outcome prevalence (Choi, 1998). Therefore, by framing enrichment surrogates $Z$ as “prior tests” of outcome $Y$ , we may gain insight into what types of variables are the best surrogates for sampling. <PARAGRAPH_SEPARATOR> # PROPOSITION 2.1 Properties of $O_{\\mathrm{ratio}}$ . <PARAGRAPH_SEPARATOR> Let an SGS design of sample size $n$ be defined with surrogate $Z$ and sampling ratio $R=P(Z=1|S=1)$ ), where $R=0.50$ corresponds to a “balanced” SGS 1:1 design. Let $Z$ has operating characteristics: $Z_{\\mathrm{sens}}:=$ $P(Z=1|Y=1)$ ), $Z_{\\mathrm{spec}}=P(Z=0|Y=0)$ . Then, if the outcome is rare $(P(Y=1)\\approx0)$ ), then <PARAGRAPH_SEPARATOR> $$ O_{\\mathrm{ratio}}(R,Z)\\approx(R)(L R+)+(1-R)(L R-). $$ <PARAGRAPH_SEPARATOR> COROLLARY 2.1 For a given Z, Oratio ∝ R. Over the set of possible Z, Oratio ∝ Zsens and Oratio ∝ 1 −1Zspec . <PARAGRAPH_SEPARATOR> Details of Proposition 2.1 and Corollary 2.1 are in Supplementary Material B available at Biostatistics online. For a given surrogate, higher values of $O_{\\mathrm{ratio}}$ can be achieved by over-representing surrogate positives. Over the range of possible surrogates, a small change in surrogate specificity can have a much higher impact on $O_{\\mathrm{ratio}}$ compared to the same change in surrogate sensitivity. Figure 1 demonstrates the impact of surrogate operating characteristics on $O_{\\mathrm{ratio}}$ for a fixed sampling ratio of $R=0.50$ (i.e., SGS 1:1 ratio of cases to control, a “balanced” design) and prevalence of $10\\%$ . An SGS design with $O_{\\mathrm{ratio}}>2$ , may be obtained by using a surrogate with (sensitivity, specificity) of $(30\\%,90\\%)$ or $(90\\%,80\\%$ ), where in order to maintain the same sample case–control odds a small decrease of surrogate specificity required a much larger increase in surrogate sensitivity. In fact, very high information designs, such as $O_{\\mathrm{ratio}}>3$ , can only be achieved when surrogate specificity is at least $90\\%$ . <PARAGRAPH_SEPARATOR> ![](images/b0224c8b98d0cd482477b2eb83b4ed8441373becdb407c60c75470385df3e6ba.jpg)  Fig. 1. $O_{\\mathrm{ratio}}$ values for surrogates of different marginal sensitivity and specificity, based on a fixed $R=0.50$ and an outcome with prevalence of $10\\%$ ."
  },
  {
    "qid": "statistic-posneg-714-1-3",
    "gold_answer": "TRUE. The paper mentions that simulations show the new LW test has a better size (i.e., better control of Type I error) when $\\Delta < 0$ compared to the CZZ test.",
    "question": "Does the paper assert that the new LW test $W_{n}$ performs better than the CZZ test when $\\Delta < 0$?",
    "question_context": "High-Dimensional Covariance Matrix Identity Tests\n\nThe paper introduces two versions of the sample covariance matrices, $S_{n}$ and $B_{n}$, and defines the CLRT and LW test statistics for testing the identity of the covariance matrix in high-dimensional settings. The CLRT is defined as $L_{n}=\\frac{1}{p}\\mathrm{tr}\\left(S_{n}\\right)-\\frac{1}{p}\\log\\left|S_{n}\\right|-1$, and the LW test is defined as $W_{n}=\\frac{1}{p}\\mathrm{tr}\\left(S_{n}-I_{p}\\right)^{2}-\\frac{p}{n-1}\\left(\\frac{1}{p}\\mathrm{tr}\\left(S_{n}\\right)\\right)^{2}-\\frac{(1+A)(n-2)(n-1)-2}{n(n-1)^{2}}$. The paper establishes the asymptotic normality of these statistics under the null hypothesis $\\Sigma_{p}=I_{p}$ and discusses their advantages over existing tests."
  },
  {
    "qid": "statistic-posneg-629-0-0",
    "gold_answer": "FALSE. The context argues that the Poisson-Exponential distribution can be applicable even when the mean frequency (m) is not absolutely small, provided that the probability of occurrence (q) is very small and the population (n) is large. The text explicitly states that the name 'Law of Small Numbers' is misleading because the applicability depends on the smallness of q and largeness of n, not just the smallness of m.",
    "question": "Is the claim that the Poisson-Exponential distribution is applicable only when the mean frequency of occurrence (m) is small and the population (n) is large, as suggested by Bortkewitsch and Mortara, correct based on the provided context?",
    "question_context": "Analysis of Binomial and Poisson Distributions in Mortality Data\n\nThe context discusses the application of binomial and Poisson distributions to mortality data, specifically deaths from small-pox and other causes across various districts. It critiques the use of Poisson's 'Law of Small Numbers' by Bortkewitsch and Mortara, arguing that the conditions for its application are often not met. The text includes various mathematical formulas and statistical tests to compare binomial and exponential (Poisson) fits to the data."
  },
  {
    "qid": "statistic-posneg-257-0-0",
    "gold_answer": "TRUE. The convergence rate is derived from the bias-variance tradeoff in wavelet estimation. The choice of j_0 ensures the resolution level grows as n^{1/(2s_*+1)}, balancing approximation error (decreasing with s_*) and estimation error (increasing with j_0). Assumption (A) provides the necessary control on the local variability of g(x), enabling this rate. The condition min(s, s-1/r+1/p) > 1/2 guarantees the smoothness is sufficient for wavelet methods to outperform naive estimators.",
    "question": "Under assumption (A) and the condition min(s, s-1/r+1/p) > 1/2, does the linear wavelet estimator achieve the convergence rate n^{-s_* p / (2s_* + 1)} as stated in Theorem 3.3?",
    "question_context": "Nonparametric Estimation of Quantile Density via Wavelet Methods\n\nThe paper discusses nonparametric estimation of a quantile density function using wavelet methods. It introduces a Lipschitz (1/2) assumption (A) to improve the efficiency of linear and hard thresholding wavelet estimators. Theorems 3.3 and 3.4 present bounds on the expected L_p error for these estimators under assumption (A), with specific conditions on the smoothness parameters (s, r, p) and choices of resolution levels (j_0, j_1). The hard thresholding estimator's convergence rate ψ_n adapts to the parameter regime (r,s,p) ∈ D."
  },
  {
    "qid": "statistic-posneg-962-0-0",
    "gold_answer": "TRUE. The inequality $\\lambda(a^*,k) \\geqslant \\lambda(a,k)$ for $a \\geqslant a^*$ follows from the TP2 (Totally Positive of Order 2) ordering property of the extended hypergeometric distribution, which ensures that the likelihood ratio $\\pi(a,b)/\\pi(a-1,b)$ is non-decreasing in $b$. This property implies stochastic dominance, meaning that for larger $a$, the tail probability $\\lambda(a,k)$ is smaller. The condition $\\pi(a,b)\\pi(a-1,b^*) \\geqslant \\pi(a,b^*)\\pi(a-1,b)$ for $b < b^*$ is a specific manifestation of the TP2 property, ensuring the stochastic ordering required for Proposition A1.",
    "question": "Is it true that the tail probability $\\lambda(a,k)$ decreases as $a$ increases, as stated in Proposition A1 ($\\lambda(a^*,k) \\geqslant \\lambda(a,k)$ for $a \\geqslant a^*$), and does this hold under the condition that $\\pi(a,b)\\pi(a-1,b^*) \\geqslant \\pi(a,b^*)\\pi(a-1,b)$ for $b < b^*$?",
    "question_context": "Extended Hypergeometric Distribution in Treatment Effect Inference\n\nTable A1 is a $2\\times2$ contingency table in which the upper left-hand corner cell has been reduced by a count of $a$ and the lower left-hand corner cell has been increased by $a$, with consequent changes in the row totals. The observed table is obtained by letting $a=0$. If $D-a$ in Table A1 has the extended hypergeometric distribution, then $\\operatorname{pr}(D-a=b)$ is $\\pi(a,b)$ given by $$\\pi(a,b)=\\left\\{\\binom{L-a}{b}\\binom{N-L+a}{m-b}\\Gamma^{b}\\right\\}\\Biggl/\\Biggl\\{\\sum_{c=\\operatorname*{max}(0,m+L-a-N)}^{\\operatorname*{min}(m,L-a)}\\binom{L-a}{c}\\binom{N-L+a}{m-c}\\Gamma^{c}\\Biggr\\},$$ if $\\operatorname*{min}(m,L-a)\\geqslant b\\geqslant\\operatorname*{max}(0,m+L-a-N),$ and is given by $\\pi(a,b)=0$ otherwise; and $\\operatorname{pr}(D-a\\geqslant k)$ is $\\lambda({a},{k})$, given by $\\begin{array}{r}{\\lambda({a},{k})=\\sum_{b=\\operatorname*{max}(k,m+L-a-N)}^{\\operatorname*{min}(m,L-a)}\\pi({a},{b})}\\end{array}$."
  },
  {
    "qid": "statistic-posneg-568-0-0",
    "gold_answer": "FALSE. The claim in Lemma A1 suggests that the error in the penalized log-likelihood ratio can be approximated solely by the error in the score function. However, the provided formulas and derivations show that the error term also involves higher-order terms, such as the Hessian of the error function and the difference between the exact and approximate Hessians. These additional terms are not accounted for in the simplified claim, making the statement incomplete and thus incorrect as presented.",
    "question": "Is the claim that the difference between the exact and approximate penalized log-likelihood ratios can be approximated by the error in the score function, as stated in Lemma A1, supported by the provided formulas and derivations?",
    "question_context": "Asymptotic Validity of Naive Inference with Approximate Likelihood\n\nThe paper discusses the asymptotic properties of estimators derived from approximate likelihood functions, focusing on the consistency and convergence rates of these estimators. Key formulas include the Taylor expansion of the approximate score function and the difference between exact and approximate log-likelihood ratios. The proofs leverage techniques from asymptotic statistics, particularly the application of Theorem 5.9 from van der Vaart (1998), to establish the behavior of estimators under large sample sizes."
  },
  {
    "qid": "statistic-posneg-1037-0-0",
    "gold_answer": "TRUE. The function g1(xi) indeed represents the probability of observing a zero count in the zero-inflated portion of the model. The formula φ / (φ + exp(xi^Tβ)) is derived from the zero-inflation component, where φ is the dispersion parameter of the negative binomial distribution and exp(xi^Tβ) relates to the Poisson mean in the count component. This formulation ensures that g1(xi) is bounded between 0 and 1, as required for a probability.",
    "question": "In the zero-inflated negative binomial model, the function g1(xi) represents the probability of observing a zero count. Is it correct that g1(xi) = φ / (φ + exp(xi^Tβ)) where φ is the dispersion parameter and β is the coefficient vector?",
    "question_context": "Zero-Inflated Negative Binomial Regression: Influence Diagnostics\n\nThe authors derive formulas for obtaining the second-order partial derivatives of the log-likelihood function in a zero-inflated negative binomial (ZINB) regression model. The model includes functions for zero-inflation probabilities (g1, g2), a combined hazard function (h), and logistic components (v). The perturbation schemes (case-weight, explanatory variable, and simultaneous) are used to assess the influence of observations on parameter estimates."
  },
  {
    "qid": "statistic-posneg-1721-0-0",
    "gold_answer": "TRUE. The paper explicitly states that for Cγ(Y) to be disconnected, there must be two local minima ξi for fY, and the value of fY at these minima must be less than λ. This is derived from the definition of Cγ(Y) and the requirement for disjoint regions in the confidence set.",
    "question": "In the asymptotic case, is it true that a necessary condition for the confidence region Cγ(Y) to be disconnected is the existence of two local minima ξi for fY with fY(ξi) < λ?",
    "question_context": "Disconnected Confidence Regions in Nonlinear Calibration\n\nThe paper discusses the formation of disconnected confidence regions in nonlinear calibration, focusing on both asymptotic and finite sample cases. In the asymptotic case, the confidence region for ξ is defined as Cγ(Y) = {ξ: fY(ξ) ≤ λ}, where fY is given and λ is the γ-fractile of the χ²(2) distribution. For the region to be disconnected, there must be two local minima ξi for fY with fY(ξi) < λ. In the finite sample case, the model Y = BX + ε is considered, with an exact γ-level confidence region Gγ(Y) defined similarly but incorporating sampling variability. The paper also provides geometric interpretations and necessary conditions for disconnected regions, including constraints on coefficients ci."
  },
  {
    "qid": "statistic-posneg-1370-0-3",
    "gold_answer": "FALSE. Explanation: The relationship between the target allocation proportions p_j* and the gains G_j is given by G_j^c = log p_j*, where G_j^c is a shifted version of G_j to ensure the probabilities sum to 1. This logarithmic relationship means that p_j* = exp(G_j^c), not p_j* = G_j. The gains G_j are used to skew the allocation probabilities but are not identical to the target proportions.",
    "question": "The target allocation proportions p_j* are directly proportional to the gains G_j, meaning that p_j* = G_j for all treatments j.",
    "question_context": "Optimal Response and Covariate-Adaptive Biased Coin Designs in Clinical Trials\n\nThe paper discusses the design of clinical trials where patients are sequentially allocated to one of t treatments based on prognostic factors, previous allocations, and responses. The model involves a regression analysis with treatment effects, covariates, and period effects. The design aims to balance efficient parameter estimation with randomization, using a utility function that combines variance reduction and randomness. The allocation probabilities are derived from a combination of information measures and treatment gains, with a focus on D_A-optimality and skewed allocations towards more effective treatments."
  },
  {
    "qid": "statistic-posneg-740-1-1",
    "gold_answer": "FALSE. The formula for ε(λ) is indeed the sum of squared prediction errors over J subsets, but it is not explicitly stated in the context that it is normalized by the total sum of squares of Y. The context describes ε(λ) as the accumulated prediction errors over the J sets of test samples, but the normalization aspect is not clearly mentioned in the provided paragraphs.",
    "question": "Is the formula for ε(λ) correctly described as the sum of squared prediction errors normalized by the total sum of squares of Y, evaluated over J subsets in cross-validation?",
    "question_context": "Regularized Least Squares (RLS) Estimation and Cross-Validation in Redundancy Analysis\n\nWe use the $J.$ -fold cross validation method (Hastie et al., 2001) to choose an optimal value of \u0002. A similar strategy can also be used to choose the value of $\\sigma$ in kernel RA. In this method, the data at hand are randomly divided into $J$ subsets. One of the $J$ sets is set aside as test samples, and model parameters are estimated from the remaining $J-1$ subsets (called calibration samples). These estimates are then applied to the test samples to estimate prediction errors. This is repeated $J$ times with the test samples changed systematically, and the prediction errors accumulated over the $J$ sets of test samples.<PARAGRAPH_SEPARATOR>Let $Y^{(-j)}$ and $X^{(-j)}$ represent matrices of criterion variables and predictor variables, respectively, with the $j$ th subset of cases, $Y^{(j)}$ and $X^{(j)}$ , removed from the original data sets. We obtain the RLS estimates of reduced rank regression coefficients based on $Y^{(-j)}$ and $X^{(-j)}$ , following the procedure described in Section 2.1. Let $\\tilde{B}^{(-j)}(\\lambda)$ denote the matrix of estimated regression coefficients. We then evaluate<PARAGRAPH_SEPARATOR>$$\\varepsilon(\\lambda)=\\sum_{j=1}^{J}\\mathrm{SS}(Y^{(j)}-X^{(j)}\\tilde{B}^{(-j)}(\\lambda))/\\mathrm{SS}(Y)$$<PARAGRAPH_SEPARATOR>with the value of $\\lambda$ systematically varied. We choose the value of $\\lambda$ associated with the smallest value of $\\varepsilon(\\lambda)$ . Essentially the same procedure can be used for choosing an “optimal” value of $\\sigma$ in kernel RA (with $X^{(-j)}$ replaced by $K^{(-j)}$ , $X^{(j)}$ replaced by $K^{(j)}$ , and $\\tilde{B}^{(-j)}(\\lambda)$ replaced by $\\tilde{G}^{(-j)}(\\lambda)$ ).<PARAGRAPH_SEPARATOR>One cautionary remark is in order, however, when the $J.$ -fold cross validation is used in situations where the space of criterion variables is totally contained in the predictor space. Such is always the case in kernel RA, and when $n\\leqslant p$ in linear RA. If we take $J=n$ , the $J.$ -fold cross validation reduces to the well-known leaving-one-out (LOO) method (or the Jackknife method). This particular choice of $J$ should be avoided in such situations. Due to the (double) centering of the predictor set $X$ or $K,$ , the test sample in the LOO method is always perfectly predicted based on the remaining $n-1$ cases when $\\lambda=0$ . This inevitably leads to the conclusion that the non-regularized case always cross-validates best (which is usually not true). This may be seen from the fact that $y^{(j)\\prime}=-1_{n-1}^{\\prime}Y^{(-j)}$ , $x^{(j)\\prime}=-1_{n-1}^{\\prime}X^{(-j)}$ , and $\\hat{B}^{(-j)}=(X^{(-j)\\prime}X^{(-j)})^{+}X^{(-j)\\prime}Y^{(-j)}$ , where $1_{n-1}$ is the $(n-1)$ -component vector of ones, so that<PARAGRAPH_SEPARATOR>$$x^{(j)\\prime}\\hat{B}^{(-j)}=-1_{n-1}^{\\prime}X^{(-j)}(X^{(-j)\\prime}X^{(-j)})^{+}X^{(-j)\\prime}Y^{(-j)}=y^{(j)\\prime},$$<PARAGRAPH_SEPARATOR>since $X^{(-j)}(X^{(-j)\\prime}X^{(-j)})^{+}X^{(-j)\\prime}=I_{n-1}.$ . The case of kernel RA is similar."
  },
  {
    "qid": "statistic-posneg-683-2-0",
    "gold_answer": "FALSE. The formula $\\mathbf{P}(r;\\mathcal{N}_{i})$ is not a blend of binomial and hypergeometric distributions. It is a compound probability distribution where the binomial term $\\pmb{\\mathscr{p}}_{s}^{\\:x}(1-\\pmb{\\mathscr{p}}_{s})^{N_{i}-x}$ represents the probability of 'x' successes (correct identifications due to sensory 'clicks') out of $\\mathcal{N}_{i}$ trials, and the hypergeometric-like term $\\frac{\\binom{\\mathcal{N}_{i}}{x}\\binom{\\mathcal{N}_{i}-x}{\\mathcal{N}_{i}-r}}{\\binom{\\mathcal{N}_{i}+\\mathcal{N}_{j}-x}{\\mathcal{N}_{j}}}$ accounts for the probability of the remaining $(\\mathcal{N}_{i}-r)$ correct placements being due to random guessing. The two components are multiplicatively combined, not blended, and the overall structure is more complex than a simple mixture of the two distributions.",
    "question": "Is the probability formula $\\mathbf{P}(r;\\mathcal{N}_{i})$ correctly described as a blend of a binomial and a hypergeometric distribution, given its structure and the context of sensory discrimination testing?",
    "question_context": "Sensory Discrimination Testing: Probabilistic Model for Sorting Tests\n\nWe now return to that common and simple and important question of taste testing for the null hypothesis that two items are sensorily indistinguishable. Pair comparison is merely the simplest of a whole family of experimental designs that can be used for this purpose. The literature is extensive. A large part of it is devoted to the problem of design effciency, and not allof this is crystal clear. There is a pervasive belief that because the aim is to test $\\mathbf{H_{0}}$ it follows that relative merits can be assessed on theoretical grounds alone. An assumption frequently made is that if, when the null hypothesis is true, an all-correct solution is statistically more improbable with design $\\mathbf{x}$ than with design Y, ergo, design X is the better. But this situation is rare in practice. Mostly, the null hypothesis is untrue, with, however, only a small difference between the compared items. So it is more realistic to discuss *betterness' in terms of the latent ability of the design to pick up a small sensory difference. <PARAGRAPH_SEPARATOR> Consider the general dichotomous sorting model in which we have $\\mathcal{N}_{i}$ samples of a product $\\mathbf{\\nabla}_{i}^{\\star}$ and ${\\mathcal{N}}_{j}$ of $\\mathfrak{f}^{\\mathfrak{v}}$ . They are presented to a subject, in coded random order (or in a coded randomly arranged group), and he is asked to unscramble them by taste differentiation alone. Attention will be restricted to the condition in which the subject is informed beforehand of the size of the $\\mathcal{N}\\mathfrak{s}.$ .We need a probabilistic model that will take into account what happens when the subject is neither completely insensitive to the difference between the items nor unambiguously recognises them. This situation often obtains: the subject sometimes places an item correctly because his sensory apparatus “clicks', and sometimes, when there is no ‘click', he makes a lucky guess. The process is known in psychology as subliminal perception or subception,7.17 because there is no necessary awarenessofwhich placements are fortuitous and which are subceived. On this basis empirical probabilities of sensory discrimination can be invoked. Let us define $\\pmb{\\phi}_{s i}$ and $\\pmb{\\phi}_{s j}$ as the probabilities of contextual subception of an $\\mathbf{\\nabla}^{6}\\vec{\\imath}^{,}$ or a $\\mathfrak{f}^{}$ ， respectively. Suppose, as the simplest hypothesis, that $\\pmb{\\phi}_{s i}=\\pmb{\\phi}_{s j}=$ , say, $\\pmb{\\mathscr{p}}_{\\pmb{\\mathscr{s}}}$ .Then it may be shownl1l that the probability of exactly $r$ 'successes' in the sorted-off subgroup of $\\mathcal{N}_{i}$ items $(\\mathcal{N}_{i}\\leq\\mathcal{N}_{j})$ will be a blend of a binomial and a hypergeometric distribution, with probability density: <PARAGRAPH_SEPARATOR> $$ \\mathbf{P}(r;\\mathcal{N}_{i})\\:=\\:\\binom{\\mathcal{N}_{j}}{\\mathcal{N}_{i}-r}\\:\\sum_{x=0}^{r}\\frac{\\binom{\\mathcal{N}_{i}}{x}\\binom{\\mathcal{N}_{i}-x}{\\mathcal{N}_{i}-r}}{\\binom{\\mathcal{N}_{i}+\\mathcal{N}_{j}-x}{\\mathcal{N}_{j}}}\\pmb{\\mathscr{p}}_{s}^{\\:x}(1-\\pmb{\\mathscr{p}}_{s})^{N_{i}-x} $$ <PARAGRAPH_SEPARATOR> An important limiting case is when $\\mathcal{N}_{i}$ is unity; then <PARAGRAPH_SEPARATOR> $$ \\mathbf{P}(r=1)=1-\\mathbf{P}(r=0)=\\frac{1+\\phi_{s}\\mathcal{N}_{j}}{1+\\mathcal{N}_{j}} $$ <PARAGRAPH_SEPARATOR> When $\\mathcal{N}_{j}$ is also unity we have pair comparison, and when it is 2 we have the 'triangle' test. Severai higher cases are to be found in the literature. <PARAGRAPH_SEPARATOR> Power functions can be obtained; i.e., we can seek the probability of the rejection of a false $\\mathrm{H}_{0}$ . But the difficulty encountered earlier arises again here: we cannot assume $\\pmb{\\phi}_{s}$ to be independent of design. However, as we said in discussing pair comparison with and without ties, intuition suggests, and experience substantiates, a negative correlation between the size of $\\pmb{\\mathscr{p}}_{s}$ and the complexity of the taster's task. With this in mind we may use power functions to arrive at tentative inferences about the merits of rival designs. Note the ‘tentative' ; here, supremely, the proof of the pudding is in the eating; only experimental observation can supply authentic answers. And we must particularise: findings on ice cream are unlikely to apply to canned soup, and still less to instant coffee. By and large, the safe rule is to aim for simplicity of design."
  },
  {
    "qid": "statistic-posneg-575-0-1",
    "gold_answer": "FALSE. The context explicitly states that the doubly robust formulation remains valid if either $\\gamma_{0}$ or $\\alpha_{0}^{\\mathrm{min}}$ is correct, not necessarily both. This property is highlighted as a key advantage of the approach, enabling robust inference under single-nuisance misspecification.",
    "question": "Does the doubly robust formulation $\\theta_{0}=E[m(W,\\gamma_{0})+\\alpha_{0}^{\\mathrm{min}}(W)\\{Y-\\gamma_{0}(W)\\}]$ remain valid only if both $\\gamma_{0}$ and $\\alpha_{0}^{\\mathrm{min}}$ are correctly specified?",
    "question_context": "Debiased Machine Learning for Nonparametric Functionals\n\nThe general inference problem is to find a confidence interval for some scalar $\\theta_{0}\\in\\mathbb{R}$ where $\\theta_{0}=$ $E\\{m(W,\\gamma_{0})\\}$ , with $\\gamma_{0}\\in\\Gamma$ and $m:\\mathcal{W}\\times\\mathbb{L}_{2}\\to\\mathbb{R}$ being an abstract formula; $W\\in\\mathcal{W}$ is a concatenation of random variables in the model excluding the outcome $Y\\in\\mathcal{Y}\\subset\\mathbb{R},\\mathbb{L}_{2}$ is the space of functions of the form $\\gamma:\\mathcal{W}\\to\\mathbb{R}$ that are square-integrable with respect to measure pr, and $\\Gamma$ is a linear subset of ${\\mathbb L}_{2}$ known by the analyst, which may be $\\mathbb{L}_{2}$ itself."
  },
  {
    "qid": "statistic-posneg-479-3-0",
    "gold_answer": "FALSE. The context indicates that the Bayesian implementation does recover from the initial oversmoothing for $\\gamma_{2}(s)$, but it does not explicitly state that the credible intervals are conservative over the remainder of the domain. The text mentions that the Bayesian implementation has conservative credible intervals for $\\gamma_{1}(s)$, but for $\\gamma_{2}(s)$, it only notes recovery from initial oversmoothing without specifying the conservativeness of the intervals over the entire domain.",
    "question": "Is the statement 'The Bayesian implementation recovers from the initial oversmoothing and has conservative credible intervals over the remainder of the domain for $\\gamma_{2}(s)$' supported by the context and the model's behavior?",
    "question_context": "Longitudinal Penalized Functional Regression Analysis\n\nFinally, in Fig. 3 we show the coverage probabilities for the $95\\%$ confidence and credible intervals produced by a subset of the simulations that were described above. We show the two extreme situations: first, we let $J=3$ , $\\sigma_{\\mathbf{W}}^{2}=0$ , $\\sigma_{\\mathbf{Y}}^{2}=5$ and $\\sigma_{\\mathbf{b}}^{2}=5$ ; second, we let $J=10$ , $\\sigma_{\\mathbf{W}}^{2}=0.5$ , $\\sigma_{\\mathbf{Y}}^{2}=10$ and $\\sigma_{\\mathbf{b}}^{2}=50$ . For $\\gamma_{1}(s)$ , we generally see that the confidence intervals have coverage probabilities that are somewhat lower than the nominal level, whereas the credible intervals are slightly conservative with the exception of one region. A more interesting situation is apparent for $\\gamma_{2}(s)$ . We note from Fig. 2 that both implementations tend to oversmooth the leftmost tail of the coefficient function; this is reflected in the very low coverage probabilities there. The Bayesian implementation recovers from this initial oversmoothing and has conservative credible intervals over the remainder of the domain, but the likelihood-based implementation has a second dip corresponding to a second region of oversmoothing before achieving coverage probabilities that are more similar to those for $\\gamma_{1}(s)$ . The oversmoothing of $\\gamma_{2}(s)$ is most likely related to the use of a single parameter $\\sigma_{g}^{2}$ to control smoothness across the domain of the coefficient function. In the case of $\\gamma_{2}(s)$ , the leftmost tail exhibits much more curvature than the remainder of the function, meaning that the single parameter may induce more smoothness than is accurate. <PARAGRAPH_SEPARATOR> ![](images/c0dfc06585091b147b2cd6d1b3aa167f2827773edd6e62c44041f9cb96a59d35.jpg)  Fig. 3. Coverage probabilities for $95\\%$ confidence $(-\\mathrm{~--~})$ and credible intervals (- - - - - - - ) for both true coefficient functions in simulations with single functional predictors: (a) $\\gamma_{1}(s)$ ; (b) $\\gamma_{2}(s)$ <PARAGRAPH_SEPARATOR> # 3.2. Multiple functional predictors Next, we generate samples from the model <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{Y_{i j}=b_{i}+\\displaystyle\\int_{0}^{10}W_{i j1}(s)\\gamma_{1}(s)\\mathrm{d}s+\\displaystyle\\int_{0}^{10}W_{i j2}(s)\\gamma_{2}(s)\\mathrm{d}s+\\varepsilon_{i j},\\qquadi=1,\\dots,100,}}\\ {{W_{i j k}^{\\prime}(s)=W_{i j k}(s)+\\delta_{i j k}(s),}}\\ {{W_{i j k}(s)=u_{i j k1}+u_{i j k2}s+\\displaystyle\\sum_{t=1}^{10}\\biggl\\{v_{i j t l1}\\sin\\biggl(\\displaystyle\\frac{\\pi t}{5}s\\biggr)+v_{i j t l2}\\cos\\biggl(\\displaystyle\\frac{\\pi t}{5}s\\biggr)\\biggr\\}}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\begin{array}{r}{\\varepsilon_{i j k}\\sim N(0,\\sigma_{\\mathbf{Y}}^{2}),\\delta_{i j k}(s)\\sim N(0,\\sigma_{\\mathbf{W}_{k}}^{2}),b_{i}\\sim N(0,\\sigma_{b}^{2}),u_{i j l1}\\sim\\mathrm{Unif}(0,5),u_{i j l2}\\sim N(1,0.2),v_{i j l1}\\sim{\\cal N}(0,0,T),}\\end{array}$ $v_{i j t l2}\\sim N(0,1/t^{2})$ and $W_{i j k}^{\\prime}(s)$ denotes the $k$ th observed functional predictor for subject $i$ at visit $j$ . Thus the $W_{i j k}(s)$ are independent functions constructed in the same manner as in Section 3.1. The $W_{i j k}(s)$ are observed on the dense grid $[s_{g}=g/10:g=0,\\dots,100]$ and we set $I=100$ . Again we choose $\\gamma_{1}(s)=2\\sin(\\pi s/5)$ and $\\gamma_{2}(s)=\\mathscr{s}$ a nd we take $J\\in\\{3,10\\},\\overset{\\cdot}{\\sigma_{\\mathrm{V}}^{2}}\\in\\{5,10\\},\\sigma_{\\mathbf{W}_{k}}^{2}\\in\\{0,\\bar{0}.5\\}$ and $\\sigma_{\\mathbf{b}}^{2}\\in\\{5,50\\}$ . For each of these combinations, we generate 100 data sets and fit model 1 using both of the implementations that were described in Section 2. Owing to the added complexity of the model, we used chains of length 1000 with the first 500 as burn-in. <PARAGRAPH_SEPARATOR> Table 2 provides the AMSEs resulting from both likelihood-based and Bayesian implementatoivoenrs  tohfe t h1e0 0l osnigmituuladtienda l dfautnac tsieotsn;a la rgeagirne, srseisounl tms ofodre l vwairtiho ums ucltoimplbei fnuanticotinosn oafl $\\overset{\\cdot}{\\sigma}_{\\mathbf{W}_{k}}^{2}$ icatnodr $\\sigma_{\\mathbf{b}}^{2}$ kaerne similar and have been omitted. We see that the results in this setting are remarkably similar to those considered in Section 3.1, despite the additional complexity of the model. Specifically, the AMSEs are negligibly affected and the comparisons between the likelihood-based and Bayesian implementations remain valid. Though not presented, figures examining the coverage probabilities of confidence and credible intervals are also largely unchanged. <PARAGRAPH_SEPARATOR> Table 2. AMSE for the likelihood-based and Bayesian implementations with multiple functional predictors over 100 repetitions for $\\sigma_{\\mathbf{\\sqrt{N}}_{k}}^{2}=0.5$ and $\\sigma_{\\mathbf{b}}^{2}=50$ , and all other possible parameter combinations and true coefficient functions†"
  },
  {
    "qid": "statistic-posneg-1718-2-2",
    "gold_answer": "FALSE. The parameter estimation for the LSD density function is based on the first two moments of the data, as indicated by the use of $\\beta_1$ and $\\beta_2$ in Theorem 6.1. The estimators $\\hat{c}_1$, $\\hat{c}_2$, $\\hat{c}$, and $\\hat{d}$ are derived from these moments, which involve both the first and second moments of the sample covariance matrix.",
    "question": "Is the parameter estimation for the LSD density function based solely on the first moment of the data?",
    "question_context": "Limiting Spectral Distribution of Large-Dimensional Sample Covariance Matrices\n\nThe paper discusses the limiting spectral distribution (LSD) of large-dimensional sample covariance matrices generated from VAR(1) and VMA(1) models. It provides explicit forms of the density functions for these LSDs and discusses estimation methods for the parameters involved. The context includes complex mathematical formulas and theorems that define the density functions and their parameters."
  },
  {
    "qid": "statistic-posneg-614-0-0",
    "gold_answer": "FALSE. While the condition is necessary for the process $(H_{t})$ to be in class $\\pmb{L}$ and ensures boundedness of the trace terms, the existence of the stochastic integral also requires the process $(H_{t})$ to be smooth $(\\mathcal{F}_{t})$-predictable and satisfy the measurability and continuity conditions specified in Definition 2. The boundedness condition alone does not guarantee the existence of the integral.",
    "question": "Is the condition $\\operatorname*{sup}_{[0,1]\\times M\\times\\Omega}[\\mathrm{tr}{H_{t}}^{*}{H_{t}}+\\mathrm{tr}{H_{t}}^{\\prime}{}^{*}{H_{t}}^{\\prime}]<\\infty$ sufficient to ensure the existence of the stochastic integral $C_{t}=\\int_{0}^{t}H_{s}\\tau_{s}d B_{s}$ in the Riemannian manifold setting?",
    "question_context": "Stochastic Integrals in Riemannian Manifolds: Definition and Existence\n\nDEFINITION 2. A stochastic process $(H_{t})_{t\\in[0,1]}$ is said to be a smooth $(\\mathcal{F}_{t})$ predictable $\\mathsf{H o m}(T M,T M)$ valued process if (i) $H\\colon[0,1]\\times M\\times\\varOmega\\to$ ${\\mathrm{Hom}}(T M,T M)$ is  measurable  on  the $\\pmb{\\sigma}$ algebra $\\mathcal{B}([0,1])\\circledast\\mathcal{B}(M)\\circledast\\mathcal{F}$ where $\\mathcal{B}$ denotes the  Borel $\\pmb{\\sigma}^{*}$ -algebra, (i) for almost all $\\pmb{\\omega}\\in\\mathcal{Q}.$ $H(\\cdot,\\cdot,\\omega)$ $[0,1]\\times M\\rightarrow\\mathrm{Hom}(T M,T M)$ is continuous on $[0,1]\\times M$ and is differentiable on $\\boldsymbol{M}$ and (ii) for each $x\\in M,H(\\cdot,x,\\cdot)\\colon[0,1]\\times\\varOmega\\to\\mathrm{Hom}(T_{x}M,T_{x}M)$ is an $(\\mathcal{F}_{t})$ -predictable process. DEFINITION 3. Let $(H_{t})_{t\\in[0,1]}$ be a smooth $(\\mathcal{F}_{t})$ -predictable $\\mathsf{H o m}(T M,T M)\\cdot$ valued process. The process $(H_{t})_{t\\in[0,1]}$ is said to be in class $\\pmb{L}$ if $$ \\operatorname*{sup}_{[0,1]\\times M\\times\\Omega}[\\mathrm{tr}{H_{t}}^{*}{H_{t}}+\\mathrm{tr}{H_{t}}^{\\prime}{}^{*}{H_{t}}^{\\prime}]<\\infty $$ where tr is the trace, the superscript \\* denotes the transpose and $\\pmb{H}^{\\prime}$ is the derivative of $H$ in the second variable. THEOREM 1. Let $(\\boldsymbol{\\mathcal{B}}_{t},\\boldsymbol{\\mathcal{F}}_{t})_{t\\in[0.1]}$ be a standard $\\pmb{n}$ -dimensional Brownian motion in the tangent space at ${\\pmb{\\alpha}}\\in\\pmb{M}$ .Let $(H_{t})_{t\\in[0,1]}$ be $\\pmb{a}$ smooth $(\\mathcal{F}_{t})$ -predictable $\\mathsf{H o m}(T M,T M)$ -valued process in class $\\pmb{L}$ . Then there is a continuous $M$ valued process, $(C_{t})_{t\\in[0.1]}$ , which is called the stochastic integral of $\\left(H_{t}\\right)$ with respect to $\\left(B_{t}\\right)$ and is formally denoted $$ C_{t}=\\int_{0}^{t}H_{s}\\tau_{s}d B_{s} $$ where $\\pmb{\\tau_{s}}$ is the parallelism along $C_{u}0\\leqslant u\\leqslant s$ that identifies vectors in the tangent space at $\\pmb{a}\\in\\pmb{M}$ with vectors in the tangent space at $C_{s}$ Furthermore, given $\\pmb{\\mathscr{u}}\\in\\cal{O}(M)$ such that $\\pmb{\\pi}(\\pmb{u})=\\pmb{a}$ where $\\pi\\colon O(M)\\to M$ is the projection, there is $\\pmb{a}$ horizontal ${\\cal O}(M)$ valued process $(L_{t})_{t\\in[0,1]}$ such that $\\pi(L_{t})=C_{t}$ for all $t\\in[0,1]$ a.s. and $L_{0}\\approxeq\\pmb{u}$"
  },
  {
    "qid": "statistic-posneg-1210-0-1",
    "gold_answer": "FALSE. The estimator explicitly accounts for the dependence between θ̂ and F̂, as indicated by the expression for 𝚷̂(r; θ̂, F̂), which uses both θ̂ and F̂(θ̂). The influence function approach further confirms this dependence by including terms that capture the joint variability of θ̂ and F̂ in the asymptotic distribution.",
    "question": "Does the proposed estimator for the risk distribution, 𝚷̂(r; θ̂, F̂), ignore the dependence between θ̂ and F̂?",
    "question_context": "Semiparametric Maximum Likelihood Estimation in Two-Phase Stratified Sampling\n\nThe paper proposes a semiparametric maximum likelihood (ML) method for estimating parameters in a two-phase stratified sampling design. It introduces a plug-in estimator for the risk distribution and AUC using R-balanced data, leveraging the empirical distribution of covariates within strata. The likelihood function incorporates both Phase I and Phase II data, with Phase I data contributing through stratum-specific information. The method involves maximizing an empirical likelihood function and derives large sample distributions using influence functions."
  },
  {
    "qid": "statistic-posneg-611-2-0",
    "gold_answer": "FALSE. The condition $r^{\\prime}\\leqslant2r$ is not necessarily a strict requirement for all $V$ paths of type (4.20). The actual bound depends on the specific constraints and free indices in the path, and it can vary based on the structure of the matrix and the indices involved.",
    "question": "Is it true that for a $V$ path of type (4.20), the number of distinct elements $r^{\\prime}$ must satisfy $r^{\\prime}\\leqslant2r$?",
    "question_context": "Eigenvector Analysis in Large Dimensional Matrices\n\nIt is evident that each nonzero term in (4.10)-(4.19) is the result of a $V$ path with $r_{1}=r_{2}=r$ <PARAGRAPH_SEPARATOR> Let $r_{0}$ and $c_{0}$ be, respectively, the number of rows and columns of $\nu_{n}$ entered by a given $V$ path. Let $l=r_{0}+c_{0}$ . It is clear that $l$ is the number of distinct indices in the $V.$ path. We will first determine a bound on $l$ for each type of $V$ -path in (4.10)-(4.19) resulting in a nonzero term. There are essentially six different types: <PARAGRAPH_SEPARATOR> Vik…: Uyk,: U)k,:. : Vu,) i# j appearing in (4.14) and (4.17), which can be seen by reversing the order on a set of indices, say, $\\underline{{i}}_{2},...,\\underline{{i}}_{r_{2}};\\underline{{k}}_{1},...,\\underline{{k}}_{r_{2}}$ $v_{1k_{1}}:\\cdots:v_{1k_{r_{1}}}:v_{1\\underline{{k}}_{1}}:\\cdots:v_{1\\underline{{k}}_{r_{2}}}$ appearing in (4.10) (4.20) and (4.21) are those $V.$ paths studied in [4]), <PARAGRAPH_SEPARATOR> $$ v_{i k_{1}}:\\cdots:v_{j k_{r_{1}}}:v_{j\\underline{{k}}_{1}}:\\cdots:v_{j\\underline{{k}}_{r_{2}}},\\quad i\not=j, $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{v_{i k_{1}}:\\cdots:v_{i k_{r_{1}}}:v_{j\\underline{{k}}_{1}}:\\cdots:v_{j\\underline{{k}}_{r_{2}}},i\\not=j,\\mathrm{app}}\\ &{v_{i k_{1}}:\\cdots:v_{i k_{r_{1}}}:v_{\\underline{{i}}\\underline{{k}}_{1}}:\\cdots:v_{\\underline{{j}}\\underline{{k}}_{r_{2}}},i,\\underline{{i}},\\underline{{j}}}\\end{array} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> For a $V$ path of type (4.20) ler $r^{\\prime}\\leqslant2r$ be the number of distinct elements of $V_{n}$ appearing in the path and let $(a_{1},b_{1}),...,(a_{r^{\\prime}},b_{r^{\\prime}})$ be a listing of the indices on the distinct elements where the a's $(b^{\\flat}\\mathfrak{s})$ correspond to the rows (columns) of $V_{n}$ . Each $\\pmb{a}_{m}\\left(\\pmb{b}_{m}\\right)$ is either constrained, that is equal, to at least one other $a_{m}$ $\\left(b_{m^{\\prime}}\\right)$ , or is free, that is, not constrained to any other index. Notice that any pair cannot have both indices free. <PARAGRAPH_SEPARATOR> We have three cases:"
  },
  {
    "qid": "statistic-posneg-700-3-0",
    "gold_answer": "FALSE. While the median and third quartile of $100\\Delta\\log(|\\mathrm{KL}|)$ are positive for all values of k when L=10, indicating a general trend of reduction in KL divergence, the statement uses the word 'always,' which is too absolute. The context mentions that the first quartile and median do not always indicate improvement for L=5, but it does not explicitly state that every single realization shows improvement for L=10. Therefore, the statement is overly strong and not fully supported by the context.",
    "question": "Is the statement 'Segmentation with L=10 always leads to a reduction in the absolute value of the KL divergence for all k-step-ahead forecasts' true based on the provided context and formula?",
    "question_context": "Bayesian Mixtures of Autoregressive Models: Segmentation Impact on KL Divergence\n\nThe paper analyzes the impact of segmentation on the Kullback-Liebler (KL) divergence in Bayesian mixtures of autoregressive models. The formula $$100\\Delta\\log(|\\mathrm{KL}|)=100\\big(\\log(|\\mathrm{KL}_{1}|)-\\log(|\\mathrm{KL}_{L}|)\\big)$$ is used to measure the reduction in the absolute value of the KL divergence due to segmentation, where subscripts 1 and L indicate no segmentation and segmentation with segments of length L, respectively. Positive values indicate a reduction in KL divergence due to segmentation. The analysis includes segment lengths L=5 and L=10, and k-step-ahead forecasts for k=1,...,5. The results show that segmentation with L=10 leads to a significant reduction in KL divergence, with the third quartile exceeding 100, indicating a reduction by a factor of almost 3. For L=5, the third quartile shows similar behavior, but the first quartile and median do not always indicate improvement."
  },
  {
    "qid": "statistic-posneg-1318-0-1",
    "gold_answer": "TRUE. The bridge function $F_{\\mathrm{BB}}(r;\\Delta_{j},\\Delta_{k})$ is correctly specified for binary variables. It accounts for the joint probability of the two binary variables being above their respective thresholds $c_j$ and $c_k$, which is given by $\\Phi_{2}(\\Delta_{j},\\Delta_{k};r)$, minus the product of their marginal probabilities $\\Phi(\\Delta_{j})\\Phi(\\Delta_{k})$. This correctly captures the dependence structure between the two binary variables in the Gaussian copula framework.",
    "question": "Does the bridge function $F_{\\mathrm{BB}}(r;\\Delta_{j},\\Delta_{k})=2\\left\\{\\Phi_{2}(\\Delta_{j},\\Delta_{k};r)-\\Phi(\\Delta_{j})\\Phi(\\Delta_{k})\\right\\}$ correctly account for the joint distribution of two binary variables in the Gaussian copula model?",
    "question_context": "Latent Correlation Estimation via Bridge Functions in Mixed Gaussian Copula Models\n\nThe mixed latent Gaussian copula model jointly models $\\mathbf{W}=$ $({\\bf W}_{1},{\\bf W}_{2},{\\bf W}_{3})\\sim\\mathrm{NPN}({\\bf0},{\\pmb\\Sigma},f)$ such that $X_{1j}=W_{1j},X_{2j}=$ $I(W_{2j}>c_{2j})$ and $W_{3j}=I(W_{3j}>c_{3j})W_{3j}$ . <PARAGRAPH_SEPARATOR> The latent correlation matrix $\\pmb{\\Sigma}$ is the key parameter in Gaussian copula models. Estimation of latent correlations is achieved via the bridge function $F$ such that $\\mathbb{E}(\\widehat{\\tau}_{j k})~=~F(\\sigma_{j k})$ , where $\\sigma_{j k}$ is the latent correlation between variables $j$ and $k,$ and $\\widehat{\\tau}_{j k}$ is the corresponding sample Kendall’s $\\tau$ . Given observed $\\mathbf{x}_{j},\\dot{\\mathbf{x}_{k}}\\in\\mathbb{R}^{n}$ , <PARAGRAPH_SEPARATOR> $$ \\widehat{\\tau}_{j k}=\\widehat{\\tau}(\\mathbf{x}_{j},\\mathbf{x}_{k})=\\frac{2}{n(n-1)}\\sum_{1\\leq i<i^{\\prime}\\leq n}\\mathrm{sign}(x_{i j}-x_{i^{\\prime}j})\\mathrm{sign}(x_{i k}-x_{i^{\\prime}k}), $$ <PARAGRAPH_SEPARATOR> where $n$ is the sample size. Using $F$ , one can construct $\\widehat{\\sigma}_{j k}~=~{\\cal F}^{-1}(\\widehat{\\tau}_{j k})$ with the corresponding estimator $\\widehat{\\pmb\\Sigma}$ being consistent for $\\pmb{\\Sigma}$ (Fan et al. 2017; Quan, Booth, and Wells 2018; Yoon, Carroll, and Gaynanova 2020). The explicit form of $F$ has been derived for all combinations of continuous(C)/binary(B)/truncated(T) variables (Fan et al. 2017; Yoon, Carroll, and Gaynanova 2020). We summarize these results below, and use CC, BC, TC, etc. to denote corresponding combinations."
  },
  {
    "qid": "statistic-posneg-148-0-1",
    "gold_answer": "TRUE. When $K=N$, each subset size $R_{i}$ is 1, because $R_{1}+R_{2}+...+R_{K}=N$ and $K=N$ implies $R_{i}=1$ for all $i$. In this case, the multinomial coefficient reduces to $N!/(1!1!\\ldots 1!)=N!$, which is the number of permutations of $N$ distinct objects. The subroutine PERMUT is designed to handle this case, enumerating all possible orderings of the $N$ distinct objects, as confirmed by the context stating that 'if $K=N$ all permutations of $N$ natural numbers $1,2,...,N$ are enumerated.'",
    "question": "Does the subroutine PERMUT generate all permutations of $N$ distinct objects when $K=N$?",
    "question_context": "Enumeration of Permutations of Multi-Sets with Fixed Repetition Numbers\n\nExact tests associated with $K$ -sample techniques, such as multi-response permutation procedures (Mielke, Berry and Brier, 1981; Mielke, Berry, Brockwell and Williams, 1981) which include the permutation analogue to one-way analysis of variance, require the generation of all permutations of multi-sets with fixed repetition numbers. SU BROUTINE PERMUT enumerates the complete set of all permutations of $N$ objectsconsidered $R_{1},R_{2},...,R_{K}$ at a time, i.e. $${\\binom{N}{R_{1}R_{2}\\ldots R_{K}}},$$ where $N$ is partitioned into $K$ non-ordered, non-empty subsets, $R_{1}+R_{2}+...+R_{K}=N $ and the sizes of subsets $R_{1},R_{2},...,R_{K}$ are fixed for all permutations. If $K=2$ all combinations are enumerated, and if $K=N$ all permutations of $N$ natural numbers $1,2,...,N$ are enumerated. A single call to PERMUT generates all permutations and these may be processed one at a time within PERMUTby calling user-supplied SU BROUTINE $J O B$ or by executing equivalent statements. The $K$ groups within each permutation are represented by the natural integers $1,2,...,K$ , but these may easily be recoded in SU BROUTINE $J O B$ to represent the integers $1,2,...,N$ Or any other natural numbers or alphameric symbols. Each permutation is represented by an integer between 1 and $N!/(R_{1}!R_{2}!\\ldots R_{K}!),$ i.e. the multinomial coefficient."
  },
  {
    "qid": "statistic-posneg-1720-1-0",
    "gold_answer": "TRUE. The warning limits (μ ± 1.25σ/√5) are indeed narrower than the action limits (μ ± 3.00σ/√5). This setup is designed to provide an early warning when sample means fall between the warning and action limits (indicating potential issues) while triggering immediate action if sample means exceed the action limits (indicating severe issues). The use of both limits helps in early detection and prevention of quality control problems.",
    "question": "In the given quality control scheme, the warning limits are set at μ ± 1.25σ/√5 and the action limits at μ ± 3.00σ/√5. Is it correct to say that the warning limits are narrower than the action limits, and this setup is designed to detect potential issues before they become severe?",
    "question_context": "Quality Control Schemes: Warning and Action Limits in Statistical Process Control\n\n6. Page (1955) has investigated the properties of inspection rules that are based on both warning and action limits operating in conjunction. The calculation of the probability that action has been taken after m samples have been observed is diffcult for such schemes. Hence, we will utilize the average run length (A.R.L.) to compare $(a)$ twospecimenschemes ofPagewith $(b)$ those described above. The A.R.L. is the average number of items that have to be observed before the machine is stopped, following the rules for stopping laid down. <PARAGRAPH_SEPARATOR> Scheme I. Samples of size 5 are used. <PARAGRAPH_SEPARATOR> $(a)$ Stop the production if 3 consecutive sample means fall between the warning and the action limits or if any one mean falls beyond the action limits: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l r}{\\mathrm{warning~limits}}&{\\mu\\pm1\\cdot25\\sigma/\\sqrt{5},}\\ {\\mathrm{action~limits}}&{\\mu\\pm3\\cdot00\\sigma/\\sqrt{5},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\pmb{\\mu}$ and $\\pmb{\\sigma}$ are the mean and standard deviation of the controlled scheme. <PARAGRAPH_SEPARATOR> (b) Alternatively, we stop the machine if a run of $\\pmb{T}$ means fall beyond some limit, the limit depending on $\\pmb{T}$ and being chosen in such a way that the average run length for the system when in control is always the same as in $(a)$ <PARAGRAPH_SEPARATOR> Table 1. Average number of samples of $\\pmb{\\mathscr{n}}$ observed before stopping"
  },
  {
    "qid": "statistic-posneg-679-0-2",
    "gold_answer": "TRUE. Theorem 3.2 explicitly assumes 'all conditions of Theorem 3.1 hold,' meaning the prediction consistency is directly contingent on the prior establishment of estimator consistency ($\\Vert\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}\\Vert=o_{p}(1)$ and $\\|\\hat{\\gamma}-\\gamma\\|=o_{p}(1)$). The prediction error convergence relies on these estimator properties carrying through the functional quadratic regression model structure.",
    "question": "Is the prediction consistency result $\\int_{\\mathcal{T}}\\Big(\\hat{Y}^{*}(t)-E(Y^{*}(t)|X^{*})\\Big)^{2}d t\\overset{P}{\\rightarrow}0$ in Theorem 3.2 dependent on the earlier estimator consistency results from Theorem 3.1?",
    "question_context": "Consistency and Asymptotic Behavior in Functional Quadratic Regression\n\nWhen the predicator $X_{i}$ is Gaussian, all $\\xi_{i j}{'s}$ are normal. Thus (C5) is satisfied for any $C\\le3$ . All conditions (C1)–(C5) are common in functional data analysis. <PARAGRAPH_SEPARATOR> Let $d_{n}=\\lVert\\hat{C}_{X}-C_{X}\\rVert$ . Denote $\\tilde{K}_{n}=\\operatorname*{min}\\{j\\ge1:\\lambda_{j}\\le2d_{n}\\}-1$ . It is clear that $d_{n}\\to0$ and thus $\\tilde{K}_{n}\\rightarrow\\infty$ as $n\\to\\infty$ . The following theorem shows the consistency of the proposed estimators. <PARAGRAPH_SEPARATOR> Theorem 3.1. Let truncations $K=K(n)\\to\\infty$ and $D=D(n)\\to\\infty$ , which satisfy $K\\leq\\tilde{K}_{n}$ and <PARAGRAPH_SEPARATOR> $$ \\frac{1}{\\sqrt{n}}\\sum_{k=1}^{D}\\sum_{m=1}^{K}\\frac{1}{\\lambda_{m}}(\\frac{1}{\\delta_{X,m}}+\\frac{1}{\\delta_{Y,k}})\\rightarrow0. $$ <PARAGRAPH_SEPARATOR> Then under conditions (C1)–(C5), we have $\\Vert\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}\\Vert=o_{p}(1).$ If we further assume that <PARAGRAPH_SEPARATOR> $$ \\frac{1}{\\sqrt{n}}\\sum_{k=1}^{D}\\sum_{l=1}^{K}\\left(\\frac{\\sqrt{\\sigma}_{k}}{\\lambda_{l}^{3}\\delta_{X,l}}+\\sum_{m=1}^{l}\\frac{1}{\\lambda_{m}\\lambda_{l}}(\\frac{1}{\\delta_{X,l}}+\\frac{1}{\\delta_{X,m}}+\\frac{1}{\\delta_{Y,k}})\\right)\\rightarrow0, $$ <PARAGRAPH_SEPARATOR> then $\\|\\hat{\\gamma}-\\gamma\\|=o_{p}(1)$ . <PARAGRAPH_SEPARATOR> To illustrate availability of assumption (15), we consider a common setting in FPCA. <PARAGRAPH_SEPARATOR> Remark 2. Similar to formula (3.2) in Hall and Horowitz (2007), suppose that <PARAGRAPH_SEPARATOR> $$ \\lambda_{j}-\\lambda_{j+1}\\geq C_{1}j^{-\\nu_{1}-1},\\quad\\sigma_{j}-\\sigma_{j+1}\\geq C_{2}j^{-\\nu_{2}-1} $$ <PARAGRAPH_SEPARATOR> for $j\\geq1$ and $\\nu_{1},\\nu_{2}>1$ . This implies that $\\lambda_{j}\\ge C_{3}j^{-\\nu_{1}}$ , $\\delta_{X,j}\\geq C_{1}j^{-\\nu_{1}-1}$ and $\\delta_{Y,j}\\geq C_{2}j^{-\\nu_{2}-1}$ , which yield that <PARAGRAPH_SEPARATOR> $$ \\frac{1}{\\sqrt{n}}\\sum_{k=1}^{D}\\sum_{m=1}^{K}\\frac{1}{\\lambda_{m}}(\\frac{1}{\\delta_{X,m}}+\\frac{1}{\\delta_{Y,k}})\\leq\\frac{C_{4}}{\\sqrt{n}}\\sum_{k=1}^{D}\\sum_{m=1}^{K}m^{\\nu_{1}}(m^{\\nu_{1}+1}+k^{\\nu_{2}+1}). $$ <PARAGRAPH_SEPARATOR> Hence we can always find suitable $K=K(n)\\to\\infty$ , $D=D(n)\\to\\infty$ such that (15) holds. (16) can be verified similarly. <PARAGRAPH_SEPARATOR> Our next theorem ensures the prediction consistency. <PARAGRAPH_SEPARATOR> Theorem 3.2. Suppose that all conditions of Theorem 3.1 hold. As $n\\to\\infty$ , we have <PARAGRAPH_SEPARATOR> $$ \\int_{\\mathcal{T}}\\Big(\\hat{Y}^{*}(t)-E(Y^{*}(t)|X^{*})\\Big)^{2}d t\\overset{P}{\\rightarrow}0. $$ <PARAGRAPH_SEPARATOR> Finally we investigate the asymptotic behaviors of the testing statistic $T_{n}$ . <PARAGRAPH_SEPARATOR> Theorem 3.3. Suppose conditions (C1)–(C5) are satisfied. For any pre-determined constants $K$ and $D$ we have <PARAGRAPH_SEPARATOR> $$ T_{n}\\stackrel{d}{\\rightarrow}\\chi^{2}(D K(K+1)/2) $$ <PARAGRAPH_SEPARATOR> under $H_{0}$ and $T_{n}{\\overset{P}{\\to}}\\infty$ if $\\pmb R\\neq0$ , where $\\pmb{R}=(\\pmb{R}_{1}^{T},\\dots,\\pmb{R}_{D}^{T})^{T}$ . <PARAGRAPH_SEPARATOR> Remark 3. In Theorem 3.3, the asymptotic distribution of $T_{n}$ depends on pre-specified finite $K$ and $D$ . The proposed test is consistent when the alternative hypothesis satisfies $\\pmb R\\neq0$ , which means that the projection of $\\gamma$ on the subspace spanned by $\\left\\{\\psi_{l}\\otimes\\psi_{m}\\otimes\\phi_{k}:1\\leq l,m\\leq K,1\\leq k\\leq l\\right.$ is not zero."
  },
  {
    "qid": "statistic-posneg-1081-0-1",
    "gold_answer": "FALSE. The decision rule $\\delta_{B}(g_{\\pi}^{*}(D_{N}))$ conserves the level asymptotically (as $N_{test} \\to \\infty$), but not necessarily for finite samples. Under $H_{0}$, the test statistic's distribution may deviate from its asymptotic limit for small $N_{test}$, leading to level inaccuracies. The proof relies on asymptotic arguments (e.g., CLT) that hold only in the limit.",
    "question": "Does the decision rule $\\delta_{B}(g_{\\pi}^{*}(D_{N}))$ always achieve exact level $\\alpha$ under $H_{0}: P = Q$ for finite sample sizes?",
    "question_context": "Asymptotic Properties of Random Forest-Based Two-Sample Tests\n\nThe paper analyzes the asymptotic properties of random forest-based two-sample tests, focusing on the decision rule's level conservation and power. Key formulas include the asymptotic normality of the U-statistic and the conditions under which the test conserves the level and achieves consistent power."
  },
  {
    "qid": "statistic-posneg-550-0-0",
    "gold_answer": "FALSE. While the paper discusses minimizing $\\|r_Y - r\\|_{\\lambda,\\eta}^2$, the actual minimization occurs over the space $\\mathcal{F}_{full,x}$ (the local function space) rather than $\\mathcal{F}_{full}$ (the global function space). The new estimator is a projection onto $\\mathcal{F}_{full,x}$ with respect to the locally and globally connected seminorm.",
    "question": "Is the statement 'The new estimator $\\hat{r}_{\\lambda,\\eta}^{(0)}(x,u)$ minimizes the penalized seminorm $\\|r_Y - r\\|_{\\lambda,\\eta}^2$ over the full function space $\\mathcal{F}_{full}$' true based on the provided formulas and context?",
    "question_context": "Locally and Globally Connected Projection Estimation in Nonparametric Regression\n\nThe paper introduces a theoretical framework for globally and locally connected inference in nonparametric regression. It defines function spaces and seminorms to establish projections of the response variable onto full and additive function spaces. The new estimator combines local linear and local additive estimators through a penalized seminorm, connecting global and local information."
  },
  {
    "qid": "statistic-posneg-216-0-0",
    "gold_answer": "FALSE. The model explicitly states that the functional coefficients $\\gamma_{k}(s)$ are population-level parameters and do not vary across visits or subjects. This is a key feature of the model, distinguishing the functional effects from the subject-specific random effects $b_{i}$.",
    "question": "Is the statement 'The functional coefficients $\\gamma_{k}(s)$ vary across visits for each subject' true based on the model specification?",
    "question_context": "Longitudinal Functional Regression with Random Effects\n\nIn Fig. 1 we display a functional predictor and cognitive disability outcome for two subjects over time. We stress that this data structure, with high dimensional predictors and scalar outcomes observed longitudinally, is increasingly common. Moreover, we emphasize that our methods are motivated by this study but are generally applicable. A single-level analysis of these data was presented in Goldsmith et al. (2011b). <PARAGRAPH_SEPARATOR> # 1.2. Model proposed <PARAGRAPH_SEPARATOR> More formally, we consider the setting in which we observe for each subject $1\\leqslant i\\leqslant I$ at each visit $1\\leqslant j\\leqslant J_{i}$ data of the form $[Y_{i j},W_{i j1}(s),\\ldots,W_{i j K}(s),X_{i j}].$ , where $Y_{i j}$ is a scalar outcome, $W_{i j k}(s)\\in\\mathcal{L}^{2}[0,1],1\\leqslant k\\leqslant K$ , are functional covariates and $X_{i j}$ is a row vector of scalar covariates. We propose the longitudinal functional regression outcome model <PARAGRAPH_SEPARATOR> $$ Y_{i j}\\sim\\mathrm{EF}(\\mu_{i j},\\eta), $$ <PARAGRAPH_SEPARATOR> $$ g(\\mu_{i j})=X_{i j}\\beta+Z_{i j}b_{i}+\\sum_{k=1}^{K}\\int_{0}^{1}W_{i j k}(s)\\gamma_{k}(s)\\mathrm{d}s $$ <PARAGRAPH_SEPARATOR> where $\\cdot\\mathrm{EF}(\\mu_{i j},\\eta)$ ’ denotes an exponential family distribution with mean $\\mu_{i j}$ and dispersion parameter $\\eta$ . Here $X_{i j}\\beta$ is the standard fixed effects component, $Z_{i j}b_{i}$ is the standard randomeffects component, $b_{i}\\sim N(0,\\sigma_{\\mathbf{b}}^{2}\\mathbf{I}_{I})$ are subject-specific random effects and the $\\begin{array}{r}{\\int_{0}^{1}W_{i j k}(s)\\gamma_{k}(s)\\mathrm{d}s}\\end{array}$ are the functional effects. Both the functional coefficients $\\gamma_{k}(s)$ and the scalar coefficients $\\beta$ are population level parameters, rather than subject-specific effects, and do not vary across visits. <PARAGRAPH_SEPARATOR> ![](images/5ecce529e0d817538c54ca8b8cde0722cd270354365458fb2f90bc178b7eabc9.jpg) <PARAGRAPH_SEPARATOR> Model (1) is novel in that it adds subject-specific random effects to the standard cross-sectional functional regression model; from another point of view, this model adds functional predictors to generalized linear mixed models. The latter viewpoint is particularly instructive in that ideas which are familiar from traditional longitudinal data analysis can be transferred seamlessly to longitudinal functional data analysis. As an example, many different structures of random effects $b_{i}$ , including random intercepts and slopes, will be needed in practice and can be implemented by appropriately specifying the random-effect design matrices and covariance structures. For expositional clarity we use a single vector of subject-specific random effects and assume that these random effects are independent, but more complex structures can be included. In practice the $W_{i j k}(s)$ are not truly functional but are observed on a dense (or sparse) grid and often with error; moreover, the predictors are not necessarily observed over the same domain. In fact, whereas the measurement error is negligible in our application, there are some missing values in the functional predictor in Fig. 1, and the mean and parallel diffusivity functional predictors have different domains. We point out that solutions to these problems are well known (Di et al., 2009; Ramsay and Silverman, 2005; Staniswalis and Lee, 1998; Yao et al., 2003) and we do not discuss them here."
  },
  {
    "qid": "statistic-posneg-657-8-0",
    "gold_answer": "TRUE. The context explicitly states that when λη→∞, the weight functions behave as W1=Op((λη)−1) and W2=1+Op((λη)−1). This is derived from the properties of the local linear–additive estimator and is consistent with the asymptotic behavior described in the paper. The explanation is supported by the mathematical derivations and the conditions provided in the context.",
    "question": "Is the statement 'When λη→∞, the weight functions satisfy W1=Op((λη)−1) and W2=1+Op((λη)−1)' consistent with the provided formulas and the context of the paper?",
    "question_context": "Local-linear-additive-estimation-for-multiple-non-parametric-regression\n\nThe paper discusses the properties of local linear–additive estimators in non-parametric regression. It provides proofs for the convergence rates and behavior of the estimators under different conditions, particularly focusing on the weights and error terms involved in the estimation process."
  },
  {
    "qid": "statistic-posneg-873-0-1",
    "gold_answer": "FALSE. Quenouille demonstrated that Scheffe's polynomial model does not adequately account for additive effects, leading to discrepancies in the interpretation of coefficients. This is because the polynomial terms do not scale appropriately with the introduction of an additive component, unlike the homogeneous models proposed in the paper.",
    "question": "Does Scheffe's polynomial model (2.6) correctly account for the additive effect of a component when the variables are proportions in a mixture, as argued by Quenouille?",
    "question_context": "Mixture Response Models and Additive Effects\n\nThe response to a mixture is taken to be dependent on the proportions of the components present but not on the total amount of mixture. The paper discusses various models for analyzing mixture responses, including polynomial models and homogeneous models of degree one. It highlights the challenges in interpreting coefficients when components have additive effects and proposes alternative models to address these issues."
  },
  {
    "qid": "statistic-posneg-1428-0-0",
    "gold_answer": "FALSE. While the GMMN RQMC estimator generally outperforms the MC estimator, its convergence rate decreases with increasing copula dimensions, as indicated by the decreasing regression coefficient α. This means that while it still performs better than MC, its efficiency advantage diminishes as the dimension increases.",
    "question": "Is it true that the GMMN RQMC estimator consistently outperforms the MC estimator in estimating both the Sobol’ g function and the expected shortfall, regardless of the copula dimension?",
    "question_context": "Efficiency of GMMN RQMC Estimator in Multivariate Risk Management\n\nThe paper evaluates the efficiency of the GMMN RQMC estimator compared to traditional MC and copula RQMC estimators in estimating expectations of test functions and expected shortfall in risk management. The test function considered is the Sobol’ g function based on the Rosenblatt transform, given by $$\\Psi_{1}(U)=\\prod_{j=1}^{d}{\\frac{|4R_{j}-2|+j}{1+j}},$$ where $R_{1}~=~U_{1}$ and, for $j=2,\\ldots,d$ and if $\\pmb{U}\\sim C,R_{j}=$ $C_{j|1,\\dots,j-1}(U_{j}\\mid U_{j-1},\\dots,U_{1})$ denotes the conditional distribution function of $U_{j}$ given $U_{1},\\dots,U_{j-1}$. The expected shortfall at level 0.99 of the aggregate loss $S=\\sum_{j=1}^{d}X_{j}$ is given by $$\\begin{array}{l}{\\displaystyle\\mathrm{ES}_{0.99}(S)=\\frac1{1-0.99}\\int_{0.99}^{1}{F_{S}^{-1}(u)\\mathrm{d}u}}\\ {\\displaystyle=\\mathbb{E}\\big(S\\mid S>F_{S}^{-1}(0.99)\\big)=\\mathbb{E}(\\Psi_{2}(X)),}\\end{array}$$ where $F_{S}^{-1}$ denotes the quantile function of S."
  },
  {
    "qid": "statistic-posneg-89-0-0",
    "gold_answer": "FALSE. While the conditions c_i'c_i = κ_1 and c_i'c_j = κ_2 are necessary, the inequality -κ_1 ≤ (k-1)κ_2 ≤ 0 is not explicitly required for Q_p to be free of nuisance parameters. The primary requirement is the uniformity of variances and covariances among the contrasts, not the specific inequality constraint.",
    "question": "Is the condition for the range-type statistic Q_p to be free of nuisance parameters correctly stated as requiring c_i'c_i = κ_1 for all i and c_i'c_j = κ_2 for all i ≠ j, with -κ_1 ≤ (k-1)κ_2 ≤ 0?",
    "question_context": "Tukey's Method Extension with Random Concomitant Variables\n\nIn many practical design and analysis of experiments situations, especially in psychometric settings, it is possible to obtain cheaply concomitant information in addition to the main response variable. Frequently, this information represents the realization of random variables. It is this situation which we shall address in the sequel. More precisely, we consider the model where the vectors are independently distributed with multivariate normal distributions having covariance matrices and with . The matrix , whose ith row is , is a matrix of known constants of rank . It is assumed that the vector lies in the column space of A. The covariance matrix is positive-definite and may be partitioned as 80 that .8 and is & 8calar. The vector and the vector of regression coefficients are to be estimated from the date; is related to (Anderson, 1958, Chapter 2). The are independent normal deviates with zero means and variances (Anderson, 1958, Chapter 2). It is assumed throughout that . Let where the a.re vectors of known constants, be & given set of parametric functions whose contrasts are estimable under model (1). We shall develop a range-type procedure by which the hypothesis may be tested and which will also allow for the construction of simultaneous confidence intervals on all contrasts of the . In order to determine the conditions under which all this is possible, it is frst necessary to consider the form of the estimates of the parameters of model (1). A set of maximum likelihood estimates for the parameters of model (1) is given by where denotes any generalized inverse of (Searle, 1971, Chapter 1), and . The quantity is estimated by where and From (3) it is seen that the differences may be estimated by where In analogy with the conditions necessary for the use of Tukey's method of multiple comparisons in the analysis of variance (Schef6, 1959, Pp. 73-7), we make the following assumptions: (i) for (i) for where and are constants satisfying (ii) for constant. Under these conditions the distribution of the range-type statistic will be derived and found to be free of nuisance parameters. It may then be used to test hypothesis (2), the hypothesis being rejected if and only if on exceeds the appropriate critical value. Further, it may be employed to form joint confidence intervals on all contrasts with . These will have upper and lower limits The derivation of these confidence limits from (7) is exactly as in Tukey's method. If , that is there are no concomitant variables, the distribution of reducestothat of the studentized range, and (8) becomes & Tukey-type joint confidence interval. If an analysis of variance model may be treated by Tukey's procedure, asumptions (i) and (i) being satisfed, then our development becomes applicable if that model is augmented by the inclusion of random normally distributed covariates, subject to assumption (i). In cases of practical interest, the applicability of Tukey's method would seem to ensure that assumption (iii) will be satisfied."
  },
  {
    "qid": "statistic-posneg-1232-0-2",
    "gold_answer": "FALSE. Explanation: The ISNI method avoids fitting complicated nonignorable models. Instead, it uses a Taylor-series approximation around the MAR model, requiring only the MAR estimates and derivatives evaluated at γ1 = 0. The paper emphasizes that this approach simplifies sensitivity analysis by avoiding high-dimensional integrations and complex model fitting.",
    "question": "The ISNI (Index of Sensitivity to Nonignorability) method requires fitting a nonignorable missing data model to compute the sensitivity of parameter estimates to nonignorable missingness. Is this statement true?",
    "question_context": "Nonignorable Missing Data Mechanism in Longitudinal Clinical Trials\n\nThe paper discusses modeling nonignorable missing data mechanisms in longitudinal clinical trials, distinguishing between intermittent missingness and dropout. The missing data model assumes that the missingness status at each visit follows a multinomial distribution, conditional on past missingness patterns and observed outcomes. The model allows for different missing data mechanisms for different past missingness patterns and includes parameters to account for the dependence of missingness on unobserved outcomes."
  },
  {
    "qid": "statistic-posneg-1408-0-0",
    "gold_answer": "FALSE. The formula suggests a Gaussian model for the increments \\(X_t - X_{t-1}\\) centered around \\(\rho_t^*\\), but it does not explicitly confirm a random walk structure. The random walk would imply \\(\rho_t^* = X_{t-1}\\), which is not specified here. The formula is more general and allows for arbitrary drift terms \\(\rho_t^*\\).",
    "question": "In the MCMC algorithm described, the likelihood process for the antibodies \\(X_t\\) is given by \\(L_{T}^{X}(\rho^{*})=\\prod_{t\\le T}\\exp\\left(-\\frac{(X_{t}-X_{t-1}-\rho_{t}^{*})^{2}}{2\\sigma^{2}}\right)\\). Is this formula consistent with a Gaussian random walk model for the antibody levels?",
    "question_context": "MCMC Algorithm for Infection Process Modeling\n\nThe paper describes an MCMC algorithm for updating the infection process, including the addition or deletion of infection points with associated marks. The likelihood processes for antibodies and AOM counting are defined, and detailed update steps for the infection process are provided, including conditions for compatibility with data and proposal distributions for durations and marks."
  },
  {
    "qid": "statistic-posneg-1081-0-2",
    "gold_answer": "TRUE. The paper proves that $g_{\\pi}^{*}$ minimizes $L_{0}^{g} + L_{1}^{g}$ (and thus $L_{1/2}^{g}$) for any $\\pi \\in (0,1)$. This follows from the inequality $1 - (L_{0}^{g_{\\pi}^{*}} + L_{1}^{g_{\\pi}^{*}}) = TV(P, Q) \\geq 1 - (L_{0}^{g} + L_{1}^{g})$ for any classifier $g$, with equality achieved by $g_{\\pi}^{*}$. The result holds regardless of the choice of $\\pi$.",
    "question": "Is the Bayes classifier $g_{\\pi}^{*}(\\mathbf{z}) = \\mathbb{I}\\{\\eta(\\mathbf{z}) > \\pi\\}$ always optimal for minimizing $L_{0}^{g} + L_{1}^{g}$ under any $\\pi \\in (0,1)$?",
    "question_context": "Asymptotic Properties of Random Forest-Based Two-Sample Tests\n\nThe paper analyzes the asymptotic properties of random forest-based two-sample tests, focusing on the decision rule's level conservation and power. Key formulas include the asymptotic normality of the U-statistic and the conditions under which the test conserves the level and achieves consistent power."
  },
  {
    "qid": "statistic-posneg-1733-3-1",
    "gold_answer": "FALSE. Explanation: The FWAS-K-means algorithm uses the formula w1 = (b1/a1) / (b1/a1 + b2/a2), where a1 and a2 are within-cluster variances, and b1 and b2 are between-cluster variances. The weight depends on the ratio of between-cluster variance to within-cluster variance (b/a). A feature with a smaller within-cluster variance (a) will have a higher weight only if the between-cluster variance (b) is sufficiently large. If b is very small, the weight may not be higher despite a smaller a. Therefore, the statement is not always true.",
    "question": "The FWAS-K-means algorithm always assigns higher weights to features with smaller within-cluster variances. Is this statement true based on the provided formula for feature weights?",
    "question_context": "Feature Weight Adjustment in Clustering Algorithms\n\nThe context discusses the w-K-means and FWAS-K-means algorithms for clustering, focusing on the calculation of feature weights. The weights are derived based on within-cluster variance and a parameter β. Two synthetic datasets are used to illustrate the importance of feature weights in clustering results."
  },
  {
    "qid": "statistic-posneg-1868-0-0",
    "gold_answer": "TRUE. The formula is correctly derived by applying the chain rule of differentiation to Sklar's theorem representation $H(x_{1},\\ldots,x_{d})=C(F_{1}(x_{1}),\\ldots,F_{d}(x_{d}))$. The numerator $h(F_{1}^{-1}(u_{1}),\\dots,F_{d}^{-1}(u_{d}))$ represents the joint density evaluated at the quantiles, while the denominator $f_{1}(F_{1}^{-1}(u_{1}))\\cdot\\cdot\\cdot f_{d}(F_{d}^{-1}(u_{d}))$ accounts for the Jacobian of the transformation from the original variables to the uniform marginals, ensuring the copula density integrates to 1 over $[0,1]^{d}$.",
    "question": "Is the copula density formula $c(u_{1},\\dots,u_{d})=\\frac{h(F_{1}^{-1}(u_{1}),\\dots,F_{d}^{-1}(u_{d}))}{f_{1}(F_{1}^{-1}(u_{1}))\\cdot\\cdot\\cdot f_{d}(F_{d}^{-1}(u_{d}))}$ correctly derived from the joint distribution $H$ and its marginals $F_{1},\\ldots,F_{d}$?",
    "question_context": "Non-parametric Copula Density Estimation via Wavelet Methods\n\nIn risk management, in the areas of finance, insurance and climatology, for example, a new tool has been developed to model the dependence structure of data: the copula. A copula is a multivariate joint distribution defined on the $d\\cdot$ - dimensional unit cube $[0,1]^{d}$ such that every marginal distribution is uniform on the interval [0, 1]. Sklar’s Theorem [1] allows us to separately study the laws of the coordinates $X^{m}$ for $m=1,\\ldots d$ , of any $d\\cdot$ -vector X, and the dependence between the coordinates. <PARAGRAPH_SEPARATOR> Theorem 1. Let $d\\geq2$ and $H$ be a $d$ -variate distribution function. If each marginal distribution $F_{m}$ , $m=1,\\ldots d,$ of $H$ is continuous, a unique $d$ -variate copula C exists, so that <PARAGRAPH_SEPARATOR> $$ \\forall(x_{1},\\ldots,x_{d})\\in\\mathbb{R}^{d},\\quad H(x_{1},\\ldots,x_{d})=C(F_{1}(x_{1}),\\ldots,F_{d}(x_{d})). $$ <PARAGRAPH_SEPARATOR> The copula model has been extensively studied within a parametric framework. Numerous classes of parametric copulas, parametric distribution functions $C$ , have been proposed. For instance there is the elliptic family, which contains the Gaussian copulas and the Student copulas, and the Archimedean family, which contains the Gumbel copulas, the Clayton copulas and the Frank copulas. The first step of such a parametric approach is to select the parametric family of the copula being considered. This is a modeling task that may require finding new copula and methodologies to simulate the corresponding data. Usual statistical inference (estimation of the parameters, goodness-of-fit test, etc) can only take place in a second step. Both tasks have been extensively studied. <PARAGRAPH_SEPARATOR> We propose here to study the copula model within a non-parametric framework. Our aim is to make very mild assumption about the copula. Thus, contrary to the parametric setting, no a priori model of the phenomenon is needed. For practitioners, non-parametric estimators could be seen as a benchmark that makes it possible to select the right parametric family by comparing them to an agnostic estimate. In fact, most of the time, practitioners observe the scatter plot of $\\{(X_{i},Y_{i}),i=1,\\dots,n\\}$ , or $\\{(R_{i},S_{i}),i=1,\\dots,n\\}$ where $R$ and S are the rank statistics of $(X,Y)$ , and then attempt, on the basis of these observations, only to guess the family of parametric copulas the target copula belongs to. Providing good non-parametric estimators of the copula makes this task easier and provides a more rigorous way to describe the copula. <PARAGRAPH_SEPARATOR> In our study, we propose non-parametric procedures to estimate the copula density c associated with the copula C. More precisely, we consider the following model. We assume that we are observing an $n$ -sample $(X_{1}^{1},\\ldots,X_{1}^{d}),\\ldots,\\bar{(}X_{n}^{1},\\ldots,X_{n}^{d})$ of independent data with the same distribution $H$ (and the same density $h$ ) as $(X^{1},\\ldots,X^{d})$ . Referring to the marginal distributions of the coordinates of the vector $(X^{1},\\ldots,X^{d})$ as $F_{1},\\ldots,F_{d}$ , we are interested in estimating the copula density $c$ defined as the derivative (if it exists) of the copula distribution <PARAGRAPH_SEPARATOR> $$ c(u_{1},\\dots,u_{d})=\\frac{h(F_{1}^{-1}(u_{1}),\\dots,F_{d}^{-1}(u_{d}))}{f_{1}(F_{1}^{-1}(u_{1}))\\cdot\\cdot\\cdot f_{d}(F_{d}^{-1}(u_{d}))} $$ <PARAGRAPH_SEPARATOR> where $F_{p}^{-1}(u_{p})=\\operatorname*{inf}\\{x\\in R:F_{p}(x)\\geq u_{p}\\},1\\leq p\\leq d$ and $u=(u_{1},\\dots,u_{d})\\in[0,1]^{d}$ . This would be a classical density model if the marginal distributions, and thus the direct observations, $(U_{i}^{1}=F_{1}(X_{i}^{1}),\\dots,U_{i}^{d}=F_{d}(X_{i}^{d}))$ for $i=1,\\ldots,n$ , were known. Unfortunately, this is not the case. We can observe that this model is somewhat similar to the non-parametric regression model with unknown random design studied in Kerkyacharian and Picard [2] with their warped wavelet families. <PARAGRAPH_SEPARATOR> Two wavelet-based methods are presented: a Local Thresholding Method and a Global Thresholding Method. Both are extensions of the methods studied by Donoho et al. [3,4] and Kerkyacharian et al. [5] in the classical density estimation framework. The copula density $c$ is estimated using a specific multiscale basis representation of $[0,1]^{d}$ , the wavelet representation. Each wavelet coefficient is estimated individually and possibly thresholded (set to 0) if it is considered to be non-significant. The two methods differ in their definition of non-significant: one is local, and individually considers considering individually each estimated coefficient; the other is global, and simultaneously considers all coefficients at each scale. Contrary to the kernel-based method, these methods do not require a fine-tuning of the smoothing parameters. The definition of non-significant is not dependent on the (unknown) regularity of the copula: the procedures are data driven and automatically provide an estimator close to the best possible estimators. We can observe that this includes the estimators that require precise knowledge of the regularity of the copula. <PARAGRAPH_SEPARATOR> We first measure the performance for both estimators on all copula densities that are bounded and that belong to a very large class of regularity. The good behavior of our procedures is due to the approximation properties of the wavelet basis. A regular copula can be approximated by few non-zero-wavelet coefficients leading to estimators with both a small bias and small variance. The wavelet representation is connected to well-known regularity spaces: Besov spaces, in particular, that contain Sobolev spaces or Holder spaces, can be defined through the wavelet coefficients. The first results of this paper are the proofs that the rate of convergence of our estimators are:"
  },
  {
    "qid": "statistic-posneg-577-4-1",
    "gold_answer": "FALSE. The formula is incorrect as written. The correct expression should use the copula's partial derivative with respect to the first argument: ℙ(X₂≤x₂|X₁≤0) = ∂C(u₁, u₂)/∂u₁ evaluated at u₁=ℙ(X₁≤0) and u₂=ℙ(X₂≤x₂). The paper's denominator ℙ(X₁≤0) alone does not account for the copula's dependence structure. The error arises from misapplying the copula's properties for discrete-continuous mixtures.",
    "question": "Does the formula ℙ(X₂≤x₂|X₁≤0) = C(ℙ(X₁≤0), ℙ(X₂≤x₂)) / ℙ(X₁≤0) correctly represent the conditional distribution of X₂ given X₁≤0 for a mixed discrete-continuous bivariate model?",
    "question_context": "Copula Constraints in Mixed Discrete-Continuous Bivariate Models\n\nThe paper discusses the challenges of using copulas to model bivariate distributions with mixed discrete and continuous variables. Specifically, it examines the case where one variable is binary (X₁ ∈ {0,1}) and the other is either discrete with multiple states (X₂ ∈ {0,1,2}) or continuous. The copula C must satisfy specific conditions to accurately represent the joint distribution, such as ℙ(X₁≤0,X₂≤x₂) = C(ℙ(X₁≤0), ℙ(X₂≤x₂)). The paper highlights that normal copulas often fail to meet these conditions due to over-determined systems, especially as the number of states increases. Graphical illustrations and conditional distributions are used to demonstrate these constraints."
  },
  {
    "qid": "statistic-posneg-1829-0-2",
    "gold_answer": "TRUE. The shrinkage operator ST(w, t) is applied in the updates for η and α to enforce sparsity. For η, it is used with the MCP penalty to produce sparse estimates, and for α, it is similarly applied to encourage homogeneity by setting some α_{kk'}^j to zero, implying that the corresponding coefficients β_{kj} and β_{k'j} are equal.",
    "question": "The shrinkage operator ST(w, t) is used to enforce sparsity in the updates for η and α. True or False?",
    "question_context": "ADMM Algorithm for High-Dimensional Integrative Analysis with Homogeneity\n\nThe paper presents an ADMM algorithm for high-dimensional integrative analysis with homogeneity. The objective function includes a least squares term, penalty terms for sparsity and homogeneity, and constraints to enforce homogeneity across groups. The augmented Lagrangian is constructed with dual variables and penalty parameters. The ADMM algorithm iteratively updates the primal variables (β, η, α) and dual variables (ν, υ) until convergence. Key steps involve solving subproblems for β, η, and α, and updating the dual variables. The algorithm uses the Sherman-Morrison-Woodbury formula for efficient matrix inversion and includes a termination criterion based on primal residuals."
  },
  {
    "qid": "statistic-posneg-59-0-3",
    "gold_answer": "FALSE. The paper does not explicitly state that the test using Z achieves asymptotic power 1 under H1 when p^(α/2)ξ→−∞. Instead, it discusses the consistency of the test and the behavior of the test statistic under H1, but does not make a definitive claim about achieving power 1 under this specific condition.",
    "question": "Does the paper suggest that the test using the statistic Z achieves asymptotic power 1 under the alternative hypothesis H1 when p^(α/2)ξ→−∞ as p→∞?",
    "question_context": "High-dimensional Hypothesis Testing for Allometric Extension Model\n\nThe paper discusses a high-dimensional hypothesis testing framework for the allometric extension model. It presents statistical methods to test hypotheses regarding the relationship between population means and covariance structures in high-dimensional settings. The key formulas involve estimators for parameters like ξ, which measures the deviation from the null hypothesis, and test statistics Z and Z^C for hypothesis testing."
  },
  {
    "qid": "statistic-posneg-584-2-1",
    "gold_answer": "TRUE. By convexity of the generator function φ, if φ'(1) < 0, then θ₁ must equal 1. This is because the limit defining θ₁ is constrained by the behavior of φ near 1, and φ'(1) < 0 ensures that the tail exhibits asymptotic independence (θ₁ = 1). The economic implication is that when φ'(1) < 0, the copula's upper tail behavior is strongly independent, meaning extreme events in one variable are unlikely to coincide with extreme events in another.",
    "question": "Does the condition φ'(1) < 0 always imply that θ₁ = 1, as stated in the paper?",
    "question_context": "Upper Tail Behavior of Archimedean Copulas\n\nThe paper analyzes the upper tail behavior of Archimedean copulas, focusing on the limit behavior of the survival copula as one or more coordinates tend to 0. The key parameter is θ₁, defined as the limit of -sφ'(1-s)/φ(1-s) as s↓0, which determines the tail dependence structure. The paper distinguishes between cases of asymptotic dependence (θ₁ > 1) and asymptotic independence (θ₁ = 1), with further subdivisions based on whether φ'(1) < 0 or φ'(1) = 0."
  },
  {
    "qid": "statistic-posneg-705-0-2",
    "gold_answer": "FALSE. The product moment E(Q₁ʰ¹Q₂ʰ²) is not given by a simple closed-form expression. Instead, it involves a series expansion with terms involving gamma functions, exponential terms, and Appell functions. This complexity arises from the noncentral nature of the distribution and the need to account for the interactions between the matrices Q₁ and Q₂ under the specified constraints.",
    "question": "Is the product moment E(Q₁ʰ¹Q₂ʰ²) given by a simple closed-form expression without any series expansion?",
    "question_context": "Noncentral Bimatrix Variate Beta Type V Distribution and Density Function of Λ₃\n\nThe paper introduces the noncentral bimatrix variate beta type V distribution, which is useful for test statistics requiring constant factors in the covariance matrices of Wishart matrix variates. The density function and CDF of Λ₃, defined as the product of determinants of matrices Q₁ and Q₂, are derived. The distribution is generated from ratios of Wishart matrices, and the paper provides detailed expressions for the density function, product moments, and CDF of Λ₃."
  },
  {
    "qid": "statistic-posneg-1889-1-0",
    "gold_answer": "TRUE. The estimator for the location parameter μ is indeed defined as the quasi-midrange v(m) with the smallest variance when r observations are censored from each extreme. This is derived from the condition that v(m) (for m ≥ r+1) is chosen to minimize variance, ensuring optimal efficiency in the context of censored samples.",
    "question": "Is it true that the estimator for the location parameter μ is defined as the quasi-midrange v(m) with the smallest variance when r observations are censored from each extreme?",
    "question_context": "Simplified Estimation of Logistic Distribution Parameters Using Quasi-Ranges\n\nSimplifed estimators of the location and scale parameters of a logistic distribution are constructed along the lines developed by Dixon (1957, 1960). Complete and symmetrically censored samples are considered. The effciency of these estimators relative to the corresponding best linear unbiased estimators is shown to be high enough to make them useful in practice. <PARAGRAPH_SEPARATOR> Let ${\\pmb x}_{\\pmb{\\mathscr{s}}}$ donote the mth smallest observation in 8 sample of size $\\pmb{x}$ from & logistic distribution with mean $\\pmb{\\mu}$ and variance ${\\pmb\\sigma}^{\\mathfrak{s}}$ . Let the mth quasi-range $\\pmb{w}_{(\\pmb{\\mathrm{\\mathbf{\\infty}}})}$ and the mth quasi-midrange ${\\pmb v}_{(\\pmb{m})}$ of the sample be defined by <PARAGRAPH_SEPARATOR> $$ w_{(m)}=x_{N-m+1}-x_{m},\\quad v_{(m)}=\\frac{1}{3}(x_{N-m+1}+x_{m}). $$ <PARAGRAPH_SEPARATOR> Here we present sirmplified estirmators for $\\pmb{\\mu}$ eand $\\pmb{\\sigma}$ based respectively on the $v_{(\\infty)}$ and the ${\\pmb w}_{(\\pmb{m})}{\\bf\\dot{s}}$ Complete 88 well as symmetricaily censored sample8 are considered. Dixon (1957, 1960) has obtained similar estimators for the parametors of a normal distribution. <PARAGRAPH_SEPARATOR> When $\\pmb{\\mathscr{r}}\\left(\\pmb{\\mathscr{r}}\\geqslant\\pmb{0}\\right)$ observations are censored from each extreme our estimator $\\pmb{\\beta}$ of ${\\pmb\\mu}$ is defined as that $v_{(\\infty)}\\left(m\\geqslant r+1\\right)$ with the smallest variance. For the same censoring the estimator $\\pmb{\\tilde{\\sigma}}$ of $\\pmb{\\sigma}$ i8 defined a8 the one with the minimum variance among estimators of the form <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\sigma\\overset{!\\dag^{N}}{\\sum}_{a_{i}w_{(i)},}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where each $\\pmb{a}_{\\ell}\\circ\\mathrm{\\bfan}$ take on the value O or 1, and $c$ is & constant, & function of the $\\pmb{a}_{i}\\star$ , determined so a8 to make the estimator unbiased. <PARAGRAPH_SEPARATOR> The estimator $\\pmb{\\mu}$ is shown in Table l for various values of the sample size $\\pmb{N}$ and the number $\\pmb{\\mathscr{r}}$ of observations consored from eaoh extreme. For & given $\\pmb{N}$ some of the possible $\\pmb{\\mathscr{r}}$ values are skipped for obvious reasons. For example, when $\\mathbf{\\hat{N}}=\\mathbf{20}$ and $\\pmb{\\mathscr{r}}=\\mathbf{0}$ , $\\pmb{\\mu}$ in given by $\\pmb{v}_{(7)}$ and thus for $1\\leqslant r\\leqslant\\theta$ $v_{(7)}$ remaine the best in the class with effciency at least as high as when $\\pmb{\\mathscr{r}}=\\mathbf{0}$ <PARAGRAPH_SEPARATOR> Table 1. Defining value of m for the eatimator $\\pmb{\\mathscr{f}}=\\pmb{v}_{(\\pmb{\\mathscr{m}})}$ and the variance of the cstimator"
  },
  {
    "qid": "statistic-posneg-2137-2-0",
    "gold_answer": "FALSE. The proof shows that under $H_1$, $c_\\alpha - n^{\\frac{1}{d+2}}(c_1 - c_0) \\rightarrow \\infty$ and $n^{\\frac{1}{d+2}}\\{\\hat{f}_n(x_{1,0},\\ldots,x_{d,0}) - c_1\\}$ is bounded in probability, which implies $P_{H_1}[T_n < c_\\alpha] \\rightarrow 1$. However, the question misrepresents the reasoning by oversimplifying it without mentioning the critical conditions involving $c_1 < c_0$ and the boundedness in probability.",
    "question": "Is the statement 'The test based on $T_n$ is consistent because $P_{H_1}[T_n < c_\\alpha] \\rightarrow 1$ as $n \\rightarrow \\infty$' supported by the given proof and formulas?",
    "question_context": "Asymptotic Properties of Multivariate Isotonic Regression Test\n\nTheorem 4. Let $c_{\\alpha}$ be the 𝛼th quantile of the distribution of $T(\\mathbb{W})^{\\prime}(0,\\ldots,0)$ . Under the assumption that the regression function $f$ is differentiable at the point $(x_{1,0},\\ldots,x_{d,0})$ , the test, which rejects $H_{0}$ if $T_{n}<c_{\\alpha}$ , will have asymptotic $s i z e=\\alpha$ . Moreover, $P_{H_{1}}[T_{n}<c_{\\alpha}]\\rightarrow1$ as $n\\to\\infty$ , that is, the test will be a consistent test. <PARAGRAPH_SEPARATOR> Proof. First note that since under $H_{0}$ , $T_{n}$ converges weakly to $T(\\mathbb{W})^{\\prime}(0,\\ldots,0)$ , the asymptotic level of the test based on $T_{n}$ will be $\\alpha$ . <PARAGRAPH_SEPARATOR> The asymptotic power of the test will be <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathbb{P}_{H_{1}}\\left[n^{\\frac{1}{d+2}}\\{\\hat{f}_{n}(x_{1,0},\\dotsc,x_{d,0})-c_{0}\\}<c_{\\alpha}\\right]=P_{H_{1}}\\left[n^{\\frac{1}{d+2}}\\{\\hat{f}_{n}(x_{1,0},\\dotsc,x_{d,0})-c_{1}+c_{1}-c_{0}\\}<c_{\\alpha}\\right]}\\ &{\\mathbb{P}_{H_{1}}\\left[n^{\\frac{1}{d+2}}\\{\\hat{f}_{n}(x_{1,0},\\dotsc,x_{d,0})-c_{1}\\}<c_{\\alpha}-n^{\\frac{1}{d+2}}(c_{1}-c_{0})\\right]}\\end{array} $$ <PARAGRAPH_SEPARATOR> The implication follows form the facts that $c_{\\alpha}-n^{\\frac{1}{d+2}}(c_{1}-c_{0})\\to\\infty$ as $n\\to\\infty$ since $c_{1}<c_{0}$ , and $n^{\\frac{1}{d+2}}\\{\\hat{f}_{n}(x_{1,0},\\ldots,x_{d,0})-c_{1}\\}$ is bounded in probability under $H_{1}$ . This completes the proof. ▪ <PARAGRAPH_SEPARATOR> To implement the test based on $T_{n}$ , one needs to compute $\\hat{f}_{n}(x_{1,0},\\ldots,x_{d,0})$ for a given data, and that can be done using the geometric property discussed in Section 2.1. Besides, in order to compute the specified quantile of the distribution of $T(\\mathbb{W})^{\\prime}(0,\\ldots,0)$ , one may adopt the methodology that will be discussed in Section 4.1. In this discussion, we would like to emphasize that the assertion in Theorem 4 implies that the test based on $T_{n}$ poses good power when the sample size is large enough. <PARAGRAPH_SEPARATOR> Furthermore, note that one can also construct the pointwise confidence interval of the multivariate isotonic regression function based on the result stated in Theorem 3. For instance, $(1-\\alpha)\\%$ $(\\alpha\\in(0,1))$ confidence interval of $f(u_{1},\\ldots,u_{d})$ at the point $(u_{1},\\ldots,u_{d})$ based on our proposed least squares estimator is $\\begin{array}{r}{\\bigg(\\widehat{f}_{n}(u_{1},\\dots,u_{d})-\\frac{c_{1-\\alpha/2}}{n^{\\frac{1}{d+2}}},\\widehat{f}_{n}(u_{1},\\dots,u_{d})-\\frac{c_{\\alpha/2}}{n^{\\frac{1}{d+2}}}\\bigg)}\\end{array}$ , where $c_{\\alpha/2}$ and $c_{1-\\alpha/2}$ are $\\alpha/2$ and $1-\\alpha/2\\mathrm{th}$ quantiles of the distribution of $T_{I_{n}}(\\mathbb{W}_{n})^{\\prime}(0,\\ldots,0)$ , respectively."
  },
  {
    "qid": "statistic-posneg-1650-0-2",
    "gold_answer": "FALSE. The paper explicitly states that while the inclusion of the correction terms $a_{ij}$ tends to reduce the effect of sampling zeros and increase the likelihood of convergence, there is no general guarantee of convergence. The success of convergence depends on the specific data and model context.",
    "question": "The bias correction method proposed in the paper guarantees convergence of the parameter estimates when solving the modified score equations. Is this claim supported by the paper's discussion?",
    "question_context": "Bias Correction in Proportional Odds Logistic Regression via Poisson Likelihood\n\nThe paper discusses a bias correction method for the proportional odds logistic regression model by transforming the multinomial likelihood into a Poisson likelihood. The key idea is that the multinomial likelihood can be expressed as a Poisson likelihood when the probabilities sum to 1 for each subject. The bias correction involves modifying the score equations by replacing the observed responses with pseudoresponses, which include a correction term derived from the second derivatives of the probabilities with respect to the parameters."
  },
  {
    "qid": "statistic-posneg-1111-0-2",
    "gold_answer": "FALSE. The representation $A = L'^{-1} P_{L'X} S P_{L'X} L'$ is valid for admissible linear estimators even when $V$ is singular, provided the conditions of Theorem 3 are met. The theorem explicitly handles cases where $r(V) = v < n$, indicating that the representation applies to singular dispersion matrices as well.",
    "question": "Is the representation $A = L'^{-1} P_{L'X} S P_{L'X} L'$ valid for admissible linear estimators only when $V$ is non-singular?",
    "question_context": "Admissible Linear Estimation in General Gauss-Markov Models\n\nThe paper investigates the validity of admissible linear estimators in general Gauss-Markov models, focusing on conditions under which estimators remain valid when the dispersion matrix is incorrectly specified. Key formulas include conditions for admissibility and representations of estimators."
  },
  {
    "qid": "statistic-posneg-412-2-1",
    "gold_answer": "FALSE. The risk bound in Corollary 2 shows that the procedure $\\delta^{*}$ performs as well as the best procedure in the class $\\{\\delta_{j}\\}$ up to a constant factor and an additive term $\\varepsilon_{n}^{2}$, but it also includes an additional penalty term $(1/N)\\log(1/\\pi_{j})$. This means the performance is not solely determined by the best procedure and $\\varepsilon_{n}^{2}$ but also depends on the weights $\\pi_{j}$.",
    "question": "Does the risk bound in Corollary 2 imply that the procedure $\\delta^{*}$ performs as well as the best procedure in the class $\\{\\delta_{j}\\}$ up to a constant factor and an additive term $\\varepsilon_{n}^{2}$?",
    "question_context": "Adaptive Risk Bounds in Nonparametric Estimation with Known Variance Classes\n\nCase 3: $\\sigma^{2}(\\boldsymbol{x})$ is known to be in a class of variance functions. More generally than the above two cases, let $\\varSigma$ be a class of variance functions bounded above and below. Let $M(\\varepsilon)$ be the $L_{2}(P_{X})$ metric entropy of $\\varSigma$ ; i.e., $M(\\varepsilon)$ is the logarithm of the smallest size of an =-cover of $\\varSigma$ under the $L_{2}(P_{X})$ distance. Let $\\varepsilon_{n}$ be determined by $M(\\varepsilon_{n})/N=\\varepsilon_{n}^{2}$ . Take $\\underline{{\\psi}}$ to be a uniform weight on a covering set, i.e., $\\omega_{k}=e^{-M(\\varepsilon_{n})}$ . This choice of $\\varepsilon_{n}$ gives a good trade-off between $(1/N)\\log(1/\\omega_{j})$ and $\\Vert{\\boldsymbol{\\sigma}}^{2}-{\\boldsymbol{\\sigma}}_{k}^{2}\\parallel^{2}$ . We have the following corollary. Corollary 2. For any given $\\varDelta$ , class of variance function $\\varSigma$ , and weight $\\underline{{\\pi}}$ , we can construct a single estimation procedure $\\delta^{*}$ such that for any underlying regression function u with $\\|u\\|_{\\infty}\\leqslant A$ and $\\sigma^{2}\\in\\Sigma$ , we have $$ \\mathsf{R}((u,\\sigma);n;\\delta^{*})\\leqslant C_{A,\\bar{\\sigma}}\\big\\{\\varepsilon_{n}^{2}+\\operatorname*{inf}_{j}\\left(\\frac{1}{N}\\log\\frac{1}{\\pi_{j}}+\\frac{1}{N}\\sum_{l=n-N+1}^{n}R((u,\\sigma);l;\\delta_{j})\\right)\\le C_{A,\\bar{\\sigma}}\\big\\}. $$ Remark. If $\\sigma$ is known to be in any one in a collection of families of variance functions $\\{\\boldsymbol{\\it{\\Sigma}}_{l},\\boldsymbol{\\it{l}}\\geqslant1\\}$ , then a similar adaptation risk bound holds with an additional penalty term $(1/N)\\log(1/\\varsigma_{l})$ , where $\\left\\{\\varsigma_{l};l\\geqslant1\\right\\}$ is the weight on $\\{\\boldsymbol{\\it{\\Sigma}}_{l},l\\geqslant1\\}$ . If $\\varSigma$ is a parametric family (i.e., of finite dimension) as in Case 2, then $M(\\varepsilon)$ is usually of order $\\log(1/\\varepsilon)$ , resulting in $\\varepsilon_{n}^{2}$ of order log $n/n$ , which is negligible for a nonparametric rate of convergence. Thus for nonparametric estimation, we lose little not knowing the variance function in a parametric class. In contrast, when $\\varSigma$ is a nonparametric class, we may pay a rather high price. For instance, if $\\varSigma$ consists of all nondecreasing functions, then $M(\\varepsilon)$ is of order $1/\\varepsilon$ , and $\\varepsilon_{n}^{2}$ determined accordingly is of order $n^{-2/3}$ , which is damaging for a fast nonparametric rate, e.g., $n^{-4/5}$ . From Theorem 1, by mixing different procedures, we have a single procedure that shares the advantages of the mixed procedures automatically in terms of the risk. Besides the $L_{2}$ risk that we consider, other performance measures (e.g., $L_{\\infty}$ risk or local risks) are useful from both theoretical and practical points of view. It then is of interest to investigate whether similar adaptation procedures exist for other loss functions and if not, what the prices are one needs to pay for adaptation."
  },
  {
    "qid": "statistic-posneg-119-0-1",
    "gold_answer": "FALSE. The paper explicitly states that the minimax rate $n^{-p/d}$ is achievable without regularity conditions on $\\mu$ only when the estimate is allowed to choose the design points adaptively. In the non-adaptive setting, additional regularity conditions on $\\mu$ (e.g., bounded support or moment conditions) are required to achieve this rate. The paper also notes that without such conditions, the rate cannot be better than $n^{-1}$ in the non-adaptive case.",
    "question": "The paper claims that the minimax rate of convergence for estimating a $(p,C)$-smooth function $m$ is $n^{-p/d}$. Is this rate achievable without any regularity conditions on the measure $\\mu$?",
    "question_context": "Optimal Rates of Convergence for Noiseless Function Estimation\n\nThe paper discusses the problem of estimating a measurable function $m:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ from $n$ observations of the function's values at points $z_{1},...,z_{n} \\in \\mathbb{R}^{d}$. The error is measured by the $L_{1}$ error with respect to a fixed but unknown probability measure $\\mu$. The function $m$ is assumed to be $(p,C)$-smooth, meaning it has partial derivatives up to order $k$ (where $p = k + \\beta$) that satisfy a Hölder condition with exponent $\\beta$ and constant $C$. The paper derives minimax rates of convergence for this estimation problem and constructs estimates that achieve these rates."
  },
  {
    "qid": "statistic-posneg-1489-0-0",
    "gold_answer": "TRUE. The condition $a > -n + p + 1$ is necessary to ensure that the integral $m(y)$ converges, making the posterior proper. This condition arises from the need to ensure integrability over $\\sigma^2$ in the marginal likelihood calculation, particularly given the form of the prior and the likelihood. Without this condition, the integral may diverge, leading to an improper posterior.",
    "question": "Is the condition $a > -n + p + 1$ necessary for the posterior to be proper when using the improper prior $\\pi(\beta,\\sigma^{2})=(\\sigma^{2})^{-(a+1)/2}I_{\\mathbb{R}_{+}}(\\sigma^{2})$ in the Bayesian linear regression model with Laplace errors?",
    "question_context": "Bayesian Linear Regression with Laplace Errors: MCMC Algorithms and Posterior Propriety\n\nLet $\\{Y_{i}\\}_{i=1}^{n}$ be independent random variables such that $Y_{i}=x_{i}^{T}\beta+\\sigma\\epsilon_{i},$ where $x_{i}\\in\\mathbb{R}^{p}$ is a vector of known covariates associated with $Y_{i}$ , $\beta\\in\\mathbb{R}^{p}$ is a vector of unknown regression coefficients, and $\\sigma\\in(0,\\infty)$ is an unknown scale parameter. The errors, $\\{\\epsilon_{i}\\}_{i=1}^{n}$ , are assumed to be i.i.d. from the Laplace distribution with scale equal to two, so the common density is $d(\\epsilon)=e^{-\\frac{|\\epsilon|}{2}}/4$. <PARAGRAPH_SEPARATOR> We consider a Bayesian model with an improper prior on $({\beta,\\sigma^{2}})$ that takes the form $\\pi(\beta,\\sigma^{2})=(\\sigma^{2})^{-(a+1)/2}I_{\\mathbb{R}_{+}}(\\sigma^{2}).$ , where $\\mathbb{R}_{+}:=(0,\\infty)$ and $a$ is a hyper-parameter. The standard default prior can be recovered by taking $a=1$. <PARAGRAPH_SEPARATOR> The joint density of the data is given by $f(y|\beta,\\sigma^{2})=\\frac{1}{4^{n}\\sigma^{n}}\\exp\\left\\{-\\frac{1}{2\\sigma}\\sum_{i=1}^{n}|y_{i}-x_{i}^{T}\beta|\right\\},$ where $y=(y_{1},\\ldots,y_{n})$. By definition, the posterior density is proper if $m(y):=\\int_{\\mathbb{R}_{+}}\\int_{\\mathbb{R}^{p}}f(y|\beta,\\sigma^{2})\\pi(\beta,\\sigma^{2})d\beta d\\sigma^{2}<\\infty.$ <PARAGRAPH_SEPARATOR> Proposition 1. The posterior is proper (that is $m(y)<\\infty,$ if and only if $X$ has full column rank, $a>-n+p+1$ , and $y\not\\in C(X)$. <PARAGRAPH_SEPARATOR> Assume now that $m(y)<\\infty$ so that the posterior density, which we denote by $\\pi(\beta,\\sigma^{2}|y)$, is well-defined. The posterior is intractable in the sense that posterior expectations cannot be computed in closed form. In this paper, we analyze a pair of Markov chain Monte Carlo (MCMC) algorithms for this problem. One is a data augmentation (DA) algorithm that is based on a representation of the Laplace density as a scale mixture of normals with respect to the inverse gamma distribution. The other is a variant of the DA algorithm that requires one additional simulation step at each iteration. <PARAGRAPH_SEPARATOR> Let $\\phi~=~\\{(\beta_{m},\\sigma_{m}^{2})\\}_{m=0}^{\\infty}$ be a Markov chain with state space $\times=\\mathbb{R}^{p}\times\\mathbb{R}_{+}$ whose dynamics are defined (implicitly) through the following three-step procedure for moving from the current state, $(\beta_{m},\\sigma_{m}^{2})=(\beta,\\sigma^{2})$ , to $(\beta_{m+1},\\sigma_{m+1}^{2})$. <PARAGRAPH_SEPARATOR> $Z_{i}\\sim\\mathrm{InverseGaussian}\\left(\\frac{\\sigma}{2|y_{i}-x_{i}^{T}\beta|},\\frac{1}{4}\right),$ <PARAGRAPH_SEPARATOR> 2. Draw $\\sigma_{m+1}^{2}\\sim\\operatorname{IG}\\left(\\frac{n-p+a-1}{2},\\frac{y^{T}Q^{-1}y-\theta^{T}\\varOmega^{-1}\theta}{2}\right)$ & ${\\mathrm{Draw}\beta_{m+1}\\sim\\mathrm{N}_{p}\\left(\theta,\\sigma_{m+1}^{2}\\varOmega\right)}$"
  },
  {
    "qid": "statistic-posneg-2049-2-1",
    "gold_answer": "FALSE. The variance formula $\\mathsf{var}(\\widehat{\\tau_{A}}-\\widehat{\\tau_{B}})=w_{\\tau}^{\\prime}w_{\\tau}\\sigma^{2}$ is correctly derived from the properties of least-squares estimators and the weight vector $w_{\\tau}^{\\prime}=\\lambda_{\\tau}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}$. This is consistent with standard linear model theory where the variance of a linear estimator $w^{\\prime}Y$ is $w^{\\prime}w\\sigma^{2}$ under homoscedasticity.",
    "question": "Is the variance of the least-squares estimator for the direct effects contrast $\\widehat{\\tau_{A}}-\\widehat{\\tau_{B}}$ given by $w_{\\tau}^{\\prime}w_{\\tau}\\sigma^{2}$ incorrect?",
    "question_context": "Robust Assessment of Two-Treatment Higher-Order Crossover Designs\n\nIt follows that if many designs are under consideration then the design $D$ with breakdown number $\\boldsymbol{\\mathrm{max}}(m_{D})$ is more robust than the competing designs and should be preferred on grounds of robustness. When several designs have the same largest breakdown number $\\boldsymbol{\\mathrm{max}}(m_{D})$ then efficiency considerations should apply to these designs. Evidently a useful preliminary step is to aim to identify those designs that possess this largest breakdown number. <PARAGRAPH_SEPARATOR> Definition 4. Given a cross-over design $D$ , let $D_{\\mathrm{min}}$ denote the eventual design which remains after all s subjects drop-out after completing two periods. Then $D_{\\mathrm{min}}$ is termed the minimal design associated with $D$ . <PARAGRAPH_SEPARATOR> Assume without loss of generality that the ordering of the rows of $X$ is such that $X_{*}$ , the submatrix of $X$ consisting of the first 2s rows, corresponds to the first two periods of the design. Then $X_{*}$ can be written as <PARAGRAPH_SEPARATOR> $$ X_{*}=\\left[X_{\\operatorname*{min}}\\mathrm{~}0_{2s}^{*}\\right] $$ <PARAGRAPH_SEPARATOR> where $X_{\\mathrm{min}}$ is the minimal design matrix and $0_{2s}^{*}$ is a $2s\\times(p-2)$ matrix consisting of zero elements. Clearly the rows of $X_{*}$ form a subspace ${\\mathcal{R}}_{*}\\subseteq{\\mathcal{R}}_{\\mathrm{plan}}$ . Using an argument of Godolphin and Godolphin (2017), it can be shown that $\\dim(\\mathcal{R}_{*})=\\operatorname{rank}\\left(X_{\\operatorname*{min}}\\right)$ , consequently there are two possible cases to consider. If the minimal design matrix $X_{\\mathrm{min}}$ is connected, i.e. it has maximal rank $s+3$ , then the deficiency in the dimension $\\mathrm{dim}\\big(\\mathcal{R}_{*}\\big)$ compared to $\\dim\\left(\\mathcal{R}_{\\mathfrak{p l a n}}\\right)$ is due solely to the fact that no measurements are made in the final $p-2$ periods which are missing in the minimal design. This implies that no drop-out activity from the design $D$ which occurs in these final $p-2$ periods will result in a disconnected eventual design, i.e. $D$ is perpetually connected. On the other hand if $X_{\\mathrm{min}}$ is disconnected then there will be other eventual designs that include $X_{\\mathrm{min}}$ which are also disconnected. <PARAGRAPH_SEPARATOR> # 3.3. Choosing a perpetually connected design <PARAGRAPH_SEPARATOR> Clearly if a perpetually connected design exists then it should be considered seriously for selection. However, it sometimes happens that the set $\\mathcal{D}$ of perpetually connected designs which are available may contain several members. In these circumstances a choice of planned design from $\\mathcal{D}$ is sensible. Considerable attention has been given to design selection based on various optimality criteria which assume no drop-out will arise: see the useful reviews in Jones and Kenward (2015, Chapter 3), and Chow and Liu (2008, Chapters 9-10). <PARAGRAPH_SEPARATOR> In addition the authors suggest making use of a complementary method of choice between members of $\\mathcal{D}$ that does not appear to be available easily in many software packages. This procedure gives the explicit forms for the unbiased estimators of ${\\tau_{A}}-{\\tau_{B}}$ and $\\rho_{A}-\\rho_{B}$ as linear forms in the observations Y , for each competing member of $\\mathcal{D}$ The method is based on an established alternative test of estimability, described by Searle (1971, Section 5.4). Suppose that $\\lambda\\in{\\mathfrak{A}}$ , where $\\varLambda$ is the estimability space defined in (3). Then $\\lambda^{\\prime}\\theta$ is estimable so it follows from Proposition 1 that there is a $w$ of size $n\\times1$ , referred to as the weight vector, such that <PARAGRAPH_SEPARATOR> $$ \\lambda^{\\prime}=w^{\\prime}X. $$ <PARAGRAPH_SEPARATOR> Let $(X^{\\prime}X)^{-}$ be a generalized inverse of $X^{\\prime}X$ , i.e. $(X^{\\prime}X)^{-}$ satisfies $X^{\\prime}X(X^{\\prime}X)^{-}X^{\\prime}X=X^{\\prime}X$ . Then $P_{X}=X(X^{\\prime}X)^{-}X^{\\prime}$ is the orthogonal projection operator on the column space of $X$ and <PARAGRAPH_SEPARATOR> $$ P_{X}X=X\\Rightarrow X^{\\prime}=X^{\\prime}P_{X}=X^{\\prime}X(X^{\\prime}X)^{-}X^{\\prime} $$ <PARAGRAPH_SEPARATOR> after noting that $P_{X}$ is symmetric. Thus if $S_{X}$ is defined as $S_{X}=X^{\\prime}X(X^{\\prime}X)^{-}$ then $S_{X}X^{\\prime}=X^{\\prime}$ , i.e. $S_{X}$ is a projection operator on the column space of $X^{\\prime}$ , which is $\\mathcal{R}_{\\sf p l a n}$ . This implies that $\\lambda\\in\\mathcal{R}_{\\mathrm{plan}}$ if and only if $\\lambda^{\\prime}S_{X}^{\\prime}=\\lambda^{\\prime}$ . From this condition it follows that, <PARAGRAPH_SEPARATOR> whenever $\\lambda^{\\prime}\\theta$ is estimable <PARAGRAPH_SEPARATOR> $$ \\lambda^{\\prime}=\\lambda^{\\prime}S_{X}^{\\prime}=\\lambda^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}X=w^{\\prime}X, $$ <PARAGRAPH_SEPARATOR> taking account of (6), where the weight vector is given by <PARAGRAPH_SEPARATOR> $$ w^{\\prime}=\\lambda^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}. $$ <PARAGRAPH_SEPARATOR> Searle (1971, page 185) remarks that ‘‘the derivation of a vector $\\lambda$ satisfying $\\lambda^{\\prime}=w^{\\prime}X$ may not always be easy’’, however the improvement in methods for computing the expression (8) in the intervening years has probably made this comment somewhat pessimistic. In particular, it follows from (8) that expressions for the least-squares estimators of the estimable contrasts $\\tau_{A}-\\tau_{B}$ and $\\rho_{A}-\\rho_{B}$ are given by <PARAGRAPH_SEPARATOR> $$ \\widehat{\\tau_{A}}-\\widehat{\\tau_{B}}=w_{\\tau}^{\\prime}Y=\\lambda_{\\tau}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}Y\\mathrm{and}\\widehat{\\rho_{A}}-\\widehat{\\rho_{B}}=w_{\\rho}^{\\prime}Y=\\lambda_{\\rho}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}Y $$ <PARAGRAPH_SEPARATOR> respectˆivelyˆ, where $\\lambda_{\\tau}$ and $\\lambda_{\\rho}$ are the $(p+s+4)\\times1$ vectors <PARAGRAPH_SEPARATOR> $$ \\lambda_{\\tau}=[1-1000_{p+s}^{\\prime}]^{\\prime}\\mathrm{and}\\lambda_{\\rho}=[001-10_{p+s}^{\\prime}]^{\\prime}. $$ <PARAGRAPH_SEPARATOR> These estimators (9) are specified uniquely as weighted sums of the elements of $Y$ and are, of course, unbiased. The leastsquares estimators of $\\tau_{A}-\\tau_{B}$ and $\\rho_{A}-\\rho_{B}$ have sampling variances <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\mathsf{v a r}(\\widehat\\tau_{A}-\\widehat\\tau_{B})=w_{\\tau}^{\\prime}w_{\\tau}\\sigma^{2}\\quad\\mathrm{and}\\mathsf{v a r}(\\widehat\\rho_{A}-\\widehat\\rho_{B})=w_{\\rho}^{\\prime}w_{\\rho}\\sigma^{2},}\\end{array} $$ <PARAGRAPH_SEPARATOR> respectively.These results can be summarized as follows: <PARAGRAPH_SEPARATOR> Proposition 2. If the design $D$ is connected then the weighted coefficients of the least-squares estimators of the direct effects contrast and the carry-over effects contrast are given by $w_{\\tau}^{\\prime}=\\lambda_{\\tau}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}$ and $w_{\\rho}^{\\prime}=\\lambda_{\\rho}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}$ respectively, where $\\lambda_{\\tau}^{\\prime}$ , $\\lambda_{\\rho}^{\\prime}$ are given by (10). The least squares contrast estimators are given by (9) and their sampling variances are given by (11) <PARAGRAPH_SEPARATOR> It should be remarked that Proposition 2 also applies to an eventual cross-over design $D_{e}$ in the event of missing observations due to subject drop-out, provided that $D_{e}$ is connected. The proposition is therefore useful for establishing the weights $w$ for eventual designs that occur after subject drop-out from a perpetually connected design. Furthermore it is evident that a choice between competing perpetually connected designs in $\\mathcal{D}$ can be made by comparing the corresponding sampling variances for the direct effects var $\\dot{\\tau}(\\widehat{\\tau_{A}}-\\widehat{\\tau_{B}})$ , specified in (11), which give the usual measure of precision of the estimator in each case."
  },
  {
    "qid": "statistic-posneg-303-0-1",
    "gold_answer": "TRUE. The paper explicitly states that the optimal estimate of ξ is the one that minimizes departure from proportionality (i.e., the sum of squares for deviations from the proportional regression assumption) and simultaneously maximizes the difference among the proportional regressions. This is achieved by solving the quadratic equation for ξ.",
    "question": "Is it true that the optimal estimate of ξ minimizes the sum of squares for departure from proportionality and maximizes the difference of proportional regressions?",
    "question_context": "Proportional Regressions in Simultaneous Equations\n\nThe paper discusses fitting proportional regression equations where coefficients are proportional across different dependent variables. For instance, in studying coal properties like carbon content and calorific value, each property is linearly related to the percentage ash content. The regression equations are restricted to have a common constant of proportionality, denoted as ξ. The paper provides methods to test the validity of this assumption and to estimate ξ using least squares and F-tests."
  },
  {
    "qid": "statistic-posneg-25-0-2",
    "gold_answer": "FALSE. The formula $T_{n}=\\int\\mid Z_{n}(t)\\mid^{2}d G(t)$ shows that $T_{n}$ involves integrating the square of $Z_{n}(t)$ with respect to a distribution function $G$, not necessarily a probability density function.",
    "question": "Does the test statistic $T_{n}$ involve integrating the square of $Z_{n}(t)$ with respect to a probability density function?",
    "question_context": "Empirical Characteristic Function and Hypothesis Testing\n\nThe empirical characteristic function is considered as a tool for large sample testing of a hypothesis that can be characterized in terms of the characteristic function. Two test statistics based upon the empirical characteristic function are proposed. The limiting distributions of these test statistics are obtained and methods are suggested for using these limiting distributions to calculate critical regions. Given a sequence of stochastic processes $Z_{n}$ and a function $\\pmb{\\mu}$ such that $\\pmb{n}^{1/2}(Z_{n}-\\mu)$ converges weakly to a zero mean Gaussian process, consider the statistics $S_{n}=\\operatorname*{sup}_{t\\in[a~b]}|Z_{n}(t)|$, and $T_{n}=\\int\\mid Z_{n}(t)\\mid^{2}d G(t)$ where $G$ is a distribution function. Statistics of these types arise naturally in the use of the empirical characteristic function for large sample hypothesis testing. Let $X_{1},X_{2},...,X_{n}$ be independent and identically distributed random variables. The empirical distribution function is $F_{n}(x)=N(x)/n,x\\in\\mathbb{R}$ where $N(x)$ is the number of $X_{j}\\leqslant x$ for $1\\leqslant j\\leqslant n$. The empirical characteristic function (e.c.f.) is defined by $c_{n}(t)=\\int e^{i t x}d F_{n}(x)={\\frac{1}{n}}\\sum_{j=1}^{n}e^{i t X_{j}},\\qquad t\\in\\mathbb{R}$."
  },
  {
    "qid": "statistic-posneg-1949-0-0",
    "gold_answer": "FALSE. The context explicitly states that the initial estimates $\\hat{S}_{0,T}^{0}$ and $\\hat{\\beta}^{0}$ are obtained from midpoint imputation, not maximum likelihood estimation. Midpoint imputation is a simpler, often less accurate method used to provide starting values for iterative algorithms like the Stochastic EM algorithm described.",
    "question": "In the Stochastic EM algorithm for doubly interval-censored data, the initial estimates $\\hat{S}_{0,T}^{0}$ and $\\hat{\\beta}^{0}$ are obtained using maximum likelihood estimation. Is this statement correct based on the provided context?",
    "question_context": "Stochastic EM Algorithm for Doubly Interval-Censored Data\n\n(1) Obtain the initial estimate $\\hat{S}_{0,T}^{0}$ and $\\hat{\\beta}^{0}$ from midpoint imputation. <PARAGRAPH_SEPARATOR> # StE-step 1 (Stochastic E-step): <PARAGRAPH_SEPARATOR> (2) Generate at iteration $k+1$ : <PARAGRAPH_SEPARATOR> $$ \\bar{u}_{1q}^{k+1},\\ldots,\\bar{u}_{n q}^{k+1},\\quad q=1,\\ldots,m, $$ <PARAGRAPH_SEPARATOR> from <PARAGRAPH_SEPARATOR> $$ \\hat{F}_{U}(u_{i}|X_{i},u_{i}\\in[u_{l i},u_{r i}],v_{i}\\in[v_{l i},v_{r i}],(\\hat{S}_{0,T}^{k})^{\\mathrm{exp}(\\hat{\\beta}^{k}X_{i})}),\\quad i=1,\\dots,n. $$ <PARAGRAPH_SEPARATOR> Appendix B describes how (3.4) can be obtained as a piecewise quadratic expression but monotone increasing thatcantherforebeeasilynvertedtoenerate. uk+ 1. <PARAGRAPH_SEPARATOR> # StE-step 2: <PARAGRAPH_SEPARATOR> (3) Generate at iteration $k+1$ : <PARAGRAPH_SEPARATOR> $$ \\bar{y}_{j q}^{k+1}\\forall j\\in D,\\quad q=1,\\dots,m, $$ <PARAGRAPH_SEPARATOR> from <PARAGRAPH_SEPARATOR> $$ \\{\\hat{S}_{0,T}^{k}(y_{j}|\\bar{u}_{j q}^{k+1},y_{j}\\in[v_{l j}-\\bar{u}_{j q}^{k+1},v_{r j}-\\bar{u}_{j q}^{k+1}])\\}^{\\exp(\\hat{\\beta}^{k}X_{i})}\\quad\\forall j\\in D, $$ <PARAGRAPH_SEPARATOR> where (3.5) is obtained by $(\\hat{S}_{0,T}^{k}(v_{l j}-\\bar{u}_{j q}^{k+1})-\\hat{S}_{0,T}^{k}(y_{j}))/(\\hat{S}_{0,T}^{k}(v_{l j}-\\bar{u}_{j q}^{k+1})-\\hat{S}_{0,T}^{k}(v_{r j}-\\bar{u}_{j q}^{k+1}))$ , swtiatnht, $\\hat{S}_{0,T}^{k}$ uasrien gs athmep lBerd efsrloomw  ae sftiinmitaet osre.t  Soifn cveal tuhees.  rReseuclatliln tgh aets,t ifmora , caerew ifsiex ecdo tnostant, the y $\\bar{y}_{j q}^{k+1}j\\in D$ $s\\in C$ $y_{s}$ $y_{s}=v_{l s}-u_{s}$ . <PARAGRAPH_SEPARATOR> # M-step: <PARAGRAPH_SEPARATOR> (4) Compute <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\hat{\\Lambda}_{0,q}^{k+1}(t)\\quad\\mathrm{and}\\quad\\hat{\\beta}_{q}^{k+1},}\\end{array} $$ <PARAGRAPH_SEPARATOR> from $\\bar{y}_{j q}^{k+1}\\forall j\\in D$ and $\\bar{y}_{s q}=v_{l s}-\\bar{u}_{s q}^{k+1}\\forall s\\in C,q=1,\\dots,m$ . Since $\\bar{y}_{1q}^{k+1},\\dots,\\bar{y}_{n q}^{k+1}$ represent right-censored survival times, we use partial likelihood and the Breslow estimator of the baseline hazard to obtain $\\hat{\\Lambda}_{0,q}^{k+1}(t)$ and $\\hat{\\beta}_{q}^{k+1}$ . <PARAGRAPH_SEPARATOR> (5) The $(k+1)$ th intermediate estimates will be <PARAGRAPH_SEPARATOR> $$ \\hat{3}^{k+1}=1/m\\sum_{q=1}^{m}\\hat{\\beta}_{q}^{k+1},\\quad\\hat{\\Lambda}_{0}^{k+1}(t)=1/m\\sum_{q=1}^{m}\\hat{\\Lambda}_{0,q}^{k+1}(t),\\quad\\mathrm{and}\\quad\\hat{S}_{0,T}^{k+1}(t)=\\exp(\\hat{\\Lambda}_{0}^{k+1}(t)). $$ <PARAGRAPH_SEPARATOR> As shown in Nielsen (2000), the StEM algorithm leads to a Markov chain $\\hat{\\beta}_{q}^{k+1}~(q=1,\\dots,m)$ and a Markov chain composed of $\\hat{\\Lambda}_{0,q}^{k+1}(t)~(q=1,\\dots,m)$ representing the distribution of $\\hat{\\beta}$ and $\\hat{\\Lambda}_{0}$ at time $t$ . Convergence is checked, e.g. by comparing the running mean of the StEM iterations to the current estimate through the mean integrated squared error. Further, Nielsen (2000) establishes the asymptotic normality of the StEM estimates at convergence (under some regularity conditions). <PARAGRAPH_SEPARATOR> In Appendix C of supplementary material available at Biostatistics online, we show that the choice of $m$ is not critical if larger than 50. <PARAGRAPH_SEPARATOR> # 3.3 Calculation of the variance of the parameters $\\beta$"
  },
  {
    "qid": "statistic-posneg-1332-0-2",
    "gold_answer": "TRUE. The MLE for $\\beta$ is correctly derived by maximizing the product of the likelihoods of individual clusters, assuming mutual independence between clusters. This formulation allows for the estimation of transmission risk factors ($\\beta$) by leveraging the observed data across multiple clusters.",
    "question": "Is the maximum likelihood estimate (MLE) for $\\beta$ given by $\\hat{\\pmb{\\beta}}=\\arg\\operatorname*{max}_{\\pmb{\\beta}}\\prod_{m=1}^{M}\\mathcal{L}(C_{m})$ under the assumption that clusters are mutually independent?",
    "question_context": "Branching Process Models for Infectious Disease Transmission Risk Factors\n\n![](images/d4325469391c936f620ffe90f3db4c7dc5e645dbd95d1f8b82e2f1691924a21d.jpg)  \nFigure 2. Unique transmission trees for an observed cluster of size three where two individuals are smear negative and one is smear positive. The first number indexes the generation number and the second is the order within that generation. There are six possible transmission trees when we consider the individuals, conditioned on their smear statuses, to be indistinguishable from one another (e.g., there is no observable difference between individuals B and C). Underneath the trees, we show the label(s) of the individuals that each tree can have. Labels in blue show the first path and labels in green show the second path for the shown tree, if we consider the individuals to be distinguishable from one another. There are 12 unique transmission trees when we uniquely label the individuals.\n\n<PARAGRAPH_SEPARATOR>\n\nto the observed data as a cluster $\\mathcal{C}$ to emphasize that no transmission paths are known. We know only the size, $|{\\mathcal{C}}|$ , of the transmission tree and the covariates of the individuals $\\mathbf{X}_{i}$ for $i=1,\\ldots,|\\mathcal{C}|$ within the transmission tree.\n\n<PARAGRAPH_SEPARATOR>\n\nLet $T$ be a transmission tree of size $|{\\mathcal{C}}|$ and $\\tau_{c}$ be the set of all such trees of size $|{\\mathcal{C}}|$ with the given covariates $X_{1},\\dots,X_{|{\\mathcal{C}}|}$ distributed among the nodes of the transmission tree. The likelihood for this cluster $({\\mathcal{L}}({\\mathcal{C}}))$ is the sum over the likelihoods of all permissible transmission trees of size $|{\\mathcal{C}}|$ ,\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\mathcal{L}(\\mathcal{C})=\\sum_{T\\in\\mathcal{T}_{\\mathcal{C}}}L_{T}(\\mathcal{C}),\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nwhere $L_{T}(\\mathcal{C})$ is the likelihood for a given transmission tree and is the product of geometric variables where $N_{i}$ is the number of individuals infected by individual $i,$\n\n<PARAGRAPH_SEPARATOR>\n\n$$\nL_{T}(\\mathcal{C})=\\prod_{i=1}^{|\\mathcal{C}|}\\mathcal{P}_{i}^{N_{i}}(1-\\mathcal{P}_{i}).\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nA limitation of the base model is that it requires a stringent assumption: complete observation of a cluster. For example, consider the transmission tree (left) in Figure 3. In this transmission tree, individuals A and E are not detected. With these individuals missing, it is impossible for our base model to reconstruct the projected transmission trees shown in the middle image, where edges between unobserved individuals have been erased. The image in the middle is not the only possible projection of the transmission of seven individuals into five; one could argue that a projection that also includes an edge between C and G is more accurate. Regardless of the projection chosen, our base model can only generate transmission trees that, for example, will connect individual B to the other observed individuals. As such, we know our estimates for any parameters will be biased as individual B’s infection should be independent from any of the observed individuals. We propose a solution to this issue with an approach we call the multiple outside transmissions model.\n\n<PARAGRAPH_SEPARATOR>\n\nAssuming that the clusters are mutually independent, the total likelihood for $M$ observed clusters is the product of the likelihoods of individual clusters. We can then estimate $\\beta$ as the maximum likelihood estimate (MLE), $\\hat{\\boldsymbol{\\beta}}$ , over the likelihood of the observed clusters,\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\hat{\\pmb{\\beta}}=\\arg\\operatorname*{max}_{\\pmb{\\beta}}\\prod_{m=1}^{M}\\mathcal{L}(C_{m}).\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nA feature of our model shown in Equation (2) is that it can be extended to multiple covariates through the logit function $p_{i}=\\mathrm{logit}^{-1}\\left(\\mathbf{X_{i}}{\\pmb\\beta}\\right)$ for a covariate matrix $\\mathbf{X}$ and a vector of covariates $\\beta$ . This formulation allows nested sets of parameters to be sequentially tested, for example, via a likelihood ratio test.\n\n<PARAGRAPH_SEPARATOR>\n\n# 2.2. The Multiple Outside Transmissions Model\n\n<PARAGRAPH_SEPARATOR>\n\nCompared to the base model, the multiple outside transmissions model allows for a more flexible set of transmission trees, and as a consequence, it does not require all individuals be observed in a cluster. The idea is that any transmission tree with unobserved cases can be projected into connected sub-trees whose primary infector is from some abstract individual outside the observed cluster. For example, in Figure 3 (middle) we can instead view the group of six observed individuals as three singletons and one transmission tree of size three, which are then “glued” together such that they originate from some abstract outside infection, which is portrayed in Figure 3 (right). In this new approximation, we have four infections $(B,C,D,G)$ resulting directly from the abstract outside infector, which is why we call this model the multiple outside transmissions model."
  },
  {
    "qid": "statistic-posneg-1390-0-1",
    "gold_answer": "FALSE. The paper clearly states that the SRRR estimator β̂ₘ(k) is biased for k > 0. The bias is given by Bias(β̂ₘ(k)) = -kMₖβ, where Mₖ = (X'V⁻¹X + R'W⁻¹R + kIₚ)⁻¹. This bias arises due to the introduction of the ridge parameter k, which shrinks the estimates towards zero. The bias is a trade-off for reduced variance, which is a common characteristic of ridge regression estimators.",
    "question": "The SRRR estimator β̂ₘ(k) is unbiased for all values of k > 0. Is this statement correct?",
    "question_context": "Stochastic Restricted Ridge Regression (SRRR) Estimator\n\nThe paper discusses the problem of multicollinearity in multiple regression models and introduces the Stochastic Restricted Ridge Regression (SRRR) estimator. This estimator combines the ideas of ridge regression and stochastic linear restrictions to address multicollinearity. The model is given by y = Xβ + ε, where y is the dependent variable, X is the matrix of explanatory variables, β is the vector of regression coefficients, and ε is the error term with E(ε) = 0 and Var(ε) = σ²V. The SRRR estimator is derived as β̂ₘ(k) = (X'V⁻¹X + R'W⁻¹R + kIₚ)⁻¹(X'V⁻¹y + R'W⁻¹r), where k > 0 is the ridge parameter, R and r are stochastic restrictions, and W is the variance matrix of the restrictions."
  },
  {
    "qid": "statistic-posneg-1033-0-1",
    "gold_answer": "TRUE. Proposition 11 explicitly states that the invariant correlation matrix $(r_{i j})_{m\\times m}$ for $(P_{i})_{i\\in\\mathcal{N}}$ has $r_{i j}=1/(n+2)$ for every $i,j\\in[m],~i\\neq j$. This is derived from the joint distribution of the conformal p-values under the null hypotheses.",
    "question": "Does the invariant correlation matrix $(r_{i j})_{m\\times m}$ for $(P_{i})_{i\\in\\mathcal{N}}$ have $r_{i j}=1/(n+2)$ for every $i,j\\in[m],~i\\neq j$ as stated in Proposition 11?",
    "question_context": "Conformal p-values and Invariant Correlation in Hypothesis Testing\n\nProposition 10 implies that $\\Theta_{d}\\subset\\mathcal{T}_{d}$ and that $\\Theta_{d}\\subset\\mathcal{K}_{d}(g)$ for wide varieties of $g$ . For instance, $\\theta_{d}$ is a subset of the compatibility sets of Blomqvist’s beta and Spearman’s rho, studied by [9,17], respectively. Since the compatibility set of Blomqvist’s beta is equal to that of Kendall’s tau [14] and is smaller than that of Gini’s gamma [11], our result implies that $\\theta_{d}$ is contained in all of these compatibility sets. The connection of the clique partition polytope to $\\mathcal{T}_{d}$ is given in Proposition 22 of [7]. This result, together with Theorem 4 and Proposition 10, leads to the following relationship between $\\theta_{d}$ and $\\mathcal{T}_{d}\\colon\\theta_{d}=\\mathcal{T}_{d}$ for $d\\leqslant4$ , and $\\Theta_{d}\\subsetneq\\mathcal{T}_{d}$ for $d\\geqslant5$ . <PARAGRAPH_SEPARATOR> # A.2. Conformal $p$ -values <PARAGRAPH_SEPARATOR> Conformal $p$ -value, studied by [1] in the context of outlier detection, is an interesting example of invariant correlation. Let $S_{1},\\ldots,S_{n}$ be scores of the null training sample (computed from some score function on the data), where $n$ is a fixed positive integer. Scores of the test sample are denoted by $S_{n+i}$ , $i\\in[d],$ , and the corresponding conformal $p$ -value is given by <PARAGRAPH_SEPARATOR> $$ P_{i}=\\frac{1}{n+1}+\\frac{1}{n+1}\\sum_{k=1}^{n}\\mathbb{1}_{\\{S_{k}\\leqslant S_{n+i}\\}}. $$ <PARAGRAPH_SEPARATOR> Assume that $\\{S_{i}:i\\in[n+d]\\}$ is a sequence of independent random variables and $S_{k}\\sim F_{0}$ , $k\\in[n]$ . The above p-values are testing the sequence of null hypotheses $\\mathrm{H}_{0,i}:S_{n+i}\\sim F_{0}$ , $i\\in[d]$ . <PARAGRAPH_SEPARATOR> Proposition 11. Let ${\\mathcal{N}}\\subseteq[d]$ be any set of null indices, that is, $\\mathrm{H}_{0,i}$ is true for $i\\in\\mathcal{N}.$ Then, <PARAGRAPH_SEPARATOR> $$ \\mathbb{P}\\left(P_{i}=\\frac{j_{i}}{n+1},i\\in\\mathcal{K}\\right)=\\frac{N_{1}!\\cdots N_{n+1}!}{(n+m)(n+m-1)\\cdots(n+1)},\\quad j_{i}\\in[n+1], $$ <PARAGRAPH_SEPARATOR> where $m=|\\mathcal{N}|$ and $N_{j}=|\\{i\\in{\\mathcal{N}}:j_{i}=j\\}|,j\\in[n+1],$ . In particular, $P_{i}$ is uniformly distributed on $[n+1]/(n+1)$ for every $i\\in\\mathcal{N}.$ Moreover, $(P_{i})_{i\\in\\mathcal{N}}$ has the invariant corelationmatix $(r_{i j})_{m\\times m}$ such that $r_{i j}=1/(n+2)$ for every $i,j\\in[m],~i\\neq j $ . <PARAGRAPH_SEPARATOR> We can take $\\mathcal{N}$ to be the set of all null indices in Proposition 11. The fact that $(P_{i})_{i\\in\\mathcal{N}}$ has an invariant correlation matrix has been shown in Lemma 2.1 of [1], which also contains the joint distribution of $(P_{i},P_{j})$ for $i,j\\in\\mathcal{N}$ . Proposition 11 further gives the joint distribution of $(P_{i})_{i\\in\\mathcal{N}}$ . On the other hand, the full vector of $\\boldsymbol{\\mathbf{p}}$ -values, $(P_{i})_{i\\in[d]}$ does not have an invariant correlation matrix in general. Note that the marginal distributions of $(P_{i})_{i\\in[d]}$ are generally different for null and non-null components, and there is some positive correlation between them by design in (A.1). Hence, our results in Section 3 explain that invariant correlation for $(P_{i})_{i\\in[d]}$ is not possible. Nevertheless, the vector of conformal p-values has PRDS on $\\mathcal{N}$ as shown by Theorem 2.4 of [1]. <PARAGRAPH_SEPARATOR> For $m=2$ , the vector of null conformal p-values follows the model (22) with $k=n+2$ , $g(x)=\\lfloor(n+1)x\\rfloor/(n+1)$ and $\\mathbf{Z}_{1},\\mathbf{Z}_{2}$ being iid multinomials with the number of trials 1 and uniform event probabilities on $[k]$ .It is left open whether the vector of null conformal p-values can be written of the form (22) for $m\\geqslant3$ ."
  },
  {
    "qid": "statistic-posneg-30-0-3",
    "gold_answer": "FALSE. The paper explicitly states that the stratified bootstrap does not address the heavy computational cost of high breakdown point robust regression estimators, unlike the FRB method, which is designed to be computationally efficient.",
    "question": "Does the paper suggest that the stratified bootstrap (SB) method is computationally more efficient than the fast and robust bootstrap (FRB) method?",
    "question_context": "Robust Model Selection Using Fast and Robust Bootstrap\n\nGiven a set of $p$ covariates, there exist efficient algorithms to compare all possible submodels of these $p$ covariates if the criterion is based on the residual sum of squares (RSS), see e.g. Furnival and Wilson (1974), Gatu and Kontoghiorghes (2006) and Hofmann et al. (2007). However, for robust regression, unless $p$ is small, it is prohibitively time consuming to compare all possible submodels of $k$ covariates with $1\\leq k\\leq p$ . One commonly used strategy is backward elimination: (i) fit all models of size $p-1$ and select the one with the best value of the selection criterion; (ii) fit all submodels of size $p-2$ that are subsets of the above optimal model of size $p-1$ , and select the best one according to the selection criterion; (iii) continue in this way until $p=0$ (a model with only an intercept term). Finally, from these $p+1$ ‘‘optimal’’ models, select the one with the smallest selection criterion. We applied the robust model selection criteria using the fast and robust bootstrap to relatively large real datasets (models with 14, 45, and 65 explanatory variables) and found it to have both a good performance and a reasonable computation time. For really high dimensional problems (e.g. $p>n.$ , a forward selection approach can be used as in Khan et al. (2007a,b). The rest of the paper is organized as follows. Section 2 contains the definition of MM-estimators for regression, which are the robust estimators used in this paper, and the criteria we use for robust model selection. The fast and robust bootstrap procedure of Salibian-Barrera and Zamar (2002) is reviewed briefly in Section 3 while the consistency of the robust model selection procedure is examined in Section 4. The results of our simulation studies are presented in Section 5 while Section 6 illustrates the robustness and feasibility of the method on some real data examples. Finally, Section 7 summarizes our conclusions while the Appendix contains the proofs."
  },
  {
    "qid": "statistic-posneg-1937-1-0",
    "gold_answer": "FALSE. The canonical representation of APH distributions includes a transition rate matrix T that is upper bidiagonal, not strictly diagonal. The matrix has negative entries on the diagonal and potentially non-zero entries immediately above the diagonal (e.g., T12, T23, etc.), representing transitions between phases. This structure is explicitly shown in the canonical form examples provided in the context, such as the matrix with elements T11, T12, T22, T23, etc.",
    "question": "In the canonical representation of APH distributions, the transition rate matrix T is always diagonal with negative entries on the diagonal and zeros elsewhere. Is this statement true based on the provided context?",
    "question_context": "Parameter Estimation and Fisher Information Matrix Computation for Phase-Type Distributions\n\nThe paper discusses parameter estimation and Fisher Information Matrix (FIM) computation for phase-type (PH) distributions, focusing on acyclic PH (APH) distributions with canonical representations. It presents examples of fitting transformed PH random variables to simulated and real-life data, including the use of finite support phase-type distributions (FSPH) for modeling incubation periods of COVID-19 cases. The canonical form of APH distributions is emphasized for its uniqueness and minimality, with transition rates represented in a specific matrix structure."
  },
  {
    "qid": "statistic-posneg-1934-2-1",
    "gold_answer": "TRUE. The normalization step applies a warping function that does not alter the cluster assignments made during the alignment step. The condition $\\lambda(\\pmb{\\varphi}_{[q]},\\tilde{\\mathbf{c}}_{i[q]})=\\lambda(\\pmb{\\varphi}_{[q]},\\mathbf{c}_{i[q]})$ confirms that the clustering structure is preserved despite the normalization.",
    "question": "Does the assignment and alignment step in the k-mean alignment algorithm preserve the clustering structure, meaning that the cluster assignment of a curve remains unchanged after normalization, as indicated by the condition $\\lambda(\\pmb{\\varphi}_{[q]},\\tilde{\\mathbf{c}}_{i[q]})=\\lambda(\\pmb{\\varphi}_{[q]},\\mathbf{c}_{i[q]})$ for all $i$?",
    "question_context": "k-Mean Alignment Algorithm for Curve Clustering\n\nLet $\\underline{{\\varphi}}_{[q-1]}=\\{\\varphi_{1[q-1]},...,\\varphi_{k[q-1]}\\}$ be the set of templates after iteration $q-1$ , and $\\{\\mathbf{c}_{1^{[q-1]}},\\dotsc,\\mathbf{c}_{N^{[q-1]}}\\}$ be the N curves aligned and clustered to $\\underline{{\\varphi}}_{\\mathrm{[}q-1\\mathrm{]}}$ . At the qth iteration the algorithm performs the following steps. <PARAGRAPH_SEPARATOR> Template identification step. <PARAGRAPH_SEPARATOR> F $\\mathbf{\\omega}_{0\\mathrm{{r}}j}=1,\\dots,k$ the template of the jth cluster, $\\varphi_{j^{[q]}}$ , is estimated using all curves assigned to cluster $j$ at iteration $q-1$ , i.e. all curves ${\\bf c}_{i[q-1]}$ such that $\\begin{array}{r}{\\lambda\\big(\\underline{{\\varphi}}_{[q-1]},\\mathbf{c}_{i[q-1]}\\big)=j.}\\end{array}$ Ideally, the template $\\varphi_{j^{[q]}}$ should be estimated as the curve $\\varphi\\in{\\mathcal{C}}$ that maximizes the total similarity: <PARAGRAPH_SEPARATOR> $$ \\sum_{i:\\lambda(\\underline{{\\varphi}}[q-1],{\\bf c}_{i}[q-1])=j}\\rho(\\varphi,{\\bf c}_{i^{[q-1]}}). $$ <PARAGRAPH_SEPARATOR> Assignment and alignment step. <PARAGRAPH_SEPARATOR> The set of curves $\\{\\mathbf{c}_{1^{[q-1]}},\\dotsc\\dotsc,\\mathbf{c}_{N^{[q-1]}}\\}$ is clustered and aligned to the set of templates $\\underline{{{\\varphi_{[q]}}}}~=~\\{\\varphi_{1^{[q]}},\\ldots,\\varphi_{k^{[q]}}\\}$ : for $i=1,\\ldots,N$ , the ith curve ${\\bf c}_{i[q-1]}$ is aligned to $\\varphi_{\\lambda(\\underline{{\\varphi}}[q],{\\bf c}_{i}[q-1])}$ and the aligned curve $\\tilde{\\mathbf{c}}_{i[q]}=\\mathbf{c}_{i[q-1]}\\circ h_{i[q]}$ is assigned to cluster $\\lambda(\\underline{{\\varphi}}_{[q]},\\mathbf{c}_{i[q-1]})\\equiv\\lambda(\\underline{{\\varphi}}_{[q]},\\widetilde{\\mathbf{c}}_{i[q]}).$ . <PARAGRAPH_SEPARATOR> Normalization step. <PARAGRAPH_SEPARATOR> After each assignment and alignment step, we also perform a normalization step. In detail, for $j=1,\\ldots,k,$ all the $N_{j[q]}$ curves $\\tilde{\\mathbf{c}}_{i[q]}$ assigned to cluster $j$ are warped along the warping function $(\\bar{h}_{j^{[q]}})^{-1}$ , where <PARAGRAPH_SEPARATOR> $$ \\bar{h}_{j^{[q]}}=\\frac{1}{N_{j^{[q]}}}\\sum_{i:\\lambda(\\underline{{\\varphi}}[q],\\tilde{\\mathbf{c}}_{i^{[q]}})=j}h_{i^{[q]}}, $$ <PARAGRAPH_SEPARATOR> thus obtaining $\\mathbf{c}_{i[q]}=\\tilde{\\mathbf{c}}_{i[q]}\\circ(\\bar{h}_{j[q]})^{-1}=\\mathbf{c}_{i[q-1]}\\circ h_{i[q]}\\circ(\\bar{h}_{j[q]})^{-1}$ . In this way, at each iteration, the average warping undergone by curves assigned to cluster $j$ is the identity transformation $h(s)=s.$ Indeed: <PARAGRAPH_SEPARATOR> $$ \\frac{1}{N_{j^{[q]}}}\\sum_{i:\\lambda(\\underline{{\\varphi}}[q],{\\bf c}_{i}[q])=j}\\big(h_{i^{[q]}}\\circ(\\bar{h}_{j^{[q]}})^{-1}\\big)(s)=s,\\quad j=1,\\dots,k. $$ <PARAGRAPH_SEPARATOR> The normalization step is thus used to select, among all candidate solutions to the optimization problem, the one that leaves the average locations of the clusters unchanged, thus avoiding the drifting apart of clusters or the global drifting of the overall set of curves. Note that the normalization step preserves the clustering structure chosen in the assignment and alignment step, i.e., $\\lambda(\\pmb{\\varphi}_{[q]},\\tilde{\\mathbf{c}}_{i[q]})=\\lambda(\\pmb{\\varphi}_{[q]},\\mathbf{c}_{i[q]})$ for all $i$ . <PARAGRAPH_SEPARATOR> The algorithm is initialized with a set of initial templates $\\underline{{\\varphi}}_{[0]}=\\{\\varphi_{1^{[0]}},\\cdot\\cdot\\cdot,\\varphi_{k^{[0]}}\\}\\subset\\mathcal{C}$ , and with $\\{{\\bf c}_{1^{[0]}},\\ldots,{\\bf c}_{N^{[0]}}\\}=$ $\\{\\mathbf{c}_{1},\\hdots,\\mathbf{c}_{N}\\}$ , and stopped when, in the assignment and alignment step, the increments of the similarity indexes are all lower than a fixed threshold. <PARAGRAPH_SEPARATOR> # 4. Technical details for the implementation of the $\\pmb{k}$ -mean alignment algorithm"
  },
  {
    "qid": "statistic-posneg-2140-2-1",
    "gold_answer": "FALSE. The convergence $\\tilde{G}_{X} \\longrightarrow \\chi_{2}^{2}$ specifically requires the process $\\mathbf{X}$ to be Gaussian and satisfy $\\sum_{t=0}^{\\infty}|\\gamma_{\\mathbf{X}}(t)|<\\infty$. Non-Gaussian processes or those violating the summability condition do not guarantee this result.",
    "question": "Does the condition $\\tilde{G}_{X} \\longrightarrow \\chi_{2}^{2}$ in Theorem 2.3 hold for any ergodic stationary process?",
    "question_context": "Testing Gaussianity via Random Projections and Characteristic Functions\n\nWe assume that all random elements (r.e.’s) are defined on the same sufficiently rich probability space $(\\varOmega,\\sigma,\\mathbb{P})$ . H denotes a separable Hilbert space with inner product $\\langle\\cdot,\\cdot\\rangle$ and norm $\\|\\cdot\\|.\\{v_{n}\\}_{n=1}^{\\infty}$ is a generic orthonormal basis of $\\mathbb{H}$ and $V_{n}$ is the $n$ -dimensional subspace spanned by $\\{v_{1},\\ldots,v_{n}\\}$ . For any $V\\subset\\mathbb{H},V^{\\perp}$ denotes its orthogonal complement. If $\\mathbf{D}$ is an $\\mathbb{H}$ -valued r.e., ${\\bf D}_{V}$ is the projection of $\\mathbf{D}$ on $V$ . <PARAGRAPH_SEPARATOR> The beta distribution with parameters $\\alpha_{1},\\alpha_{2}$ will be denoted $\\beta(\\alpha_{1},\\alpha_{2});N(\\nu,\\rho)$ is the one-dimensional normal distribution with mean $\\nu$ and variance $\\rho$ , and $\\Phi_{\\nu,\\rho}$ is its ch.f. <PARAGRAPH_SEPARATOR> Let $\\mathbf{X}$ be a stationary process and let $\\mu_{X}:=\\mathbb{E}[X_{0}],\\mu_{X,k}:=\\mathbb{E}[(X_{0}-\\mu_{X})^{k}]k>1$ , and $\\gamma_{X}(t):=\\mathbb{E}[(X_{0}-\\mu_{X})(X_{t}-\\mu_{X})],t\\in\\mathbb{Z}$ . In this section we will handle $X_{1},X_{2},\\ldots,X_{n}$ , $n\\in\\mathbb{N}$ a sample of equally spaced observations of X. We set ${\\hat{\\mu}}_{X}:=n^{-1}\\sum_{i=1}^{n}X_{i}$ , $\\begin{array}{r}{\\hat{\\mu}_{X,k}:=n^{-1}\\sum_{i=1}^{n}(X_{i}-\\hat{\\mu}_{X})^{k}k\\in\\mathbb{N}}\\end{array}$ and <PARAGRAPH_SEPARATOR> $$ \\hat{\\gamma}_{X}(t):=n^{-1}\\sum_{i=1}^{n-|t|}(X_{i}-\\hat{\\mu}_{X})(X_{i+|t|}-\\hat{\\mu}_{X}),\\quad|t|\\leq n-1. $$ <PARAGRAPH_SEPARATOR> When it is clear, we write $\\mu_{X,k}$ as $\\mu_{k}$ , $\\gamma_{X}(0)$ as $\\gamma_{X}$ and $\\hat{\\gamma}_{X}(0)$ as $\\hat{\\gamma}_{X}$ . <PARAGRAPH_SEPARATOR> The dissipative distributions (see Definition 2.1) were introduced in Cuesta-Albertos et al. (2007). In the finite dimensional case, the dissipative distributions and the absolutely continuous distributions with respect to the Lebesgue measure coincide. Thus, the dissipative distributions can be considered as a generalization of the absolutely continuous distributions to the infinite dimensional case in which there is no measure to play the role of the Lebesgue measure. <PARAGRAPH_SEPARATOR> It should be noted that all non-degenerate Gaussian distributions are dissipative. In Section 3 we introduce a nonGaussian dissipative distribution well suited for the problem at hand. <PARAGRAPH_SEPARATOR> Definition 2.1. Let $\\mathbf{D}$ be an $\\mathbb{H}$ -valued r.e. We will say that its distribution is dissipative if there exists an orthonormal basis $\\{v_{n}\\}_{n=1}^{\\infty}$ of $\\mathbb{H}$ , such that $\\mathbb{P}(\\mathbf{D}_{V_{n}^{\\perp}}=0)=0$ , for all $n\\geq2$ , and if the conditional distribution of ${\\bf D}_{V_{n}}$ given ${\\bf D}_{V_{n}^{\\perp}}$ is absolutely continuous with respect to the $n$ -dimensional Lebesgue measure. <PARAGRAPH_SEPARATOR> # 2.1. The Epps test <PARAGRAPH_SEPARATOR> Let <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{A_{N}:=\\{\\lambda:=(\\lambda_{1},\\dots,\\lambda_{N})^{T}\\in\\mathbb{R}_{N}^{+}:\\lambda_{i}\\neq\\lambda_{j},i\\neq j,i,j=1,\\dots,N\\},}\\end{array} $$ <PARAGRAPH_SEPARATOR> and for $\\lambda\\in\\varLambda_{N}$ , let <PARAGRAPH_SEPARATOR> $$ \\hat{g}(\\lambda):=\\frac{1}{n}\\sum_{i=1}^{n}(\\cos(\\lambda_{1}X_{i}),\\sin(\\lambda_{1}X_{i}),\\ldots,\\cos(\\lambda_{N}X_{i}),\\sin(\\lambda_{N}X_{i}))^{T}. $$ <PARAGRAPH_SEPARATOR> We set <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{g_{\\nu,\\rho}(\\lambda):=(\\mathsf{R e}(\\phi_{\\nu,\\rho}(\\lambda_{1})),\\mathsf{I m}(\\phi_{\\nu,\\rho}(\\lambda_{1})),\\ldots,\\mathsf{R e}(\\phi_{\\nu,\\rho}(\\lambda_{N})),\\mathsf{I m}(\\phi_{\\nu,\\rho}(\\lambda_{N})))^{T}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> The spectral density matrix of the process <PARAGRAPH_SEPARATOR> $$ (g(X_{t},\\lambda))_{t\\in\\mathbb{Z}}:=((\\cos(\\lambda_{1}X_{t}),\\sin(\\lambda_{1}X_{t}),\\dots,\\cos(\\lambda_{N}X_{t}),\\sin(\\lambda_{N}X_{t})))_{t\\in\\mathbb{Z}}^{T} $$ <PARAGRAPH_SEPARATOR> at frequency 0 is denoted by $f_{\\mathbf{X}}(0,(\\mu_{X},\\gamma_{X}),\\lambda)$ and is estimated by <PARAGRAPH_SEPARATOR> $$ \\widehat{f}(0,\\lambda)=(2\\pi n)^{-1}\\left(\\sum_{t=1}^{n}\\widehat{G}(X_{t,0},\\lambda)+2\\sum_{i=1}^{\\lfloor n^{2/5}\\rfloor}(1-i/\\lfloor n^{2/5}\\rfloor)\\sum_{t=1}^{n-i}\\widehat{G}(X_{t,i},\\lambda)\\right), $$ <PARAGRAPH_SEPARATOR> where $\\hat{G}(X_{t,i},\\lambda)=(g(X_{t},\\lambda)-\\hat{g}(\\lambda))(g(X_{t+i},\\lambda)-\\hat{g}(\\lambda))^{T}$ and $\\lfloor\\cdot\\rfloor$ denotes the integer part. <PARAGRAPH_SEPARATOR> Let $\\Theta\\subset\\mathbb{R}\\times\\mathbb{R}^{+}$ be an open bounded set and let $\\lambda\\in\\varLambda_{N}$ . Let $G_{n}^{+}(\\lambda)$ be the generalized inverse of $2\\pi\\hat{f}(0,\\lambda)$ and <PARAGRAPH_SEPARATOR> $$ \\mathsf Q_{n}(\\nu,\\rho,\\lambda):=\\left(\\hat{g}(\\lambda)-g_{\\nu,\\rho}(\\lambda)\\right)^{T}G_{n}^{+}(\\lambda)\\left(\\hat{g}(\\lambda)-g_{\\nu,\\rho}(\\lambda)\\right). $$ <PARAGRAPH_SEPARATOR> Given $i=1,\\ldots,N$ , using the modulus-argument form to write complex numbers, it is obvious that there exist $(\\nu_{i},\\rho_{i})$ such that <PARAGRAPH_SEPARATOR> $$ \\{(\\nu,\\rho)\\in\\mathbb{R}\\times\\mathbb{R}^{+}:e^{i\\nu\\lambda_{i}}e^{-\\lambda_{i}^{2}\\rho/2}=\\phi_{X}(\\lambda_{i})\\}=\\{(\\nu_{i}+2k\\pi/\\lambda_{i},\\rho_{i}):k=1,\\ldots\\}. $$ <PARAGRAPH_SEPARATOR> Thus, the set <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\Theta_{0}(\\lambda):=\\{(\\nu,\\rho)\\in\\Theta:\\varPhi_{\\nu,\\rho}(\\lambda_{i})=\\varPhi_{X}(\\lambda_{i}),i=1,\\ldots,N\\}}\\end{array} $$ <PARAGRAPH_SEPARATOR> discrete. Notice that this set will contain at most one element unless the ${\\lambda_{j}}^{\\prime}$ s are rational multiple of $\\lambda_{1}$ . <PARAGRAPH_SEPARATOR> Next, we include an assumption on some regularity conditions of the involved functions on the points in $\\Theta_{0}(\\lambda)$ . This assumption, taken from Epps (1987), will be employed in the results related to the Epps test. <PARAGRAPH_SEPARATOR> Assumption A. For each $(\\nu,\\rho)\\in\\mathcal{O}_{0}(\\lambda)$ it happens that $f_{\\mathbf{X}}(0,(\\nu,\\rho),\\lambda)=f_{\\mathbf{X}}(0,(\\mu_{X},\\gamma_{X}),\\lambda)$ and that <PARAGRAPH_SEPARATOR> $$ \\left.\\frac{\\partial\\phi_{x,y}(\\lambda_{i})}{\\partial(x,y)}\\right|_{(x,y)=(\\nu,\\rho)}=\\left.\\frac{\\partial\\phi_{x,y}(\\lambda_{i})}{\\partial(x,y)}\\right|_{(x,y)=(\\mu_{X},\\gamma_{X})},\\quad i=1,\\ldots,N. $$ <PARAGRAPH_SEPARATOR> The following theorem, proved in Epps (1987), shows the asymptotic distribution of the statistic involved in the Epps test under the null hypothesis. <PARAGRAPH_SEPARATOR> Theorem 2.2 (Epps, 1987). Let $\\mathbf{X}$ be a stationary Gaussian process satisfying <PARAGRAPH_SEPARATOR> $$ \\sum_{t\\in\\mathbb{Z}}|t|^{\\zeta}|\\gamma_{\\mathbf{X}}(t)|<\\infty,\\quad f o r s o m e\\zeta>0. $$ <PARAGRAPH_SEPARATOR> Let $\\Theta\\subset\\mathbb{R}\\times\\mathbb{R}^{+}$ be open and bounded and let $\\lambda\\in\\varLambda_{N}$ such that Assumption A holds. Let $(\\mu_{n},\\gamma_{n})$ be the minimizer on $\\Theta$ nearest to $(\\hat{\\mu}_{X},\\hat{\\gamma}_{X})$ of the map $(\\nu,\\rho)\\to Q_{n}(\\nu,\\rho,\\lambda).$ Assume moreover that $f_{\\mathbf{X}}(0,(\\mu_{X},\\gamma_{X}),\\lambda)$ is positive definite. Then, $n Q_{n}(\\mu_{n},\\gamma_{n},\\lambda)$ converges in distribution to $\\chi_{2N-2}^{2}$ . <PARAGRAPH_SEPARATOR> Remark 2.2.1. Since the Epps test only checks whether the ch.f. of the marginal of the involved process coincides with that of a Gaussian distribution on a finite number of points fixed in advance (see Theorem 2.2), this test is non-consistent against alternatives with Gaussian marginals or, even, against distributions with non-Gaussian marginals whose ch.f.s take the appropriate values on the selected points. <PARAGRAPH_SEPARATOR> Below, in Theorem 3.6, we show that this last problem is alleviated if the set employed in the Epps test is selected at random, thus making this test consistent against every alternative with non-Gaussian one-dimensional marginals. <PARAGRAPH_SEPARATOR> # 2.2. The Lobato and Velasco test <PARAGRAPH_SEPARATOR> Theorem 2.3 shows the behavior of the Lobato and Velasco test. We use the following functions. <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\displaystyle F_{k}:=\\sum_{t=-\\infty}^{\\infty}\\gamma_{X}(t)^{k}\\mathrm{,}}\\ &{\\displaystyle\\tilde{F}_{k}:=2\\sum_{t=1}^{n-1}\\hat{\\gamma}_{X}(t)(\\hat{\\gamma}_{X}(t)+\\hat{\\gamma}_{X}(n-t))^{k-1}+\\hat{\\gamma}_{X}^{k}\\quad\\mathrm{and}}\\ &{\\displaystyle\\tilde{G}_{X}:=n\\hat{\\mu}_{X,3}^{2}/(6\\tilde{F}_{3})+n(\\hat{\\mu}_{X,4}-3\\hat{\\mu}_{X,2}^{2})^{2}/(24\\tilde{F}_{4})\\mathrm{.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Theorem 2.3 (Lobato and Velasco, 2004). Let $\\mathbf{X}$ be an ergodic stationary process. If $\\mathbf{X}$ is Gaussian and satisfies $\\begin{array}{r}{\\sum_{t=0}^{\\infty}|\\gamma_{\\mathbf{X}}(t)|<\\infty,}\\end{array}$ , then $\\tilde{G}_{X}\\longrightarrow\\chi_{2}^{2}$ in distribution. <PARAGRAPH_SEPARATOR> $\\tilde{G}_{X}$ diverges to infinity whenever $\\mu_{X,3}\\neq0$ or $\\mu_{X,4}\\neq3\\mu_{X,2}^{2}$ , if $\\mathbb{C}[X_{t}^{16}]<\\infty$ and <PARAGRAPH_SEPARATOR> - $\\begin{array}{r}{\\sum_{t_{1}=\\infty}^{\\infty}\\cdot\\cdot\\cdot\\sum_{t_{q-1}=\\infty}^{\\infty}|k_{q}(t_{1},\\ldots,t_{q-1})|<\\infty,}\\end{array}$ for $q=2,\\ldots,16$ , where $k_{q}(t_{1},\\dots,t_{q-1})$ denotes the qth-order cumulant of $X_{1},X_{1+t_{1}},\\dots,X_{1+t_{q-1}}$ ,   - $\\begin{array}{r}{\\sum_{t=1}^{\\infty}[\\mathbb{E}|(\\mathbb{E}(X_{0}-\\mu)^{k}|\\mathcal{F}_{-t})-\\mu_{k}|^{2}]^{1/2}<\\infty,}\\end{array}$ , for $k=3$ , 4, where $\\mathcal{F}_{-t}$ denotes the $\\sigma$ -field generated by Xj, $j\\le-t$ , and - $\\begin{array}{r}{\\mathbb{E}[(X_{0}-\\mu)^{k}-\\mu_{k}]^{2}+2\\sum_{t_{1}=\\infty}^{\\infty}\\mathbb{E}([(X_{0}-\\mu)^{k}-\\mu_{k}][(X_{t}-\\mu)^{k}-\\mu_{k}])>0,}\\end{array}$ $k=3$ , 4."
  },
  {
    "qid": "statistic-posneg-1207-8-0",
    "gold_answer": "TRUE. The paper demonstrates that RF-based tests exhibit increasing power with higher dimensions, while MMD-based tests fail to detect the marginal difference due to their reliance on joint structure rather than marginal changes. The RF tests' ability to adapt to the problem structure allows them to identify the variance difference in the middle component, even as the number of samples per blob decreases.",
    "question": "Is it true that the Random Forest-based tests (hypoRF, CPT-RF) outperform MMD and other tests in detecting the marginal difference between P and Q, particularly as the dimensionality (d) increases?",
    "question_context": "Random Forest Performance in Detecting Marginal Distribution Differences\n\nThe paper discusses the performance of Random Forest (RF) tests in detecting differences between two multivariate distributions, P and Q, which are defined as products of univariate mixtures. The distributions are constructed such that P_X and P_Y are mixtures of three normal distributions with different variances. The key difference between P and Q lies in the variance of the middle component (ν_{2,Y} = 2 vs. ν_{2,X} = 1). The paper evaluates the power of various tests, including RF-based tests (hypoRF, CPT-RF), Maximum Mean Discrepancy (MMD), and others, in detecting this subtle difference across increasing dimensions (d). The results show that RF-based tests outperform MMD and other methods, especially as the dimensionality increases."
  },
  {
    "qid": "statistic-posneg-1450-0-3",
    "gold_answer": "TRUE. The paper's Remark 1 clarifies that for $d=1$, the set $C_{I}$ is simply the set of convex functions on $I$. This is because in one dimension, the condition that $\\partial_{\\ell}G$ is coordinatewise monotone is automatically satisfied for convex functions, as the derivative of a convex function is nondecreasing.",
    "question": "For $d=1$, does the set $C_{I}$ of $d$-convex functions reduce to the set of all convex functions on $I$?",
    "question_context": "Multivariate Isotonic Regression and Geometric Characterization\n\nThe paper discusses the extension of monotonicity and linearity to multivariate cases in isotonic regression. It defines a function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ as monotone (or linear) if for every coordinate $i\\in\\{1,2,\\ldots,d\\}$ and every choice of $x_{1},\\ldots x_{d}\\in\\mathbb{R}$, the function $y\\mapsto f(x_{1},\\dots,x_{i-1},y,x_{i+1},\\dots,x_{d})$ is monotone (or linear). The isotonic regression estimator (IRE) for $f$ is obtained by minimizing the squared error loss over a constrained set, ensuring coordinatewise monotonicity. The paper also introduces a geometric characterization of the IRE using a generalized notion of the greatest convex minorant (GCM) for $\\mathbb{R}^{d}$, termed $d$-GCM, and defines the left-slope $\\partial_{\\ell}G$ of a function $G$ on $\\mathbb{R}^{d}$."
  },
  {
    "qid": "statistic-posneg-1407-0-1",
    "gold_answer": "FALSE. The paper explicitly mentions that the variance κ can be calculated directly from the multinomial distribution of x_ir under the null hypothesis (no linkage), but it also provides a more rapid method using the equation analogous to (24), which accounts for parental mating types. The paper clearly states that the pairs are treated as independent units only when the parental mating type is known, and this assumption does not hold when the mating type is unknown.",
    "question": "Does the paper incorrectly state that the variance κ of the efficient score λ can be calculated directly from the multinomial distribution of x_ir without considering parental mating types?",
    "question_context": "Linkage Detection in Human Genetics: Sib-Pair Analysis and Efficient Scores\n\nThe paper discusses the detection of linkage in human genetics through sib-pair analysis, focusing on the efficient scores derived for different parental mating types. The probability of offspring genotypes is modeled based on whether parents are in coupling or repulsion, with recombination fractions accounted for. The paper presents detailed formulas for calculating probabilities, efficient scores, and variances under different genetic scenarios."
  },
  {
    "qid": "statistic-posneg-1816-4-1",
    "gold_answer": "TRUE. Explanation: The formula for $D_{\\theta}^{2}m(\\theta;x)$ includes the term $-D_{\\theta\\psi}^{2}\\nu\\left(D_{\\psi\\psi}^{2}\\nu\\right)^{-1}D_{\\theta\\psi}^{2}\\nu^{T}$, which accounts for the implicit dependence of $\\hat{\\psi}(\\theta;x)$ on $\\theta$ through the optimization condition. This term arises from applying the chain rule to the first derivative $D_{\\theta}m(\\theta;x)$, which itself depends on $\\hat{\\psi}(\\theta;x)$.",
    "question": "Does the formula for $D_{\\theta}^{2}m(\\theta;x)$ correctly account for the dependence of $\\hat{\\psi}(\\theta;x)$ on $\\theta$ when computing the second derivative?",
    "question_context": "Sandwich Covariance Estimation in Variational Inference\n\nWe now discuss computation of consistent covariance estimators. Recall that in practice, $m(\\theta;X)$ is often not available in closed form. Fortunately, the derivatives of $m(\\theta;X)$ can be expressed in terms of the derivatives of $\\nu(\\theta,\\psi;X)$ , which are always available, because $\\nu(\\theta,\\psi;X)$ is a result of the model and variational family used. Thus, using the chain rule, the asymptotic variance can be estimated whether or not $m(\\theta;X)$ is available explicitly. We denote by $D_{\\theta}\\nu$ and $D_{\\psi}\\nu$ the first partial derivatives of $\\nu_{:}$ and $D_{\\theta\\theta}^{2}\\nu,D_{\\theta\\psi}^{2}$ , and $D_{\\psi\\psi}^{2}\\nu$ the second derivatives of $\\nu$ . <PARAGRAPH_SEPARATOR> Concerning $D_{\\theta}m(\\theta;x)$ , which appears in Equation (4), since $\\hat{\\psi}(\\boldsymbol{\\theta};\\boldsymbol{x})$ maximizes $\\nu$ for fixed $\\theta,x,D_{\\theta}m(\\theta;x)=D_{\\theta}$ $\\nu(\\theta,\\psi;x)|_{\\psi=\\hat{\\psi}(\\theta;x)}$ For $D_{\\theta}^{2}m(\\theta;x)$ in Equation (3), <PARAGRAPH_SEPARATOR> $$ D_{\\theta}^{2}m(\\theta;x)=\\left[D_{\\theta\\theta}^{2}\\nu-D_{\\theta\\psi}^{2}\\nu\\left(D_{\\psi\\psi}^{2}\\nu\\right)^{-1}D_{\\theta\\psi}^{2}\\nu^{T}\\right]_{\\psi=\\hat{\\psi}(\\theta;x)}, $$ <PARAGRAPH_SEPARATOR> as we show in the supplementary materials, where we abbreviate $\\nu(\\theta,\\psi;X)$ as $\\nu$ for presentation. Replacing the appropriate derivatives in the definition of $V(\\theta)$ with the above expressions and the population expectations with empirical ones gives a way to calculate the asymptotic covariance only knowing $\\nu(\\theta,\\psi;X)$ and its derivatives (which can be calculated numerically), as opposed to $m(\\theta;x)$ , the computation of which involves optimization. <PARAGRAPH_SEPARATOR> We now have $\\hat{A}_{n}(\\hat{\\theta}_{n})^{-1}\\hat{B}_{n}(\\hat{\\theta}_{n})\\hat{A}_{n}(\\hat{\\theta}_{n})^{-1}\\overset{\\mathrm{~P~}}{\\longrightarrow}V(\\bar{\\theta})$ where <PARAGRAPH_SEPARATOR> $$ {\\hat{A}}_{n}(\\theta)={\\frac{1}{n}}\\sum_{i=1}^{n}\\left[D_{\\theta\\theta}^{2}\\nu-D_{\\theta\\psi}^{2}\\nu\\left(D_{\\psi\\psi}^{2}\\nu\\right)^{-1}D_{\\theta\\psi}^{2}\\nu^{T}\\right]_{\\psi={\\hat{\\psi}}_{i},x=X_{i}}, $$ <PARAGRAPH_SEPARATOR> $$ \\hat{B}_{n}(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}\\left[\\left(D_{\\theta}\\nu\\right)\\left(D_{\\theta}\\nu\\right)^{T}\\right]_{\\psi=\\hat{\\psi}_{i},x=X_{i}}. $$ <PARAGRAPH_SEPARATOR> Equations (5) and (6) provide a formula for constructing an asymptotic covariance matrix for the variational estimator $\\hat{\\theta}_{n}$ . This covariance can be used to construct asymptotically calibrated Wald intervals, regions, and hypothesis tests about $\\theta_{0}$ if $\\bar{\\theta}=\\theta_{0}$ . Furthermore, the sandwich covariance is model-robust in the sense that it is valid even if $P_{0}\\notin\\mathcal{P}$ . <PARAGRAPH_SEPARATOR> For an MLE under correct model specification, $A(\\theta)=B(\\theta)$ and the asymptotic covariance reduces to $A(\\theta)^{-1}$ , the inverse Fisher information matrix. In this case the sandwich covariance is only needed for model-robust uncertainty estimation. However, when $m$ is not proportional to the log-likelihood, as is often true with variational inference, $A$ and $B$ are not necessarily equal even under correct model specification. Therefore, the sandwich covariance is necessary even if $P_{0}\\in\\mathcal{P}$ ."
  },
  {
    "qid": "statistic-posneg-196-1-0",
    "gold_answer": "FALSE. Explanation: The functions $g(n) = \\max(0.5, 0.9 \\frac{1}{1+0.025n})$ and $g_{2}(n) = \\max(0.5, 0.9 \\frac{1}{1+0.02n})$ initially decrease as $n$ increases, but they are bounded below by 0.5. Therefore, once the term $0.9 \\frac{1}{1+0.025n}$ or $0.9 \\frac{1}{1+0.02n}$ falls below 0.5, the functions remain constant at 0.5 regardless of further increases in $n$. This design ensures that the thresholds do not become too stringent as the trial progresses.",
    "question": "Is it true that the adaptive threshold functions $g(n)$ and $g_{2}(n)$ decrease as the number of enrolled patients $n$ increases, based on the given formulas?",
    "question_context": "Adaptive Dose-Finding Design with Toxicity and Efficacy Constraints\n\n(a) $P(p_{\\mathrm{T1}}>T\\mathrm{T1}+\\epsilon_{1})<g(n)$ , (b) $P(p_{\\mathrm{T}2}>\\tau_{\\mathrm{T}2}+\\epsilon_{2})<g(n_{2})^{\\mathbf{I}_{n_{2}>1}}$ and (c) $P(p_{\\mathrm{E}}<\\tau_{\\mathrm{E}}-\\epsilon_{\\mathrm{E}})<g_{2}(n)^{\\mathbf{l}_{n>11}},$ <PARAGRAPH_SEPARATOR> where $\\epsilon_{1},\\epsilon_{2}$ and $\\epsilon_{\\mathrm{E}}$ are specified constants as discussed below. <PARAGRAPH_SEPARATOR> Finally, the highest efficient dose under toxicity constraints is selected. $\\mathbf{1}_{(...)}$ refers to the indicator function, which assumes a value of 1 when the condition in the subscript holds and 0 otherwise. In this way T2 and efficacy constraints influence the dose escalation only when available. Adaptive choices of the thresholds, $g(n)$ and $g_{2}(n)$ , are proposed depending on the number of patients who were already enrolled in the trial for which we have data: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l c r}{{g(n)=\\operatorname*{max}\\biggl(0.5,0.9\\frac{1}{1+0.025n}\\biggr),}}\\ {{g_{2}(n)=\\operatorname*{max}\\biggl(0.5,0.9\\frac{1}{1+0.02n}\\biggr).}}\\end{array} $$ <PARAGRAPH_SEPARATOR> The errors $\\epsilon_{\\mathrm{E}},\\epsilon_{1}$ and $\\epsilon_{2}$ were set equal to 0:02, based on a sensitivity analysis, and in LEVNEONAT clinical trial $\\tau_{\\mathrm{T1}}=\\tau_{\\mathrm{T2}}=0.1$ and $\\tau_{\\mathrm{E}}=0.6$ . In the case of no eligible dose, because the minimum effective dose is a dose that is higher than the maximum tolerated dose, the trial is stopped. Furthermore, the trial is stopped if $P(p_{\\mathrm{T1}}>\\tau_{\\mathrm{T1}}|d_{1})>0.9$ , $P(p_{\\mathrm{T}2}>\\tau_{\\mathrm{T}2}|d_{1})>0.9$ or $P(p_{\\mathrm{E}}<\\tau_{\\mathrm{E}}|d_{K})>0.9$ , i.e. if the first dose is too toxic or the last dose is not sufficiently efficient, similarly to what is proposed in Thall and Cook (2004). The no-skipping rule is applied, i.e. a dose level can be assigned only if at least one patient is allocated to all lower doses. <PARAGRAPH_SEPARATOR> At the end of the trial, the minimum effective dose is computed as <PARAGRAPH_SEPARATOR> $$ d_{\\mathrm{e,min}}=\\arg\\operatorname*{min}_{d\\in D}|\\hat{p}_{\\mathrm{E}}(d)-\\tau_{\\mathrm{e}}| $$ <PARAGRAPH_SEPARATOR> and the maximum tolerated dose as <PARAGRAPH_SEPARATOR> $$ d_{\\mathrm{t,max}}=\\operatorname*{min}(\\arg\\operatorname*{min}_{d\\in D}|\\hat{p}_{\\mathrm{T1}}(d)-\\tau_{p1}|,\\arg\\operatorname*{min}_{d\\in D}|\\hat{p}_{\\mathrm{T1}}(d)-\\tau_{p2}|). $$ <PARAGRAPH_SEPARATOR> The dose that is recommended at the end is equal to $d_{\\mathrm{t,max}}$ if $d_{\\mathrm{{e,min}}}\\leqslant d_{\\mathrm{{t}}}$ , max and none otherwise. The $\\hat{p}_{\\mathrm{E}}(d),\\hat{p}_{\\mathrm{T1}}$ and $\\hat{p}_{\\mathrm{TI}}(d)$ are defined as the posterior median values of those probabilities given the dose $d$ . <PARAGRAPH_SEPARATOR> # 4. Evaluation of the method proposed <PARAGRAPH_SEPARATOR> # 4.1. Simulation setting"
  },
  {
    "qid": "statistic-posneg-822-0-0",
    "gold_answer": "FALSE. The conditional expectation $g_f$ is not necessarily unique based solely on the condition $\\int_{A}g_{f}d\\mu=\\int_{A}f d\\mu$ for all $A\\in\\varLambda$. However, the paper states that $g_f$ is essentially unique when it exists, meaning it is unique up to sets of measure zero. The uniqueness is guaranteed under additional conditions, such as when $f\\in L^{p}(\\Sigma)$ for $1\\leqslant p<\\infty$ and the measure space $(\\Omega,\\Sigma,\\mu)$ is localizable. Without these conditions, the uniqueness may not hold.",
    "question": "Is the conditional expectation $g_f$ relative to $\\varLambda$ uniquely determined by the condition $\\int_{A}g_{f}d\\mu=\\int_{A}f d\\mu$ for all $A\\in\\varLambda$?",
    "question_context": "Conditional Expectation and Contractive Projections in Measure Spaces\n\nThe concepts and notations to be used here generally follow [3]. Thus a measure space $(\\Omega,\\Sigma,\\mu)$ consists of a set $\\pmb{\\mathcal{Q}}_{i}$ ,a 8-ring $\\boldsymbol{\\Sigma}$ (i.e., closed under finite unions, differences and countable intersections), and a measure $\\mu:{\\mathcal{Z}}\\mapsto{R}^{+}$ which is complete (i.e., $N\\in{\\mathcal{Z}},$ $\\pmb{\\mu}(N)=0$ implies, for every $A\\subset N,A\\in\\Sigma).$ Let $\\boldsymbol{\\Sigma_{10\\mathrm{{c}}}}=\\{\\boldsymbol{A}\\subset\\mathcal{Q}:\\boldsymbol{A}\\cap\\boldsymbol{E}\\in\\boldsymbol{\\Sigma}$ for all $\\pmb{\\mathcal{E}}\\in\\pmb{\\mathcal{Z}}\\}$ , so that it is a $\\pmb{\\sigma}$ algebra. Define $\\mu^{*}:\\mathcal{Z}_{\\mathrm{loc}}\\mapsto\\widetilde{R}^{+},$ as $\\mu^{*}(A)=\\operatorname*{sup}\\{\\mu(B):B\\subset A,B\\in\\Sigma\\},$ $\\scriptstyle A\\in\\Sigma_{\\log}$ . Then $\\mu^{*}$ is a measure [3]. Let ${\\mathcal{N}}({\\mathcal{L}})={\\mathcal{N}}=\\{N\\in{\\varSigma}_{\\mathrm{loc}}:\\mu^{*}(N)=0\\}$ . Hence $\\mathcal{N}$ is the $\\pmb{\\sigma}^{.}$ -ideal of $\\pmb{\\mu}$ -negligible sets, so that $\\mu(N\\cap A)=0$ for all $A\\in\\Sigma$ We call $\\mathcal{N}$ the class of $\\pmb{\\Sigma}.$ -negligible sets, though a more appropriate term would be the locally $\\pmb{\\mu}$ (or $\\pmb{\\Sigma}$ )-negligible sets, to conform with other [17] terminology. Set $\\tilde{\\varSigma}_{\\mathrm{loc}}=\\ensuremath{\\varSigma_{\\mathrm{loc}}}/\\mathcal{N}_{:}$ i.e,the faetor $\\pmb{\\sigma}$ -algebra which is the set of equivalence casses $\\pmb{\\tilde{A}}$ of sets $A\\in\\varSigma_{\\mathrm{loc}}\\mathrm{mod}(\\mathcal{N})$ . The sets of $\\pmb{\\mathcal{\\Sigma}}_{\\mathbf{loc}}$ will be called $\\pmb{\\Sigma}.$ measurable, and thus all concepts of $\\boldsymbol{\\Sigma}.$ measurability are understood in terms of $\\pmb{\\mathcal{\\Sigma}}_{\\mathbf{loc}}$ ， as in [3]. (\"Supp\" stands for “support\" in what follows). <PARAGRAPH_SEPARATOR> For $1\\leqslant p<\\infty$ ,let $L^{p}({\\boldsymbol{\\Sigma}})$ be the space of real $\\Sigma\\cdot$ measurable functions vanishing outside a sequence of sets of $\\pmb{\\Sigma},$ . and which are limits a.e. $(\\Sigma),$ and also of $\\pmb{\\phi}$ -mean, of sequences of $\\Sigma\\cdot$ step functions. If $A\\subset E$ is a 8-ring containing all the $\\pmb{\\Sigma}.$ -null sets of $\\pmb{\\Sigma}$ (i.e., $\\mathcal{N}$ is contained in the $\\boldsymbol{\\mathfrak{A}}$ -locally null class), then $L^{\\boldsymbol{p}}(\\boldsymbol{A})\\subset D^{\\boldsymbol{p}}(\\boldsymbol{\\it{\\sum}})$ for $1\\leqslant p<\\infty$ , and the inclusion is an isometric embedding, where the usual metric given by the seminorm in $L^{{\\mathfrak{p}}}({\\boldsymbol{\\Sigma}})$ is used here. In general, $\\scriptstyle{A_{\\mathrm{loc}}}$ and $\\Sigma_{\\mathrm{loc}}$ are not comparable (even though each $\\pmb{\\Sigma}.$ -negligible set is $\\boldsymbol{\\mathfrak{A}}$ -negligible), so that $L^{\\infty}(\\varLambda)$ and $L^{\\infty}(\\Sigma)$ are also not comparable. However, if $\\operatorname*{sup}\\{\\tilde{A}:A\\in\\mathcal{A}\\}=\\tilde{\\Omega}$ then $L^{\\infty}({\\cal{A}})\\subset L^{\\infty}({\\cal{Z}})$ $(L^{\\infty}({\\mathcal{Z}})=\\{f:f\\varphi_{A}$ is in ordinary $L^{\\infty}(\\sigma({\\mathcal{Z}}))$ ,all $A\\in\\Sigma\\}.$ $\\scriptstyle(\\varphi_{A}$ being the indicator function of $\\pmb{A}$ <PARAGRAPH_SEPARATOR> If $f:\\varOmega\\mapsto R$ is $\\pmb{\\Sigma}.$ -measurable such that $f_{\\varphi_{A}}\\in L^{1}(\\mathcal{Z})$ for each $\\pmb{A}\\in\\pmb{A}$ (thus $f$ is A-locally integrable) then any $\\varLambda$ measurable $g_{f}:\\varOmega\\mapsto R$ satisfying <PARAGRAPH_SEPARATOR> $$ \\int_{A}g_{f}d\\mu=\\int_{A}f d\\mu,A\\in\\varLambda, $$ <PARAGRAPH_SEPARATOR> such that $\\operatorname*{sup}\\{\\tilde{A}:A\\in\\varLambda,A\\subset G\\}=\\tilde{G}$ where $G=\\operatorname{supp}(g_{f})_{!}$ . is called a conditional expectation of $f$ relative to $\\varLambda.$ It was shown in [4] that $g_{f}$ is essentially unique when it exists, and that it exists if $f\\in L^{p}(\\Sigma)$ $1\\leqslant p<\\infty$ ,and the same is true if $\\phi=\\infty$ also, provided $(\\Omega,\\Sigma,\\mu)$ is localizable in addition. [Recall that $(\\mathcal{Q},\\mathcal{Z},\\mu)$ is loalizable fevyry subfamiy of $\\widetilde{\\Sigma}_{\\mathrm{1oc}}$ has a supremum in $\\tilde{\\Sigma}_{\\mathrm{loc}}$ See [17] for a discussion of this concept.] Thus the mapping $E^{\\lambda}:f\\mapsto g_{f}$ is well defined, and is a linear contractive positive projection (also denoted $E(\\cdot\\mid A)$ and $E_{A}(\\cdot).$ ${\\cal E}^{\\varLambda}$ has many of the properties of the corresponding operator when $A,D$ are ${\\pmb{\\sigma}}^{*}$ -algebras, but sometimes additional hypotheses are needed to compensate for the $\\pmb{\\delta}.$ -ring case. (For proofs of these and other properties of $E^{\\varLambda}$ see [4].) <PARAGRAPH_SEPARATOR> With this background information, we shall proceed to find conditions for some characterizations of subclasses of contractive projections on $L^{p}({\\mathcal{Z}}),{\\mathcal{p}}\\geqslant1;$ to coincide with conditional expectations. <PARAGRAPH_SEPARATOR> # 2. MEASURABLE SUBSPACES OF $L^{p}(\\varSigma)$"
  },
  {
    "qid": "statistic-posneg-1524-2-2",
    "gold_answer": "TRUE. The parameter $\\gamma$ scales the distance $d_{i j}$ in the exponential function, thus controlling how quickly the weights decay as the distance between sites increases. A larger $\\gamma$ would result in slower decay, meaning weights remain significant over larger distances.",
    "question": "The parameter $\\gamma$ in the weight function $b_{i j}$ is a scale parameter that determines the rate of decay of weights with distance. Is this statement true?",
    "question_context": "Comparison of Gaussian Geostatistical Models and Gaussian Markov Random Fields\n\nTable 14 AMSPE in approximations of GGMs with $\\phi=3$ , $\\sigma^{2}=1$ and various values of $v$ by GMRFs with parametrized weights and neighborhood II <PARAGRAPH_SEPARATOR> <html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">AMSPE</td></tr><tr><td>v = 0.1</td><td>v =0.5</td><td>U=1</td></tr><tr><td>CSDS</td><td>0.79</td><td>0.38</td><td>0.19</td></tr><tr><td>KLS</td><td>0.77</td><td>0.37</td><td>0.19</td></tr><tr><td>SRDC</td><td>0.81</td><td>0.37</td><td>0.21</td></tr><tr><td>AOM</td><td>0.77</td><td>0.36</td><td>0.16</td></tr></table></body></html> <PARAGRAPH_SEPARATOR> To study the potential impact of the weight function of GMRFs on the approximations of GGMs by GMRFs, we use a distance-based weight parametrized by $\\gamma$ , and estimate $\\gamma$ in addition to the $\\alpha$ and $\\tau$ , <PARAGRAPH_SEPARATOR> $$ b_{i j}=\\frac{\\exp{\\left(-\\frac{d_{i j}}{\\gamma}\\right)}}{\\displaystyle\\sum_{j=1}^{N}\\exp{\\left(-\\frac{d_{i j}}{\\gamma}\\right)}}, $$ <PARAGRAPH_SEPARATOR> where $b_{i j}$ is the weight for sites $i$ and $j,\\gamma$ is a scale parameter, and $d_{i j}$ is the distance between sites $i$ and $j$ . We performed this study on 16 by 16 lattices. The estimated parameters are slightly different from those based on the scaled binary weights in GMRFs (Table 13). However, we still obtain smaller AMSPEs using the CSDS and the KLS (Table 14). <PARAGRAPH_SEPARATOR> We also compare the suggested metrics taking into account the computational time. Tables 15 and 16 display the computational time in approximations of GGMs by GMRFs, and approximations of GMRFs by GGMs. The unit of time is seconds, and these results are obtained using a Pentium PC with a $3.20\\mathrm{GHz}$ CPU, showing that the approximation methods based on the spectral density are faster than those based on covariance functions. <PARAGRAPH_SEPARATOR> Table 15 Computation times in approximations of GGMs with $\\sigma^{2}=1$ , $\\phi=3$ and $v=0.5$ by GMRFs with neighborhood II"
  },
  {
    "qid": "statistic-posneg-851-0-1",
    "gold_answer": "FALSE. Unlike bootstrap methods, the NMCT procedure does not require plug-in estimation of $\\varSigma$. Instead, it generates conditional counterparts of the test statistic by simulating random variables, bypassing the need for covariance matrix estimation.",
    "question": "Does the NMCT procedure require the estimation of the covariance matrix $\\varSigma$ for approximating the null distribution of the test statistic?",
    "question_context": "Nonparametric Monte Carlo Test (NMCT) for Multivariate Regression Diagnostics\n\nFrom Theorem 2.1 we can easily determine $p$ -values through chi-square distributions. However, for $\textstyle{\\mathcal{T}}_{n}$ a deterioration for the power comes from a plug-in estimation for the covariance matrix $\\varSigma=\\operatorname{Cov}(V\bullet\\varepsilon)$ . This is because under the alternative $\\pmb\\varepsilon$ is no longer centered and this covariance matrix will be larger than that under $H_{0}$ . In this section, we propose a nonparametric Monte Carlo test (NMCT) procedure to approximate the null distribution of a test. An interesting point of this approximation is that although the test itself we will use is not scale-invariant because we do not use a plug-in estimator of the covariance matrix, NMCT can be scale-invariant. In comparison with the existing proposals for approximating the null distributions of the tests, for instance, classical bootstrap Efron [5], and their variants such as the wild bootstrap [11,19], we know that a bootstrap procedure also requests the estimation for $\\varSigma$ . More importantly, NMCT has a special usefulness in multivariate response cases because if the distribution of the error is of semiparametric structure, we can benefit from it to better mimic the null distribution, and if the error is fully nonparametrically distributed, NMCT is also consistent. To this end, we first give a definition about the independent decomposition for a random variable."
  },
  {
    "qid": "statistic-posneg-2084-1-1",
    "gold_answer": "TRUE. According to Theorem 12 in the context, $\\mathrm{Cov}(A_{kl}, A_{rs}) = 0$ when $k\\neq r$ and $l\\neq s$. This is due to the independence of the bilinear forms under these conditions, as they involve distinct pairs of random vectors.",
    "question": "Does the covariance between two bilinear forms $A_{kl}$ and $A_{rs}$ equal zero when $k\\neq r$ and $l\\neq s$?",
    "question_context": "Cumulants and Covariance Structures in Quadratic and Bilinear Forms\n\nThe paper discusses the properties of quadratic and bilinear forms under normal distribution assumptions, providing detailed derivations for their cumulants and covariance structures. Key formulas include the rth cumulant of quadratic forms, the representation of bilinear forms as quadratic forms, and various covariance expressions between different forms."
  },
  {
    "qid": "statistic-posneg-591-0-2",
    "gold_answer": "FALSE. The model $\\left\\{X_{t}\\right\\}$ can be strictly stationary and ergodic under certain conditions even though $\\left\\{\\theta_{t}\\right\\}$ is a Markov chain, as mentioned in the context. This is an advantage over deterministic time changes, which would produce a non-stationary process.",
    "question": "The Markov-switching autoregressive model $\\left\\{X_{t}\\right\\}$ cannot be strictly stationary and ergodic under any conditions when $\\left\\{\\theta_{t}\\right\\}$ is a Markov chain. True or False?",
    "question_context": "Markov-Switching Autoregressive Model with State Transitions\n\nWe shall limit ourselves to the AR(1) case, but our definitions and theory can be extended to the $\\mathbf{AR}(p)$ case (see Tyssedal and Tjostheim (1985)). We assume that the observations are generated by a stochastic process $\\left\\{X_{t}\\right\\}$ given by $X_{t}=\\theta_{t}X_{t-1}+e_{t},~t=0,\\pm1,\\pm2,...$ where $\\left\\{\\theta_{t}\\right\\}$ is a Markov chain which is irreducible and aperiodic and has a finite state space consisting of $k$ states $s_{1},\\ldots,s_{k}$. Furthermore, $\\left\\{\\boldsymbol{e}_{t}\\right\\}$ is a sequence of independent identically distributed random variables with zero mean and variance $\\sigma^{2}$. We assume that the processes $\\left\\{\\theta_{t}\\right\\}$ and $\\left\\{\\boldsymbol{e}_{t}\\right\\}$ are independent. We denote by S the $k$ dimensional vector defined by $\\mathbf{S}=[s_{1},\\ldots,s_{k}]$. The transition probability matrix is given by $Q=(q_{i j})$ where $q_{i j}=P(\\theta_{t}=s_{j}|\\theta_{t-1}=s_{i}),\\quad i,j=1,\\ldots,k$. For an irreducible and aperiodic Markov chain with a finite state space there is a unique vector of stationary probabilities which will be denoted by ${\\pmb{\\pi}}=[{\\pmb{\\pi}}_{1},\\ldots,{\\pmb{\\pi}}_{k}]$. We have found it advantageous to introduce an indicator process $\\left\\{\\Delta_{t}\\right\\}$ indicating at each time point which state the process $\\left\\{\\theta_{t}\\right\\}$ is in. It is defined by $\\Delta_{t}=[\\delta_{1t},\\delta_{2t},...,\\delta_{k t}]$ where $\\delta_{j t}=\\left\\{{1\\atop{0}}\\quad{\\mathrm{if}}\\theta_{t}=s_{j}\\right.$. In the case $k=1$ there is only one state and $\\left\\{X_{t}\\right\\}$ degenerates to an ordinary AR(1) process. For several states, each $s_{i},i=1,...,k,$ corresponds to the underlying mechanism being in a certain state. One advantage obtained by modelling the changes in terms of a strictly stationary Markov chain as opposed to using a deterministic function is that $\\left\\{{{X}_{t}}\\right\\}$ may still under certain conditions be a strictly stationary and ergodic process, even though the separate realizations of $\\left\\{X_{t}\\right\\}$ may have a very non-stationary outlook. The stationarity problem is discussed in Tjostheim (1986a). The properties of stationarity and ergodicity mean that the strong law of large numbers and central limit theorems are available in an asymptotic analysis, whereas working with deterministic time changes will produce a non-stationary process $\\left\\{X_{t}\\right\\}$. The Markov property is also essential for forecasting, which will be the subject of a forthcoming publication. The difficulty of the estimation problem increases by an order of magnitude as the number of states increases from one to two or more. Unknown parameters are $k_{:}$ S, $Q,\\pi,\\sigma^{2}$ and the process $\\left\\{\\Delta_{t}\\right\\}$. The maximum number of states that we shall look at is $k=3$. For time series consisting of 100 samples or fewer this seems to be close to what is possible in practice. Once $k$ is known, $\\left\\{\\Delta_{t}\\right\\}$ is estimated using a separation method. On the basis of estimates of $k$ and $\\left\\{\\Delta_{t}\\right\\}$ it is simple to estimate $Q$ and $\\pmb{\\pi}$ by counting the number of transitions and the number of periods that the process stays in each state. The properties of such estimates are well known when $\\left\\{\\Bar{\\Delta}_{t}\\right\\}$ is known (Billingsley, 1961). We do not know the asymptotic properties of such estimates when they are based on an estimate of $\\left\\{\\Delta_{t}\\right\\}$. Two methods will be used for estimating S (and $\\sigma^{2}$), namely the method of moments and a least squares procedure. Sclove (1983) presents a maximum likelihood method for estimating transition points and states."
  },
  {
    "qid": "statistic-posneg-1587-0-1",
    "gold_answer": "FALSE. While the equation shows that $k_{t}$ depends on past wages $w_{i}$, it also includes the term $s^{\\prime}$, which is a composite term involving $s$, $a_{k}$, $n$, and $\\log(1-\\alpha)$. Therefore, $k_{t}$ is not solely determined by past wages but also by these other factors.",
    "question": "Does the equation $k_{t}=s^{\\prime}+(1-\\delta)\\displaystyle\\sum_{i=-\\infty}^{t}\\delta^{t-i}w_{i}$ imply that capital stock $k_{t}$ is solely determined by past wages $w_{i}$?",
    "question_context": "Stability and Dynamics in Aggregate Economic Processes\n\nSecond, the estimations reported in Table 2 point to an AR(2) specification for the process of TFP shocks. In addition, this representation is likely to be stationary with complex roots and with a dominant frequency around 0.15, which is consistent with (exogenous) cycles lasting 6–7 years. From now on, we thus assume that $a_{t}$ admits a strong, stationary AR(2) representation. <PARAGRAPH_SEPARATOR> The investigation above provides useful information that can be used to perform a more structural analysis. It also serves as an indirect indication that a model with a constant externality would probably be rejected by the data. However, it cannot be considered as a formal test of (in)stability. We now propose a more direct way to assess the interest of our model. To this end, we start from Eqs. (6)–(7) to get <PARAGRAPH_SEPARATOR> $$\\begin{array}{r l}&{k_{t}=a_{k}+(1-\\delta)s+\\delta k_{t-1}+(1-\\delta)(w_{t}-\\log(1-\\alpha)+n),}\\\\ &{w_{t}-\\log(1-\\alpha)+n=\\gamma_{t}^{\\prime}k_{t-1}+a_{t}+(1-\\alpha)n,}\\\\ &{a_{t}=w_{t}+\\alpha n-\\log(1-\\alpha)-\\gamma_{t}^{\\prime}k_{t-1},}\\end{array}$$ <PARAGRAPH_SEPARATOR> where $\\gamma_{t}^{\\prime}=\\alpha+\\gamma_{t}$ . As $0<\\delta<1$ we may write <PARAGRAPH_SEPARATOR> $$\\begin{array}{l}{{k_{t}=s^{\\prime}+(1-\\delta)\\displaystyle\\sum_{i=-\\infty}^{t}\\delta^{t-i}w_{i},}}\\\\ {{\\quad a_{t+1}-\\alpha n+\\log(1-\\alpha)=w_{t+1}-\\gamma_{t+1}^{\\prime}k_{t},}}\\end{array}$$ <PARAGRAPH_SEPARATOR> where $\\begin{array}{r}{s^{\\prime}=s+\\frac{a_{k}}{1-\\delta}+n-\\log(1-\\alpha)}\\end{array}$ . Finally, we derive the dynamics of wages <PARAGRAPH_SEPARATOR> $$w_{t+1}-\\gamma_{t+1}^{\\prime}(1-\\delta)\\sum_{i=-\\infty}^{t}\\delta^{t-i}w_{i}=a_{t+1}-\\alpha n+\\log(1-\\alpha)+\\gamma_{t+1}^{\\prime}s^{\\prime}.$$ <PARAGRAPH_SEPARATOR> Table 3 P-values of ADF tests. <PARAGRAPH_SEPARATOR> <html><body><table><tr><td>Series</td><td>P-values</td></tr><tr><td>wt</td><td>0.9135</td></tr><tr><td>xt</td><td>0.9994</td></tr><tr><td>Residuals</td><td>6.38E-08</td></tr></table></body></html> <PARAGRAPH_SEPARATOR> Table 4 ARIMA(1, 1, 1) estimates for log of squared residuals."
  },
  {
    "qid": "statistic-posneg-816-0-0",
    "gold_answer": "FALSE. The formula $v=\\sqrt{(l^{8}+2u^{8}+4b^{8})}$ is not correctly derived from the given context. The correct coefficient of variation should be based on the standard deviation and mean of the volume V, which is given by $V=kLB^{2}$. The correct formula for the coefficient of variation should involve terms like $\\lambda$, $\\kappa$, and $\\beta$ as shown in the context, not the eighth powers of $l$, $u$, and $b$. The error of 4% is mentioned, but it refers to a different approximation, not this specific formula.",
    "question": "Is the coefficient of variation v correctly approximated by the formula $v=\\sqrt{(l^{8}+2u^{8}+4b^{8})}$ with an error of about 4% when $l$, $b$, and $u$ are of the order of 0.05?",
    "question_context": "Coefficient of Variation in Egg Volume Analysis\n\nIt is required to find the coefficient of variation v of the volume V, supposing that ${\\pmb V}={\\pmb k}{\\pmb L}{\\pmb B}^{2}$ where $\\pmb{k}$ is a constant. That is to say the eggs are all supposed to be of the same shape apart from changes of scale in $\\pmb{L}$ and $\\pmb{\\mathcal{B}}$ .Wealsosuppose $\\pmb{L}$ and $\\pmb{B}$ to be normally correlated with coefficient $\\pmb{\\rho}$ . Let ${\\pmb u}^{\\pmb{\\Omega}}={\\pmb{2}}\\pmb{\\rho}\\pmb{b}\\pmb{l}$ <PARAGRAPH_SEPARATOR> Then $\\bar{L}=\\bar{L}(1+l x)$ ,and $\\pmb{\\mathcal{B}}=\\pmb{\\mathcal{B}}(\\mathbf{1}+\\pmb{\\mathit{b y}})$ ,where $\\pmb{\\mathscr{x}}$ and $\\pmb{y}$ a&re correlated reduced normal variates (with zero mean and unit variance). The means of odd powers and products of $\\pmb{z}$ and $\\pmb{y}$ vanish, and <PARAGRAPH_SEPARATOR> Hence <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{=\\rho,\\quad\\overrightarrow{x^{4}}=\\overrightarrow{y^{4}}=3,\\quad\\overrightarrow{x^{3}y}=\\overrightarrow{x y^{3}}=3\\rho,\\quad\\overrightarrow{x^{2}y^{2}}=1+2\\rho^{3},\\quad\\overrightarrow{x^{4}y^{2}}=3(1+4\\rho^{4}).}\\ &{\\qquad\\quad V=k\\overrightarrow{L}(\\overrightarrow{B})^{?}(1+l x)(1+b y)^{2}.}\\ &{\\qquad\\quad V=k\\overrightarrow{L}(\\overrightarrow{B})^{2}(1+2l b\\overrightarrow{x y}+b^{3}\\overrightarrow{y^{2}})=k\\overrightarrow{L}(\\overrightarrow{B})^{3}(1+b^{2}+u^{3}),}\\ &{=k^{3}(\\overrightarrow{L})^{2}(\\overrightarrow{B})^{*}[1+l^{2}+4u^{2}+6b^{2}+3(2l^{3}b^{2}+u^{4}+4b^{2}u^{3}+b^{4})+3b^{2}(l^{3}b^{2}+u^{4})].}\\ &{\\qquad\\quad\\mathbf{3}=l^{3}+2u^{3}+4b^{2}+2(2l^{3}b^{2}-l^{2}u^{2}-u^{4}-5u^{2}b^{2}-3b^{4})+\\ldots.}\\end{array} $$ <PARAGRAPH_SEPARATOR> So <PARAGRAPH_SEPARATOR> Since $\\pmb{l},\\pmb{b},$ and $\\pmb{\\mathscr{u}}$ are of the order of 0.05, we may take <PARAGRAPH_SEPARATOR> $$ v={\\sqrt{(l^{8}+2u^{8}+4b^{8})}}, $$ <PARAGRAPH_SEPARATOR> with an error of the order of $4\\%$ , which is generally negligible <PARAGRAPH_SEPARATOR> Since <PARAGRAPH_SEPARATOR> $$ \\lambda=(\\bar{L})^{3}l^{2},\\quad\\beta=(\\bar{B})^{3}b^{2},\\quad\\kappa=\\rho\\bar{L}\\bar{B}l b=\\pm\\bar{L}\\bar{B}u^{2},\\quad $$ <PARAGRAPH_SEPARATOR> $$ {v}=\\sqrt{\\left(\\frac{\\lambda}{(\\widetilde L)^{2}}+\\frac{4\\kappa}{\\widetilde L\\widetilde B}+\\frac{4\\beta}{(\\widetilde B)^{2}}\\right)}. $$ <PARAGRAPH_SEPARATOR> If i is given, but not $\\pmb{\\kappa}$ , we proceed as follows: <PARAGRAPH_SEPARATOR> $$ I=\\frac{\\overline{{B}}}{\\overline{{L}}}\\left({\\bf l}+b y\\right)({\\bf l}+l x)^{-1}. $$ <PARAGRAPH_SEPARATOR> This may be expanded in a series, and the moments calculated. Theoretically they are all infinite, since if $\\pmb{L}$ is normally distributed it can be zero. However, the series for the moments are asymptotic expansions, and the first few terms give good approximations: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\overline{{I}}=\\frac{\\overline{{B}}}{\\overline{{L}}}\\biggl[1+(l-\\rho\\dot{b})\\sum_{r=1}^{\\infty}\\frac{l^{2r-1}(2r!)}{2^{r}r!}\\biggr]=\\frac{\\overline{{B}}}{\\overline{{L}}}\\bigl[1+(l^{2}-\\frac{1}{2}u^{2})\\left(1+3l^{2}+15l^{3}+\\ldots\\right)\\bigr],}\\ &{\\overline{{I^{2}}}=\\biggl(\\frac{\\overline{{B}}}{\\overline{{L}}}\\right)^{2}+\\overline{{I^{2}}}\\dot{i}^{2}=\\biggl(\\frac{\\overline{{B}}}{\\overline{{L}}}\\biggr)^{2}[1+3l^{2}-2u^{2}+b^{2}+3(5l^{4}-4l^{2}u^{2}+3l^{2}b^{2}+3u^{4})+\\ldots],}\\ &{i^{2}=l^{2}+b^{2}-u^{2}+\\mathrm{etc.,}}\\ &{\\qquad\\quad\\qquad\\quad\\qquad\\quad\\qquad\\quad\\qquad\\quad\\qquad\\quad\\qquad\\quad\\qquad\\quad\\qquad\\quad\\qquad\\quad\\qquad\\quad\\qquad\\quad\\qquad\\quad\\qquad\\quad}\\end{array} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> again with an crror of thc order of about $4\\%$ <PARAGRAPH_SEPARATOR> # APPENDIX 2. INTERPRETATION OF THE GREATER VARIANCE OF THE UNHATCHED EGGS"
  },
  {
    "qid": "statistic-posneg-2090-2-1",
    "gold_answer": "FALSE. The context explicitly states that 'Linear is nonconservative for the target probability 0.9 in test case (b).' This means that the Linear interpolation method does not provide a conservative estimate in this scenario.",
    "question": "In the test case (b) with $(\\mu_{0},\\mu_{1},\\sigma_{0},\\sigma_{1},\\rho)=(0,2,4,1,0.9)$ and $u=-0.1$, is the Linear interpolation method conservative?",
    "question_context": "Interpolation Methods and Uncertainty in Contour Maps\n\nFor the Step and Log method, zero-width needle sets are avoided in the set construction by first eliminating any triangle where $F_{\\mathbf{u}}(\\mathbf{s})=0$ for any of the vertices. <PARAGRAPH_SEPARATOR> A practical approach for visualizing the interpolated function is to compute a triangle subdivision, splitting each triangle into four new ones in each step, with the chosen interpolation method used to generate the values on the new vertices. Then, linear interpolation on the subdivided triangles, can be used when plotting, as illustrated in Figure 4. <PARAGRAPH_SEPARATOR> The qualitative behavior of the interpolation methods can be assessed by considering test cases consisting of only two spatial locations, which allows exact calculation of $F_{\\mathbf{u}}(s)$ for a single level $u$ along the line segment between the two locations. We define the joint distribution for $x(0)$ and $x(1)$ to be <PARAGRAPH_SEPARATOR> $$ \\left[\\begin{array}{c c}{x(0)}\\ {x(1)}\\end{array}\\right]\\sim N\\left(\\left[\\begin{array}{c c}{\\mu_{0}}\\ {\\mu_{1}}\\end{array}\\right],\\left[\\begin{array}{c c}{\\sigma_{0}^{2}}&{\\rho\\sigma_{0}\\sigma_{1}}\\ {\\rho\\sigma_{0}\\sigma_{1}}&{\\sigma_{1}^{2}}\\end{array}\\right]\\right), $$ <PARAGRAPH_SEPARATOR> and define the linear basis representation for $s\\in[0,1]$ as $x(s)=$ $(1-s)x(0)+s x(1)$ . The true $F_{\\mathbf{u}}(s)$ function is compared with its interpolation approximations for different combinations of distribution parameters $(\\mu_{0},\\mu_{1},\\sigma_{0},\\sigma_{1},\\rho)$ and thresholds $u$ . The three test cases shown in Figure 5 are <PARAGRAPH_SEPARATOR> (a) $(\\mu_{0},\\mu_{1},\\sigma_{0},\\sigma_{1},\\rho)=(0,1,1,1,0.9)$ with $u=-0.5$ , (b) $(\\mu_{0},\\mu_{1},\\sigma_{0},\\sigma_{1},\\rho)=(0,2,4,1,0.9)$ with $u=-0.1;$ , and (c) $(\\mu_{0},\\mu_{1},\\sigma_{0},\\sigma_{1},\\rho)=(0,2,4,1,0)$ with $u=-0.1$ . <PARAGRAPH_SEPARATOR> The interpolation method Step is always conservative, Log is conservative for large target probabilities, whereas Linear is nonconservative for the target probability 0.9 in test case (b). When both Linear and Log are nonconservative, Log stays closer to the true $F_{\\mathbf{u}}(s)$ function. In this simple example, $x(0)$ and $x(1)$ could be thought of as two of the neighboring locations on a mesh in two dimensions, and no major differences arise if one were to consider 2D interpolation over a triangle in the mesh."
  },
  {
    "qid": "statistic-posneg-286-0-0",
    "gold_answer": "TRUE. The transformation is correctly applied. The original inequality can be rewritten as $(\\lambda_{1}-x)u_{1}^{2} + (\\lambda_{2}-x)u_{2}^{2} + ... + (\\lambda_{m}-x)u_{m}^{2} < c$, which is equivalent to $\\uplambda_{1}^{*}u_{1}^{2}+\\uplambda_{2}^{*}u_{2}^{2}+\\hdots+\\uplambda_{m}^{*}u_{m}^{2}<c$ where ${\\boldsymbol{\\lambda}}_{j}^{*}={\\lambda}_{j}-{\\boldsymbol{x}}$. This simplification is valid and aligns with the standard approach to evaluating such probabilities.",
    "question": "Is the transformation ${\\boldsymbol{\\lambda}}_{j}^{*}={\\lambda}_{j}-{\\boldsymbol{x}}$ correctly applied to simplify the inequality $\\lambda_{1}u_{1}^{2}+\\lambda_{2}u_{2}^{2}+...+\\lambda_{m}u_{m}^{2}<x(u_{1}^{2}+...+u_{m}^{2})+c$ into $\\uplambda_{1}^{*}u_{1}^{2}+\\uplambda_{2}^{*}u_{2}^{2}+\\hdots+\\uplambda_{m}^{*}u_{m}^{2}<c$?",
    "question_context": "Probability Evaluation of a Linear Combination of Chi-Square Variates\n\nLet $\\lambda_{1}>\\lambda_{2}>...>\\lambda_{m}$ $x$ and $c$ be fixed numbers and let $u_{1},u_{2},....u_{m}$ be independent standard normal variates, then AS 153 evaluates the probability that $$\\lambda_{1}u_{1}^{2}+\\lambda_{2}u_{2}^{2}+...+\\lambda_{m}u_{m}^{2}<x(u_{1}^{2}+...+u_{m}^{2})+c$$ $$\\uplambda_{1}^{*}u_{1}^{2}+\\uplambda_{2}^{*}u_{2}^{2}+\\hdots+\\uplambda_{m}^{*}u_{m}^{2}<c,$$ where ${\\boldsymbol{\\lambda}}_{j}^{*}={\\lambda}_{j}-{\\boldsymbol{x}}$ and $c=0$ . Grad and Solomon (1955) and Iohnson and Kotz (1970, pp. 155-156 and 174-175) have  shown that $(1^{\\prime})$ maybe evaluated  when $c$ is nonzero by inserting exp $[-c/2(y-x)]$ after the integral signs in Farebrother's (1980) equations (2), (3) and (4). Equation_ (2)  should be used  when $c$ is positive and equation (3) when $c$ is negative as exp $[-c/2(y-x)]$ will then be less than unity."
  },
  {
    "qid": "statistic-posneg-1043-0-1",
    "gold_answer": "TRUE. The variance of the regression estimator $\\bar{y}_{r}$ decreases as the correlation between $\\mathcal{X}$ and $\\pmb{\\mathcal{B}}$ increases. When the correlation is perfect (i.e., absolute value of 1), the variance of $\\bar{y}_{r}$ approaches zero because the secondary outcome $\\mathcal{X}$ perfectly predicts the primary outcome $\\pmb{\\mathcal{B}}$, making the regression estimator extremely precise.",
    "question": "Does the variance of the regression estimator $\\bar{y}_{r}$ approach zero as the absolute value of the correlation between $\\mathcal{X}$ and $\\pmb{\\mathcal{B}}$ approaches 1?",
    "question_context": "Regression Estimator in Survey Sampling and Clinical Trials\n\nIn survey sampling of finite populations, a regression estimator can be used in the setting where the population mean of a secondary outcome $\\mathcal X$ is known and the values of both $\\mathscr{X}$ and the primary outcome of interest $\\pmb{\\mathcal{G}}$ are known in a simple random sample. The regression estimator ${\\bar{y}}_{r}$ of the population mean of $\\pmb{\\mathcal{G}}$ is then $\\bar{y}_{r}=\\bar{y}+c(\\bar{\\mathcal{X}}-\\bar{x}).$ where $\\bar{y}$ and $\\bar{x}$ are the sample means of $\\mathcal{B}$ and $\\mathcal X$ , respectively, $\\bar{\\mathcal X}$ is the population mean of $\\mathcal X$ and $c$ is the slope of the least squares regression line of $\\pmb{\\mathcal{G}}$ on $\\mathscr{X}$ in the sample. This estimator is asymptoticaily unbiased for the population mean of $\\pmb{\\mathcal{D}}(\\pmb{\\mathcal{D}})$ and has lower variance than the standard estimator y: in fact, as the absolute value of the correlation between $\\mathcal{X}$ and $\\pmb{\\mathcal{B}}$ approaches 1, the variance of the regression estimator approaches 0. The motivation for using such an estimator is cost effectiveness. For a given level of precision, it may be cheaper to completely enumerate the secondary outcome in the finite population and then measure the primary outcome in a small sample of individuals rather than ignoring the secondary outcome and measuring $\\pmb{\\mathcal{G}}$ on a larger sample of individuals (Cochran, 1977, p. 189). <PARAGRAPH_SEPARATOR> The regression estimator can readily be expressed as $\\bar{y}_{r}=\\bar{y}-c\\bar{d}_{}$ where $\\bar{d}=\\bar{x}-\\bar{\\mathcal{X}}$ and $^{c}$ is a consistent estimator of <PARAGRAPH_SEPARATOR> $$ c_{0}=\\frac{\\mathrm{cov}\\left(\\bar{y},\\bar{d}\\right)}{\\mathrm{var}\\left(\\bar{d}\\right)}. $$ <PARAGRAPH_SEPARATOR> If $\\Tilde{y}_{r}$ is the regression estimator with $c$ replaced by $c_{0}$ , then it is easy to show that $\\Tilde{y}_{r}$ is the minimum variance linear combination of $\\bar{y}$ and $\\bar{x}$ that is unbiased for the population mean of $\\pmb{\\mathcal{P}}$ and that $\\tilde{y}_{r}$ is asymptotically equivalent to ${\\bar{y}}_{r}$ . The properties of this regression estimator can be further generalized. Suppose that we have statistics $Y_{n}$ and $X_{n}$ , for sample size $\\pmb{n}$ , based on the primary and secondary outcome data, respectively, and that $\\smash{\\{Y_{n},X_{n}\\}^{\\prime}}$ approaches $\\{Y,X\\}^{\\prime}$ in distribution, where $E(Y)=\\theta$ and $E(X)=0$ . Then, provided we have a consistent estimator $c$ of <PARAGRAPH_SEPARATOR> $$ {\\boldsymbol{c}}_{0}=\\operatorname{cov}\\left({\\boldsymbol{Y}},{\\boldsymbol{X}}\\right)\\left\\{\\operatorname{var}\\left({\\boldsymbol{X}}\\right)\\right\\}^{-1}, $$ <PARAGRAPH_SEPARATOR> ${\\hat{\\theta}}=Y_{n}-c X_{n}$ has asymptotically minimum variance of all linear combinations of $Y_{n}$ and $X_{n}$ which are asymptotically unbiased for $\\theta$ <PARAGRAPH_SEPARATOR> The estimator proposed in this paper is of the form ${\\hat{\\theta}},$ where $Y_{n}$ is a linear rank statistic assessing treatment effect on the primary failure time and $X_{n}$ is an asymptotically mean zero statistic based on the secondary failure time data. The development of this estimator is possible, in part, because randomizing patients to both extended and limited follow-up is statistically similar to the finite sampling setting described above. Knowledge of the population mean of the secondary outcome, $\\bar{\\mathcal X}$ , in the survey setting is parallel to knowledge of the secondary outcome information in both the limited and extended follow-up arms in the clinical setting, while knowledge of both the primary and secondary outcomes obtained from the simple random sample, $\\bar{y}$ and $\\bar{x}_{:}$ parallels knowledge obtained from the extended follow-up arm."
  },
  {
    "qid": "statistic-posneg-31-0-1",
    "gold_answer": "FALSE. The simple adjustment method of regressing residuals (e.g., ẽỸ|U on ẽX̃|U) is consistent for the additive distortion model but fails for the multiplicative distortion model. In the multiplicative case, the estimator targets ξ1 = γ1Δ, where Δ = E{ψ(U)ϕ(U)}/E{ϕ²(U)}, which can deviate arbitrarily from γ1, leading to inconsistent estimates.",
    "question": "The simple adjustment method of regressing the residuals of the distorted response on the residuals of the distorted predictors is consistent for estimating γ1 in the multiplicative distortion model. Is this claim correct?",
    "question_context": "Covariate-Adjusted Regression with Multiplicative Distortion\n\nWe address the problem of parameter estimation in multiple regression when the actual predictors and response are not observable. Instead, one observes contaminated versions of these variables, where the distortion is multiplicative, with a factor that is a smooth unknown function of an observed covariate. The simultaneous dependence of response and predictors on the same covariate may lead to artificial correlation and regression relationships which do not exist between the actual hidden predictor and response variables. An example is the fibrinogen data of Kaysen et al. (2003), where the regression of fibrinogen level on serum transferrin level in haemodialysis patients is of interest. Both observed response and predictor are known to depend on body mass index, defined as weight/height2, which thus has a confounding effect on the regression relationship. The theme of this paper is to explore such confounding in regression and to develop appropriate adjustment methods."
  },
  {
    "qid": "statistic-posneg-1236-0-4",
    "gold_answer": "FALSE. The identifiability of the global spatial trend and local CAR effects in the Smooth-CAR model is not guaranteed and may require further research, as noted in the context.",
    "question": "The Smooth-CAR model combines a global spatial trend modeled by P-splines with local regional effects modeled by a CAR structure, and the two components are always identifiable separately. True or False?",
    "question_context": "P-spline Smoothing for Spatial Data with CAR Structure\n\nThe paper discusses the use of P-splines for smoothing spatial data, incorporating conditionally autoregressive (CAR) structures to model both global spatial trends and local regional effects. The models are formulated within a mixed model framework, allowing for flexible smoothing and handling of overdispersion in count data."
  },
  {
    "qid": "statistic-posneg-1575-0-1",
    "gold_answer": "TRUE. The formula correctly represents the probability distribution of the symmetric-statistic T3, where k̂ is a normalization constant, f(s) ensures the sample contains n distinct population units, and the remaining terms account for the probabilities of selection and the multiplicities of the units in the sample.",
    "question": "Is the probability distribution of the symmetric-statistic T3 given by the formula P[T3] = k̂f(s) * (r! / (α(1)!...α(n)!)) * p(1)^α(1)...p(n)^α(n) * (1 - p(1) - ... - p(n))?",
    "question_context": "Inverse Sampling with Unequal Probabilities: Estimator Comparison\n\nIn inverse sampling with unequal probabilities, population units are selected with unequal probabilities (with replacement), $P_{j}$ being the probability of selection of $U_{j}$ ${\\mathit{J}}_{j}~(j{=}1,{\\ldots},N)$ and the selection of units is stopped at the $(\\pmb{r}+1)$ th draw when the sample first contains $({\\pmb{n}}+1)$ different population units. The last unit is rejected and the recorded sample consists of sample units corresponding to $\\pmb{\\mathscr{n}}$ different population units. The rejection of the last sample unit introduces simplicity in the distribution of the sample units as then for a given sample size the recorded sample units behave as interchangeable random variables. An observed sample is recorded as\n\n$$\n\\begin{array}{r}{\\boldsymbol{\\vartheta}=(u_{1},u_{2},...,u_{r})\\quad(r=n,n+1,...),}\\end{array}\n$$\n\nvhere $u_{1},u_{2},...,u_{r}$ are respectively the first, second and the rth sample units.\n\nThe probability of selecting a sample $\\pmb{\\vartheta}$ is given by\n\n$$\nP(\\varepsilon)=k f(s)p_{1}p_{2}\\ldots p_{r}(1-p_{(1)}-p_{(2)}-\\ldots-p_{(n)}),\n$$\n\nwhere $\\pmb{\\mathcal{P}\\mathfrak{i}}$ denotes the probability of selection of the ith sample unit, $u_{i}(i=1,...,r)_{i}$\n\n$$\nf(\\varepsilon)=\\left\\{{\\begin{array}{l l}{1}&{{\\mathrm{if~}}s{\\mathrm{~contains~}}n{\\mathrm{~distinct~population~units,}}}\\ {0}&{{\\mathrm{otherwise,}}}\\end{array}}\\right.\n$$\n\n$\\mathcal{P}(\\mathfrak{u})\\cdots,\\mathcal{P}(\\mathfrak{n})$ are the probabilities of selection associated with $\\pmb{\\mathscr{n}}$ differentpopulation units selected in the sample, and $\\pmb{k}$ is to be determined such that the summation of the right side over all possible samples equals unity.\n\nFrom (2) it is evident that for a given $\\pmb{\\mathscr{r}}$ $u_{1},u_{2},...,u_{r}$ are interchangeable random variables.\n\nUnder this sampling scheme, the order-statistic, the serial statistic and the symmetricstatistic are all sufficient. The probability distribution of the symmetric-statistic\n\n$$\nT_{3}=[(u_{(1)},\\alpha_{(1)}),...,(u_{(n)},\\alpha_{(n)})]\n$$\n\nis easily seen to be given by\n\n$$\nP[T_{3}]=\\stackrel{\\smile}{k}f(s)\\frac{r!}{\\alpha_{(1)}!\\ldots\\alpha_{(n)}!}p_{(1)}^{\\alpha_{(1)}}\\ldots p_{(n)}^{\\alpha_{(n)}}(1-p_{(1)}-\\ldots-p_{(n)}),\n$$\n\nwhere $\\pmb{k}$ and $f(\\pmb\\theta)$ have been defined in (2) and $\\mathbfcal{P}(\\mathfrak{s})$ i8 the probability of selection of $u_{(i)}$ $(i=1,...,n)$\n\nA little consideration will show that the probability distribution of the serial-statistic $T_{\\ell}=[u_{(1)},u_{(2)},...,u_{(n)}]$ is given by\n\n$$\nP[{\\mathcal{T}}_{2}]=p_{11}{\\frac{p_{12}}{(1-p_{11})}}\\cdots\\cdots{\\frac{p_{1n}}{(1-p_{[1]}-\\ldots-p_{[n-1]})}},\n$$\n\nwhere $\\pmb{\\mathcal{p}}_{[\\pmb{i}]}$ is the probability of selection of $\\mathscr{u}_{(i)}(i=1,...,n)$\n\nLastly the probability distribution of the order-statistic $T_{1}=[u_{(1)},...,u_{(n)}]$ is given by\n\n$$\nP[{\\cal T}_{1}]=\\Sigma^{\\prime}p_{(1)}\\frac{p_{(2)}}{(1-p_{(1)})}\\dots\\dots\\dots\\frac{p_{(n)}}{(1-p_{(1)}-\\dots-p_{(n-1)})},\n$$\n\nwhere the summation $\\Sigma^{\\prime}$ extends over all possible orderings of $p_{(\\mathbf{u})},...,p_{(\\pmb{n})}$\n\nIt can be seen from (4) that the distribution of the serial-statistic is the same as that of the serial-statistic in sampling with unequal probabilities (without replacement). Thus in this sense the two sampling schemes are equivalent. The author (1961) has earlier proved & similar result. The equivalence can be made precise as follows. Suppose $g_{1}(s)$ &nd $g_{\\mathfrak{L}}(\\pmb{\\mathscr{s}})$ areestimators of the population total based on inverse sampling with unequal probabilities and sampling with unequal probabilities (without replacement) respectively. Now if to the statistician the outcome of only one of these sampling schemes is given, he could, if he wished, compute with the help of & random device both $g_{\\mathbf{1}}(s)$ &nd $g_{\\mathfrak{L}}(s)$ such that their probability distributions will be identical with their original probability distributions under the two given sampling schemes. Thus theoretically, the knowledge of the outcome of one of these sampling schemes enables the statistician to produce an outcome of the second sampling scheme and therefore the two sampling schemes can be said as equivalent. This also shows that in inverse sampling with unequal probabilities if one is using estimators which are based on the serial-statistic then sampling can be stopped as soon as the nth population unit has been selected."
  },
  {
    "qid": "statistic-posneg-917-0-2",
    "gold_answer": "TRUE. The working correlation matrix R in the multivariate spatial GEEs is indeed assumed to be separable into spatial and between-response components, as R = R_spatial(θ) ⊗ R_resp. This separability simplifies the inversion and estimation of R and is motivated by both computational and statistical grounds, as explained in the context.",
    "question": "Is the working correlation matrix R in the multivariate spatial GEEs assumed to be separable into spatial and between-response components, specifically R = R_spatial(θ) ⊗ R_resp?",
    "question_context": "Spatial GLLVM and Multivariate Spatial GEEs for Correlated Data Analysis\n\nEach latent variable in the spatial GLLVM is assumed to follow a Gaussian process. That is, for $k=1,\\ldots,d,$ the $N$ - vector $z_{*k}=(z_{1k},\\bar{\\dots},z_{N k})^{\\top}$ follows a multivariate normal distribution with mean vector zero and a spatial correlation matrix characterized by the Matérn correlation function, $f(z_{*k})~=~$ $\\mathcal{N}\\{\\mathbf{0},\\pmb{\\Sigma}(\\gamma)\\}$ and $\\Sigma(\\pmb{\\gamma})_{s s^{\\prime}}=\\{2^{1-\\nu}/\\Gamma(\\nu)\\}(\\Delta_{s s^{\\prime}}/h)^{\\nu}\\dot{\\mathcal{K}}_{\\nu}(\\Delta_{s s^{\\prime}}/h)$ for $s,s^{\\prime}=1,\\ldots,N,$ , where $\\Gamma(\\cdot)$ is the gamma function, $\\mathcal{K}_{\\nu}(\\cdot)$ is the modified Bessel function of the second kind, and $\\Delta_{s s^{\\prime}}$ is a distance measure between spatial locations $s$ and $s^{\\prime}$ . The Matérn correlation function is characterized by the smoothness $\\nu~>~0$ and spatial scale parameters $h>0$ , such that we have $\\ensuremath{\\boldsymbol\\gamma}=(\\ensuremath{\\boldsymbol\\nu},h)^{\\top}$ . Note the smoothness parameter is sometimes fixed a-priori in real application for example, $\\nu~=~1$ is a common choice when $\\mathcal{D}=\\mathbb{R}^{2}$ (Lindgren and Rue 2015). Also, for ease of notation we have assumed the same smoothness and spatial scale parameters for all $d$ latent variables, although this can be generalized.<PARAGRAPH_SEPARATOR>It is a straightforward to see how the spatial GLLVM accounts for both between response and spatial correlations. On the linear predictor scale, we have $\\mathrm{cov}(x_{s}^{\\top}\\beta_{j}+z_{s}^{\\top}\\lambda_{j},x_{s^{\\prime}}^{\\top}\\beta_{j^{\\prime}}+z_{s^{\\prime}}^{\\top}\\lambda_{j^{\\prime}})=$ $\\Sigma(\\pmb{\\gamma})_{s s^{\\prime}}\\lambda_{j}^{\\top}\\lambda_{j^{\\prime}}$ , implying that the correlation between two responses decays from a maximum of $\\lambda_{j}^{\\top}\\lambda_{j^{\\prime}}$ , achieved at $s=s^{\\prime}$ , with increasing distance between the spatial units. Moreover, by employing latent variables the spatial GLLVM is able to model the between response correlation in a parsimonious, rank-reduced manner. Recently, Hui, Hill, and Welsh (2021a) demonstrated that if the true model is a spatial GLLVM but one misspecifies and fit a spatially independent GLLVM that is, assumes $f(z_{i k})\\sim\\mathcal{N}(0,1)$ for all $i=1,\\ldots,N$ and $k=1,\\ldots,d,$ then it generally has severe consequences on estimation and inference for the $\\beta_{j}\\mathrm{{^{>}s,}}$ especially if the corresponding covariates are also spatially correlated. This result demonstrates the importance of accounting for spatial correlation when it is present in (the underlying) GLLVM.<PARAGRAPH_SEPARATOR>Let $\\begin{array}{l l l}{\\phi}&{=}&{(\\phi_{1},...,\\phi_{m})^{\\top}}\\end{array}$ and $\\begin{array}{r c l}{\\Psi}&{=}&{(\\pmb{\\beta}_{1}^{\\top},\\dots,\\pmb{\\beta}_{m}^{\\top},\\pmb{\\phi}^{\\top},\\rho,}\\end{array}$ $\\lambda_{1}^{\\top},\\dots,\\lambda_{j}^{\\top},\\pmb{\\gamma}^{\\top})^{\\top}$ denote the full vector of parameters in the spatial GLLVM. If $\\phi$ are known, then these are omitted from $\\Psi$ . Note there are typically restrictions placed on the loadings $\\lambda_{j}$ to avoid rotation invariance (Skrondal and Rabe-Hesketh 2004; Niku et al. 2019a), although we suppress these in the definition of $\\Psi$ for ease of notation. The marginal log-likelihood of the spatial GLLVM is then defined as<PARAGRAPH_SEPARATOR>$$\\ell(\\Psi)=\\log\\left(\\int\\prod_{s=1}^{N}\\prod_{j=1}^{m}f(y_{s j}|x_{s},z_{s})\\prod_{k=1}^{d}f(z_{*k})d\\prod_{k=1}^{d}z_{*k}\\right).$$<PARAGRAPH_SEPARATOR>It is clear from (1) that the marginal likelihood function involves a high-dimensional integral, scaling with the number of spatial units $N$ and latent variables $d$ . Along with the fact that the integral itself does not possess a tractable form for nonnormally distributed responses, which is the case for the three response distributions outlined above, then estimation and thus variable selection for spatial GLLVMs presents a substantial computational burden.<PARAGRAPH_SEPARATOR># 3. Multivariate Spatial Generalized Estimating Equations<PARAGRAPH_SEPARATOR>To overcome the computational challenges of fitting and performing variable selection in spatial GLLVMs, we instead turn to multivariate spatial GEEs, which are defined by assumptions on the marginal moments and the working correlation matrix. Specifically, we assume the marginal moments $\\mathrm{E}(y_{s j}|\\pmb{x_{s}})=\\mu_{s j}$ and $\\begin{array}{c c c c c}{\\log(\\mu_{s j})}&{=}&{\\eta_{s j}}&{=}&{x_{s}^{\\top}\\pmb{\\alpha}_{j},}\\end{array}$ where $\\pmb{\\alpha}_{j}$ denotes the GEE regression coefficients for response $j$ . Note the mean $\\mu_{s j}$ is different from the one defined for the spatial GLLVM, and unless otherwise stated we will use $\\mu_{s j}$ to represent the mean in the multivariate spatial GEE. For the marginal variance, we consider the same mean-variance relationships as induced by conditional distribution of the responses in the spatial GLLVM (this is common practice e.g., Fitzmaurice et al. 2008). That is, we can consider $\\mathrm{var}(y_{s j}|{\\pmb x}_{s})=\\nu_{s j}=\\phi_{j}\\mu_{s j}$ and $\\mathrm{var}(y_{s j}|\\pmb{x}_{s})=$ $\\nu_{s j}=\\mu_{s j}+\\phi_{j}\\mu_{s j}^{2}$ for (overdispersed) counts, and $\\mathrm{var}(y_{s j}|\\pmb{x}_{s})=$ vsj = φjμsρj with ρ ∈ (1, 2) for nonnegative continuous data. Again, the parameters defining the marginal variance function differ from those in the underlying spatial GLLVM.<PARAGRAPH_SEPARATOR>Let $y=(y_{11},y_{12},\\dotsc,y_{1m},y_{21},\\dotsc,y_{N m})^{\\top}$ denote the full mN-vector responses, sorted by responses within spatial units, and likewise define $\\pmb{\\mu}=(\\mu_{11},\\stackrel{.}{\\mu}_{12},\\stackrel{.}{.}\\cdot\\cdot,\\mu_{N m})^{\\top}$ . Next, let $\\pmb{\\alpha}=$ $(\\pmb{\\alpha}_{1}^{\\top},\\dots,\\pmb{\\alpha}_{m}^{\\top})^{\\top}$ denote the full mp-vector of marginal coefficients in the multivariate spatial GEE, and define the $m N\\times m p$ model matrix<PARAGRAPH_SEPARATOR>$$X=\\left(\\begin{array}{c}{{I_{m}\\otimes x_{\\mathrm{I}}^{\\top}}}\\ {{I_{m}\\otimes x_{2}^{\\top}}}\\ {{\\vdots}}\\ {{I_{m}\\otimes x_{N}^{\\top}}}\\end{array}\\right).$$<PARAGRAPH_SEPARATOR>With the above set up, the full vector of linear predictors $\\pmb{\\eta}~=~(\\eta_{11},\\eta_{12},.~.~.,\\eta_{N m})^{\\top}$ can be written as $\\pmb{\\eta}~=~\\pmb{X}\\pmb{\\alpha}$ and subsequently $\\pmb{\\mu}=\\exp(\\pmb{\\eta})$ where the exponential function is applied element-wise. The saturated multivariate spatial GEE is then defined as<PARAGRAPH_SEPARATOR>$$\\boldsymbol{S}(\\boldsymbol{\\alpha})=\\boldsymbol{X}^{\\top}\\boldsymbol{W}\\boldsymbol{A}^{-1/2}\\boldsymbol{R}^{-1}\\boldsymbol{A}^{-1/2}(\\boldsymbol{y}-\\boldsymbol{\\mu})=\\mathbf{0},$$<PARAGRAPH_SEPARATOR>where $W$ is a diagonal matrix with elements equal to $\\pmb{\\mu}$ and $\\pmb{A}$ is a diagonal matrix with elements $\\pmb{\\nu}=(\\nu_{11},\\nu_{12},...,\\nu_{N m})^{\\top}$ . We refer to (2) as a saturated GEE since it involves all $p$ covariates, or equivalently all mp columns of $X$ . This contrasts to candidate GEEs that we will discuss in Section 3.2, which involve a subset of the columns of $X$ .<PARAGRAPH_SEPARATOR>Regarding the working correlation matrix, while there are a wide array of possible choices for $\\pmb R$ as reviewed in Section 1, here we propose a relatively simple, separable structure $R=R_{\\mathrm{spatial}}(\\theta)\\otimes R_{\\mathrm{resp}}$ , where $R_{\\mathrm{resp}}$ is assumed to be an $m\\times m$ unstructured between response correlation matrix, and $R_{\\mathrm{spatial}}(\\pmb\\theta)$ is an $N\\times N$ spatial working correlation matrix parameterized by a Matérn correlation function with smoothness parameter $\\nu$ and spatial scale $h,$ such that $\\mathbf{\\theta}\\theta~=~(\\nu,h)^{\\top}$ . Again, these parameters differ from those in the spatial GLLVM. The assumption of separability between the spatial and between response correlation structures is motivated from both computational and statistical grounds. In the former, inversion and estimation of $\\pmb R$ is greatly simplified upon assuming separability, and this is exploited in the estimation process below. In the latter, note that without independent samples, the empirical estimate of the fully unstructured $m N\\times m N$ working correlation matrix would reduce to $A^{-1/2}(y-\\pmb{\\mu})(y-\\pmb{\\mu})^{\\top}A^{-\\top/2}$ . This is not only a poor estimate of the correlation matrix in general, but in fact does not lead to a sensible score statistic as we will see in Section 4. Assuming separability will circumvent this issue. With regards to $R_{\\mathrm{spatial}}(\\pmb\\theta)$ , we use a Matérn correlation function to ensure the spatial correlation matrix is well-defined and stable; see also the discussion in Section 4. On the other hand, since we typically have no simple concept of distance between two responses (species) without further knowledge, then so we choose to keep $R_{\\mathrm{resp}}$ unstructured. With $N$ sufficiently large, we found that this unstructured matrix can usually be estimated in a stable manner.<PARAGRAPH_SEPARATOR>As a simpler and even more computationally efficient alternative to the above, we also examine the case of the independence estimation equations (IEEs), where $\\textit{\\textbf{R}}=\\textit{\\textbf{I}}_{m N}$ that is, a working correlation assuming all spatial units and responses are uncorrelated with each other. Note in this setting Equation (2) is equivalent to fitting a separate regression model to each response for example, the case of $\\mathrm{var}(y_{s j}|{\\pmb x}_{s})=\\nu_{s j}=\\phi_{j}\\mu_{s j}$ is the same as fitting a separate Poisson generalized linear model (GLM) to response, while the case $\\mathrm{var}(y_{s j}|\\pmb{x}_{s})=\\nu_{s j}=\\phi_{j}\\mu_{s j}^{\\rho}$ is the same as fitting separate Tweedie GLMs to each response."
  },
  {
    "qid": "statistic-posneg-337-0-1",
    "gold_answer": "TRUE. Although the notation simplifies the penalized least squares criterion to a single smoothing parameter $\\lambda$, the text explicitly states that extra smoothing parameters are included in $P(\\eta,\\eta)$ to adjust the strength of the different components in the model. These additional parameters are omitted from the notation for simplicity.",
    "question": "Does the penalized least squares criterion $$ \\frac{1}{n}\\sum_{i=1}^{n}\\{y_{i}-\\eta(x_{i})\\}^{2}+\\lambda P(\\eta) $$ include multiple smoothing parameters to adjust the strength of different components in the model, despite the notation simplifying it to a single $\\lambda$?",
    "question_context": "Smoothing Spline ANOVA Model and Parameter Selection\n\nIn this article, we consider a nonparametric model of the form $$ y_{i}=\\eta(x_{i})+\\epsilon_{i}\\quad(i=1,\\ldots,n), $$ where $y_{i}\\in\\mathbb{R}$ is the response variable for the ith observation, $\\eta$ is a nonparametric function varying in an infinite-dimensional functional space, $x_{i}=(x_{i\\langle1\\rangle},\\dots,x_{i\\langle d\\rangle})^{\\mathrm{T}}$ is a $d$ -dimensional vector of predictors for the ith observation, and the $\\epsilon_{i}$ are independent and identically distributed random errors with mean zero and unknown variance $\\sigma^{2}$ . We focus on the smoothing spline ANOVA model (Wahba et al., 1995) for a multi-dimensional problem, i.e., with $d>1$ . In the smoothing spline ANOVA model we decompose the function $\\eta$ as $$ \\eta(x)=\\eta_{\\mathsf{o}}+\\sum_{j=1}^{d}\\eta_{j}(x_{\\langle j\\rangle})+\\sum_{j<k}\\eta_{j,k}(x_{\\langle j\\rangle},x_{\\langle k\\rangle})+\\cdot\\cdot\\cdot+\\eta_{1,2,\\ldots,d}(x_{\\langle1\\rangle},x_{\\langle2\\rangle},\\ldots,x_{\\langle d\\rangle}), $$ where $\\eta_{\\alpha}$ is a constant, the $\\eta_{j}$ are main-effect functions, the $\\eta_{j,k}$ are two-way interaction functions, and $\\eta_{1,2,\\ldots,d}(x_{\\langle1\\rangle},x_{\\langle2\\rangle},\\ldots,x_{\\langle d\\rangle})$ is a $d$ -way interaction function. Side conditions on the components are imposed to guarantee a unique decomposition. The nonparametric function $\\eta$ can be estimated by minimizing the penalized least squares $$ \\frac{1}{n}\\sum_{i=1}^{n}\\{y_{i}-\\eta(x_{i})\\}^{2}+\\lambda P(\\eta), $$ where $P(\\eta)=P(\\eta,\\eta)$ is a quadratic roughness penalty and the smoothing parameter $\\lambda$ controls the trade-off between the lack of fit of $\\eta$ and the roughness of $\\eta$ . Extra smoothing parameters are included in $P(\\eta,\\eta)$ to adjust the strength of the components in (2), but for simplicity we omit them from the notation. The explicit formula for $P(\\eta,\\eta)$ can be found in $\\S\\-2.1$ . Since the minimizer of (3), denoted by $\\eta_{n,\\lambda}$ , is sensitive to the selection of $\\lambda$ , it is crucial to choose an effective and efficient method for selecting the smoothing parameter."
  },
  {
    "qid": "statistic-posneg-1955-0-1",
    "gold_answer": "FALSE. The IBF sampler is explicitly designed as a noniterative method, leveraging the SIR technique with a single evaluation of the importance weights $\\omega_j$. Unlike Gibbs sampling, which requires iterative conditional sampling until convergence, the IBF sampler directly resamples from a proposal distribution $f(Z|Y_{\\mathrm{obs}},\\theta^{*})$ weighted by $f^{-1}(\\theta^{*}|Y_{\\mathrm{obs}},Z^{(j)})$. The noniterative nature is highlighted by the one-step generation of $\\{Z^{(j)}\\}_1^J$ and subsequent resampling, with accuracy improving as $J$ increases, not through iteration.",
    "question": "Does the IBF sampling method always require iterative procedures like Gibbs sampling to generate iid samples from $f(Z|Y_{\\mathrm{obs}})$?",
    "question_context": "Inverse Bayes Formulas (IBF) Sampling for Hierarchical Models\n\nLet $Y_{\\mathrm{obs}}$ denote the observed data and $\\theta$ the parameter vector of interest. Using the concept of data augmentation (Tanner and Wong, 1987), we augment the observed data $Y_{\\mathrm{obs}}$ with latent variables $Z$ so that both the augmented posterior distribution $f(\\theta|Y_{\\mathrm{obs}},Z)$ and the conditional predictive distribution $f(Z|Y_{\\mathrm{obs}},\\theta)$ are available. Let $\\hat{\\theta}_{\\mathrm{obs}}$ denote the mode of the observed posterior density $f(\\theta|Y_{\\mathrm{{obs}}})$ and $\\mathcal{S}_{(\\theta,Z|Y_{\\mathrm{obs}})},\\mathcal{S}_{(\\theta|Y_{\\mathrm{obs}})}$ and $\\mathcal{S}_{(Z|Y_{\\mathrm{{obs}}})}$ denote the joint and conditional supports of $(\\theta,Z)|Y_{\\mathrm{obs}}$ , $\\theta|Y_{\\mathrm{obs}}$ and $Z|Y_{\\mathrm{obs}}$ , respectively. The IBF sampler requires two basic assumptions: (a) the observed posterior mode $\\hat{\\theta}_{\\mathrm{obs}}$ is already obtained, say, by the EM algorithm; and (b) the joint support is a product space, i.e., $\\mathcal{S}_{(\\theta,Z|Y_{\\mathrm{obs}})}=\\mathcal{S}_{(\\theta|Y_{\\mathrm{obs}})}\\times\\mathcal{S}_{(Z|Y_{\\mathrm{obs}})},$ . The goal is to obtain iid samples from $f(\\theta|Y_{\\mathrm{{obs}}})$ . <PARAGRAPH_SEPARATOR> If we can obtain independent samples $\\left\\{Z^{(k_{i})}\\right\\}_{i=1}^{m}$ from $f(Z|Y_{\\mathrm{{obs}}})$ and generate $\\theta^{(i)}\\sim$ $f\\left(\\theta|Y_{\\mathrm{obs}},Z^{(k_{i})}\\right)$ for $i=1,\\ldots,m$ , then $\\left\\{\\theta^{(i)}\\right\\}_{i=1}^{m}\\overset{\\mathrm{iid}}{\\sim}f(\\theta|Y_{\\mathrm{obs}}).$ . Therefore, the key is to be able to generate iid samples from $f(Z|Y_{\\mathrm{obs}}).$ . This can be achieved by using the samplingwise IBF <PARAGRAPH_SEPARATOR> $$ f(Z|Y_{\\mathrm{obs}})\\propto\\frac{f\\left(Z|Y_{\\mathrm{obs}},\\theta^{*}\\right)}{f\\left(\\theta^{*}|Y_{\\mathrm{obs}},Z\\right)}\\quad\\mathrm{~for~some~arbitrary~}\\theta^{*}\\in\\mathcal{S}_{\\left(\\theta|Y_{\\mathrm{obs}}\\right)} $$ <PARAGRAPH_SEPARATOR> IBF sampling is realized via sampling/importance resampling (SIR) technique (Rubin, 1987, 1988; Smith and Gelfand, 1992). The SIR aims at drawing a random sample from a target distribution $\\pi$ . First, a sample is drawn from a proposal distribution $q$ , and then from this a smaller sample is drawn with sample probabilities proportional to the importance ratios $\\pi/q$ . An overview of the method may also be found in Tanner (1996) and Gelman et al. (1996). Recently, Skare et al. (2003) proposed an improved SIR and showed that this gives faster convergence than the original SIR and mentioned many applications of the SIR algorithm. <PARAGRAPH_SEPARATOR> Now, from (2.1), the target distribution is $f(Z|Y_{\\mathrm{{obs}}})$ and the proposal distribution is the conditional predictive distribution $f\\left(Z|Y_{\\mathrm{obs}},\\theta^{*}\\right)$ . Therefore, the IBF sampling is as follows: (i) draw $J$ independent samples of $Z$ from $f\\left(Z|Y_{\\mathrm{obs}},\\theta^{*}\\right)$ , denoted by $\\left\\{Z^{(j)}\\right\\}_{1}^{J}$ ; (ii) calculate the weights $\\omega_{j}\\propto f^{-1}\\left(\\theta^{*}|Y_{\\mathrm{obs}},Z^{(j)}\\right)$ for $j=1,\\ldots,J$ such that $\\textstyle\\sum_{j=1}^{J}\\omega_{j}=1$ ; (iii) choose a subset from $\\left\\{Z^{(j)}\\right\\}_{1}^{J}$ via resampling without replacement fro m the discrete distribution on $\\left\\{Z^{(j)}\\right\\}_{1}^{J}$ with probabilities $\\left\\{\\omega_{j}\\right\\}_{1}^{J}$ to obtain an iid sample of size m $(<J)$ approximately fro\u0001m $f(Z|Y_{\\mathrm{{obs}}})$ with the appr\u0001oxi \u0002mation “improving” as $J$ increases (Smith and Gelfand, 1992), denoted by $\\left\\{Z^{(k_{i})}\\right\\}_{i=1}^{\\hat{m}^{\\star}}$ ) i 1. The efficiency of the IBF sampling depends on the choice of $\\theta^{*}$ . Tan et al. (2003) suggested an effective choice of $\\theta^{*}=\\hat{\\theta}_{\\mathrm{obs}}$ , the observed posterior mode."
  },
  {
    "qid": "statistic-posneg-1604-0-0",
    "gold_answer": "TRUE. The conditional linear mixed model approach conditions on the sufficient statistics for the subject-specific intercepts (nuisance parameters), effectively removing them from the model. This allows the focus to remain on the longitudinal parameters of interest (fixed and random effects), while the residual variance remains as the only other parameter. The transformation of the data vectors Yi to Ai'Yi, where Ai is a matrix satisfying specific conditions, facilitates this conditioning and maintains the linear mixed model framework for standard software application.",
    "question": "Is it true that the conditional linear mixed model eliminates subject-specific intercepts by conditioning on their sufficient statistics, allowing inference for longitudinal parameters of interest?",
    "question_context": "Conditional Linear Mixed Models for Longitudinal Data Analysis\n\nA classical statistical procedure that is often used to correct for differences between subjects is the inclusion of covariates which may explain the observed heterogeneity. In our ADL example, available potential covariates are the age of the patient at entry, as well as the living situation of the patient before the hip fracture. The living situation is a nominal variable indicating whether the patient was living on his or her own, living with a partner, family or friend(s), or living in a nursing home. Both variables are believed to be highly predictive of the functional status of the patients. In model (1), age and living situation can be formally corrected for by including them as covariates (after recoding the living situation by two dummy variables). Obviously, the advantage of this procedure is that the new model is still a linear mixed model such that the same estimation and inferential procedures can be used as for the original uncorrected model. Among the disadvantages is the fact that predictive covariates must be known and available, but also the fact that the correct parametric functional relationship between the covariates and the response of interest must be specified."
  },
  {
    "qid": "statistic-posneg-2086-1-0",
    "gold_answer": "FALSE. The context states that there is high dependence between $V$ and $k_{e}$ in the Gibbs sequence, which implies that the alternative parameterization does not reduce dependence. The high dependence between $V$ and $k_{e}$ (since $k_{e}=C/V$) suggests that the alternative parameterization may not be more effective in reducing dependence.",
    "question": "Is the alternative parameterization $\\phi=(V,k_{e})$ where $k_{e}=C/V$ more effective than the original parameterization $\\pmb\\theta=(V,C)$ in reducing dependence between parameters in the Gibbs sampler?",
    "question_context": "Gibbs Sampler Parameterization and Convergence in Pharmacokinetic Modeling\n\nJon Wakefield (Imperial College of Science, Technology and Medicine, London): I would like to present a simple example which demonstrates that the choice of parameterization can not only speed up convergence but also ease the diagnosis of convergence. Consider drug concentrations $y$ (2.03,1.28, 1.20, 1.02, 0.83, 0.28) measured at sampling times t (2, 4, 6, 8, 10, 24). The model $$E[Y|V,C]={\\frac{D}{V}}\\exp\\left(-{\\frac{C t}{V}}\\right),$$ where $^D$ is the dose, is appropriate. An alternative parameterization to $\\pmb\\theta=(V,C)$ is $\\phi=(V,k_{e})$ where $k_{e}=C/V$ .For simplicity we assume additive, independent normal errors with variance $\\sigma^{2}$ ,areference prior (Bernardo, 1979) for $\\pmb\\theta$ and ${\\pmb\\sigma}^{-1}$ for $\\pmb{\\sigma}$ . The Gibbs sampler was applied by using the ratio of uniforms method (Wakefield et al., 1991) to generate from the conditionals for $\\pmb\\theta$ Samples from each of the posteriors are shown in Fig. 2. There is high dependence between $V$ and $k_{e}$ which produces high dependence in the Gibbs sequence. The method of Raftery and Lewis (1992) was used to diagnose convergence. Suppose that $\\boldsymbol{U}$ is afunction of interest and we wish to estimate $P(U{\\leqslant}u|\\mathbf{y})$ towithin $\\pm r$ withprobability $\\pmb{S}$ Themethod takes a short run of the Gibbs sampler and provides recommendations for an initial number of iterates to discard and a further number $N$ for which every $k$ th is stored. The parameter of interest chosen is the half-life, defined as. $U{=}k_{e}^{-1}\\log2$ . The quantile $q=0.025$ with $r=0.005$ and $s=0.95$ is used.For each of the parameterizations 10 independent chains each of length 1000 are run. Table 1 shows the recommended number of iterations, $N.$"
  },
  {
    "qid": "statistic-posneg-1357-0-4",
    "gold_answer": "FALSE. The estimator $\\widehat{\\mathbf{R}}$ constructed using the inverse bridge functions $F^{-1}$ is not guaranteed to be positive definite. While the individual pairwise estimates $\\widehat{r}_{jk} = F^{-1}(\\widehat{\\tau}_{jk})$ are consistent for the true latent correlations, the resulting matrix $\\widehat{\\mathbf{R}}$ may not be positive definite, especially in high dimensions or with small sample sizes. Additional steps, such as eigenvalue adjustment or projection onto the space of positive definite matrices, may be required to ensure positive definiteness.",
    "question": "Is the latent correlation estimator $\\widehat{\\mathbf{R}}$ constructed using the inverse bridge functions $F^{-1}$ for all combinations of continuous, binary, and truncated variables guaranteed to be positive definite?",
    "question_context": "Latent Correlation Estimation via Bridge Functions in Mixed Gaussian Copula Models\n\nThe mixed latent Gaussian copula model jointly models $\\mathbf{W}=$ $({\\bf W}_{1},{\\bf W}_{2},{\\bf W}_{3})\\sim\\mathrm{NPN}({\\bf0},{\\pmb\\Sigma},f)$ such that $X_{1j}=W_{1j},X_{2j}=$ $I(W_{2j}>c_{2j})$ and $W_{3j}=I(W_{3j}>c_{3j})W_{3j}$ . <PARAGRAPH_SEPARATOR> The latent correlation matrix $\\pmb{\\Sigma}$ is the key parameter in Gaussian copula models. Estimation of latent correlations is achieved via the bridge function $F$ such that $\\mathbb{E}(\\widehat{\\tau}_{j k})~=~F(\\sigma_{j k})$ , where $\\sigma_{j k}$ is the latent correlation between variables $j$ and $k,$ and $\\widehat{\\tau}_{j k}$ is the corresponding sample Kendall’s $\\tau$ . Given observed $\\mathbf{x}_{j},\\dot{\\mathbf{x}_{k}}\\in\\mathbb{R}^{n}$ , <PARAGRAPH_SEPARATOR> $$ \\widehat{\\tau}_{j k}=\\widehat{\\tau}(\\mathbf{x}_{j},\\mathbf{x}_{k})=\\frac{2}{n(n-1)}\\sum_{1\\leq i<i^{\\prime}\\leq n}\\mathrm{sign}(x_{i j}-x_{i^{\\prime}j})\\mathrm{sign}(x_{i k}-x_{i^{\\prime}k}), $$ <PARAGRAPH_SEPARATOR> where $n$ is the sample size. Using $F$ , one can construct $\\widehat{\\sigma}_{j k}~=~{\\cal F}^{-1}(\\widehat{\\tau}_{j k})$ with the corresponding estimator $\\widehat{\\pmb\\Sigma}$ being consistent for $\\pmb{\\Sigma}$ (Fan et al. 2017; Quan, Booth, and Wells 2018; Yoon, Carroll, and Gaynanova 2020). The explicit form of $F$ has been derived for all combinations of continuous(C)/binary(B)/truncated(T) variables (Fan et al. 2017; Yoon, Carroll, and Gaynanova 2020). We summarize these results below, and use CC, BC, TC, etc. to denote corresponding combinations."
  },
  {
    "qid": "statistic-posneg-621-0-0",
    "gold_answer": "FALSE. The exact solution involves averaging over the joint probability distribution of the estimated variances, and the condition given is a condensed form of the solution. The operator Θ must be expanded and applied correctly to derive the exact solution, which is more complex than the simple equation suggests.",
    "question": "Is the condition for the exact solution of the generalized Student's problem given by ΘI{h(w)/√(Σλiσi²)} = P, where Θ is an operator involving partial differentiations and I is the normal probability integral, correct?",
    "question_context": "Generalization of Student's t-Test for Heteroscedastic Variances\n\nThe paper discusses the generalization of Student's t-test when several different population variances are involved. It introduces a method to make probability statements about an estimate y of a population parameter η, where the variance of y is a weighted sum of unknown variances. The paper derives an exact solution and compares it with an approximate method using a Pearson Type III distribution."
  },
  {
    "qid": "statistic-posneg-762-0-0",
    "gold_answer": "FALSE. The context does not provide a clear derivation or justification for the formula r1 = 2S3/S1 - d(1 + d)b. The explanation is incomplete, and the term 'b' is not defined or explained in the provided context. Additionally, the context mentions that the method is practical but does not explicitly link the formula to the summation process described.",
    "question": "Is the formula for r1, given as r1 = 2S3/S1 - d(1 + d)b, correctly derived from the summation method described in the context?",
    "question_context": "Summation Method for Calculating Moments in Statistical Processes\n\nThe text discusses a summation method for calculating rough moments about a mean in statistical processes. The method involves using sums of function values and their multiples to derive moments. The formulas provided are for calculating the first, third, and fourth moments (r1, r3, r4) using sums S1, S3, S4, and S6, and a distance parameter d. The method is compared to direct calculation and other integration methods, noting its advantages for longer series and its application to correlation tables."
  },
  {
    "qid": "statistic-posneg-657-3-2",
    "gold_answer": "TRUE. The global AIC criterion evaluates the overall model fit (using $\\hat{\\sigma}^2 = \\|\\mathbf{Y} - \\tilde{\\mathbf{Y}}\\|^2/n$) and complexity (via $\\operatorname{tr}(H)/n$) across all observations. Unlike the local AIC, it is not restricted to a specific region $R_k$ and instead optimizes the global parameters $h$ and $\\lambda$ for the entire estimator.",
    "question": "The global AIC criterion $AIC(h, \\lambda) = \\log(\\hat{\\sigma}^2) + 2\\operatorname{tr}(H)/n$ is used to select the global bandwidth $h$ and regularization parameter $\\lambda$ after fitting the local N-W-additive estimator. Does this criterion operate on the entire dataset rather than within individual regions?",
    "question_context": "Local Linear-Additive Estimation and Bandwidth Selection via AIC\n\nThe paper discusses a method for selecting local regularization parameters and bandwidths in nonparametric regression using an AIC-type criterion. The approach involves splitting the support of covariates into rectangular regions, fitting local additive estimators within each region, and smoothing the selected parameters. The AIC criterion is used to choose optimal bandwidths and regularization parameters both locally and globally."
  },
  {
    "qid": "statistic-posneg-1938-0-0",
    "gold_answer": "FALSE. The context explicitly states that calculating $\\operatorname{E}(X_{i,j}|W_{i})$ under high-dimensional predictors can be tedious, which implies it is not always feasible. The paper then proposes an alternative approach using pseudo data to address this issue.",
    "question": "Is the statement 'The induced model (4.1) suggests that using $\\operatorname{E}(X_{i,j}|W_{i})$ for a missing $X_{i,j}$ in prediction is always feasible under high-dimensional predictors' correct based on the provided context?",
    "question_context": "Imputation Methods for Missing Predictors in High-Dimensional Data\n\nThe above gradient boosting is for the purpose of calculating the conditional expectation in (ii) (a) of the iterative conditional mean method described in Section 3.1. That is, this subsection is a tool for the purpose of imputing missing predictors, based on the imputation model (3.2). The gradient boosting in this subsection will also be used in another imputation method which will be described in the Section 4. In (ii) (a) of the above algorithm, it is important to select the most important variable. One consequence of not selecting the most important variable is that the convergence of the algorithm will be slower. More importantly, if in case irrelevant variables are consecutively selected, then the algorithm may stop earlier than it should, such that the conditional expectation is poorly estimated.<PARAGRAPH_SEPARATOR># 4. PSEUDO DATA CONDITIONAL MEAN IMPUTATION<PARAGRAPH_SEPARATOR>In this section, we propose another imputation method. Note that in Section 3, we proposed an iterative conditional mean imputation which was based on the imputation model $X_{i,k}^{*}$ given $\\delta_{i,j}X_{i,j},j\neq k$ , and an iterative procedure is involved since $X_{i,k}^{*}$ is iteratively updated. We now investigate another type of imputation for high-dimensional predictors. We now explain the motivation of the method by considering a simple additive model such that $\begin{array}{r}{\\operatorname{E}(Y_{i}|X)=\\beta_{0}+\\sum_{j=1}^{p}\\beta_{j}X_{i,j}}\\end{array}$ . Let $W_{i}$ be a vector consisting the observed elements of $X_{i}$ ; that is, $X_{i,j}$ such that $\\delta_{i,j}=1$ for $j=1,\\dots,p$ . We note that an induced model is<PARAGRAPH_SEPARATOR>$$\\mathrm{E}(Y_{i}|W_{i})=\\beta_{0}+\\sum_{j=1}^{p}\\beta_{j}\\delta_{i,j}X_{i,j}+\\sum_{j=1}^{p}\\beta_{j}(1-\\delta_{j})\\mathrm{E}(X_{i,j}|W_{i}).$$<PARAGRAPH_SEPARATOR>The above induced model (4.1) suggests that we may use $\\operatorname{E}(X_{i,j}|W_{i})$ for a missing $X_{i,j}$ in prediction. However, it is seen from the created missing data example in the last section, the calculation of $\\operatorname{E}(X_{i,j}|W_{i})$<PARAGRAPH_SEPARATOR>under high-dimensional predictors can be tedious. Alternatively, we consider the following equation:<PARAGRAPH_SEPARATOR>$$\\mathrm{E}\\{Y_{i}-\\beta_{0}-\\sum_{j=1}^{p}\\beta_{j}\\delta_{i,j}X_{i,j}-\\sum_{j=1}^{p}\\beta_{j}(1-\\delta_{j})\\mathrm{E}(X_{i,j}|X_{i,-j})\\}=0,$$<PARAGRAPH_SEPARATOR>where $X_{i,-j}$ is $X_{i}$ but excluding $X_{i,j}$ . It is clear that $\\operatorname{E}(X_{i,j}|X_{i,-j})$ may be a function of both the observed and the missing data. Therefore, an alternative idea is to consider “pseudo data” for those $X_{i,k}$ with $\\delta_{i,k}=0$ .e cLtieot $X_{i,k}^{[0]}$ nboet eas t, ews hoifl ea  tmhies $X_{i,k}$ feo lra astn sy $i,k$ osnu cdhe ntohtaet $\\delta_{i,k}=0$ $X_{i,k}^{[r]}$ $\\operatorname{E}(X_{i,j}|X_{i,-j})$ $X_{i,k}^{(r)}$ $\\{\\mathrm{E}(X_{i,k}|\\delta_{i,j}X_{i,j},j\\neq k)\\}$ Define Xi[rk∗] $X_{i,k}^{[r*]}=\\delta_{i,k}X_{i,k}+(1-\\delta_{i,k})X_{i,k}^{[r]}$ for $r=0,1,\\ldots$ . We would consider an iterative algorithm at iteration $r$ , we assume the regression model<PARAGRAPH_SEPARATOR>$$X_{i,k}^{[r*]}=\\eta_{0}^{[r]}+\\sum_{j=1,j\\neq k}^{p}\\eta_{j}^{[r]}X_{i,j}^{[r*]}+e_{i,k},$$<PARAGRAPH_SEPARATOR>where $e_{i,k}$ is a mean 0 residual in the above linear regression and $\\eta_{0}^{\\left[r\\right]}$ and η[jr] are the regression parameters at the $r$ th step. We note that the induced imputation model (4.2) is to use the pseudo data Xi[,r j∗] as the covariates for imputation. This is different from the induced imputation model (3.2) given in the last section, which uses $\\delta_{i,j}X_{i,j}$ as the covariates for imputation. These 2 imputation models have different meanings in terms of prediction, and the regression coefficients in the 2 models are different. Nevertheless, the 2 different imputation models have the same goal; which is to estimate a missing variable. However, from our numerical experience, the imputation model (4.2) often does not converge when using an iterative procedure. Therefore, we modify the imputation model by considering $X_{i,k}^{[\\bar{r*}]}=\\eta_{0}^{[r]}+\\bar{\\sum_{\\substack{j=1,j\\neq k}}}\\eta_{j}^{[r]}\\bar{X_{i,j}^{*}}+e_{i,k}$ $X_{i,j}^{*}$ $X_{i,j}$ $\\delta_{i,j}=1$ ,a taendd  eiss ttihmea tuendc tiaot nthale $\\delta_{i,j}=0$ $\\begin{array}{r}{X_{i,k}^{[r+1]}=\\widehat{\\eta}_{0}^{[r]}+\\sum_{j=1,j\\neq k}^{p}\\widehat{\\eta}_{j}^{[r]}X_{i,j}^{*}}\\end{array}$ $X_{i,j}$ $(r+1)\\operatorname{th}$ iteration. Therefore, the iterative a lbgorith m can be  dbescribed as below:<PARAGRAPH_SEPARATOR>(i) Choose a naive imputed value for $X_{i,k}$ , say mean imputation if $\\delta_{i,k}=0$ for $i=1,\\ldots,n,k=$ 1, . . . , p. Let Xi[,0k] b e a starting value, and let Xi[,0k∗] $X_{i,k}^{[0*]}=\\delta_{i,k}X_{i,k}+(1-\\delta_{i,k})X_{i,k}^{[0]}$ .<PARAGRAPH_SEPARATOR>(ii) For each $r=0,1,\\ldots$"
  },
  {
    "qid": "statistic-posneg-334-0-1",
    "gold_answer": "FALSE. The correct convergence rate under the given assumptions is $O(n^{-s_{*}p/(2s_{*}+1)})$, as shown in the proof of Theorem 3.3. The initial rate mentioned is derived under different conditions and is not applicable here.",
    "question": "The paper claims that the expected Lp error of the estimator $\\hat{g}_{L}(x)$ converges at the rate $O(n^{-s_{*}p/(2s_{*}+4)})$. Is this convergence rate correct under the given assumptions?",
    "question_context": "Nonparametric Estimation of a Quantile Density Function Using Wavelets\n\nThe paper discusses nonparametric estimation of a quantile density function using wavelet methods. It presents several lemmas and theorems that establish bounds on the estimation error and convergence rates. Key formulas include bounds on the probability of large deviations of wavelet coefficient estimates and the expected Lp error of the wavelet-based estimator."
  },
  {
    "qid": "statistic-posneg-861-0-0",
    "gold_answer": "TRUE. Explanation: The GPD's survival function is $\\bar{G}(y) = \\left[1 + \\frac{\\xi}{\\psi}(y - u)\\right]_{+}^{-1/\\xi}$. For $\\xi < 0$, the support is $u < y < y^+ = u - \\psi/\\xi$ (with $\\psi$ as the scale parameter, equivalent to $\\sigma$ in the GEV). This ensures the term inside the brackets remains positive. As $y \\to y^+$, $\\bar{G}(y) \\to 0$, indicating a finite upper bound. The condition $y^+ = u - \\sigma/\\xi < \\infty$ is explicitly stated in the context when $\\xi < 0$, confirming the finite endpoint property for negative shape parameters.",
    "question": "The generalized Pareto distribution (GPD) is used to model the exceedances over a high threshold $u$ in the Peaks-over-Threshold (PoT) approach. The shape parameter $\\xi$ determines the tail behavior: $\\xi > 0$ corresponds to heavy tails, $\\xi = 0$ to exponential tails, and $\\xi < 0$ to light tails with a finite upper endpoint. Is it correct to say that for $\\xi < 0$, the upper endpoint $y^+$ is finite and given by $y^+ = u - \\sigma/\\xi$?",
    "question_context": "Extreme Value Theory: Generalized Pareto Distribution for Threshold Exceedances\n\nThe theory on which the vanilla versions of both block maxima and PoT models are based starts with the assumption of a sequence $Y_{1}$ , . . . , $Y_{m}$ of independent replicates of a random variable Y, with distribution $F(y)=\\operatorname*{Pr}[Y\\leq y]$ . Under some nonrestrictive conditions on $F$ , Von Mises (1936) and Gnedenko (1943) showed that in the limit as $m\\rightarrow\\infty$ , the distribution of the normalized block maximum $(M_{m}-b_{m})/a_{m}=(\\operatorname*{max}\\{Y_{1},...,Y_{m}\\}-b_{m})/a_{m}$ is one of the three types of max-stable distribution: Weibull, Fréchet, or Gumbel. While both the rate of convergence to the limiting dis­ tribution and the sequences $\\{a_{m}>0\\}$ and $\\{b_{m}\\}$ are determined by the underlying distribution $F$ which, in a modelling context is unknown, this result is nevertheless used to justify the use of the generalized extreme-value distribution (GEV) to model the maxima of large but finite blocks. The GEV is a class of distributions combining the three types of max-stable distribution. The resulting cumulative distribution function is, $$G(z)=\\exp\\left\\{-\\biggl[1+\\frac{\\xi}{\\sigma}(z-\\mu)\\biggr]^{-1/\\xi}\\right\\},~-\\infty<z<y^{+}$$ where $y^{+}<\\infty$ in the case $\\xi<0$ . This distribution has location $\\mu$ , scale $\\sigma>0$ and shape $\\xi$ parame­ ters, with the Weibull, Fréchet, and Gumbel distributions corresponding to $\\xi<0$ , $\\xi>0$ and $\\xi\\rightarrow$ 0, respectively. As a model the GEV has the advantage of increased flexibility over choosing one of the three types a priori. The vanilla PoT model is motivated by a result of Pickands (1975) who showed that, under some nonrestrictive regularity conditions, as $u\\to y^{+}$ for $y>u$ , $$F(y)=1-\\operatorname*{Pr}[Y>u]\\operatorname*{Pr}[Y>y\\mid Y>u]\\approx1-\\phi{\\bar{G}}(y)=1-\\phi{\\left[1+{\\frac{\\xi}{\\psi}}(y-u)\\right]}_{+}^{-1/\\xi}$$ where $a_{+}=\\operatorname*{max}{\\{a,0\\}},0\\leq\\phi\\leq1,\\psi$ $\\psi>0$ and $\\bar{G}(y)$ is the survival function of the generalized Pareto distribution. The parameters $\\phi,\\psi$ , and $\\xi$ are known as the rate, scale and shape, respectively; the rate $\\phi=\\operatorname*{Pr}[Y>u]$ is sometimes called the exceedance probability. The value of the shape param­ eter is determined by the rate at which $F(y)\\rightarrow1$ as $y\\to y^{+}$ ; exponential decay corresponds to $\\xi=0$ , and heavy (light) decay to $\\xi>0(\\xi<0)$ . For $\\xi\\ge0$ , there is no finite end-point, while $y^{+}=u-$ $\\sigma/\\xi<\\infty$ if $\\xi<0$ . By assuming that this limit result is a suitable approximation to the behaviour of the exceedan­ ces of a finite, but high, level $\\boldsymbol{\\mathscr{u}}$ , it can be used to motivate a model in which the magnitudes of the threshold exceedances $\\{y_{i}-u:y_{i}>u,i=1,...,n\\}$ are a random sample from the generalized Pareto distribution and the exceedance times a random sample from a homogeneous Poisson pro­ cess. While not essential an assumption of independence is often made about the exceedances. If there is evidence of serial dependence, a routine and theoretically justified approach is to identify all clusters of extremes and model only the frequency and magnitude of the cluster maxima (Davis et al., 2013; Drees, 2008; Laurini $\\&$ Tawn, 2003; Smith & Weissman, 1994). In either case, the parameters $(\\phi,\\psi,\\xi)$ are estimated with standard statistical inference procedures, e.g maximum likelihood or Bayesian inference. A crucial modelling consideration is the choice of threshold $\\boldsymbol{u}$ , which requires a compromise be­ tween retaining a sufficiently large number of exceedances such that estimation of the model pa­ rameters is possible, while still ensuring that the assumption of the limiting approximation is plausible. Various tools are available to aid threshold selection. Most of these use two key thresh­ old stability properties of the generalized Pareto distribution to identify the minimum plausible threshold above which the distribution is a suitable description of the data."
  },
  {
    "qid": "statistic-posneg-1684-4-0",
    "gold_answer": "TRUE. The definition states that for any β ∈ ker X, ‖β_S0‖1 ≤ ρ‖β_S0^c‖1. Since ρ ∈ (0,1), this inequality implies that ‖β_S0‖1 < ‖β_S0^c‖1, meaning the L1 norm on S0 is strictly less than on its complement.",
    "question": "The stable nullspace property with constant ρ ∈ (0,1) implies that for any β ∈ ker X, the L1 norm of β on S0 is strictly less than the L1 norm of β on the complement of S0. Is this statement true based on the given definition?",
    "question_context": "Stable Nullspace Property and Thresholded Basis Pursuit in Model Selection\n\nThe paper discusses the stable nullspace property of a matrix X, which is crucial for the analysis of thresholded basis pursuit (TBP) in model selection. The property is defined as: for any β in the kernel of X, the L1 norm of β restricted to the support S0 is bounded by ρ times the L1 norm of β restricted to the complement of S0. The paper also provides conditions under which the TBP estimator equals the true parameter vector β0 with high probability, given a sufficiently large minimal signal strength and appropriate threshold τ."
  },
  {
    "qid": "statistic-posneg-193-0-2",
    "gold_answer": "FALSE. The convergence result is independent of the specific minimization algorithm used, as long as the algorithm satisfies the complexity and convergence assumptions outlined in the paper. The paper explicitly states that any suitable gradient descent algorithm with complexity $\\delta_{(m)}$ can be used to achieve the result.",
    "question": "Is the convergence result $\\hat{\\pmb{\\theta}}_{y}^{(m_{n})} \\xrightarrow[n\\rightarrow+\\infty]{\\mathbb{P}} \\pmb{\\theta}_{y}^{\\star}$ dependent on the choice of the minimization algorithm?",
    "question_context": "Convergence Rates in Multivariate Expectile Estimation\n\nThe paper discusses the convergence rates for estimating multivariate expectiles using a minimization algorithm. Key formulas include the scaling sequence conditions and the convergence results for the approximated loss function and its gradient. The main theorem states that under certain hypotheses, the estimated parameters converge to the true optimum with a specified rate."
  },
  {
    "qid": "statistic-posneg-1585-0-1",
    "gold_answer": "TRUE. The probability of a child inheriting a GT chromosome from the mother is $(\\frac{1}{2}-\\frac{1}{2}x)$, and the probability of inheriting a $\\mathbf{g^{\\prime}t^{\\prime}}$ chromosome from the father is $(\\frac{1}{2}-\\frac{1}{2}x^{\\prime})$. Since these events are independent, the joint probability is the product of the individual probabilities, as derived in the context. This illustrates the application of Mendelian principles to calculate offspring genotypes considering recombination events.",
    "question": "In the context of Mendelian inheritance, is it correct to say that the probability of a child being of type $\\mathbf{GT/g^{\\prime}t^{\\prime}}$ is given by $(\\frac{1}{2}-\\frac{1}{2}x)(\\frac{1}{2}-\\frac{1}{2}x^{\\prime})$, where $x$ and $x^{\\prime}$ are the recombination fractions in the mother and father, respectively?",
    "question_context": "Mendelian Inheritance and Recombination Fractions in Human Genetics\n\nThis separation is not always made, however, as it is not required for blood transfusion. Thus for statistical purposes it will be necessary to consider $\\mathbf{A_{1}}$ and $\\mathbf{A_{2}}$ as “identical” genes when no attempt has been made to distinguish them in the data under analysis. It is also possible for things which are not genes in the strict sense of the word to be inherited exactly like genes--for example, if a very short segment of chromosome is absent (a deletion), or if there is a group of genes very close together on the chromosome (as in Fisher's theory of the rhesus blood-groups). For most practical purposes such objects can be treated as single genes, and will be considered as such in this paper. <PARAGRAPH_SEPARATOR> # 4. Mendelian Ratios <PARAGRAPH_SEPARATOR> Gametes (sperm and eggs) contain only 24 single chromosomes instead of 24 pairs. Thus a mother having chromosomes $\\pmb{A}^{(1)}$ $\\pmb{\\mathcal{A}}^{(2)}$ ， $B^{(1)}$ ， $B^{(2)}($ $X^{(1)}$ $X^{(2)}$ , will produce an egg with chromosomes $A^{e},B^{e},$ $X^{e}$ ：The chromosome $A^{e}$ will not in general be completely identical with either of the original chromosomes $\\b{A^{(1)}}$ or $\\pmb{A}^{(2)}$ but consists of parts of each (Fig. 1). The process by which this occurs is somewhat complicated, and will not be considered here. But if there is a gene $\\mathbf{G}$ (say) at a given locus on $\\pmb{A^{(1)}}$ , and an allele $\\mathbf{g}$ at the corresponding locus on $\\pmb{\\mathcal{A}}^{(2)}$ then the chromosome $A^{e}$ in the egg may contain either that part of $\\pmb{A}^{(1)}$ which carries $\\mathbf{G}$ or instead that part of $\\pmb{A^{(2)}}$ carrying g, but not both. Experiments show (as originally perceived by Mendel) that these two possibilities are equally likely, so that half the eggs contain $\\mathbf{G}$ , and half contain g; in a fairly obvious notation <PARAGRAPH_SEPARATOR> $$ \\bf{G g}\\rightarrow\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm\\pm. $$ <PARAGRAPH_SEPARATOR> (The arrow will be used to mean “\"produces gametes\" or “\"produces children\" according to context.) <PARAGRAPH_SEPARATOR> If the husband has genotype $\\mathbf{G}^{\\prime}\\mathbf{g}^{\\prime}$ his sperm will similarly be $\\textstyle{\\frac{1}{2}}\\mathbf{G}^{\\prime}+{\\frac{1}{2}}\\mathbf{g}^{\\prime}$ : by union of egg and sperm we see that the children of such a mating are of 4 equally probable types, GG', $\\mathbf{G}\\mathbf{g}^{\\prime}$ gG', $\\mathbf{88}^{\\prime}$ <PARAGRAPH_SEPARATOR> $$ \\mathbf{G}\\mathbf{g}\\times\\mathbf{G}^{\\prime}\\mathbf{g}^{\\prime}\\to(\\mathbf{\\frac{1}{2}}\\mathbf{G}\\mathbf{\\frac{1}{2}}\\mathbf{g})(\\mathbf{\\frac{1}{2}}\\mathbf{G}^{\\prime}+\\mathbf{\\frac{1}{2}}\\mathbf{g}^{\\prime})={\\textstyle\\frac{1}{4}}\\mathbf{G}\\mathbf{G}^{\\prime}+{\\textstyle\\frac{1}{4}}\\mathbf{G}\\mathbf{g}^{\\prime}+{\\textstyle\\frac{1}{4}}\\mathbf{g}\\mathbf{G}^{\\prime}+{\\textstyle\\frac{1}{4}}\\mathbf{g}\\mathbf{g}^{\\prime}. $$ <PARAGRAPH_SEPARATOR> This is the most general case. There is no need for all the genes $\\mathbf{G},\\mathbf{g},\\mathbf{G}^{\\prime},\\mathbf{g}^{\\prime}$ to be different. Ifwetake $\\mathbf{G}^{\\prime}=\\mathbf{g}^{\\prime}=\\mathbf{g}$ the formula becomes that for a backcross mating <PARAGRAPH_SEPARATOR> $$ \\mathbf{Gg}\\times\\mathbf{gg}\\to\\mathbf{\\downarrowGg}+\\mathbf{\\downarrow}\\mathbf{gg}. $$ <PARAGRAPH_SEPARATOR> and other special cases can be readily worked out. <PARAGRAPH_SEPARATOR> Now suppose that the chromosome $\\pmb{A^{(1)}}$ in the mother carries, in addition to the gene G, a gene $\\mathbf{T}$ (say) at another locus--this chromosome is said to be of the type GT. The homologous chromosome $\\pmb{A}^{(2)}$ will carry alleles of $\\mathbf{G}$ and $\\mathbf{T}.$ , say gt (Fig. 1). The mother's genotype is then usually written $\\mathbf{GT/gt}$ or equivalently as $\\mathbf{\\deltagt}/\\mathbf{GT}$ .There will be four possible types of eggs: two of these carry the genes GT and gt respectively on the $A^{e}$ chromosome; these resemble the original chromosomes $\\pmb{\\mathcal{A}}^{(1)}$ and $\\pmb{A^{(2)}}$ of the mother. The other two types are Gt and $\\mathbf{g}\\mathbf{T}_{i}$ which are accordingly named recombinations. The probability that an egg contains a $\\mathbf{G}$ gene is known to be $\\frac{1}{2}$ . and therefore <PARAGRAPH_SEPARATOR> $$ \\mathrm{Prob}\\left(\\mathrm{eggisGT}\\right)+\\mathrm{Prob}\\left(\\mathrm{eggisGt}\\right)=\\ddagger.\\quad. $$ <PARAGRAPH_SEPARATOR> There are three similar equations for $\\mathbf{g},\\mathbf{T}$ and t respectively. Let us call the probability that an egg is Gt $\\cdots+x^{3}$ ; then these equations show that the probabilities of GT, $\\mathbf{gT}_{\\mathrm{~-~}}$ gt are respectively $\\frac{1}{2}-\\frac{1}{2}x,\\frac{1}{2}x,$ and $\\frac{1}{2}-\\frac{1}{2}x$ <PARAGRAPH_SEPARATOR> $$ \\mathbf{GT}/\\mathbf{gt}\\rightarrow(\\frac{1}{2}-\\frac{1}{2}\\gamma)\\mathbf{GT}+\\frac{1}{2}\\chi\\mathbf{Gt}+\\frac{1}{2}\\chi\\mathbf{gT}+(\\frac{1}{2}-\\frac{1}{2}\\gamma)\\mathbf{gt}. $$ <PARAGRAPH_SEPARATOR> The probability of a recombination type, i.e., Gt or $\\mathbf{gT}$ is $\\frac{1}{2}x+\\frac{1}{2}x=x.$ andso $x$ is named the recombination fraction. <PARAGRAPH_SEPARATOR> Similarly a father of genotype ${\\bf G^{\\prime}T^{\\prime}}/{\\bf g^{\\prime}t^{\\prime}}$ produces 4 types of sperm in the proportions <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{{\\bf G}^{\\prime}{\\bf T}^{\\prime}/{\\bf g}^{\\prime}{\\bf t}^{\\prime}\\rightarrow\\left(\\frac{1}{2}-\\frac{1}{2}\\chi^{\\prime}\\right){\\bf G}^{\\prime}{\\bf T}^{\\prime}+\\frac{1}{2}\\chi^{\\prime}{\\bf G}^{\\prime}{\\bf t}^{\\prime}+\\frac{1}{2}\\chi^{\\prime}{\\bf g}^{\\prime}{\\bf T}^{\\prime}+\\left(\\frac{1}{2}-\\frac{1}{2}\\chi^{\\prime}\\right){\\bf g}^{\\prime}{\\bf t}^{\\prime},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\acute{x}$ is the recombination fraction in the male. From these formulae the probabilities of occurrence of different types of children can be readily calculated: thus a child of type $\\mathbf{GT/g^{\\prime}t^{\\prime}}$ must get a GT chromosome from the mother, $\\mathbf{g^{\\prime}t^{\\prime}}$ from the father, and so has a probability $(\\frac{1}{2}-\\frac{1}{2}x)(\\frac{1}{2}-\\frac{1}{2}x^{\\prime})$ . If the genes $\\mathbf{G},\\mathbf{g},\\mathbf{G}^{\\prime},\\mathbf{g}^{\\prime}$ or the genes $\\mathbf{T},\\mathbf{t},\\mathbf{T}^{\\prime},$ $\\mathbf{\\Delta_{t}}\\prime$ are not all different the expressions may simplify. A particularly important case is the double backcross, in which the father is of type $\\mathbf{gt}/\\mathbf{gt}$ .All sperm are of type gt, and so, by (5), the children occur in proportions <PARAGRAPH_SEPARATOR> $$ {\\textstyle(\\frac{1}{2}-\\frac{1}{2}\\gamma)\\mathbf{G}\\mathbf{T}/\\mathbf{g}\\mathbf{t}+\\frac{1}{2}\\chi\\mathbf{G}\\mathbf{t}/\\mathbf{g}\\mathbf{t}+\\frac{1}{2}\\chi\\mathbf{g}\\mathbf{T}/\\mathbf{g}\\mathbf{t}+(\\frac{1}{2}-\\frac{1}{2}\\chi)\\mathbf{g}\\mathbf{t}/\\mathbf{g}\\mathbf{t}}. $$ <PARAGRAPH_SEPARATOR> ![](images/36b9242d63b4caf6f52c624a0593fb0827884a5e02586af8de8d079a51b95215.jpg)  FiG. 1.A diagrammatic representation of the mechanism of chromosomal inheritance, showingamating $\\mathbf{GT/gt}\\times\\mathbf{GT/g^{\\prime}t^{\\prime}}$ givingachild $\\mathbf{Gt}/\\mathbf{G}^{\\prime}\\mathbf{T}^{\\prime}$ <PARAGRAPH_SEPARATOR> The recombination fraction $\\boldsymbol{\\mathfrak{X}}$ is then the expected proportion of children of types $\\mathbf{Gt}/\\mathbf{gt}$ and ${\\bf\\delta g}{\\bf T}/{\\bf g t},$ and no more complicated statistical problem is involved than that of sampling from a binomial distribution. Such a double backcross can usually be arranged in animal or plant breeding. But to depend on it in human genetics would mean at best the rejection of almost all the available data, and would often be entirely impracticable. <PARAGRAPH_SEPARATOR> Other simplified cases can be similarly dealt with: it is only necessary to add together the probabilities of all types which become indistinguishable. The same will hold when dealing with the combination of distinct genotypes which form a single phenotype. It would be a straightforward but lengthy procedure to write down all possible cases. In the mating $\\mathbf{GT/gt}\\times\\mathbf{GT/g^{\\prime}t^{\\prime}}$ each probability is theproduct of a linear function of $\\mathfrak{X}$ and a linear function of $\\textsf{z}^{\\prime}$ .Therefore in any simplified table each probability must be linear in $\\boldsymbol{\\mathsf{\\xi}}$ and in $\\chi^{\\prime}$ (including special cases which do not involve $\\pmb{\\chi}.$ or $\\acute{x}$ , or both)."
  },
  {
    "qid": "statistic-posneg-1977-0-2",
    "gold_answer": "FALSE. The paper states that the model is fit with $n_{I}$ ranging from 3 to 6, and the most appropriate number of ice components for each cell is determined by comparing the fits using the Bayesian Information Criterion (BIC). This approach allows for flexibility in the number of components to account for the diversity of distributions across the ice sheet.",
    "question": "Is the number of ice components $n_{I}$ fixed at 3 for all cells in the ice sheet?",
    "question_context": "Truncated Gaussian Mixture Model for Ice Sheet Temperature Analysis\n\nSince melt occurs when the IST exceeds $0^{\\circ}C$ , positive temperatures are of most concern. For the majority of cells, particularly inland and/or at higher altitudes, there are too few positive observa­tions to take $0^{\\circ}\\mathrm{C}$ as the PoT modelling threshold. However, when taking a modelling threshold below $0^{\\circ}\\mathrm{C}$ , there are problems with the PoT fit, as seen in Figure 4. Many cells have a secondary mode corresponding to a large mass of observations close to $0^{\\circ}\\mathrm{C}$ and a much lower-weighted tail covering small positive temperature values. The most likely explanation of this is that once the temperature exceeds $0^{\\circ}\\mathrm{C}$ the ice melts. This change in state from ice to melt water, which is no longer considered the surface of the ice sheet, creates a soft upper limit close to $0^{\\circ}C$ on ISTs. We cannot determine from the data which of the positive observations are water temperatures and which are due to measurement or recording uncertainty $(\\pm1^{\\circ}\\mathrm{C}$ Hall et al., 2018). Regardless of the reason, the soft upper limit results in a distribution mode close to $0^{\\circ}\\mathrm{C}$ , rendering the generalized Pareto distribution unsuitable. Lastly, we note that the shape of the tail distribu­tion, and especially the behaviour at or near $0^{\\circ}C$ , varies primarily in response to altitude and prox­imity to the coast. <PARAGRAPH_SEPARATOR> The objective now is to build a model to reflect these insights and establish how the melt thresh­old impacts the distribution of ISTs. To facilitate ease of application at all locations, we propose a truncated Gaussian mixture model for the full distribution of ISTs. This circumvents the need to define a tail region—as discussed previously, for this particular data set such a definition is non­trivial due to the varying cross-site behaviour of the upper tail. Utilizing information on behaviour in sub-tail regions should help better identify behaviour close to the melt threshold. Furthermore, a mixture model allows joint modelling of the ice and melt observations; this is critical since we have no information on which category each observation belongs to. <PARAGRAPH_SEPARATOR> The truncated Gaussian mixture model has separate model components for ice and melt temper­atures. Observations are not preassigned to a component, rather the fit provides an estimate of the probability with which each observation belongs to each component. For $n_{I}$ ice components and a single melt component, let $\\phi_{i}$ be the weight associated with ice component $i$ and $\\phi_{M}$ be the weight associated with the melt component, such that $\\begin{array}{r}{\\sum_{i=1}^{n_{I}}\\phi_{i}+\\phi_{M}=1.}\\end{array}$ Let $f_{i}(x)$ and $f_{M}(x)$ be the probabil­ity density functions of the ice $i\\in\\{1,...,n_{I}\\}$ and melt components, respectively, such that for each $i,X\\sim T\\dot{N}(\\mu_{i},\\sigma_{i}^{2},a_{i},b_{i})$ , where $\\mu_{i}$ is the mean, $\\sigma_{i}>0$ the standard deviation, and $a_{i}$ and $b_{i}$ the lower and upper truncation points. Then the mixture model probability density function of IST $x$ is: <PARAGRAPH_SEPARATOR> $$ \\boldsymbol{p}(\\boldsymbol{x})=\\sum_{i=1}^{n_{I}}\\phi_{i}f_{i}(\\boldsymbol{x})+\\phi_{M}f_{M}(\\boldsymbol{x}). $$ <PARAGRAPH_SEPARATOR> Ice temperature components are bounded above by $b=0$ , since these should not exceed $0^{\\circ}\\mathrm{C}$ , and have no lower bound since the only physical lower limit is absolute zero, which is far below the range of the data. For the melt component, there is no upper bound as ISTs are not feasibly phys­ically limited on the ice sheet. The lower bound is set to $-1.65$ as this has previously been estimated as the melting point of saline ice and thereby acts as a conservative minimum estimate for the ice sheet. It follows that data in the range $[-1.65,0]$ can be modelled by either the ice or melt compo­nents, capturing the uncertainty in the true nature of the ice sheet when these observations were measured. The probability of melt $\\rho(x)$ for given IST $x$ is defined as the ratio of the densities of the ice and melt components, such that: <PARAGRAPH_SEPARATOR> $$ \\rho(x)={\\frac{\\phi_{M}\\slash_{M}(x)}{\\phi_{M}\\slash_{M}(x)+\\sum_{i=1}^{n_{I}}\\phi_{i}\\slash_{i}(x)}}. $$ <PARAGRAPH_SEPARATOR> As a result of the model truncation points, $\\rho(x)=0$ below $-1.65^{\\circ}\\mathrm{C}$ and $\\rho(x)=1$ above $0^{\\circ}\\mathrm{C}$ . ISTs between these values vary smoothly within this range. Small discontinuities in the melt probability may arise at $-1.65^{\\circ}\\mathrm{C}$ and $0^{\\circ}\\mathrm{C}$ due to the truncation of the mixture components at these points; the magnitude of the discontinuity depends on the severity of the truncation. To find the most appro­priate number of ice components for the model and allow for the diversity of distributions across the ice sheet, we fit the model with $n_{I}$ from 3 to 6 and compare the fits using Bayesian information criterion (BIC). In contrast, a single melt component is used since, in theory, the melt temperatures should show much less cross-cell variability than the ice temperatures. <PARAGRAPH_SEPARATOR> ![](images/946fbb3e55b278b361ea825e14bfc28d82c86fcb067af7b52bee37d602888c0a.jpg)  Figure 3. Estimated scale parameter annual random effects for each weather station (thin black lines), the annual mean for all stations (central bold red line), and GMST (bold blue line, in degrees). Results cover the period 1950 to 2021 inclusive. <PARAGRAPH_SEPARATOR> We use the Expectation-maximization (EM) algorithm to estimate the parameters $\\phi,\\mu$ and $\\sigma^{2}$ for each model component. We found 800 iterations of the algorithm optimal in terms of both convergence and computational time. To improve the stability of the algorithm, particularly for cells with few or no observations above $-1.65^{\\circ}\\mathrm{C}$ , we set additional restrictions on two param­eters: $\\phi\\ge0.0005$ and $\\sigma\\ge0.35$ . This allows a melt component—however small—to be fitted even if there are no observations in the melt temperature range, and avoids melt components with $\\sigma\\approx0$ since this results in a point-mass density for cells with only a single observation above $-1.65^{\\circ}\\mathrm{C}$ . We apply the model to 1,139 cells across the ice sheet; this represents a subsample of 1 in every 50 available cells in both horizontal and vertical directions. The cells are equally spaced in distance and are taken from those cells identified as ice by the MODIS ice mask. The number and spread of the sub-sampled cells accurately reflects the variety in glaciological and climatological conditions across the ice sheet. A range of average temperatures, surface conditions, latitudes, lon­gitudes, elevations, and distances to the coast are all observed within the sample."
  },
  {
    "qid": "statistic-posneg-584-3-1",
    "gold_answer": "TRUE. The regular variation of $\\phi^{\\leftarrow}$ at infinity with index $-1/\theta_{0}$ and the uniform convergence theorem ensure that the right-hand side of the copula expression converges to $(x_{1}^{-\theta_{0}}+\\cdot\\cdot\\cdot+x_{d}^{-\theta_{0}})^{-1/\theta_{0}}$ as $s\\downarrow0$.",
    "question": "Does the function $\\phi^{\\leftarrow}$ being regularly varying at infinity of index $-1/\theta_{0}$ imply that the copula $C(s x_{1},\\ldots,s x_{d})$ converges to $(x_{1}^{-\theta_{0}}+\\cdot\\cdot\\cdot+x_{d}^{-\theta_{0}})^{-1/\theta_{0}}$ as $s\\downarrow0$ when $0<\theta_{0}<\\infty$?",
    "question_context": "Multivariate Archimedean Copula Tail Behavior Analysis\n\nThe distribution function of $(U_{i})_{i\\in I}$ is given by the $|I|$ -variate copula with generator $\\phi$ . Hence, it suffices to show (3.3) for the case $I=\\{1,\\ldots,d\\}$ . <PARAGRAPH_SEPARATOR> First, suppose $\theta_{0}=0$ . F $\\mathrm{ix}0<\\varepsilon<1.$ . Since $\\phi$ is slowly varying, $\\phi(s\\varepsilon)\\leqslant2\\phi(s)$ and thus $s\\varepsilon\\geqslant\\phi^{\\leftarrow}(2\\phi(s))$ for all sufficiently small $s>0$ . Hence $\\phi^{\\leftarrow}\\{2\\phi(s)\\}=o(s)$ as s $\\downarrow0$ . Denoting $x=x_{1}\\lor\\cdots\\lor x_{d}$ , we arrive at $C(s x_{1},...,s x_{d})\\leqslant\\phi^{\\leftarrow}(d\\phi(s x))\\leqslant$ $\\phi^{\\leftarrow}(2\\phi(s x))=o(s)$ as $s\\downarrow0$ . <PARAGRAPH_SEPARATOR> Second, suppose $0~<~\theta_{0}~<\\infty$ . The function $t\\mapsto f(t)=\\phi(1/t)$ is increasing and regularly varying at infinity of index $\theta_{0}$ . By Bingham et al. [40, Theorem 1.5.12], its inverse, $f^{\\leftarrow}$ is regularly varying at infinity of index $1/\theta_{0}$ . Since $\\phi^{\\leftarrow}=1/f^{\\leftarrow}$ , we find that $\\phi^{\\leftarrow}$ is regularly varying at infinity of index $-1/\theta_{0}$ . Now write <PARAGRAPH_SEPARATOR> $$ s^{-1}C(s x_{1},\\ldots,s x_{d})={\\frac{1}{\\phi^{\\leftarrow}(\\phi(s))}}\\phi^{\\leftarrow}\\left(\\phi(s)\\left\\{{\\frac{\\phi(s x_{1})}{\\phi(s)}}+\\cdots+{\\frac{\\phi(s x_{d})}{\\phi(s)}}\right\\}\right). $$ <PARAGRAPH_SEPARATOR> Since $\\phi(s)\to\\infty\\mathrm{a}s s\\downarrow0$ and by the uniform convergence theorem for regularly varying functions [40, Theorem 1.5.2], the right-hand side of the previous display converges to $(x_{1}^{-\theta_{0}}+\\cdot\\cdot\\cdot+x_{d}^{-\theta_{0}})^{-1/\theta_{0}}$ as $s\\downarrow0$ . <PARAGRAPH_SEPARATOR> Finally, suppose $\theta_{0}=\\infty$ . Denote $m=x_{1}\\wedge\\cdots\\wedge x_{d}$ . We have <PARAGRAPH_SEPARATOR> $$ s^{-1}\\phi^{\\leftarrow}(d\\phi(s m))\\leqslant s^{-1}C(s x_{1},\\ldots,s x_{d})\\leqslant m. $$ <PARAGRAPH_SEPARATOR> Fix $0~<~\\lambda~<~1$ . Since $\\phi$ is regularly varying at zero with index $-\\infty$ we have $\\phi(\\lambda s m)~\\geqslant~d\\phi(s m)$ and thus $\\lambda m\\leqslant$ $s^{-1}\\phi^{\\leftarrow}(d\\phi(s m))$ for all sufficiently small $s>0$ . Let $\\lambda$ increase to one to see that $\begin{array}{r}{\\operatorname*{lim}_{s\\downarrow0}s^{-1}C(s x_{1},\\dots,s x_{d})=m}\\end{array}$ This finishes the proof of Theorem 3.1. □"
  },
  {
    "qid": "statistic-posneg-758-0-2",
    "gold_answer": "TRUE. The paper shows that λ_U = 0 for the specified bivariate PH distribution, indicating tail independence due to the asymptotic behavior of the survival function.",
    "question": "Is the upper tail dependence coefficient λ_U always zero for the bivariate PH distribution with the given structure?",
    "question_context": "Bivariate PH Distribution and EM Algorithm Estimation\n\nThe paper discusses the estimation of bivariate phase-type (PH) distributions using EM algorithms. It presents a special form of bivariate PH distribution with an explicit density, allowing for tail independence analysis and parameter estimation. The EM algorithm is detailed with conditional expectations and steps for parameter updates."
  },
  {
    "qid": "statistic-posneg-458-0-0",
    "gold_answer": "TRUE. The paper derives that under the assumption of jointly normal inputs, the linear regression coefficients are proportional to the parameters α_j in the nonlinear model, provided that cov{G(U), U} ≠ 0. This is shown in equations (1.3) and (1.6), where the covariance between Y and X_k is expressed as a sum involving α_j and cov{G(U), U}/var(U).",
    "question": "In the context of the paper, is it true that the linear regression coefficients of Y on X are proportional to the parameters α_j in the nonlinear model Y = μ + G(Σα_jX_j) + ε when the inputs X_j are jointly normal?",
    "question_context": "Nonlinear Time Series System Identification with Gaussian Inputs\n\nThe paper discusses the identification of nonlinear time series systems with Gaussian inputs. It presents a model where the output series Y(t) is generated by a nonlinear transformation G of a linear combination of input series X(t), plus noise. The key results involve relationships between covariances and cumulants of the input and output series, which are used to identify the system parameters. The paper also considers the case where the input series are jointly normal, showing that linear regression coefficients are proportional to the parameters of a broader class of nonlinear models."
  },
  {
    "qid": "statistic-posneg-279-3-1",
    "gold_answer": "FALSE. The formula correctly includes $(\\mathbf{I} - \\mathbf{R})^{-1}$ to account for the infinite series of possible transitions before signaling, weighted by the steady-state probabilities $\\pi_s$. The subtraction of the identity matrix is not present in the ATS formula but appears in the AATS (Average Adjusted Time to Signal) formula as $\\{(\\mathbf{I} - \\mathbf{R})^{-1} - \\frac{\\mathbf{I}}{2}\\}$.",
    "question": "Does the steady-state ATS calculation formula $\\textsf{ATS} = \\pi_s' \\cdot (\\mathbf{I} - \\mathbf{R})^{-1} \\cdot \\mathbf{t}$ incorrectly include the identity matrix subtraction?",
    "question_context": "Adaptive CUSUM Control Chart with Variable Sampling Intervals\n\nThe paper discusses the design and performance of Variable Sampling Interval (VSI) Adaptive CUSUM control charts for detecting shifts in process mean. It introduces a Markovian model for computing Average Run Length (ARL) and Average Time to Signal (ATS) metrics, utilizing transition probabilities between states defined by the current CUSUM statistic and estimated process mean. Key formulas include the transition probability calculations and the steady-state ATS evaluation."
  },
  {
    "qid": "statistic-posneg-1405-0-0",
    "gold_answer": "FALSE. The condition (b) is incorrectly stated. The correct condition should involve the orthonormal basis functions being uniformly bounded and the tail sum of the squared coefficients being positive, but the Lebesgue measure condition for 's' is not relevant. The strict inequality arises from the properties of the orthogonal series and the behavior of the coefficients, not directly from the Lebesgue measure of 's'.",
    "question": "Is the condition for the inequality in (4.1) to become strict correctly stated as requiring either (a) a positive Lebesgue measure for the set S or (b) uniformly bounded orthogonal polynomials with certain properties?",
    "question_context": "Nonparametric Density Estimation via Orthogonal Series\n\nThe inequality in (4.1) becomes strict in either of the following two cases: (a) $S=\\left\\{x;\\sum_{0}^{N}a_{i}\\varPhi_{i}(x)<0\\right\\}$ has a positive Lebesgue measure; (b). $s$ has Lebesgue measure O, $\\begin{array}{r}{\\sum_{N+1}^{2N}a_{i}^{2}>0}\\end{array}$ and $\\pmb{\\varPhi}_{i}(x)=\\sqrt{\\phi(x)}Q_{i}(x)$ are uniformly bounded, where $\\phi$ is $\\pmb{a}$ density with support $\\pmb{A}$ and $Q_{i}$ is $^{a}$ polynomial of degree i."
  },
  {
    "qid": "statistic-posneg-1914-0-2",
    "gold_answer": "FALSE. The paper explicitly mentions that simulating a correlation matrix from (A.11) is difficult and recommends using the Parameter-Extended MH (PX-MH) algorithm, not a Gibbs sampler.",
    "question": "For the full conditional distribution of the correlation matrix Φ_k, the paper suggests using a standard Gibbs sampler. Is this statement accurate?",
    "question_context": "Bayesian Mixture Models with Latent Variables: Full Conditional Distributions and MH Algorithm\n\nThe paper presents a Bayesian approach to mixture models with latent variables, detailing the full conditional distributions and the Metropolis-Hastings (MH) algorithm for sampling. The context includes complex mathematical formulas for the full conditional distributions of parameters like Ω, ζ, μ_k, A_k, Ψ_k, Π_k, Φ_k, α_k, Y*, and δ. The MH algorithm is described with proposal distributions and acceptance probabilities for generating samples from these non-standard distributions."
  },
  {
    "qid": "statistic-posneg-2121-13-2",
    "gold_answer": "FALSE. The statistic T provides a more tangible measure of non-representativeness than the product β̂σ̂, as it directly compares the sample mean of the observations with the integrated predicted values over the study region, offering a clearer interpretation of non-representativeness.",
    "question": "Is the product β̂σ̂ a more tangible measure of non-representativeness than the statistic T = n^{-1}∑_{i=1}^{n}Y_i - |A|^{-1}∫Ŝ(x)dx?",
    "question_context": "Geostatistical Inference and Preferential Sampling\n\nProfessor Besag’s more fundamental point, which was also raised in different ways by Fuentes and by Rue, Martino, Mondal and Chopin, is whether a stationary Gaussian process with Matérn correlation structure is appropriate at all. Within the Gaussian process framework, the most important practical difference between stationary and intrinsic models is that, when data are irregularly distributed over the spatial region of interest, the predictions from stationary models are mean reverting in sparsely sampled areas, whereas those from intrinsic models are not. Which of these properties is preferable could be context dependent. On balance, we agree that, in the absence of explanatory variables, allowing a global mean to have a major influence on local predictions is a questionable strategy. When explanatory variables are available, we are less convinced. <PARAGRAPH_SEPARATOR> Professor Myers questions whether any form of Gaussian process is an appropriate model for geostatistical data. Our counter to this is that we think of the model-based approach as a device through which we determine a principled solution to a prediction problem. The Gaussian assumption leads naturally to linear prediction, whereas, in the classical geostatistical approach that is known as ordinary kriging, linearity is imposed as an a priori constraint. In similar vein, a model-based way to answer Professor Scott’s request for a ‘measure of representativeness is to derive the score statistic for the relevant parameter. For our model, if we assume that all parameters other than $\\beta$ are known, the score statistic to test $\\beta=0$ is <PARAGRAPH_SEPARATOR> Fig. 16. (a) Empirical and (b) fitted (by maximum likelihood, assuming non-preferential sampling) variograms for the Galicia 1997 data: the different curve styles correspond to four different imputations to replace the two outliers (see the text for details) <PARAGRAPH_SEPARATOR> $$ T=n^{-1}\\sum_{i=1}^{n}Y_{i}-|A|^{-1}\\int\\hat{S}(x)\\mathrm{d}x. $$ <PARAGRAPH_SEPARATOR> This has an obvious interpretation beyond the model assumed. It does not provide a useful test for preferential sampling because, as shown explicitly by Professor Anderson, when $\\beta\\neq0$ fitting under the incorrect assumption that $\\beta=0$ leads to biased estimation of the mean. However, it provides a more tangible measure of non-representativeness than does the product $\\hat{\\beta}\\hat{\\sigma}$ . <PARAGRAPH_SEPARATOR> Professor Myers’s comment about non-point support is well taken, but we reply that, if the spatially continuous process $S(\\cdot)$ is Gaussian, then so is any spatially averaged process of the form $T(x)=$ $\\begin{array}{r}{\\int w(u)S(x-\\mathbf{\\dot{\\mu}}u)\\mathrm{d}u}\\end{array}$ . Whether one should then derive the correlation structure of $T(\\cdot)$ from an assumed correlation structure for $S(\\cdot)$ and a specified weighting function $w(\\cdot)$ or, more pragmatically, specify the assumed correlation structure of $T(\\cdot)$ directly is open to debate in view of the empirical status of most geostatistical models. <PARAGRAPH_SEPARATOR> Somewhat analogous to the non-point support problem is the aggregated data problem that was raised by Professor Gelfand and Professor Chakraborty, in which observations $Y_{i}$ relate to each of $n$ subregions $A_{i}$ that together form a partition of the study region. Most applications involving data in this form treat the study region as discrete and use Markov random-field models for the vector $Y=(Y_{1},\\dots,Y_{n})$ (Rue and Held, 2005). An alternative is to use a spatially continuous latent process $S(\\cdot)$ ; for example, in an epidemiological setting where the $Y_{i}$ are disease counts a starting point might be to assume that, conditional on $S(\\cdot)$ , the $Y_{i}$ are independent Poisson variates with conditional means $\\begin{array}{r}{\\mu_{i}=N_{i}\\int_{A_{i}}S(x)\\mathrm{{d}}x}\\end{array}$ , where $N_{i}$ denotes the number of people at risk in the subregion $A_{i}$ . Note, however, that both approaches are susceptible to aggregation bias if spatial variation in population density within subregions is correlated with spatial variation in disease risk (Diggle and Elliott, 1995)."
  },
  {
    "qid": "statistic-posneg-1863-0-1",
    "gold_answer": "FALSE. The paper explicitly states that simple kriging, while being the minimum mean squared error linear predictor, is suboptimal for truncated data. The conditional expectation provides a more accurate predictor, as it accounts for the truncation and the full conditional distribution of the data, not just the first two moments.",
    "question": "Does the paper suggest that simple kriging is always the optimal predictor for truncated spatial data?",
    "question_context": "Comparison of Optimal Predictors for Truncated Spatial Data\n\nThe paper compares different predictors for truncated spatial data, including simple kriging, conditional expectation, and indicator cokriging. It provides formulas for the expectation and variance of truncated normal variables, as well as the covariance between two truncated normal variables. The paper also discusses the limitations of linear predictors like simple kriging and the advantages of using conditional distributions for more accurate predictions."
  },
  {
    "qid": "statistic-posneg-2107-0-3",
    "gold_answer": "FALSE. The context distinguishes between arrows representing free parameters in the model: bidirectional error arrows (E1 ↔ E2) denote correlated errors (ψ12 ≠ 0), while bidirectional arrows between endogenous variables (Y1 ↔ Y2) would imply a feedback loop (non-zero entries in both B12 and B21). These have distinct implications: the former affects only the error covariance ψ, while the latter modifies the structural coefficient matrix B. The paper’s Example I highlights this by showing that correlated errors (E1 ↔ E2) alone do not create a directed cycle among Y variables.",
    "question": "In the path diagram representation of an LSEM, a bidirectional arrow between two error terms (e.g., E1 ↔ E2) is mathematically equivalent to a bidirectional arrow between the corresponding endogenous variables (Y1 ↔ Y2). Is this interpretation valid?",
    "question_context": "Linear Structural Equation Models and Path Diagrams\n\nThe paper discusses linear structural equation systems for Gaussian variables, their associated path diagrams, and the set of probability distributions defined by such systems. It introduces the LISREL-type specification of a linear structural equations system and explains the relationships between observed and unobserved variables, error terms, and the covariance structure. The paper also covers the concept of recursive and non-recursive systems, path diagrams, and the implications of directed cycles in such diagrams."
  },
  {
    "qid": "statistic-posneg-1338-0-0",
    "gold_answer": "FALSE. The condition $B_{1}V = B_{1}^{2}V$ alone is not sufficient to conclude $u = 0$. The full equivalence requires additional conditions such as $\\mathcal{C}(B_{1}V) \\subseteq \\mathcal{C}(V)$ and $\\text{Rank } B_{1}V + \\text{Rank } B_{2}V = \\text{Rank } V$ (condition (iv) and (v)), which are not implied solely by $B_{1}V = B_{1}^{2}V$.",
    "question": "Is the statement 'If $B_{1}V = B_{1}^{2}V$, then $u = 0$' true based on the equivalence conditions provided in the context?",
    "question_context": "Properties of BLUE in a Linear Model and Canonical Correlations\n\n(b) The following statements are equivalent: <PARAGRAPH_SEPARATOR> (i) ${\\pmb u}={\\bf0},$ (ii) $B_{1}V=B_{1}^{2}V=V V^{-}B_{1}V.$ (i) $\\left(I-V V^{-}\\right)B_{1}V=B_{2}B_{1}V=0,$ (iv) $B_{1}V=B_{1}^{2}V a n d\\mathcal{C}(B_{1}V)\\subseteq\\mathcal{C}(V), $ (v) Rank $B_{1}V+1$ Rank $B_{2}V=\\mathsf{R a n k}V,$ (vi) $B_{i}V=B_{i}V V^{-}B_{i}V$ for $i=1,2.$ (vi) $B_{i}V V^{-}B_{j}V{=}0$ for all $i\\neq j,i,j=1,2,$ where $\\mathcal{C}(P)$ denotes the linear vector space spanned by the column vectors of $P$"
  },
  {
    "qid": "statistic-posneg-868-0-0",
    "gold_answer": "TRUE. The paper explicitly states that under these conditions, GEE-assisted forward regression in combination with SIC is screening consistent for the true spatial GLLVM. This means that with probability tending to one, the method selects a model containing all truly nonzero predictors, even if the working correlation matrix R is misspecified. The choice of τ = log(N)loglog(p) is shown to satisfy the necessary conditions for screening consistency, unlike simpler penalties like τ = 2 (AIC) or τ = log(N) (BIC).",
    "question": "Is it true that the Score Information Criterion (SIC) proposed in the paper guarantees screening consistency for the true spatial GLLVM when the penalty parameter τ is set to log(N)loglog(p), under the conditions p = o(N^(1/3)), τ = o(N^(1/2)), and log(p) = o(τ)?",
    "question_context": "GEE-Assisted Forward Regression with Score Information Criterion for Model Selection\n\nand $\\mathcal{M}^{(k)}=\\mathcal{M}^{(1)}\\cup\\{\\hat{l}_{1},...,\\hat{l}_{k-1}\\}$ for $k=2,\\ldots,m(p-1)$ with $\\hat{l}_{k}$ being the predictor chosen for inclusion in the $k$ -step. Note that at each step in the forward regression path, we only require estimates from the most recent GEE fit to determine which candidate predictor to include next, as judged by the score statistic. This contrasts to, say, using the Wald or some form of a pseudo-likelihood ratio statistic, which would require estimating the parameters for each of the alternative models. Finally, in the special case where IEEs are used to estimate candidate models, the score statistic is modified as follows: for step 2 in Algorithm 1, we set $S\\{\\hat{\\alpha}(\\mathcal{M})\\}=X_{\\mathcal{M}^{c}}^{\\top}\\hat{W}(\\mathcal{M})\\hat{A}^{-1}(\\mathcal{M})\\{y-$ $\\hat{\\pmb{\\mu}}(\\mathcal{M})\\}$ and ${\\mathcal{Z}}\\{{\\hat{\\pmb{\\alpha}}}({\\mathcal{M}})\\}=X_{{\\mathcal{M}}^{c}}^{\\top}{\\hat{W}}({\\mathcal{M}}){\\hat{\\pmb{A}}}^{-1}({\\mathcal{M}}){\\hat{\\pmb{W}}}({\\mathcal{M}})X_{{\\mathcal{M}}^{c}}$ The remainder of the forward regression procedure remains the same. <PARAGRAPH_SEPARATOR> # 4.1. Score Information Criterion <PARAGRAPH_SEPARATOR> To select a model on the forward regression path, as well as make use of an early stopping rule, we propose minimizing a modified version of the score information criterion (SIC, Stoklosa, Gibb, and Warton 2014). At model $\\mathcal{M}^{(k)}\\in\\mathbb{M}$ , define the SIC as <PARAGRAPH_SEPARATOR> $$ \\mathrm{SIC}(\\mathcal{M}^{(k)})=-\\sum_{k^{\\prime}=1}^{k}\\mathcal{U}_{\\hat{l}_{k^{\\prime}}}\\{\\hat{\\alpha}(\\mathcal{M}^{(k^{\\prime})})\\}+\\tau\\mathrm{dim}(\\mathcal{M}^{(k)}), $$ <PARAGRAPH_SEPARATOR> where $\\tau{}>0$ is a model complexity penalty. We select the best model as $\\begin{array}{r}{\\hat{\\mathcal{M}}~=~\\arg\\operatorname*{min}_{\\mathcal{M}^{(k)}}\\mathrm{SIC}(\\mathcal{M}^{(k)})}\\end{array}$ . The SIC is a reasonable choice as it builds on the score statistics computed as part of constructing the GEE-assisted forward regression path in Algorithm 1, and thus requires little added computation. The first component in SIC is the cumulative sum of the score statistics as we progress along the forward regression path, while the second component in SIC penalizes for the corresponding increase in the degree of model complexity. While Stoklosa, Gibb, and Warton (2014) considered AIC- (which uses a penalty of 2) and BIC- (which uses a penalty of $\\log(N)$ type model complexity penalties, here we use stronger penalties that are appropriate in the setting where $\\boldsymbol{p}$ grows with $n$ . Specifically, in supplementary material A we show that under the same setting used to established slope consistency with $p~=~o(N^{1/3})$ , and assuming the conditions $\\tau~=~o(N^{\\dot{1}/2})$ and $\\log(p)~=~o(\\tau)$ , then GEE-assisted forward regression in combination with SIC is screening consistent for the true spatial GLLVM. That is, if the underlying spatial GLLVM is sparse with only some of the elements in each of the $\\beta_{j}\\mathrm{^s}$ being truly nonzero (note the number and location of the truly nonzero coefficients can differ across the $m$ responses), then with probability tending to one GEEassisted forward regression using SIC is guaranteed to select a model containing at least all of these important predictors. Moreover, this screening consistency holds even if the structure of the working correlation matrix $\\pmb R$ is misspecified. <PARAGRAPH_SEPARATOR> In the simulations and application, we set $\\tau=\\log(N)\\operatorname{l}$ og $\\log(p)$ (similar to Wang, Li, and Leng 2009). Note the conditions imposed on $\\tau$ mean that choosing $\\tau=\\log(N)$ for the model complexity penalty, as characterizes the BIC, does not guarantee screening consistency. Indeed, in the simulations below we also tested two other choices of the model complexity parameter in $\\tau{}=2$ (as characterizes the AIC) and $\\tau~=~\\log(N)$ , and perhaps not surprisingly found both performed worse than our proposed choice of $\\tau~=~\\log(N)\\log\\log(p)$ . At the same time however, we stress this particular choice of $\\tau$ is by no means definitive; other, perhaps more data driven choices are possible (see for instance Bai, Rao, and Wu 1999, in a different context using information criteria), although they would likely increase detract from the computational efficiency of the proposed approach. We offer a more detailed discussion regarding the choice of $\\tau$ in Supplementary Material C. <PARAGRAPH_SEPARATOR> Finally, as seen in Algorithm 1, note that SIC can also be used as an early stopping rule to further reduce computation time. Specifically, in Step 3 of Algorithm 1, we only include a new predictor and proceed on the forward solution path if it reduces the current value of SIC. Otherwise, we terminate the solution path early, and select the last fitted candidate GEE. We employ this approach in both our simulations and application below. It is important to emphasize that the use of an early stopping rule based on SIC is theoretically justified, in the sense that it asymptotically still guarantees screening consistency. We provide further discussion of this, as well as empirical studies to support the SIC-based early stopping rule, in supplementary material B."
  },
  {
    "qid": "statistic-posneg-91-0-2",
    "gold_answer": "TRUE. The monotonicity assumption $(D(1)\\geq D(0))$ implies that no patient would receive the experimental treatment if assigned to the standard treatment but not if assigned to the experimental treatment (i.e., no defiers). Defiers are defined as patients with $D(1)=0$ and $D(0)=1$, which directly violates the monotonicity assumption. The context explicitly states that the monotonicity assumption is plausible because defiers are unlikely in practice, as they would refuse the proffered treatment and not participate in the study.",
    "question": "Is the monotonicity assumption $(D(1)\\geq D(0))$ violated if there are defiers in the population?",
    "question_context": "Proportion Ratio Estimation in Non-Compliance RCT with MAR Outcomes\n\nIn this paper, we focus the discussion on estimation of the $P R$ under a non-compliance RCT with outcomes missing at random (MAR). Using an approach similar to that proposed elsewhere (Brunner and Neumann, 1985; Bernhard and Compagnone, 1989), we derive in Sections 2.1–2.3 the maximum likelihood estimator (MLE) for the $P R$ under various assumptions, including the model in which there is no missingness, the model in which missingness depends on only assigned treatments, and the model in which missingness depends on both assigned and received treatments. In Section 2.4, we develop three asymptotic interval estimators in closed form for the $P R$ under the model in which missingness depends on assigned and received treatments. We use Monte Carlo simulation to study the performance of point estimators developed in Section 3.1 and the performance of the three asymptotic interval estimators in Section 3.2. We note that the point estimator for missing completely at random (MCAR) can be subject to a serious bias under various models with MAR. By contrast, we note that the point estimators developed in this paper can perform well when the number of patients per assigned treatment is large. Further, we find that the asymptotic interval estimator using Wald’s statistic tends to lose accuracy with respect to the coverage probability, while the interval estimator using the logarithmic transformation (Katz et al., 1978; Fleiss, 1981; Lui, 2002) tends to lose precision with respect to the average length. Finally, we note that an interval estimator, formed by an ad hoc combination of the previous two estimators, can perform well in almost all situations considered here. We apply the data taken from a multiple risk factor intervention trial for reducing the mortality of coronary heart disease (Multiple Risk Factor Intervention, 1982) in Section 3.3 to illustrate the bias of the point estimator for MCAR, and the use of the above point and interval estimators under the assumed MAR."
  },
  {
    "qid": "statistic-posneg-1038-0-0",
    "gold_answer": "TRUE. The probability mass function for the ZINB model is given by: $$P r(Y_{i}=y_{i})=\\left\\{\\begin{array}{l l}{p_{i}+(1-p_{i})\\Big(\\displaystyle\\frac{\\phi}{\\mu_{i}+\\phi}\\Big)^{\\phi},}&{y_{i}=0,}\\ {(1-p_{i})\\displaystyle\\frac{T(\\phi+y_{i})}{T(y_{i}+1)T(\\phi)}\\Big(\\displaystyle\\frac{\\mu_{i}}{\\mu_{i}+\\phi}\\Big)^{y_{i}}\\Big(\\displaystyle\\frac{\\phi}{\\mu_{i}+\\phi}\\Big)^{\\phi},}&{y_{i}=1,2,\\dots,}\\end{array}\\right.$$ For yi = 0, the probability includes both the zero-inflation term pi and the Negative Binomial term (1-pi)(φ/(μi+φ))^φ, which accounts for the probability of a zero count from the Negative Binomial distribution.",
    "question": "In the ZINB regression model, the probability mass function for a zero count includes both the zero-inflation component and the Negative Binomial component. Is this statement true based on the provided formula for Pr(Yi = yi)?",
    "question_context": "Zero-Inflated Negative Binomial (ZINB) Regression Model and Influence Diagnostics\n\nThe paper discusses the Zero-Inflated Negative Binomial (ZINB) regression model, which is used for modeling count data with overdispersion and excess zeros. The model's probability mass function, log-likelihood function, and estimation methods including Maximum Likelihood (ML) estimation, EM algorithm, jackknife, and bootstrap methods are presented. The paper also covers sensitivity analysis through global and local influence diagnostics."
  },
  {
    "qid": "statistic-posneg-2140-11-0",
    "gold_answer": "FALSE. The formula appears to be a variant of the Benjamini-Hochberg (BH) procedure for controlling the False Discovery Rate (FDR), but it is not standard. The correct BH procedure would compare each ordered p-value $p_{(i)}$ to $(i/m) \\cdot q$, where $m$ is the total number of tests and $q$ is the desired FDR level, then find the largest $i$ where $p_{(i)} \\leq (i/m) \\cdot q$. The given formula seems to be an ad-hoc combination method rather than the standard FDR control procedure.",
    "question": "Is the formula $p_{0}=4k\\sum_{i=1}^{4k}i^{-1}\\operatorname*{min}_{i=1,\\ldots,4k}p_{(i)}/i$ correctly representing the FDR-based combination of p-values from multiple tests?",
    "question_context": "Random Projection-Based Test of Gaussianity: FDR p-value Combination\n\nThe $p$ -value <PARAGRAPH_SEPARATOR> Finally, we have $p^{(i,j)}$ , $i=1,\\ldots,k,j=1,\\ldots,4$ that we combine using the FDR to obtain a $p$ -value for the global procedure, $p_{0}$ . For that we first order the $p$ -values increasingly $p_{(1)}\\leq\\cdots\\leq p_{(4k)}.$ Thus, <PARAGRAPH_SEPARATOR> $$ p_{0}=4k\\sum_{i=1}^{4k}i^{-1}\\operatorname*{min}_{i=1,\\ldots,4k}p_{(i)}/i. $$ <PARAGRAPH_SEPARATOR> # 5. An empirical study <PARAGRAPH_SEPARATOR> We analyze two real datasets: the Canadian lynx and the Wolfer sunspot, which were previously found to be nonGaussian (Rao and Gabr, 1980; Epps, 1987). <PARAGRAPH_SEPARATOR> The Canadian lynx data consists in the annual record of the number of lynxes trapped in the Mackenzie River district of the North-West Canada for the period from 1821 to 1934 while the Wolfer sunspot data consists in the annual record of the sunspot activity in the period from 1700 to 1960. We have applied the $\\phantom{}_{1}\\mathrm{RP}$ -test to both, with the $p$ -values displayed in Table 8 together with those obtained in previous studies. The similarity between all of them is quite clear."
  },
  {
    "qid": "statistic-posneg-877-2-1",
    "gold_answer": "FALSE. The EM algorithm described in Algorithm 4 does not necessarily guarantee convergence to the global maximum of the likelihood function. While the EM algorithm is known to converge to a local maximum, the likelihood function for bivariate matrix-Pareto distributions can be non-convex and may have multiple local maxima. The algorithm's convergence to the global maximum depends on the initialization of the parameters $(\\pmb{\\alpha}, \\pmb{T}, \\pmb{\\beta})$ and the specific structure of the likelihood surface. Practical implementations often use multiple random initializations to mitigate this issue.",
    "question": "Does the EM algorithm described in Algorithm 4 guarantee convergence to the global maximum of the likelihood function for the bivariate matrix-Pareto distribution?",
    "question_context": "Parameter Estimation for Bivariate Matrix-Pareto Distributions\n\nIn the sequel we will provide an algorithm for parameter estimation for which an explicit expressions of the bivariate density is needed. We therefore restrict it to the bivariate case. <PARAGRAPH_SEPARATOR> # 4.2 Parameter estimation in the bivariate case <PARAGRAPH_SEPARATOR> In the bivariate case, for any ${\\pmb Y}\\sim\\mathrm{{MPH}}^{*}({\\pmb\\pi},{\\pmb T},{\\pmb R})$ with parameters (5), it is easy to see that the density of $\\pmb{X}=(g_{1}(Y^{(1)}),g_{2}(Y^{(2)}))^{\\prime}$ is given by <PARAGRAPH_SEPARATOR> $$ f_{X}(x^{(1)},x^{(2)})=\\alpha\\mathrm{e}^{T_{11}g_{1}^{-1}(x^{(1)})}T_{12}\\mathrm{e}^{T_{22}g_{2}^{-1}(x^{(2)})}(-T_{22})e~{\\frac{1}{g_{1}^{\\prime}(g_{1}^{-1}(x^{(1)}))g_{2}^{\\prime}(g_{2}^{-1}(x^{(2)}))}}. $$ <PARAGRAPH_SEPARATOR> If we assume that $g_{j}(\\cdot;\\beta_{j})$ is a parametric nonnegative function depending on the vector $\\beta_{j}$ , $j=1,2$ , and let $\\beta=(\\beta_{1},\\beta_{2})$ . Then, we can formulate an algorithm analogous to Algorithm 1: <PARAGRAPH_SEPARATOR> Algorithm 4. (EM algorithm for bivariate inhomogeneous $\\mathrm{MPH^{*}}$ distributions) <PARAGRAPH_SEPARATOR> 0. Initialize with some “arbitrary” $(\\pmb{\\alpha},\\pmb{T},\\pmb{\\beta})$ . 1. Transform the data into $y_{i}^{(j)}:=g_{j}^{-1}(x_{i}^{(j)};\\beta_{j}),i=1,\\dots,N,j=1,2,$ , and apply the $E$ - and M-steps of Algorithm 3 by which we obtain the estimators $(\\hat{\\pmb{\\alpha}},\\hat{\\pmb{T}})$ . <PARAGRAPH_SEPARATOR> 2. Compute <PARAGRAPH_SEPARATOR> $$ \\hat{\\beta}=\\arg\\operatorname*{max}_{\\beta}\\sum_{i=1}^{N}\\log(f_{X}(x_{i}^{(1)},x_{i}^{(2)};\\hat{\\alpha},\\hat{T},\\beta)). $$ <PARAGRAPH_SEPARATOR> 3. Assign $(\\pmb{\\alpha},\\pmb{T},\\pmb{\\beta})=(\\hat{\\pmb{\\alpha}},\\hat{\\pmb{T}},\\hat{\\pmb{\\beta}})$ and GOTO 1. <PARAGRAPH_SEPARATOR> We now consider particular multivariate distributions obtained through such a transformation of an $\\mathrm{MPH^{*}}$ random vector. <PARAGRAPH_SEPARATOR> # 4.3 Multivariate matrix–Pareto models <PARAGRAPH_SEPARATOR> Let $X=(g_{1}(Y^{(1)}),\\dots,g_{d}(Y^{(d)}))^{\\prime}$ , where ${\\pmb Y}\\sim\\mathrm{{MPH}}^{*}({\\pmb\\pi},{\\pmb T},{\\pmb R})$ and <PARAGRAPH_SEPARATOR> $$ g_{j}(y)=\\beta_{j}(e^{y}-1),\\quad\\beta_{j}>0,j=1,\\dots,d. $$ <PARAGRAPH_SEPARATOR> Then we say that $\\pmb{X}$ follows a multivariate matrix–Pareto distribution. Some special properties of this class of distributions are: <PARAGRAPH_SEPARATOR> 1. Marginal distributions are matrix–Pareto distributed.   2. Moments and cross–moments of $\\pmb{X}$ can be obtained from the moment generating function of Y (see (Bladt & Nielsen, 2017, Theorem 8.1.2)), provided that they exist.   3. Products of the type $\\prod_{i=1}^{M}{(X^{(j_{i})}/\\beta_{j_{i}}+1)^{a_{i}}}$ are matrix–Pareto distributed, $a_{i}>0,j_{i}\\in\\{1,\\ldots,d\\}.$ , $i=1,\\ldots,M,M{\\le}N.$ ,  since linear combinations of $\\mathbf{Y}$ are PH distributed. <PARAGRAPH_SEPARATOR> In the bivariate case, (8) and (7), lead to <PARAGRAPH_SEPARATOR> $$ f_{X}(x^{(1)},x^{(2)})=\\alpha\\Biggl(\\frac{x^{(1)}}{\\beta_{1}}+1\\Biggr)^{T_{11}-I}T_{12}\\Biggl(\\frac{x^{(2)}}{\\beta_{2}}+1\\Biggr)^{T_{22}-I}(-T_{22})e\\frac{1}{\\beta_{1}\\beta_{2}}. $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\overline{{F}}_{X}(x^{(1)},x^{(2)})=\\alpha(-T_{11})^{-1}\\left(\\frac{x^{(1)}}{\\beta_{1}}+1\\right)^{T_{11}}T_{12}\\left(\\frac{x^{(2)}}{\\beta_{2}}+1\\right)^{T_{22}}e. $$ <PARAGRAPH_SEPARATOR> Moreover, in this bivariate case, linear combinations of $X^{(i)}$ are regularly varying, and the respective index is the real part of the eigenvalue with largest real part of the sub-intensity matrices of the marginals, which follows from (Davis & Resnick, 1996, Lem. 2.1) and asymptotic independence of $\\mathbf{Y}.$ . The general case is not clear since the condition of asymptotic independence does not hold. <PARAGRAPH_SEPARATOR> Remark 3. The Marshall–Olkin Pareto distribution (see Hanagal (1996a)) is a particular case of this class of distributions. <PARAGRAPH_SEPARATOR> # 4.3.1 Parameter estimation"
  },
  {
    "qid": "statistic-posneg-1519-0-3",
    "gold_answer": "TRUE. The invertibility of the matrix $(\\mathbf{B^{\\prime}B}+\\delta\\mathbf{P}^{0}+\\gamma\\mathbf{P}^{1}+\\lambda\\mathbf{P}^{2})$ is assessed by the condition number, which is the ratio of the largest to smallest eigenvalue. If this ratio exceeds $10^{10}$, the matrix is considered numerically singular, and $\\delta$ is increased to improve the condition number and ensure invertibility. This threshold is chosen to maintain numerical stability in the computations.",
    "question": "The matrix $(\\mathbf{B^{\\prime}B}+\\delta\\mathbf{P}^{0}+\\gamma\\mathbf{P}^{1}+\\lambda\\mathbf{P}^{2})$ is considered invertible if the ratio of the largest to smallest eigenvalue is less than $10^{10}$. True or False?",
    "question_context": "Penalized Regression with Difference Penalties for Air Pollution Prediction\n\nThe vector $\\hat{\\boldsymbol{\\alpha}}$ of coefficients that minimizes (10) is rather simple to compute. First, the original design matrix $\\mathbf{X}$ is transformed to a new $n\\mathbf{x}(1+r p)$ design matrix $\\mathbf{B}=(\\mathbf{1}|\\mathbf{B}_{1}|\\cdot\\cdot\\cdot|\\mathbf{B}_{p})$ , where each $\\mathbf{B}_{j}$ is a nxr matrix with the values of $b_{l}(x_{j})$ in the lth column. <PARAGRAPH_SEPARATOR> Then define the three matrices $\\mathbf{D}^{d}$ , $d=0$ , 1, 2 of dimension $(r-d)\\mathbf{x}r\\colon\\mathbf{D}^{0}$ is the identity matrix; $\\mathbf{D}^{1}$ has the value $^{-1}$ in element $(l,l)$ , the value 1 in element $(l,l+1)$ for $l=$ $1,\\ldots,r-1$ , and is 0 elsewhere; $\\mathbf{D}^{2}$ is 1 in element $(l,l)$ , $^{-2}$ in element $(l,l+1)$ and 1 in element $(l,l+2)$ for $l=1,\\ldots,r-1$ , and 0 elsewhere. Then the three matrices $\\mathbf{P}^{d}$ , $d=0,1,2$ , are defined by blockdiag $(0,\\mathbf{D}^{d^{\\prime}}\\mathbf{D}^{d},\\ldots,\\mathbf{D}^{d^{\\prime}}\\mathbf{D}^{d})$ , where the $\\mathbf{D}^{d}\\mathbf{D}^{d}$ is repeated $p$ times. <PARAGRAPH_SEPARATOR> Marx and Eilers (1998) notice that there is no unique solution to (10) because $\\mathbf{B}$ is singular, since the columns of each $\\mathbf{B}_{j}$ sum to 1. To overcome this, they add a small penalization to the 0th order difference, i.e. the term $\\delta\\cdot{\\textstyle\\sum_{j=1}^{p}}{\\textstyle\\sum_{l=1}^{p}}(\\tilde{\\alpha}_{j l})^{2}$ is added to (10), where $\\delta$ is a small positive constant. The solution to t his slightly modified version of (10) is then <PARAGRAPH_SEPARATOR> $$ {\\hat{\\boldsymbol{\\alpha}}}=(\\mathbf{B^{\\prime}B}+\\delta\\mathbf{P}^{0}+\\gamma\\mathbf{P}^{1}+\\lambda\\mathbf{P}^{2})^{-1}\\mathbf{B^{\\prime}y}. $$ <PARAGRAPH_SEPARATOR> The role of $\\delta$ is only to ensure that the solution is unique, i.e. to ensure that $({\\bf B}^{\\prime}{\\bf B}+\\delta{\\bf P}^{0}+{\\bf\\delta}$ $\\gamma\\mathbf{P}^{1}+\\lambda\\mathbf{P}^{2})$ is invertible. This matrix is considered as invertible if the ratio of the largest and smallest eigenvalue is less than $10^{10}$ . If not, $\\delta$ is increased until this is fulfilled. The minimum value of \u0006 is 10−4. <PARAGRAPH_SEPARATOR> The effective number of parameters in the model may be computed as the trace of the so called hat matrix defined by <PARAGRAPH_SEPARATOR> $$ \\hat{\\mathbf{H}}=\\mathbf{B}(\\mathbf{B^{\\prime}B}+\\delta\\mathbf{P}^{0}+\\gamma\\mathbf{P}^{1}+\\lambda\\mathbf{P}^{2})^{-1}\\mathbf{B^{\\prime}}. $$ <PARAGRAPH_SEPARATOR> # Appendix B. Description of two air pollution data sets <PARAGRAPH_SEPARATOR> Two data sets used in the simulation experiment are unpublished, but are available at StatLib (http://lib.stat.cmu/edu). They have been collected by The Norwegian Public Roads Administration, and originate in a study where air pollution at a road is related to traffic volume and meteorological variables. The response variables consist of hourly values of the logarithm of the concentration of $P M_{10}$ (particles) in data set 13 and of $\\mathrm{NO}_{2}$ in data set 14, measured at Alnabru in Oslo, Norway, between October 2001 and August 2003. The predictor variables are the logarithm of the number of cars per hour, temperature $2\\mathrm{m}$ above ground (degree C), wind speed (meters/second), the temperature difference between 25 and $2\\mathrm{m}$ above ground (degree C), wind direction (degrees between 0 and 360), hour of day and day number from 1 October 2001. The observations in the two data sets are taken at different time points, so their $\\mathbf{X}$ -matrices are different. <PARAGRAPH_SEPARATOR> # References"
  },
  {
    "qid": "statistic-posneg-1366-1-0",
    "gold_answer": "FALSE. The estimation of regression coefficients is valid under the assumption that the errors $\\epsilon$ are independent of $\\pmb{x}$ and normally distributed with zero mean and constant variance. While the distribution of $\\pmb{x}$ does not affect the estimation under these conditions, the validity of the estimation depends on the linearity assumption and the properties of the error term, not just the distribution of $\\pmb{x}$.",
    "question": "Is it correct to assume that the estimation of regression coefficients remains valid regardless of the distribution of the predictor variable $\\pmb{x}$ when the hypothesis of linearity is satisfied?",
    "question_context": "Regression Analysis and Functional Relationship\n\n(c) On the other hand, if pre-selection of values of the predicated variate destroys or seriously impairs the validity of significance tests we may purchase extra accuracy too dearly. We then arrive at the familiar dilemma in the systomatization of design—whether we prefer to be closer to the truth without knowing how close, or to run the risk of being further from it while knowing how far away we are likely to be. <PARAGRAPH_SEPARATOR> $(d)$ It should be observed that I do not adduce as an example the case occurring frequently in physics where we assign values of one variable and measure the values of the other; as, for example, in a laboratory experiment on Hooke's law, where we hang pre-selected weights on & spring and measure the extension. This looks like a case where the weight variable is pre-selected completely. But, in fact, it does not fall within the ambit of regression analysis at all unless there are no errors in the determination of the weights. <PARAGRAPH_SEPARATOR> 9. Let us suppose for simplicity that the regression of $\\pmb{y}$ on $\\pmb{\\mathscr{x}}$ is linear and that the variates are measured about their mean. The regression line is then <PARAGRAPH_SEPARATOR> $$ Y=\beta_{0}+\beta_{1}X, $$ <PARAGRAPH_SEPARATOR> where $\beta_{0}$ and $\beta_{1}$ are unknown constants. We can also write <PARAGRAPH_SEPARATOR> $$ {\\pmb y}_{\\pmb X}={\\pmb\beta}_{\\pmb0}+{\\pmb\beta}_{\bf1}{\\pmb X}+{\\pmb\\epsilon}_{\\pmb X}, $$ <PARAGRAPH_SEPARATOR> meaning that the random variate ${\\pmb y}_{\\pmb x}$ depends on a mathematical variable $\\pmb{X}$ , two unknown constants $\beta_{\\mathfrak{o}}$ and $\beta_{\\mathbf{1}}$ and a random variate $\\epsilon$ which also depends on $\\pmb{X}$ . What we should not write for the regression line is an expression such as <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{{\\pmb y}=\\beta_{0}+\\beta_{1}{\\pmb x}+\\epsilon.}\\end{array} $$ <PARAGRAPH_SEPARATOR> This means something quite diferent, namely, that $\\pmb{y}$ is the weighted sum of two random variates $\\pmb{x}$ and $\\pmb{\\epsilon}$ . It also seems to me confusing in this context, if not actually improper, to introduce the‘conditional′ variate ${\\pmb y}|{\\pmb x}$ and write <PARAGRAPH_SEPARATOR> $$ \\pmb{y}\\vert\\pmb{x}=\\beta_{0}+\\beta_{1}\\pmb{x}+\\epsilon\\vert\\pmb{x}, $$ <PARAGRAPH_SEPARATOR> which means that $\\pmb{x}$ is a random variate, that for any value of ${\\pmb x},{\\pmb y}|{\\pmb x}$ and $\\epsilon|{\\pmb x}$ are random variates, and that ${\\pmb y}|{\\pmb x}$ is a weighted sum of $\\pmb{z}$ and $\\pmb{\\epsilon}|\\pmb{x}$ <PARAGRAPH_SEPARATOR> The difference between (5) and (7) is that (5) asserts something about a random variate $\\pmb{y}$ and avariable $\\pmb{X}$ , whereas (7) asserts something about & pair of random variates $\\pmb{y}$ &nd $\\pmb{\\mathscr{x}}$ <PARAGRAPH_SEPARATOR> 10. Consider now the problem of estimating the constants $\beta_{\\bullet}$ and $\\beta_{1}$ in (4) from & random sample of values of $\\pmb{y}$ and $\\pmb{x}$ .We have two cases to consider: (1) when the sample is drawn so that values of $\\pmb{y}$ and $\\pmb{x}$ both occur at random, (2) when it is drawn by specifying values of $\\pmb{x}$ and only $\\pmb{y}$ varies at random. <PARAGRAPH_SEPARATOR> To make any headway at all we require certain further assumptions. We may be content to rely on the principle of least squares, without looking for any deeper justifcation of that principle, and hence to minimize the sum of squares <PARAGRAPH_SEPARATOR> $$ \\sum_{i=1}^{n}(y_{i}-b_{0}-b_{1}x_{i})^{2}, $$ <PARAGRAPH_SEPARATOR> which leads to the familiar estimators <PARAGRAPH_SEPARATOR> $$ b_{0}=0,\\quad b_{1}={\\frac{\\Sigma x y}{\\Sigma x^{2}}}, $$ <PARAGRAPH_SEPARATOR> both variates being measured about their observed mean. There is something to be said for accepting the principle of least squares in its own right (cf. Lindley, 1947). If, however, we do not accept least squares as a basic principle of estimation we must make certain assumptions about the random variate $\\epsilon_{x}$ . What we do in practice is to suppose (1) that $\\pmb{\\epsilon_{x}}$ i8 independent of $\\pmb{X}$ ; (2) that it is & normal variable with zero mean and variance, say ${\\pmb{\\sigma}}^{\\mathfrak{s}}$ , (3) that the observed values of $\\epsilon_{x}$ are independent of each other. The joint distribution of & set of $\\textbf{\\em n}\\epsilon^{\\prime}\\mathfrak{s}$ ,whatever the values of the $X^{\\mathfrak{s}}$ is then <PARAGRAPH_SEPARATOR> $$ \\left.\\frac{1}{(2\\pi\\sigma)^{\\dagger n}}\\exp\\left\\{-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}\\epsilon_{i}^{2}\\right\\}d\\epsilon_{1}\\ldots d\\epsilon_{n}. $$ <PARAGRAPH_SEPARATOR> We now use (5) to transform this to the random variables ${\\pmb y}_{\\pmb x}$ and obtain <PARAGRAPH_SEPARATOR> $$ \\frac{1}{(2\\pi\\sigma)^{\\mathbf{i}n}}\\exp\\left\\lbrace-\\frac{1}{2\\sigma^{2}}\\Sigma(y_{X}-\\beta_{0}-\\beta_{1}X)^{\\mathbf{2}}\\right\\rbrace d y_{X,1}\\dots d y_{X,n}, $$ <PARAGRAPH_SEPARATOR> We then use the principle of maximum likelihood to determine $\\beta_{\\bullet}$ and $\\beta_{1}$ 80 a8 to minimize the sum inside the braces, which leads us back to (8). It does not matter if we are ignorant ofthevalueof $\\pmb{\\sigma}$ <PARAGRAPH_SEPARATOR> 11. This process is legitimate only if we use equation (5) to transform from the random variable element $d\\epsilon_{1}\\dots d\\epsilon_{n}$ to the element $d y_{X,1}\\dots d y_{X,n}$ . It is not legitimate if we use (6) or (7), for we are not entitled to express an $\\pmb{\\mathscr{n}}$ -fold element $d\\epsilon_{1}\\dots d\\epsilon_{n}$ in terms of an element comprising $\\scriptstyle2\\pi$ variates, ny's and $\\pmb{n x}^{*}\\lefteq$ . The method therefore provides a solution only in the case when values of $\\pmb{X}$ are given. What can be said about the alternative case when $\\pmb{y}$ and $\\pmb{x}$ are both random variates? <PARAGRAPH_SEPARATOR> The usual answer to this question is that $\\mathbf{\\epsilon}^{6}\\mathbf{w}\\mathbf{e}$ may regard the z's as fixed' by an argument of the following type. Suppose the joint distribution of $\\pmb{y}$ and $_{\\pmb{x}}$ .9 $f(x,y)$ The probability of the observed sample is then <PARAGRAPH_SEPARATOR> $$ P=f(x_{1},y_{1})\\ldots f(x_{n},y_{n})d x_{1}\\ldots d x_{n}d y_{1}\\ldots d y_{n}. $$ <PARAGRAPH_SEPARATOR> Transform to new random variates by the equations <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\pmb x}={\\pmb\\xi},}\\\\ {{\\pmb y}=\\beta_{0}+\\beta_{1}{\\pmb\\xi}+\\epsilon.}\\end{array}\\biggr\\} $$ <PARAGRAPH_SEPARATOR> The Jacobian of the transformation is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{\\displaystyle\\frac{\\partial({\\boldsymbol{x}},{\\boldsymbol{y}})}{\\partial({\\boldsymbol{\\xi}},{\\boldsymbol{\\epsilon}})}=1,}}\\\\ {{{}}}\\\\ {{P=f(\\xi_{1},\\beta_{0}+\\beta_{1}\\xi_{1}+\\epsilon_{1})\\ldots f(\\xi_{n},\\beta_{0}+\\beta_{1}\\xi_{n}+\\epsilon_{n})d\\xi_{1}\\ldots d\\xi_{n}d\\epsilon_{1}\\ldots d\\epsilon_{n}.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> If this splits into two factors such that the $\\epsilon$ -distribution is independent of the $\\pmb{\\xi}$ distribut <PARAGRAPH_SEPARATOR> we must have <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{f(\\xi,\\beta_{0}+\\beta_{1}\\xi+\\epsilon)=g(\\xi)h(\\epsilon),}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\pmb{g}$ and $\\pmb{h}$ are frequency functions. This is equivalent to <PARAGRAPH_SEPARATOR> $$ f(x,y)=g(x)h(y-\\beta_{0}-\\beta_{1}x). $$ <PARAGRAPH_SEPARATOR> If and only if this condition is satisfied we may estimate $\\beta_{\\bullet}$ and $\\beta_{\\mathbf{1}}$ from $h(\\epsilon)$ alone.In particular, if $\\epsilon$ is distributed normally the maximum likelihood solution leads back to (9) and thence to (8). Thus, as the random origin of the $\\pmb{\\mathscr{x}}$ 's does not affect the estimation we can regard them as the assigned values of a variable or as 'fixed'. <PARAGRAPH_SEPARATOR> 12. I cannot soe any other way of arriving at the desired result unless we import yet another principle of inference, which is usually a confession of failure and in any case is not to be done where avoidable. The points to be noticed are : (1)that we have assumed thee's to be independent of the x's and (2) that we have used in equations (12) a transformation depending on the view that both $\\pmb{x}$ a.nd $\\pmb{y}$ are random variates. It is usually stated that ‘by regarding the x's as fixed' we avoid making any assumption about the distribution of $\\pmb{x}$ ; and, indeed, it is sometimes implied that the estimation remains valid whatever the distribution of $\\mathbf{\\hat{\\phi}}_{x}$ .This is true when the hypothesis as to linearity is satisfied, but not necessarily in alternative cases. <PARAGRAPH_SEPARATOR> In questions of estimation we are mainly concerned with the distribution of the e's. Where tests of significance are required we are interested in the distribution of the residuals about & fitted regression line, which is quite another thing."
  },
  {
    "qid": "statistic-posneg-1451-0-1",
    "gold_answer": "TRUE. The covariance function $\\Sigma(B_{1}, B_{2})$ is defined as $|B_{1}|^{-1}|B_{2}|^{-1}\\int_{B_{1}}\\int_{B_{2}}c(s_{1},s_{2})\\mathrm{d}s_{1}\\mathrm{d}s_{2}$, which explicitly includes the inverse of the areas of $B_{1}$ and $B_{2}$ to normalize the integral of the point-level covariance function $c(s_{1}, s_{2})$ over the regions.",
    "question": "Does the covariance function $\\Sigma(B_{1}, B_{2})$ account for the areas of the regions $B_{1}$ and $B_{2}$ in its calculation?",
    "question_context": "Comparison of Gaussian Geostatistical Models and Gaussian Markov Random Fields\n\nIn many applications in spatial and environmental epidemiology, data concerning a spatial process of interest are often observed at different spatial resolutions, and the overall problem of incompatible spatial data has been encountered very commonly when relating two spatial variables with different supports. For example, in studies of the association between air pollution exposure and adverse health effects, relevant health outcomes are usually available as areal data due to confidentiality while the pollution data are available as a point level [6,19]. To investigate the relationships between two variables with different spatial resolutions, the mismatch problem in the support of the two variables needs to be resolved. One common solution to this spatial misalignment problem is to aggregate the point-referenced data to the area level, and create a common support for both variables. Once the point-referenced data are aggregated to the relevant level, the process representing the aggregated data is modeled using integrals of spatial continuous process [14,15]. Consider a continuous Gaussian process $Y(s)$ with mean function $\\mu(s)$ and covariance function $c(s_{i},s_{j})$ for $s_{i},s_{j}\\subset D\\in R^{d}$ , where $s_{i}$ is a location in a fixed domain $D$ . The aggregated process over a region $B$ , $\\begin{array}{r}{Y(B)=\\int_{B}Y(s)\\mathrm{d}s}\\end{array}$ has a multivariate normal distribution with mean function $\\mu(B)$ and covariance function $\\Sigma(B_{1},B_{2})$ , $$\\begin{array}{l}{\\displaystyle\\mu(B)=E(Y(B))=|B|^{-1}\\int_{B}\\mu(s)\\mathrm{d}s}\\ {\\sum(B_{1},B_{2})=\\mathrm{cov}(Y(B_{1}),Y(B_{2}))=|B_{1}|^{-1}|B_{2}|^{-1}\\int_{B_{1}}\\int_{B_{2}}c(s_{1},s_{2})\\mathrm{d}s_{1}\\mathrm{d}s_{2},}\\end{array}$$ where $|B_{i}|$ denotes the area of a region $B_{i}$ for $i=1,2$ . Modeling aggregated data using these spatial integrals requires lots of computation. Hence, instead of using the aggregated models directly in modeling aggregated point-referenced data, it is becoming very common to use Gaussian Markov random fields."
  },
  {
    "qid": "statistic-posneg-584-0-2",
    "gold_answer": "FALSE. The survival copula $\\overline{C}$ is defined in terms of the joint survival probabilities and is not necessarily Archimedean, nor does it generally share the same generator $\\phi$ as the original copula $C$. The properties of $\\overline{C}$ depend on the specific form of $C$ and the margins, and it does not automatically inherit the Archimedean structure from $C$.",
    "question": "Is it correct to say that the survival copula $\\overline{C}$ of an Archimedean copula $C$ is also Archimedean with the same generator $\\phi$?",
    "question_context": "Tail Behavior Classification of Multivariate Archimedean Copulas\n\nA $d$ -variate copula $C$ is the restriction to $[0,1]^{d}$ of a $d$ -variate distribution function with uniform (0, 1) margins. By Sklar’s theorem, a copula is what remains of an arbitrary (continuous) $d$ -variate distribution function $F$ when stripped of its margins. Margin-free measures of dependence such as Kendall’s $\\tau$ or Spearman’s $\\rho$ depend on $F$ only through C. More generally, copulas form a natural way to describe the dependence between variables when making abstraction of their marginal distributions. Overviews of the probabilistic and statistical aspects of copulas are to be found for instance in [1–6]. A copula $C$ is called Archimedean [7,8] if it is of the form $$ C(u_{1},\\dots,u_{d})=\\phi^{\\leftarrow}(\\phi(u_{1})+\\cdot\\cdot\\cdot+\\phi(u_{d})) $$ for $(u_{1},\\dots,u_{d})\\in[0,1]^{d}$ , where the Archimedean generator $\\phi:[0,1]\\rightarrow[0,\\infty]$ is convex, decreasing and satisfies $\\phi(1)=0$ and where $$ \\phi^{\\leftarrow}(y)=\\operatorname*{inf}\\{t\\in[0,1]:\\phi(t)\\leqslant y\\} $$ for $y\\in[0,\\infty]$ is the generalized inverse of $\\phi$ . See Table 1 for a list of some common parametric families of Archimedean generators. A necessary and sufficient condition for the right-hand side of (1.1) to specify a copula is that the function $\\phi^{\\leftarrow}$ is $d\\cdot$ -monotone on $(0,\\infty)$ , that is, $\\phi^{\\leftarrow}$ is $d-2$ times continuously differentiable, $(-D)^{k}\\phi^{\\leftarrow}\\geqslant0$ for all $k\\in\\{0,1,\\ldots,d-2\\}$ (with $D$ the derivative operator), and $(-D)^{d-2}\\phi^{\\leftarrow}$ is convex [3]. Note that if $d=2$ , then the conditions on $\\phi$ mentioned in the beginning of this paragraph are necessary and sufficient. If $\\phi^{\\leftarrow}$ is completely monotone, that is, if $(-\\stackrel{.}{D})^{k}\\phi^{\\leftarrow}\\geqslant0$ for all integer $k\\geqslant0$ , then $\\phi^{\\leftarrow}$ is also $d$ -monotone for every integer $d\\geqslant2$ and the resulting model can be interpreted as a frailty model [9,10]. Because of their analytic tractability, Archimedean copulas have enjoyed a great popularity in applied work, from insurance [11,12] and finance [13,14] to hydrology [1,15,16] and survival analysis [17–19], to mention just a few references. More flexible models can be obtained by forming mixtures of Archimedean copulas [20], or by constructing hierarchical (or nested) Archimedean copulas [21]; however, this will not be considered in this paper."
  },
  {
    "qid": "statistic-posneg-1077-0-1",
    "gold_answer": "FALSE. While the orthogonality condition ensures the existence of full rank transformations $\\beta$ and $\\beta^{*}$, the equality $P_{2}^{*}Q_{2}^{*}=P_{2}Q_{2}$ specifically depends on the derived relationship $\\mathcal{Q}_{2}^{*}=(\\beta^{*})^{-1}\\beta Q_{2}$ and the invertibility of $P$ and $P^{*}$. The proof requires substituting $P_{2}^{*}=P_{2}\\beta^{-1}\\beta^{*}$ and $\\mathcal{Q}_{2}^{*}=(\\beta^{*})^{-1}\\beta Q_{2}$ into the product, analogous to the $P_{1}^{*}\\mathcal{Q}_{1}^{*}$ case. The orthogonality alone does not guarantee the equality; the exact form of the transformations is necessary.",
    "question": "Does the orthogonality condition between the column spaces of $M$ and $G$ imply that $P_{2}^{*}Q_{2}^{*}=P_{2}Q_{2}$ must hold for any choice of full rank matrices $\\beta$ and $\\beta^{*}$?",
    "question_context": "Orthogonal Matrix Transformations in Exact Maximum Likelihood Estimation\n\nIn a similar way, consider $k\\times r$ matrices $P_{2}$ and $P_{2}^{*}$ such that $P=[P_{1},P_{2}]$ and $P^{*}=$ $\\left[P_{1}^{*},P_{2}^{*}\\right]$ are invertible, and the column vectors of $P_{2}$ and $P_{2}^{*}$ are orthogonal to those of $P_{1}$ and $P_{1}^{*}$ , respectively. Of course, the space spanned by the column vectors of $M=[v_{1},\\ldots,v_{r}]$ is orthogonal to the one spanned by those of $G$ . Hence, there exist full rank $r\\times r$ matrices $\\beta$ and $\\beta^{*}$ such that $P_{2}=M\\beta$ and $P_{2}^{*}=M\\beta^{*}$ . Thus, $P_{2}^{*}=P_{2}\\beta^{-1}\\beta^{*}$ . <PARAGRAPH_SEPARATOR> Let $Q={\\left[\\begin{array}{l}{Q_{1}}\\ {Q_{2}}\\end{array}\\right]}=P^{-1}$ and $Q^{*}=\\left[{{Q}_{1}^{*}}\\right]=(P^{*})^{-1}$ , we can easily check that $\\mathcal{Q}^{*}=\\left[\\begin{array}{l}{(\\alpha^{*})^{-1}\\alpha Q_{1}}\\ {(\\beta^{*})^{-1}\\beta Q_{2}}\\end{array}\\right].$ <PARAGRAPH_SEPARATOR> Now turning back to the product $P_{1}^{*}Q_{1}^{*}$ , we have that <PARAGRAPH_SEPARATOR> $$ P_{1}^{*}\\mathcal{Q}_{1}^{*}=P_{1}\\alpha^{-1}\\alpha^{*}(\\alpha^{*})^{-1}\\alpha\\mathcal{Q}_{1}=P_{1}\\mathcal{Q}_{1}. $$ <PARAGRAPH_SEPARATOR> In a similar way we can check that $P_{2}^{*}Q_{2}^{*}=P_{2}Q_{2}$ , which completes the proof."
  },
  {
    "qid": "statistic-posneg-1516-3-1",
    "gold_answer": "FALSE. While the condition $SD(Z^{(b)}) < \\tau$ ensures that the Monte Carlo error is controlled (i.e., the sampling variability is below a threshold $\\tau$), it does not by itself guarantee that $Z^{(b)}$ is normally distributed. The normality assumption is an additional modeling assumption stated in the context: 'The idea behind this is an assumption that the Wald statistics $Z^{(b)}$ has a Monte Carlo Normal distribution conditioned on all the data $\\mathcal{D}$.' The condition $SD(Z^{(b)}) < \\tau$ is used to ensure the Monte Carlo approximation is sufficiently precise, but normality is a separate assumption.",
    "question": "Does the condition $SD(Z^{(b)}) < \\tau$ ensure that the Monte Carlo error is controlled, and the Wald statistic $Z^{(b)}$ is approximately normally distributed?",
    "question_context": "Monte Carlo Sampling for Branching Process Models in Infectious Disease Transmission\n\nOur sampling algorithm requires us to input $K$ , the number of trees of given size and covariates to sample for each observed cluster. The law of large numbers tells us that the maximum likelihood of $\\beta$ based on the estimated likelihood will converge to the maximum likelihood estimator of $\\beta$ based on the proper likelihood as the number number $K$ of sampled trees of size $n$ grows large. To perform our algorithm, we need to determine how large $K$ needs to be so that the average likelihood from sampling is close to the proper average likelihood. Our solution is to find the minimum number of samples $K$ such that the Monte Carlo sample error is less than some threshold $\\tau$ . More plainly, we want to verify that our approximate MC estimate is close enough to the proper likelihood MLE. <PARAGRAPH_SEPARATOR> We first describe how to determine $K$ when there is a single covariate, $X_{1}$ . For a dataset with $M$ clusters, we then sample $K$ trees of size $|C_{i}|$ for each observed cluster $i,$ estimate the likelihood over all clusters, and estimate $\\hat{\\beta}_{1}^{(1)},\\widehat{\\mathrm{SE}}\\left(\\hat{\\beta}_{1}^{(1)}\\right)$ . We repeat this process $B$ times to obtain $\\begin{array}{r}{\\hat{\\beta}_{1}^{(b)},\\widehat{\\mathrm{SE}}\\left(\\hat{\\beta}_{1}^{(b)}\\right),b=1,\\ldots,B.}\\end{array}$ . We then form Wald statistics $Z^{(b)}=\\hat{\\beta}_{1}^{(b)}/\\widehat{\\mathrm{SE}}\\left(\\hat{\\beta}_{1}^{(b)}\\right)$ If the sample standard deviation over the B samples, $S D\\left(Z^{(b)}\\right)<\\tau$ , then we say $K$ is enough Monte Carlo samples. Otherwise, we double $K$ and repeat the process until we find a large enough $K$ . If we have multiple covariates, we repeat this process for all covariates. In our simulations we use $B=100$ sets of trees to determine the Wald statistics and their standard error. <PARAGRAPH_SEPARATOR> The idea behind this is an assumption that the Wald statistics $Z^{(b)}$ has a Monte Carlo Normal distribution conditioned on all the data $\\mathcal{D}$ . That is <PARAGRAPH_SEPARATOR> $$ Z^{(b)}|\\mathcal{D}\\sim N\\left(\\widehat{\\beta}_{1}^{(\\mathrm{MLE})}/\\widehat{\\mathrm{SE}}\\left(\\widehat{\\beta}_{1}^{(\\mathrm{MLE})}\\right),\\mathrm{Var}(Z^{(b)})\\right), $$ <PARAGRAPH_SEPARATOR> where $\\hat{\\beta}_{1}^{(\\mathrm{MLE})},\\widehat{\\mathrm{SE}}(\\hat{\\beta}_{1}^{(\\mathrm{MLE})})$ , are the MLE and estimated standard error that would be obtained from the correct likelihood, i.e. summed over all permissible transmission trees. Note that as $K\\rightarrow\\infty,Z^{(b)}$ becomes the true Wald statistic and var $(Z^{(b)})$ approaches 0. We can thus choose $K$ so that the sample $\\begin{array}{r}{\\hat{\\mathrm{SD}}(Z^{(b)})\\le\\tau}\\end{array}$ . If we let $\\tau=0.1$ , then we will be about $95\\%$ confident that <PARAGRAPH_SEPARATOR> $$ Z^{(b)}|\\mathcal{D}\\in\\biggr[\\hat{\\beta}_{1}^{(\\mathrm{MLE})}/\\widehat{\\mathrm{SE}}\\left(\\hat{\\beta}_{1}^{(\\mathrm{MLE})}\\right)-0.196,\\hat{\\beta}_{1}^{(\\mathrm{MLE})}/\\widehat{\\mathrm{SE}}\\left(\\hat{\\beta}_{1}^{(\\mathrm{MLE})}\\right)+0.196\\biggr], $$ <PARAGRAPH_SEPARATOR> so a two-sided p-value would lie in 2[1 - Φ(BMLE /SE(β(MLE) $\\pm0.196)_{.}$ ]. <PARAGRAPH_SEPARATOR> # 4. Results <PARAGRAPH_SEPARATOR> # 4.1. Simulation Results <PARAGRAPH_SEPARATOR> We first show our base model’s ability to properly estimate $\\beta$ from simulated data generated from a known branching process. <PARAGRAPH_SEPARATOR> Since we know the proper likelihood for the cluster of size 3 in Figure 2, we first confirmed that our algorithm works for that configuration. We then explored how our model performs for different parameter values of the model shown below, again where $p_{i}$ is the probability of onward transmission and $X_{i}$ is a binary covariate, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\displaystyle p_{i}=\\mathrm{logit}^{-1}\\left(\\beta_{0}+\\beta_{1}X_{i}\\right),}}\\ {{\\displaystyle X_{i}\\sim\\mathrm{Bernoulli}\\left(\\gamma\\right).}}\\end{array} $$ <PARAGRAPH_SEPARATOR> In our simulation study, we selected parameter choices such that the maximum cluster size is $\\leq70$ with high probability. These included some combinations of $\\beta_{0}=-2.75,-2.50,-1.5\\beta_{1}=$ $-0.50,0.00,0.5,1,2.25$ ; and $P(+)=\\gamma=0.1,0.5,0.7$ . <PARAGRAPH_SEPARATOR> To select $K$ , the number of MC transmission trees required to be sampled for each observed cluster, we implement our criterion mentioned in Section 3. The algorithm is computationally costly and so we only find $K$ for the first parameter configuration in Table 1. For this we use $N_{B}=100$ simulations with $\\tau=0.1$ . We find that $K=5000$ samples are enough. We double that number and use $K=10{,}000$ MC for the remaining simulations. <PARAGRAPH_SEPARATOR> The results of our simulations for the base model are displayed in Table 1. We report the estimated mean and median along with the standard error from bootstrap sampling (where entire clusters are sampled with replacement) and the inner quartile range (IQR). We additionally report the coverage for a $95\\%$ CI of the mean estimate $\\pm1.96\\times\\mathrm{SE}$ When the parameter configurations are such that the maximum cluster size is $<$ 35, we find that, across these different parameter values, our method estimates $\\beta_{0}$ and $\\beta_{1}$ well, on average, and also have approximately $95\\%$ coverage. When the maximum cluster size $>35$ , we see that our algorithm does not have good coverage and when the maximum cluster size $>52$ , we also see large bias in the average estimate. These results suggest that $10{,}000~\\mathrm{MC}$ samples are enough for clusters $<35$ individuals but when the clusters are $\\geq35$ our algorithm does not perform well, which we suspect is due to not sampling enough “informative” trees from the very large sampling space. Another possible reason is that our observed data does not have enough clusters."
  },
  {
    "qid": "statistic-posneg-2140-6-0",
    "gold_answer": "TRUE. According to Theorem 3.9, if the process $\\mathbf{X}$ is Gaussian, then $G_{Y}$ converges in distribution to a chi-squared distribution with 2 degrees of freedom ($X_{2}^{2}$). This is a key property for the test's validity under the null hypothesis, ensuring that the test statistic has a known asymptotic distribution when the process is Gaussian.",
    "question": "Is the test statistic $G_{Y}$ guaranteed to converge to a chi-squared distribution with 2 degrees of freedom under the null hypothesis of Gaussianity, as stated in Theorem 3.9?",
    "question_context": "Random Projection-Based Test of Gaussianity\n\nCorollary 3.8. Under the assumptions of Theorem 3.6, $(P_{\\lambda}\\otimes P_{\\mathbf{H}})[B]\\in\\{0,1\\}$ and $\\mathbf{X}$ is Gaussian if, and only if, $(P_{\\lambda}\\otimes P_{\\mathbf{H}})[B]=1$ . Analogously, under the assumptions of Corollary 3.7, $P_{\\lambda}(C)\\in\\{0,1\\}$ and $\\mathbf{X}$ is Gaussian if, and only if, $P_{\\lambda}(C)=1$ . <PARAGRAPH_SEPARATOR> Remark 3.8.1. From Theorem 2.2, we have that Theorem 3.6 and Corollaries 3.7 and 3.8 remain true if we substitute in the definition of sets $B$ and $C$ ‘‘non-degenerate distribution’’ by ‘‘chi-squared distribution with $2(N-1)$ degrees of freedom’’; this allows a test to be constructed based on the asymptotic distribution of $n Q_{n}(\\mu_{n},\\gamma_{n},\\lambda)$ . <PARAGRAPH_SEPARATOR> We end this section with a result which shows the applicability of the LV-test to the projected process under different assumptions than the ones used in Lobato and Velasco (2004). To this end, we replace the statistics $\\tilde{G}_{Y}$ by <PARAGRAPH_SEPARATOR> $$ G_{Y}=n\\hat{\\mu}_{3}^{2}/(6|\\hat{F}_{3}|)+n(\\hat{\\mu}_{4}-3\\hat{\\mu}_{2}^{2})^{2}/(24|\\hat{F}_{4}|), $$ <PARAGRAPH_SEPARATOR> with <PARAGRAPH_SEPARATOR> $$ \\hat{F}_{k}=2\\sum_{t=1}^{\\tau_{n}}\\hat{\\gamma}(t)(\\hat{\\gamma}(t)+\\hat{\\gamma}(\\tau_{n}+1-t))^{k-1}+\\hat{\\gamma}^{k},\\tau_{n}<c n^{\\beta_{0}},0<\\beta_{0}<.5\\mathrm{and}c>0. $$ <PARAGRAPH_SEPARATOR> Thus, the differences between $G_{Y}$ and $\\tilde{G}_{Y}$ are the absolute values in the denominator and the number of terms involved in $\\hat{F}_{k}$ . <PARAGRAPH_SEPARATOR> Theorem 3.9. Let X be an ergodic and stationary process which satisfies $\\begin{array}{r}{\\sum_{t=0}^{\\infty}|\\gamma_{X}(t)|<\\infty}\\end{array}$ . Then, <PARAGRAPH_SEPARATOR> 1. If $\\mathbf{X}$ is a Gaussian process, then $G_{Y}\\longrightarrow d\\ X_{2}^{2}$ . 2. Assume that $\\begin{array}{r}{X_{t}-\\mu_{X}=\\sum_{i=1}^{\\infty}k(i)\\epsilon_{t-i}}\\end{array}$ with $\\begin{array}{r}{\\sum_{i=1}^{\\infty}|k(i)|<\\infty,\\sum_{i=1}^{\\infty}i k(i)<\\infty,}\\end{array}$ , and $\\left(\\epsilon_{t}\\right)$ are i.i.d. r.v.’s with $\\mathbb{E}[\\epsilon_{n}]=0$ , and $\\mathbb{E}[X_{0}^{4}]<\\infty$ . Then, condi tionally on h, $G_{Y}$ div erges a.s. to infinit y whenever $\\mu_{3}\\neq0$ or $\\mu_{4}\\neq3\\mu_{2}^{2}$ ."
  },
  {
    "qid": "statistic-posneg-698-2-1",
    "gold_answer": "FALSE. Theorem 3.3 states that the asymptotic distribution holds for any pre-determined finite constants $K$ and $D$, but this is contingent on conditions (C1)–(C5) being satisfied. The result does not universally hold for arbitrary choices of $K$ and $D$ without these underlying regularity conditions governing the functional data structure and eigenvalue behavior.",
    "question": "Does the asymptotic distribution $T_{n}\\stackrel{d}{\\rightarrow}\\chi^{2}(D K(K+1)/2)$ under $H_{0}$ hold for any choice of $K$ and $D$ as long as they are finite?",
    "question_context": "Consistency and Asymptotic Behavior in Functional Quadratic Regression\n\nWhen the predicator $X_{i}$ is Gaussian, all $\\xi_{i j}{'s}$ are normal. Thus (C5) is satisfied for any $C\\le3$ . All conditions (C1)–(C5) are common in functional data analysis. <PARAGRAPH_SEPARATOR> Let $d_{n}=\\lVert\\hat{C}_{X}-C_{X}\\rVert$ . Denote $\\tilde{K}_{n}=\\operatorname*{min}\\{j\\ge1:\\lambda_{j}\\le2d_{n}\\}-1$ . It is clear that $d_{n}\\to0$ and thus $\\tilde{K}_{n}\\rightarrow\\infty$ as $n\\to\\infty$ . The following theorem shows the consistency of the proposed estimators. <PARAGRAPH_SEPARATOR> Theorem 3.1. Let truncations $K=K(n)\\to\\infty$ and $D=D(n)\\to\\infty$ , which satisfy $K\\leq\\tilde{K}_{n}$ and <PARAGRAPH_SEPARATOR> $$ \\frac{1}{\\sqrt{n}}\\sum_{k=1}^{D}\\sum_{m=1}^{K}\\frac{1}{\\lambda_{m}}(\\frac{1}{\\delta_{X,m}}+\\frac{1}{\\delta_{Y,k}})\\rightarrow0. $$ <PARAGRAPH_SEPARATOR> Then under conditions (C1)–(C5), we have $\\Vert\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}\\Vert=o_{p}(1).$ If we further assume that <PARAGRAPH_SEPARATOR> $$ \\frac{1}{\\sqrt{n}}\\sum_{k=1}^{D}\\sum_{l=1}^{K}\\left(\\frac{\\sqrt{\\sigma}_{k}}{\\lambda_{l}^{3}\\delta_{X,l}}+\\sum_{m=1}^{l}\\frac{1}{\\lambda_{m}\\lambda_{l}}(\\frac{1}{\\delta_{X,l}}+\\frac{1}{\\delta_{X,m}}+\\frac{1}{\\delta_{Y,k}})\\right)\\rightarrow0, $$ <PARAGRAPH_SEPARATOR> then $\\|\\hat{\\gamma}-\\gamma\\|=o_{p}(1)$ . <PARAGRAPH_SEPARATOR> To illustrate availability of assumption (15), we consider a common setting in FPCA. <PARAGRAPH_SEPARATOR> Remark 2. Similar to formula (3.2) in Hall and Horowitz (2007), suppose that <PARAGRAPH_SEPARATOR> $$ \\lambda_{j}-\\lambda_{j+1}\\geq C_{1}j^{-\\nu_{1}-1},\\quad\\sigma_{j}-\\sigma_{j+1}\\geq C_{2}j^{-\\nu_{2}-1} $$ <PARAGRAPH_SEPARATOR> for $j\\geq1$ and $\\nu_{1},\\nu_{2}>1$ . This implies that $\\lambda_{j}\\ge C_{3}j^{-\\nu_{1}}$ , $\\delta_{X,j}\\geq C_{1}j^{-\\nu_{1}-1}$ and $\\delta_{Y,j}\\geq C_{2}j^{-\\nu_{2}-1}$ , which yield that <PARAGRAPH_SEPARATOR> $$ \\frac{1}{\\sqrt{n}}\\sum_{k=1}^{D}\\sum_{m=1}^{K}\\frac{1}{\\lambda_{m}}(\\frac{1}{\\delta_{X,m}}+\\frac{1}{\\delta_{Y,k}})\\leq\\frac{C_{4}}{\\sqrt{n}}\\sum_{k=1}^{D}\\sum_{m=1}^{K}m^{\\nu_{1}}(m^{\\nu_{1}+1}+k^{\\nu_{2}+1}). $$ <PARAGRAPH_SEPARATOR> Hence we can always find suitable $K=K(n)\\to\\infty$ , $D=D(n)\\to\\infty$ such that (15) holds. (16) can be verified similarly. <PARAGRAPH_SEPARATOR> Our next theorem ensures the prediction consistency. <PARAGRAPH_SEPARATOR> Theorem 3.2. Suppose that all conditions of Theorem 3.1 hold. As $n\\to\\infty$ , we have <PARAGRAPH_SEPARATOR> $$ \\int_{\\mathcal{T}}\\Big(\\hat{Y}^{*}(t)-E(Y^{*}(t)|X^{*})\\Big)^{2}d t\\overset{P}{\\rightarrow}0. $$ <PARAGRAPH_SEPARATOR> Finally we investigate the asymptotic behaviors of the testing statistic $T_{n}$ . <PARAGRAPH_SEPARATOR> Theorem 3.3. Suppose conditions (C1)–(C5) are satisfied. For any pre-determined constants $K$ and $D$ we have <PARAGRAPH_SEPARATOR> $$ T_{n}\\stackrel{d}{\\rightarrow}\\chi^{2}(D K(K+1)/2) $$ <PARAGRAPH_SEPARATOR> under $H_{0}$ and $T_{n}{\\overset{P}{\\to}}\\infty$ if $\\pmb R\\neq0$ , where $\\pmb{R}=(\\pmb{R}_{1}^{T},\\dots,\\pmb{R}_{D}^{T})^{T}$ . <PARAGRAPH_SEPARATOR> Remark 3. In Theorem 3.3, the asymptotic distribution of $T_{n}$ depends on pre-specified finite $K$ and $D$ . The proposed test is consistent when the alternative hypothesis satisfies $\\pmb R\\neq0$ , which means that the projection of $\\gamma$ on the subspace spanned by $\\left\\{\\psi_{l}\\otimes\\psi_{m}\\otimes\\phi_{k}:1\\leq l,m\\leq K,1\\leq k\\leq l\\right.$ is not zero."
  },
  {
    "qid": "statistic-posneg-1013-0-0",
    "gold_answer": "TRUE. The condition λ_i = O(p^γ) with γ < 1/2 ensures that the normalized sum in T1 satisfies the Lindeberg condition for asymptotic normality. This is explicitly justified in the paper's Theorem 3.1, which shows convergence to N(0,1) under this eigenvalue growth constraint. The key steps involve bounding the contribution of individual eigenvalues through the variance condition σ₁² < ∞.",
    "question": "Is the asymptotic normality of the statistic T1 under the null hypothesis guaranteed when the maximum eigenvalue condition λ_i = O(p^γ) with 0 ≤ γ < 1/2 holds?",
    "question_context": "Asymptotic Distribution of Test Statistics in High-Dimensional MANOVA\n\nThe paper models a high-dimensional MANOVA scenario where the number of observations N is less than the dimension p. It introduces test statistics T1 and T2 for hypothesis testing, derives their asymptotic distributions under null and alternative hypotheses, and compares their powers. Key formulas include the characteristic function expansions for the normalized trace statistics u and v, and the limiting distributions of T1 and T2 under local alternatives."
  },
  {
    "qid": "statistic-posneg-1776-0-0",
    "gold_answer": "TRUE. The paper derives the correct error rate for AGHQ, showing that it is $O(p^{-\\lfloor(m+2)/3\\rfloor})$, which differs from the previously suggested rate of $O(p^{-\\lfloor m/3+1\\rfloor})$ by Liu & Pierce (1994). This correction applies to both unidimensional and multi-dimensional integrals, as demonstrated in the paper's theoretical analysis and numerical illustrations.",
    "question": "Is the error rate of adaptive Gauss-Hermite quadrature approximation correctly given as $O(p^{-\\lfloor(m+2)/3\\rfloor})$ for both unidimensional and multi-dimensional integrals?",
    "question_context": "Error Rate Analysis of Adaptive Gauss-Hermite Quadrature in Latent Variable Models\n\nThe paper discusses the error rate of adaptive Gauss-Hermite quadrature (AGHQ) approximation in latent variable models. It corrects the previously derived error rate by Liu & Pierce (1994), showing that the correct error rate for AGHQ is $O(p^{-\\lfloor(m+2)/3\\rfloor})$ rather than $O(p^{-\\lfloor m/3+1\\rfloor})$. The paper also compares AGHQ with Laplace approximation, demonstrating that AGHQ with $m=4,5,6$ points per dimension has an error rate of $O(p^{-2})$, similar to second-order Laplace approximation."
  },
  {
    "qid": "statistic-posneg-2121-3-0",
    "gold_answer": "TRUE. Explanation: The context explicitly states that when $Y$ is measured without error, the ratio $[Y|S_{0j}]/[S_{0j}|Y]$ equals 1. This is because, in the absence of measurement error, the conditional distribution of $Y$ given $S_{0j}$ and the conditional distribution of $S_{0j}$ given $Y$ are directly related in a way that their ratio simplifies to 1, reflecting perfect compatibility between the observed data and the simulated realizations of $S$.",
    "question": "In the Monte Carlo approximation of the likelihood function $L_{\\mathrm{MC}}(\\theta)$, the term $[Y|S_{0j}]/[S_{0j}|Y]$ simplifies to 1 when $Y$ is measured without error. Is this simplification correct based on the provided context?",
    "question_context": "Monte Carlo Maximum Likelihood Estimation in Preferential Sampling Models\n\nFor the shared latent process model (3), the likelihood function for data $X$ and $Y$ can be expressed as $L\\left(\\theta\\right)=\\left[X,Y\\right]=E_{S}[\\left[X|S\\right]\\left[Y|X,S\\right]]$, where $\\theta$ represents all the model parameters and the expectation is with respect to the unconditional distribution of $S$. Evaluation of the conditional distribution $[X|S]$ strictly requires the realization of $S$ to be available at all $x\\in A$. However, and as previously noted in Section 3.1, we approximate the spatially continuous realization of $S$ by the set of values of $S$ on a finely spaced lattice to cover $A$ and replace the exact locations $X$ by their closest lattice points. We then partition $S$ into $S=\\{S_{0},S_{1}\\}$, where $S_{0}$ denotes the values of $S$ at each of $n$ data locations $x_{i}\\in X$, and $S_{1}$ denotes the values of $S$ at the remaining $N-n$ lattice points. To evaluate $L(\\theta)$ approximately, a naive strategy would be to replace the intractable expectation on the right-hand side of equation (6) by a sample average over simulations $S_{j}$. This strategy fails when the measurement error variance $\\dot{\\tau}^{2}$ is 0, because unconditional simulations of $S$ will then be incompatible with the observed $Y$. It also fails in practice when the measurement error is small relative to the variance of $S$, which is the case of most practical interest. We therefore rewrite the exact likelihood (6) as the integral $L(\\theta)=\\int[X|S][Y|X,S]\\frac{[S|Y]}{[S|Y]}[S]\\mathrm{d}S.$ Now, write $[S]=[S_{0}][S_{1}|S_{0}]$ and replace the term $[S|Y]$ in the denominator of expression (7) by $[S_{0}|Y][S_{1}|S_{0},Y]=[S_{0}|Y][S_{1}|S_{0}]$. Note also that $[Y|X,S]=[Y|S_{0}]$. Then, equation (7) becomes $L(\\theta)=\\int[X|S]\\frac{[Y|S_{0}]}{[S_{0}|Y]}[S_{0}][S|Y]\\mathrm{d}S= E_{S|Y}\\left[[X|S]\\frac{[Y|S_{0}]}{[S_{0}|Y]}[S_{0}]\\right]$ and a Monte Carlo approximation is $L_{\\mathrm{MC}}(\\theta)=m^{-1}\\sum_{j=1}^{m}[X|S_{j}]\\frac{[Y|S_{0j}]}{[S_{0j}|Y]}[S_{0j}]$, where now the $S_{j}$ are simulations of $S$ conditional on $Y.$."
  },
  {
    "qid": "statistic-posneg-2104-0-1",
    "gold_answer": "FALSE. A p-value of .557 is much higher than conventional significance levels (e.g., 0.05), indicating that the data does not provide strong evidence against the null hypothesis. The context states that the p-value 'favors the said hypothesis' (i.e., y > 1) because y=1.0468 > 1, but this does not imply statistical significance—it merely means the point estimate aligns with the alternative hypothesis directionally.",
    "question": "The p-value of .557 obtained from the permutation test for the hypothesis that bodyfat (y) is larger than one when $x_{1}=71$ and $x_{2}=-209.25$ strongly supports rejecting the null hypothesis. True or False?",
    "question_context": "Multivariate Isotonic Regression Estimation and Hypothesis Testing\n\nFor this data also, we compute the $p$ -value of the test based on $T_{n}$ to check whether bodyfat (i.e., y) is larger than one or not when $x_{1}=71$ and $x_{2}=-209.25$ . To compute the $p$ -value, as we did for Auto MPG dataset, we carry out well-known permutation test and obtain the $p$ -value $=.557$ (i.e., favors the said hypothesis) as expected since for $x_{1}=71$ and $x_{2}=-209.25$ , $y=1.0468>1$ . <PARAGRAPH_SEPARATOR> # 6 CONCLUDING REMARKS <PARAGRAPH_SEPARATOR> In this article, we propose a least squares estimator of the multivariate isotonic regression function and study its asymptotic properties along with applications in the testing of hypothesis and the formulation of the pointwise confidence interval. In this context, we would like to point out that even for the smooth (i.e., differentiable) multivariate isotonic regression function, our estimator is not smooth enough; strictly speaking, it is not a differentiable function. To overcome this issue, one may consider the kernalized version of the IRE described here, which can be defined as follows. <PARAGRAPH_SEPARATOR> $$ \\hat{f}_{n,s m}(u_{1}^{*},\\ldots,u_{d}^{*})=\\frac{1}{n h_{n}}\\sum_{i=1}^{n}k\\left(\\frac{\\mathbf{u}^{*}-\\mathbf{x}_{i}}{h_{n}}\\right)\\hat{f}_{n}(u_{1}^{*},\\ldots,u_{d}^{*}), $$ <PARAGRAPH_SEPARATOR> where $k$ is a sufficiently smooth kernel function with bandwidth ${\\bf\\alpha}=h_{n}$ , $\\mathbf{u}^{*}=(u_{1}^{*},\\ldots,u_{d}^{*})$ , and $\\mathbf{x}_{i}$ is the ith covariate. One can hope that under some conditions, the smoothness of $\\cdot\\hat{f}_{n,s m}(u_{1}^{*},\\ldots,u_{d}^{*})$ will be the same as that of the kernel $k$ , and the isotonicity property of the estimator depends on the choice of the kernel $k$ . <PARAGRAPH_SEPARATOR> The issue of robustness is another topic of research since $\\hat{f}_{n}(.)$ is an average based estimator, and hence, it is expected that $\\hat{f}_{n}(.)$ will be influenced by the presence of the outliers/influential observations. Technically speaking, the breakdown point or the influence function may give us an idea about how much the estimator will be robust against the outliers. Moreover, one may also consider the median or the trimmed mean type of estimator so that the estimator possess good robustness property. For instance, in the case of univariate isotonic regression function, Dhar (2016) showed that trimmed mean IRE may achieve $25\\%$ asymptotic breakdown point. <PARAGRAPH_SEPARATOR> In order to implement the test described in Section 3, one needs to compute a certain quantile of $T(\\mathbb{W})^{\\prime}(0,\\ldots,0)$ , which is not easily tractable. As we indicated earlier, note here that $T(\\mathbb{W})^{\\prime}(0,\\ldots,0)$ can be thought as a multivariate extension of well-known Chernoff's distribution; however, unlike the univariate Chernoff's distribution, the accurate computation of the quantiles of multivariate version of Chernoff's distribution is not available in the literature. For univariate case, the readers may see Groeneboom and Wellner (2001). Instead of the simulation-based procedure of computing quantiles of $T(\\mathbb{W})^{\\prime}(0,\\ldots,0)$ described in Section 4.2, it may be of future interest of research about how to compute the quantiles exactly of the distribution of $T(\\mathbb{W})^{\\prime}(0,\\ldots,0)$ like the univariate Chernoff's distribution."
  },
  {
    "qid": "statistic-posneg-469-0-1",
    "gold_answer": "FALSE. The nearest-neighbour property Pr(ξ_j|ξ_1,ξ_2,...,ξ_{j-1},ξ_{j+1},...,ξ_s)=Pr(ξ_j|ξ_{j-1},ξ_{j+1}) is not equivalent to the Markov property Pr(ξ_t|ξ_0,ξ_1,...,ξ_{t-1})=Pr(ξ_t|ξ_{t-1}) in one-dimensional chains. The Markov property implies a directional dependence (typically time), whereas the nearest-neighbour property implies a symmetric dependence on both immediate neighbours (ξ_{j-1} and ξ_{j+1}), without any directional bias. This distinction is crucial in generalizing Markov chains to systems without directional properties.",
    "question": "Is the nearest-neighbour property Pr(ξ_j|ξ_1,ξ_2,...,ξ_{j-1},ξ_{j+1},...,ξ_s)=Pr(ξ_j|ξ_{j-1},ξ_{j+1}) equivalent to the Markov property Pr(ξ_t|ξ_0,ξ_1,...,ξ_{t-1})=Pr(ξ_t|ξ_{t-1}) in one-dimensional chains?",
    "question_context": "Conditional vs. Joint Probability Approaches in Nearest-Neighbour Systems\n\nIn the treatment of Markov Chains, the almost exclusive use of time as the parameter has led to a concept of a favoured direction. This way of thinking creates difficulties when one is forced to consider more general cases of chains in which there is a limited range of dependence, but no directional properties are implied by the parameter. In the one-dimensional Markov Chain we have the two means of specification; we may define Pr(ξ_t|ξ_0,ξ_1,...,ξ_{t-1})=Pr(ξ_t|ξ_{t-1}), which leads to Pr(ξ_t,ξ_2,...,ξ_t|ξ_0)=∏_{j=1}^t Pr(ξ_j|ξ_{j-1}), where ξ_0 may be assumed to have some constant value given by the boundary conditions. Alternatively, Pr(ξ_1,ξ_2,...,ξ_t|ξ_0)=∏_{j=1}^t Q_j(ξ_j,ξ_{j-1}). In this case there is no difficulty and the Q_j(ξ_j,ξ_{j-1}) can be readily identified with the conditional probabilities, Pr(ξ_j|ξ_{j-1}). However, if we now consider a chain which is specified by the nearest-neighbour property, the natural extension of the form of definition given by equation (1) is Pr(ξ_j|ξ_1,ξ_2,...,ξ_{j-1},ξ_{j+1},...,ξ_s)=Pr(ξ_j|ξ_{j-1},ξ_{j+1}), and the extension of the form of definition given in equation (3) is Pr(ξ_1,ξ_2,...,ξ_s|ξ_0,ξ_{s+1})=∏_{j=1}^s Q_j(ξ_{j-1},ξ_j,ξ_{j+1}), where ξ_0 and ξ_{s+1} have constant values given by the boundary conditions."
  },
  {
    "qid": "statistic-posneg-1592-0-1",
    "gold_answer": "TRUE. The original parameterization, although formally incorrect because $\\Gamma$ was not a correlation matrix, still correctly represented the probability distribution. This is due to the equivalence between the original density and the corrected density, as highlighted in the corrigendum. The correction ensures compliance with the formal condition but does not alter the underlying distribution.",
    "question": "Is it true that the original parameterization of the SUN distribution, despite not meeting the condition that $\\Gamma$ is a correlation matrix, still correctly represents the probability distribution?",
    "question_context": "Parameterization Correction in Skew-Normal Distributions\n\n# Corrigendum to “On the unification of families of skew-normal distributions” published in Scand. J. Stat. vol. 33, pp. 561–574\n\n<PARAGRAPH_SEPARATOR>\n\nAlthough the expression indicated at the end of Appendix A provides the correct density of the required conditional distribution, it is formally incorrect to denote it as a SUN distribution with the stated parameters, since they do not meet the condition introduced in Section 2.1 that the $\\Gamma$ component of the parameter set is a correlation matrix. The expression of $\\Gamma_{1.2}$ given in Appendix A has instead diagonal elements less than 1.\n\n<PARAGRAPH_SEPARATOR>\n\nThere is a fix to the problem, based on the simple fact the probability $\\Phi_{m}(\\gamma_{1.2};\\Gamma_{1.2})$ appearing in the denominator of $f_{Y_{1}|Y_{2}=y_{2}}$ can be rewritten as $\\Phi_{m}(D\\gamma_{1.2};D\\Gamma_{1.2}D)$ for any positive definite diagonal matrix $D$ , and a |similar equality holds for the probability in the numerator. Denote by Diag $(\\Gamma_{1.2})$ the diagonal matrix formed by the diagonal elements of $\\Gamma_{1.2}$ and define\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\begin{array}{r l}&{\\overline{{\\Delta}}_{1,2}=\\Delta_{1,2}\\mathrm{{Diag}}(\\Gamma_{1,2})^{-1/2}}\\ &{\\overline{{\\Gamma}}_{1,2}={\\mathrm{Diag}}(\\Gamma_{1,2})^{-1/2}\\Gamma_{1,2}\\mathrm{{Diag}}(\\Gamma_{1,2})^{-1/2},}\\ &{\\overline{{\\gamma}}_{1,2}={\\mathrm{Diag}}(\\Gamma_{1,2})^{-1/2}\\gamma_{1,2},}\\ &{\\overline{{\\Omega}}_{1,2}^{*}=\\left(\\overline{{\\Gamma}}_{1,2}\\quad\\overline{{\\Delta}}_{1,2}^{\\top}\\right)}\\ &{\\overline{{\\Delta}}_{1,2}=\\left(\\overline{{\\Delta}}_{1,2}\\quad\\overline{{\\Omega}}_{11,2}\\right)}\\end{array}\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nso that we can write $Y_{1}|Y_{2}=y_{2}\\sim\\mathrm{SUN}_{d_{1},m}(\\xi_{1.2},\\overline{{{\\gamma}}}_{1.2},\\overline{{{\\omega}}}_{1},\\overline{{{\\Omega}}}_{1.2}^{*})$ , whose parameters identify the same probability distribution| as the one indicated in the paper, but complying with the condition that the $\\Gamma$ component is a correlation matrix.\n\n<PARAGRAPH_SEPARATOR>\n\nIt is certainly unfortunate that an inappropriate statement was made in the paper, but we underline that anyone using the distribution with the parameters originally indicated did not incur in an error because of the equivalence between that density and the one indicated here. A second consideration is that the present adjustment is based on the existence of equivalent densities, if the condition on the scale factor of the $\\Gamma$ matrix is not imposed. This reinforces the recurrent indication in the paper of the appropriateness of this condition to avoid overparameterization.\n\n<PARAGRAPH_SEPARATOR>\n\n# ACKNOWLEDGEMENTS"
  },
  {
    "qid": "statistic-posneg-334-0-0",
    "gold_answer": "TRUE. The inequality is derived from the Massart version of the Dvoretzky–Kiefer–Wolfowitz inequality and holds under the assumption that $2^{j}\\leq{\\sqrt{n/\\ln(n)}}$. The proof leverages the properties of the empirical distribution function and the wavelet basis functions.",
    "question": "The inequality $\\mathbb{P}\\left(|\\hat{d}_{j,k}-d_{j,k}|\\ge\\lambda_{j}\\right)\\le2\\frac{1}{n^{p}}$ holds for all $j$ such that $2^{j}\\leq{\\sqrt{n/\\ln(n)}}$. Is this statement true given the assumptions in the paper?",
    "question_context": "Nonparametric Estimation of a Quantile Density Function Using Wavelets\n\nThe paper discusses nonparametric estimation of a quantile density function using wavelet methods. It presents several lemmas and theorems that establish bounds on the estimation error and convergence rates. Key formulas include bounds on the probability of large deviations of wavelet coefficient estimates and the expected Lp error of the wavelet-based estimator."
  },
  {
    "qid": "statistic-posneg-713-0-0",
    "gold_answer": "TRUE. The Stieltjes transform is indeed defined as $m^{F^{A_{n}}}(z) = \\int \\frac{1}{x - z} dF^{A_{n}}(x)$, which is a standard definition in random matrix theory. This transform is crucial for analyzing the spectral properties of large dimensional random matrices, as it provides a bridge between the empirical spectral distribution and its limiting behavior.",
    "question": "Is the Stieltjes transform $m^{F^{A_{n}}}(z)$ correctly defined as the integral of $(x - z)^{-1}$ with respect to the empirical spectral distribution $F^{A_{n}}(x)$?",
    "question_context": "Empirical Spectral Distribution and Stieltjes Transform in Random Matrix Theory\n\nSuppose $A_{n}$ is an $n\\times n$ Hermitian matrix with eigenvalues $\\lambda_{1},\\dots,\\lambda_{n}$ . Define the empirical spectral distribution (ESD) of $A_{n}$ as $F^{A_{n}}(x)=\\frac{1}{n}\\sum_{i=1}^{n}I(\\lambda_{i}\\leq x)$. The limit distribution of $F^{A_{n}}$ is called the limiting spectral distribution (LSD) of the sequence $\\left\\{A_{n}\\right\\}$ And the Stieltjes transform of $F^{A_{n}}$ is given by $m^{F^{A_{n}}}(z)=\\int\\frac{1}{x-z}d F^{A_{n}}(x)=\\frac{1}{n}\\mathrm{tr}(A_{n}-z I_{n})^{-1}$, where $z=\\mu+i\\nu\\in\\mathcal{C}^{+}$ . By the inverse formula, $F^{A_{n}}\\{[a,b]\\}=\\operatorname*{lim}_{v\\rightarrow0^{+}}\\frac{1}{\\pi}\\int_{a}^{b}\\mathrm{Im}(m^{F^{A_{n}}}(x+i v))d x$."
  },
  {
    "qid": "statistic-posneg-1078-0-2",
    "gold_answer": "FALSE. In the error-correction representation, the matrix C = -Φ(1) has rank k - d, where d is the number of unit roots (0 < d < k). Thus, C is not full rank unless d = 0, which corresponds to the stationary case.",
    "question": "In the error-correction representation of an I(1) VARMA model, the matrix C is always full rank. True or False?",
    "question_context": "Structured VARMA Models and Exact Maximum Likelihood Estimation\n\nThe paper discusses structured VARMA models, focusing on echelon forms based on Kronecker indices and scalar component models (SCM). It covers I(1) VARMA models, their transformation to stationary VARMA models, and the evaluation of exact likelihood using recurrence equations. The paper also compares exact maximum likelihood estimation with conditional methods."
  },
  {
    "qid": "statistic-posneg-264-0-2",
    "gold_answer": "FALSE. The stationarity condition requires that all eigenvalues of the matrix $A$ have negative real parts, not positive. This is explicitly mentioned in the context: 'For a constant volatility function $\\sigma(t),\\mathbf{X}(t)$ is stationary as long as all eigenvalues of $A$ have negative real part.' This ensures that the variance matrix converges as $t\\to\\infty$.",
    "question": "In the CAR(p) model, the stationarity of the process $\\mathbf{X}(t)$ requires that all eigenvalues of the matrix $A$ have positive real parts. Is this condition correctly stated?",
    "question_context": "Temperature Derivatives and Continuous-Time Autoregressive Models\n\nThe CME organizes trade in futures and options written on the temperatures in 18 US cities, 9 European cities and 2 Japanese cities. The European cities are Amsterdam, Barcelona, Berlin, Essen, London, Madrid, Paris, Rome and Stockholm. Later, we shall study the temperature dynamics in Stockholm based on a long record of daily measurements, and analyze the implications on relevant futures and options traded at the CME for this city. <PARAGRAPH_SEPARATOR> The market for temperature derivatives at the CME is based on three types of temperature indices: HDD, CDD and CAT. The HDD over a time period from, say, day $\\tau_{1}$ to $\\tau_{2}>\\tau_{1}$ is the accumulated temperatures below a certain threshold, usually determined to be $c=18^{\\circ}\\mathrm{C}$ , or equivalently $c=65^{\\circ}\\mathrm{F}.$ The index measures how many degrees that has accumulated below the threshold over the measurement period, being for instance an indicator of the demand of household heating. On the other hand, the CDD measures the accumulated temperature above $c$ , and measures the need for cooling. In the US, air-conditioners are usually switched on at a temperature of $65^{\\circ}\\mathrm{F}$ or higher. The CAT index is simply the accumulation of the daily average temperatures over the measurement period. <PARAGRAPH_SEPARATOR> Letting $T_{i}$ be the average of the minimum and maximum temperature on day $i$ , the HDD is defined to be <PARAGRAPH_SEPARATOR> $$ \\mathrm{HDD}(\\tau_{1},\\tau_{2})=\\sum_{i=\\tau_{1}}^{\\tau_{2}}\\operatorname*{max}(c-T_{i},0), $$ <PARAGRAPH_SEPARATOR> while the CDD is <PARAGRAPH_SEPARATOR> $$ \\mathrm{CDD}(\\tau_{1},\\tau_{2})=\\sum_{i=\\tau_{1}}^{\\tau_{2}}\\operatorname*{max}(T_{i}-c,0). $$ <PARAGRAPH_SEPARATOR> The CAT is defined as <PARAGRAPH_SEPARATOR> $$ \\mathbf{CAT}(\\tau_{1},\\tau_{2})=\\sum_{i=\\tau_{1}}^{\\tau_{2}}T_{i}. $$ <PARAGRAPH_SEPARATOR> There is an index only relevant for the two Japanese cities called the Pacific Rim. However, this is the average of the CAT, and we shall not discuss this in further detail as all results valid for the CAT index are easily modified for the Pacific Rim. <PARAGRAPH_SEPARATOR> At the CME, the measurement periods for the different indices are standardized to be each month of the year and two seasons. The winter season is from October to April, and summer season from April to October, where there are contracts with measurement periods from two to seven months in each season. For the US cities, contracts are written on the HDD and CDD for the respective periods, with CDDs for the summer, and HDDs for the winter. The CDDs are substituted with the CAT index in the summer season for the European cities. <PARAGRAPH_SEPARATOR> The futures contracts are all cash settled, and the owner of a futures receives 20 times the index value at the end of the measurement period, in return for a fixed price (which is called the futures price of the contract). The currency is British pounds for the European futures contracts, while the US contracts are naturally denominated in US dollars. Trading of the futures contracts goes on up to the beginning of the measurement period. By definition of a futures contract, it does not cost anything to buy it. The payment takes place in the future, and entering the contract is simply an agreement of a future trade. In the context of temperatures, it is an agreement to exchange a fixed amount of money for a variable amount linked to the temperature index. In popular terms, the buyer of a temperature futures is changing a variable and an uncertain future temperature event against a fixed and known. Such an agreement may of course end with either a profit or a loss, depending on the outcome of the temperature event. The futures price is set based on the market’s expectations of the value of the index, and on supply and demand of contracts. <PARAGRAPH_SEPARATOR> The CME organizes a market for call and put options written on the temperature futures. A call option is a contract that gives the owner the right to buy the underlying asset for a fixed price at an agreed time. The owner is not obliged to buy, but exercises his or her option only if this is to his or her advantage (being the case if the fixed price in the option contract is lower than the market value of the underlying, giving him or her a discount when purchasing). The underlying asset in the CME market is a temperature future (written on the HDD, CDD or CAT index, with a specified measurement period). The fixed price in the option contract is called the strike price, whereas the agreed time for using the option is called the exercise time of the contract. A put option gives the owner the right to sell the underlying. To fix some notation, suppose the futures price at exercise time $\\tau\\leq\\tau_{1}$ is $F(\\tau,\\tau_{1},\\tau_{2})$ for a futures with measurement period $[\\tau_{1},\\tau_{2}]$ . Then, the owner of a call option written on that futures will receive <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\operatorname*{max}(F(\\tau,\\tau_{1},\\tau_{2})-K,0),}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $K$ is the strike price. The options at CME are cash settled, and so the owner receives the difference between the futures price and the strike if exercise takes place. The owner does not use the option if the futures price is lower. In the market, options with different strike prices and exercise times are offered for futures on different indices at all times. Options are a way for the traders in the market to trade in beliefs of high or low futures prices at a low cost, providing a tool for speculation or hedging. The fair price to buy an option contract is derived via a hedging strategy and the principle of no-arbitrage. However, this requires a stochastic model for the temperature dynamics, which implies a dynamics for the futures price being the basic ingredient in the option valuation. In the next section we propose and analyze a continuous-time autoregressive model for the temperature dynamics. <PARAGRAPH_SEPARATOR> # 3. A continuous-time autoregressive model with seasonal variation <PARAGRAPH_SEPARATOR> In this section we define a general continuous-time autoregressive model, which is the continuous-time analogue of an $\\operatorname{AR}(p)$ time series. The class of models is called ${\\mathrm{CAR}}(p)$ , and was first studied by Phillips (1959). <PARAGRAPH_SEPARATOR> Suppose that $(\\Omega,{\\mathcal{F}},P)$ is a probability space with a filtration $\\{\\mathcal{F}_{t}\\}_{0\\le t\\le\\tau_{\\mathrm{max}}}$ satisfying the usual assumptions. Here, $\\tau_{\\mathrm{max}}$ denotes a maximal time covering all times of interest in the market. Denote by $B(t)$ a Brownian motion defined on this filtered probability space. <PARAGRAPH_SEPARATOR> # 3.1. Continuous-time AR-models <PARAGRAPH_SEPARATOR> Let $\\mathbf{X}(t)$ be a stochastic process in $\\mathbb{R}^{p}$ for $p\\geq1$ defined by the vectorial Ornstein–Uhlenbeck equation <PARAGRAPH_SEPARATOR> $$ \\mathrm{d}\\mathbf{X}(t){=}A\\mathbf{X}(t)\\mathrm{d}t{+}\\mathbf{e}_{p}\\sigma(t)\\mathrm{d}B(t), $$ <PARAGRAPH_SEPARATOR> with $\\mathbf{e}_{k}$ being the $k$ th unit vector in $\\mathbb{R}^{p}$ $^p,k=1,\\ldots,p$ . Further, $\\sigma(t)>0$ is a real-valued and square integrable function (over any finite time interval) and $A$ is the $p\\times p$ matrix <PARAGRAPH_SEPARATOR> $$ A={\\left[\\begin{array}{l l l l l l}{0}&{1}&{0}&{\\cdots}&{0}\\ {0}&{0}&{1}&{\\cdots}&{0}\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}\\ {0}&{0}&{0}&{0}&{1}\\ {-\\alpha_{p}}&{-\\alpha_{p-1}}&{-\\alpha_{p-2}}&{\\cdots}&{-\\alpha_{1}}\\end{array}\\right]}. $$ <PARAGRAPH_SEPARATOR> We suppose that $\\upalpha_{k}$ , $k=1,\\hdots,p$ , are constants. With $\\Lambda(t)$ being a deterministic seasonal function representing the average temperature, we introduce the following ${\\mathrm{CAR}}(p)$ model for the temperature dynamics: <PARAGRAPH_SEPARATOR> $$ T(t)=\\Lambda(t)+X_{1}(t). $$ <PARAGRAPH_SEPARATOR> Here, $X_{q}$ is the qth coordinate of the vector $\\mathbf{X}$ , $q=1,\\ldots,p$ . As we shall see in Subsection 3.2, the volatility function $\\sigma(t)$ will play an important role in the dynamics of the temperature. It turns out that the temperature residuals possess a variation that is a function of the season. It has a rather characteristic shape, which we model by using a truncated Fourier series. The variation function $\\sigma(t)$ will be strictly positive, continuous, and bounded. <PARAGRAPH_SEPARATOR> We can represent the stochastic process $\\mathbf{X}(t)$ explicitly by solving the stochastic differential equation (5): <PARAGRAPH_SEPARATOR> Lemma 1 The stochastic process $\\mathbf{X}(t)$ has the explicit form <PARAGRAPH_SEPARATOR> $$ \\mathbf{X}(s){=}\\exp\\left(A(s-t)\\right)\\mathbf{x}+\\int_{t}^{s}\\exp\\left(A(s-u)\\right)\\mathbf{e}_{p}\\sigma(u)\\mathrm{d}B(u), $$ <PARAGRAPH_SEPARATOR> for $s\\geq t\\geq0$ and $\\mathbf{X}(t)=\\mathbf{x}\\in\\mathbb{R}^{p}$ . <PARAGRAPH_SEPARATOR> Proof. Follows by an application of the multi-dimensional Ito Formula, which can be found in Øksendal (1998, theorem 4.2.1, page 48). <PARAGRAPH_SEPARATOR> For a constant volatility function $\\sigma(t),\\mathbf{X}(t)$ is stationary as long as all eigenvalues of $A$ have negative real part. This can be seen from proposition 6.2 of Ichihara & Kunita (1974). In our case, the stationarity holds when the variance matrix <PARAGRAPH_SEPARATOR> $$ \\int_{0}^{t}\\sigma^{2}(t-s)\\exp({A s})\\mathbf{e}_{p}\\mathbf{e}_{p}^{\\prime}\\exp({A^{\\prime}s})\\mathrm{d}s $$ <PARAGRAPH_SEPARATOR> converges as $t\\to\\infty$ , which is the case if all the eigenvalues of $A$ have negative real part and $\\sigma(t)$ is bounded. <PARAGRAPH_SEPARATOR> # 3.2. Empirical analysis of Stockholm temperatures <PARAGRAPH_SEPARATOR> We have available daily average temperatures (DAT) measured on Celsius scale from Stockholm over a period ranging from 1 January 1961 to 25 May 2006, resulting in 16,581 records. The DATs are computed as the average of the minimum and maximum temperature over the day. February 29 was removed from the sample in each leap year to have years of equal size, leading to a time series of 16,570 observations. Our aim is to fit a time-series model that we can associate to the ${\\mathrm{CAR}}(p)$ -model (5). A straightforward way suggested by Phillips (1959) is to estimate the parameters in the corresponding $\\operatorname{AR}(p)$ -model, which is the path we follow here. <PARAGRAPH_SEPARATOR> Define the seasonal AR(3) time series: <PARAGRAPH_SEPARATOR> $$ x_{i}=T_{i}-\\Lambda_{i},\\quad i=0,1,2,3,\\ldots. $$ <PARAGRAPH_SEPARATOR> where $T_{i}$ is the DAT on day $i$ and $\\Lambda_{i}$ is the seasonal function defined as $\\Lambda_{i}:=\\Lambda(i)$ , with <PARAGRAPH_SEPARATOR> $$ \\Lambda(t)=a_{0}+a_{1}t+a_{2}\\cos(2\\pi(t-a_{3})/365). $$ <PARAGRAPH_SEPARATOR> Further, we suppose that $x_{i}$ is an AR(3)-process with seasonal residuals, modelled as <PARAGRAPH_SEPARATOR> $$ y_{i+3}=\\beta_{1}y_{i+2}+\\beta_{2}y_{i+1}+\\beta_{3}y_{i}+\\sigma_{i}\\epsilon_{i}, $$ <PARAGRAPH_SEPARATOR> with $\\sigma_{i}:=\\sigma(i)$ , for $\\sigma(t)$ defined as <PARAGRAPH_SEPARATOR> $$ \\sigma^{2}(t)=c_{1}+\\sum_{k=1}^{4}[c_{2k}\\cos(2k\\pi t/365)+c_{2k+1}\\sin(2k\\pi t/365)]. $$ <PARAGRAPH_SEPARATOR> In Fig. 1, we have plotted the observed DATs together with the seasonal function $\\Lambda(t)$ , which we fitted using least squares minimization. The parameters of $\\Lambda(t)$ are reported in Table 1. <PARAGRAPH_SEPARATOR> The partial autocorrelation function (PACF) plot in Fig. 2 of the deseasonalized DAT-data indicates that an AR(3) process may be suitable for modelling the time evolution of these residuals. We regress today’s detrended and deseasonalized temperatures against the temperatures of the three previous days. The values of the regression parameters are all significant at the level of $1\\%$ and are presented in Table 2. Admittedly, the $p$ -values here have to be treated with care as the assumptions of the regression model are violated. However, the successive analysis clearly indicates that the AR(3) process fits data well, and therefore we believe that the obtained $p$ -values are reasonable. The constant in the regression is insignificant, as expected, and we assume it to be zero. The $R^{2}$ of the regression is $94.1\\%$ . <PARAGRAPH_SEPARATOR> In Fig. 3 the residuals and their squares for the last 10 years are plotted. We observe a persistent variation in the variance of the noise. The autocorrelation function (ACF) of the residuals and their squares are presented in Fig. 4. The autocorrelations for the first few lags are insignficant according to the Box–Ljung statistic; however, we see a clear seasonal pattern here. The seasonality pattern is confirmed by the ACF for squared residuals."
  },
  {
    "qid": "statistic-posneg-786-1-0",
    "gold_answer": "FALSE. The formula incorrectly includes the term $\\sum_{j=1\\atop j\\neq i}^{N}\\boldsymbol{\\theta}_{j}^{-1}$ as an additive component within the summation. The correct interpretation should involve the conditional expectation density $\\boldsymbol{\\mathcal{E}}_{i}(\\tau)$ weighted by the probability $\\theta_{\\hat{\\imath}}^{-1}/\\Sigma\\theta_{\\hat{\\jmath}}^{-1}$, without adding the inverse rates of other sequences. The economic intuition is that the expectation density for the pooled process should reflect the weighted average of individual sequence densities, not an arbitrary sum of inverse rates.",
    "question": "Is the formula for $\\boldsymbol{\\mathcal{E}}(\\tau)$ correctly derived as the sum over all sequences $i$ of the product of the probability that an event belongs to sequence $i$ and the sum of $\\boldsymbol{\\mathcal{E}}_{i}(\\tau)$ and the inverse rates $\\boldsymbol{\\theta}_{j}^{-1}$ of all other sequences $j \\neq i$?",
    "question_context": "Superposition of Random Sequences: Expectation Density and Recurrency Analysis\n\nPooling of independent impulse series has been extensively dealt with by Cox & Smith (1953, 1954) with reference to another problem in theoretical neurophysiology, the probabilistic release of quantal transmitter substance at the neuromuscular junction. <PARAGRAPH_SEPARATOR> In their 1953 paper these authors considered sources at each of which events occur at strictly regular intervals with mutually irrational periods. In the 1954 paper the intervals <PARAGRAPH_SEPARATOR> Because the probability that the event at $t_{0}$ belongs to the ith sequence equals $\\theta_{\\hat{\\imath}}^{-1}/\\Sigma\\theta_{\\hat{\\jmath}}^{-1}$ the function $\\pmb{\\mathcal{E}}(\\tau)$ is found by summing the product of this probability and {prob $\\left(a\\right)+\\mathbf{prob}\\left(b\\right)\\}$ over i from 1 to $\\pmb{N}$ . In symbols <PARAGRAPH_SEPARATOR> $$ \\boldsymbol{\\mathcal{E}}(\\tau)=\\boldsymbol{\\theta}\\sum_{i=1}^{N}\\left\\{\\boldsymbol{\\mathcal{E}}_{i}(\\tau)+\\sum_{j=1\\atop j\\neq i}^{N}\\boldsymbol{\\theta}_{j}^{-1}\\right\\}\\boldsymbol{\\theta}_{i}^{-1}. $$ <PARAGRAPH_SEPARATOR> As mentioned in $\\S1$ , & measure of deviation from recurrency is obtained by comparing this expression with the expectation density if $f(t)$ were recurrent, given by (cf. Appendix) <PARAGRAPH_SEPARATOR> $$ E(\\tau)=p(\\tau)+\\sum_{l=2}^{\\infty}p^{[\\ l]}(\\tau), $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ \\mathcal{P}^{\\perp}(\\tau)=\\int_{0}^{\\tau}\\mathcal{P}^{\\perp-1}(\\overline{{{\\tau}}})\\mathcal{P}(\\tau-\\overline{{{\\tau}}})d\\widetilde{\\tau},\\quad\\mathrm{with}\\quad\\mathcal{P}^{\\perp1}(\\tau)=\\mathcal{p}(\\tau). $$"
  },
  {
    "qid": "statistic-posneg-906-4-0",
    "gold_answer": "FALSE. The condition $[\\widehat{\\bf X}^{\\perp}\\Sigma\\widehat{\\bf X}_{2}^{\\perp}\\widehat{\\bf X}_{1},\\widehat{\\bf X}^{\\perp}\\Sigma\\widehat{\\bf X}_{1}^{\\perp}\\widehat{\\bf X}_{2}]={\\bf0}$ is derived from the joint matrix equalities in Corollary 4.3(a)(iv) and (b)(iv), which are part of the proof that establishes the equivalence between statements (a), (b), and (c) in Theorem 4.5. However, the direct equivalence is not explicitly stated in the context. The condition is a mathematical representation of the orthogonality and independence between the estimators of $\\mathbf{X}_{1}{\\boldsymbol{\\beta}}_{1}$ and $\\mathbf{X}_{2}{\\boldsymbol{\\beta}}_{2}$, but it is not the sole condition for the equality of OLSE and BLUE for the combined parameters.",
    "question": "Is the condition $[\\widehat{\\bf X}^{\\perp}\\Sigma\\widehat{\\bf X}_{2}^{\\perp}\\widehat{\\bf X}_{1},\\widehat{\\bf X}^{\\perp}\\Sigma\\widehat{\\bf X}_{1}^{\\perp}\\widehat{\\bf X}_{2}]={\\bf0}$ equivalent to the statement that the OLSE and BLUE of the combined parameters $\\mathbf{X}_{1}{\\boldsymbol{\\beta}}_{1}+\\mathbf{X}_{2}{\\boldsymbol{\\beta}}_{2}$ are equal?",
    "question_context": "Equivalence Conditions for OLSE and BLUE in Linear Regression Models\n\nTheorem 4.5. Let $\\mathcal{P}_{r}$ be as given in (1.4), and assume that $\\mathbf{X}_{1}\\boldsymbol{\\beta}_{1}$ and $\\mathbf{X}_{2}\\pmb{\\beta}_{2}$ in (1.4) are estimable, respectively. Then, the following statements are equivalent: (a) $\\operatorname{OLSE}_{\\mathcal{P}_{r}}(\\mathbf{X}_{1}{\\boldsymbol{\\beta}}_{1}+\\mathbf{X}_{2}{\\boldsymbol{\\beta}}_{2})=\\operatorname{BLUE}_{\\mathcal{P}_{r}}(\\mathbf{X}_{1}{\\boldsymbol{\\beta}}_{1}+\\mathbf{X}_{2}{\\boldsymbol{\\beta}}_{2})$ holds definitely. (b) $\\operatorname{OLSE}_{\\mathcal{P}_{r}}(\\mathbf{X}_{1}\\pmb{\\beta}_{1})=\\operatorname{BLUE}_{\\mathcal{P}_{r}}(\\mathbf{X}_{1}\\pmb{\\beta}_{1})$ and $\\operatorname{OLSE}_{\\mathcal P_{r}}(\\mathbf{X}_{2}\\pmb{\\beta}_{2})=\\operatorname{BLUE}_{\\mathcal P_{r}}(\\mathbf{X}_{2}\\pmb{\\beta}_{2})$ hold definitely. (c) $\\operatorname{OLSE}_{\\mathcal{P}_{r}}(\\widehat{\\mathbf{X}}_{2}^{\\perp}\\mathbf{X}_{1}\\pmb{\\beta}_{1})=\\operatorname{BLUE}_{\\mathcal{P}_{r}}(\\widehat{\\mathbf{X}}_{2}^{\\perp}\\mathbf{X}_{1}\\pmb{\\beta}_{1})$ and $\\operatorname{OLSE}_{\\mathcal{P}_{r}}(\\widehat{\\mathbf{X}}_{1}^{\\perp}\\mathbf{X}_{2}\\pmb{\\beta}_{2})=\\operatorname{BLUE}_{\\mathcal{P}_{r}}(\\widehat{\\mathbf{X}}_{1}^{\\perp}\\mathbf{X}_{2}\\pmb{\\beta}_{2})$ hold definitely."
  },
  {
    "qid": "statistic-posneg-1154-0-1",
    "gold_answer": "TRUE. The MBD is defined as the average proportion of time the curve x lies within bands formed by pairs of curves from the sample, as given by the formula: MBD(x; x_{1:n}) = (n choose 2)^{-1} Σ λ[{s ∈ [0,1] | min(x_{i1}(s), x_{i2}(s)) ≤ x(s) ≤ max(x_{i1}(s), x_{i2}(s))}].",
    "question": "Is it correct to say that the modified band depth (MBD) of a curve x with respect to a sample x_{1:n} is calculated by averaging the proportion of time x lies within bands formed by all possible pairs of curves from the sample?",
    "question_context": "Functional Time Series Analysis and Depth Measures\n\nThe article discusses functional time series analysis, where data are collections of functional observations. It introduces the concept of functional random variables and their properties, such as stationarity and the functional autoregressive model (FAR). The modified band depth (MBD) and extremal depth (ED) are presented as measures to evaluate the centrality or extremality of a curve with respect to a distribution function. The MBD computes the proportion of time a curve is within a band formed by two other curves, while the ED measures the pointwise extremeness of a curve."
  },
  {
    "qid": "statistic-posneg-1531-0-0",
    "gold_answer": "TRUE. The condition 'coarsened completely at random' requires that the function $f_{\\gamma}^{G|x}(g|x)$ is constant for all $x\\in\\Xi$ when $G$ is observed exactly. This ensures that the coarsening mechanism does not depend on the underlying data $X$, making the coarsening ignorable for frequentist inferences. This is explicitly stated in the context: 'the data are coarsened completely at random if and only if, for the observed $\\pmb{g}$ and for all $\\gamma_{:}$ the function $f_{\\gamma}^{G|x}(g|x)$ takes the same value for all $\\boldsymbol{x}\\in\\Xi$'.",
    "question": "Is the condition for ignorability in frequentist inferences, as defined by 'coarsened completely at random', equivalent to the condition that the function $f_{\\gamma}^{G|x}(g|x)$ takes the same value for all $x\\in\\Xi$ when $G$ is observed exactly?",
    "question_context": "Ignorability in General Incomplete-Data Models\n\nIn studying ignorability for frequentist inferences, it is convenient to consider the degree of coarsening as a distinct data element, a feature that was absent from the Heitjan-Rubin model. Accordingly, this paper extends the Heitjan-Rubin model and uses it to establish the appropriate frequentist ignorability condition, which we call 'coarsened completely at random' because it generalises 'missing completely at random', the condition for the missing-data model. We also update the Bayes/likelihood ignorability theory to the extended model. We illustrate the model and ignorability conditions in a number of common incomplete-data situations."
  },
  {
    "qid": "statistic-posneg-1497-1-0",
    "gold_answer": "FALSE. The table shows that the coefficient of variation for range-based estimates (a) increases from 0.83 to 1.41 as β₂ increases from 1 to 9, while the r.m.s.-based estimates (b) increase from 0.50 to 1.80. Although range-based estimates have lower coefficients of variation at lower β₂ values, the difference narrows as β₂ increases. The claim of greater robustness for range-based tests is not uniformly supported across all β₂ values, especially at higher levels of non-normality (β₂ > 3).",
    "question": "Is the claim 'tests based on range are more robust than those based on r.m.s.' supported by the coefficients of variation presented in the table for β₂ values from 1 to 9?",
    "question_context": "Robustness of Range vs. RMS in Small Sample Dispersion Tests\n\nThe paper discusses tests for dispersion based on small samples, comparing the robustness of tests based on sample range versus sample r.m.s. for normal and non-normal populations. It presents coefficients of variation for estimates derived from range and r.m.s. as functions of β₂, showing that range-based tests are more robust. The context also explores transformations of observations to meet normality and additivity assumptions, using residuals (z) and fitted values (Y) to detect non-additivity, heteroscedasticity, and non-normality."
  },
  {
    "qid": "statistic-posneg-703-3-2",
    "gold_answer": "FALSE. The context states that under the null hypothesis, all permutations of the scores {t_rn} are equally likely, but this does not imply that the test statistic v is invariant under these permutations. Instead, the distribution of v is determined by considering all possible permutations of the scores.",
    "question": "Is the test statistic v = Σx_j t_j invariant under permutations of the scores {t_rn} when the null hypothesis is true?",
    "question_context": "Exponential Regression Analysis and Hypothesis Testing\n\nSuppose that we want to test the observations $y_{1},...,y_{n}$ for regression on the fixed quantities $x_{1},...,x_{n}$ .It is worth outlining briefly the procedure to be followed if $y_{1},...,y_{n}$ are assumed exponentially distributed. In the absence of specific reason to the contrary, it is sensible to consider a simple regression model chosen for mathematical convenience. This is that the $y_{j}$ are independently exponentially distributed with means $1/(\\alpha+\\beta x_{j})$ . The null hypothesis is that $\\beta=0$ <PARAGRAPH_SEPARATOR> The likelihood is <PARAGRAPH_SEPARATOR> $$ \\exp{(-\\alpha\\Sigma y_{j}-\\beta\\Sigma x_{j}y_{j})}\\Pi(\\alpha+\\beta x_{j}), $$ <PARAGRAPH_SEPARATOR> the sufficient statistic is $(\\Sigma y_{j},\\Sigma x_{j}y_{j})$ and the “exact’ test is based on the conditional distribution of $\\Sigma{}x_{j}y_{j}$ given $\\Sigma{y}_{j}$ which is independent of the nuisance parameter $\\upalpha$ Vincent (1961) has discussed this test in the different context of tests on sample estimates of variance. <PARAGRAPH_SEPARATOR> An interesting special case arises when $x_{j}=j$ ; if the $x_{j}$ are interpreted as intervals between events in a point proces, the nuil hypothesis is that we have a Poisson process, whereas under the alternative process there is a trend with the serial number of the event. This can be compared with the model and test considered by Cox (1955) of a time-dependent Poisson process in which the probability rate of occurrence at time $\\boldsymbol{\\mathscr{r}}$ is $\\alpha e^{\\beta\\tau}$ .  The two significant tests of the null hypothesis $\\beta=0$ are almost identical; the models are, of course, different when $\\beta\\neq0$ , although equivalent in a deterministic approximation. <PARAGRAPH_SEPARATOR> Another special case is the two-sample problem of Section 3 obtained when the $x_{i}$ takesvalues $1,0$ Only. <PARAGRAPH_SEPARATOR> Turning now to the test based on scores, we rank the $y_{j}$ replace them by the appropriate scores and consider the test statistic <PARAGRAPH_SEPARATOR> $$ v=\\Sigma x_{j}t_{j}, $$ <PARAGRAPH_SEPARATOR> where the random variable $t_{j}$ is the score attached to the jth observation. Under the null hypothesis, all permutations of the scores $\\{t_{r n}\\}$ are equally likely. <PARAGRAPH_SEPARATOR> We shall not develop the theory of $v$ beyond the simplest normal approximation, for which we need the mean and variance of $v$ over all permutations of $\\{t_{r n}\\}$ .Now $E(v)$ is bilinear in $\\{x_{j}\\}$ and $\\{t_{r n}\\}$ and is symmetric in the two sets separately. Hence <PARAGRAPH_SEPARATOR> $$ E(v)=a_{n}s_{1n}\\Sigma x_{j}=a_{n}n\\Sigma x_{j}, $$ <PARAGRAPH_SEPARATOR> where $a_{n}$ depends only on $n$ . Consideration of the special case $x_{1}=\\ldots=x_{n}=1$ shows that $a_{n}=1/n$ and that <PARAGRAPH_SEPARATOR> $$ E(v)=\\Sigma x_{j}. $$ <PARAGRAPH_SEPARATOR> Similarly, we can show from considerations of degree and invariance that <PARAGRAPH_SEPARATOR> $$ \\mathrm{var}(v)=b_{n}\\Sigma(x_{j}-\\bar{x})^{2}K_{2n}, $$ <PARAGRAPH_SEPARATOR> where $b_{n}$ depends only on $n$ . If we consider the special case $x_{1}=1$ ， $x_{2}=...=x_{n}=0$ we find on substituting into (10) that $b_{n}=1$ <PARAGRAPH_SEPARATOR> Thus the large-sample test is to take <PARAGRAPH_SEPARATOR> $$ (v-\\Sigma x_{j})/\\{\\Sigma(x_{j}-\\bar{x})^{2}K_{2n}\\}^{\\frac{1}{2}} $$ <PARAGRAPH_SEPARATOR> as normally distributed with mean zero and unit variance. <PARAGRAPH_SEPARATOR> In special cases, as in Section 3, it may be possible to obtain a better approximation fairly easily."
  },
  {
    "qid": "statistic-posneg-951-0-0",
    "gold_answer": "FALSE. The χ² test statistic (8.9) with 2 degrees of freedom yields a P-value of 0.011, which is below conventional significance thresholds (e.g., 0.05). This indicates we reject the null hypothesis that δ_c values are equal across occupational categories. The point estimates (0.027, 0.027, 0.019) further show numerical differences, particularly for unskilled manual workers.",
    "question": "Is the statement 'The estimated excess mortality risk for the unemployed (δ_c) is identical across all occupational categories (non-manual, skilled manual, unskilled manual)' supported by the statistical evidence (χ²=8.9, f=2, P=0.011) provided in the context?",
    "question_context": "Occupation-Specific Mortality Modeling: Unemployment Excess Risk\n\nIt was already apparent in the first publication of the data (Andersen, 1985) that the patterns of age-specific mortality deviated considerably between the main occupational groups self-employed, non-manual, skilled manual and unskilled manual. We omit self-employed (including “helping spouse') from the data in the rest of this paper, since in this group unemployment is poorly defined. Taking occupational category $^{c}$ into account amounts to specifying the death intensities $$\\uplambda_{a,c,\\mathrm{unemp}}^{\\gamma_{c}}=\\uplambda_{a,c,\\mathrm{emp}}^{\\gamma_{c}}+\\updelta_{c},$$ and we first note that the three separately estimated $\\hat{\\gamma}_{c}$ come out very close, and indeed close to 0.5 which we choose as a common value for $\\pmb{\\gamma}$ . With this assumption, the $\\hat{\\delta}_{c}$ for non-manual, skilled manual and unskilled manual become 0.027, 0.027 and 0.019 respectively. These values cannot be considered equal using the Poisson likelihood: $\\chi^{2}=8.9,f=2$ $P=0.011$"
  },
  {
    "qid": "statistic-posneg-1565-1-0",
    "gold_answer": "FALSE. While $E(\\omega_{k})=1$ ensures the estimator $\\tilde{\\Theta}$ is unbiased for $\\Theta$ under the given interpolation scheme, the context does not explicitly verify that the interpolation preserves unbiasedness in all cases. The interpolation method (averaging nearest neighbors) may introduce bias if the missingness pattern or the function $f$ has specific properties not accounted for in the model.",
    "question": "Is the statement 'The estimator $\\tilde{\\Theta}$ remains unbiased for $\\Theta$ even when some values $f(y_{k})$ are missing and interpolated, as shown by $E(\\omega_{k})=1$' true based on the provided context?",
    "question_context": "Variance Estimation for Generalized Cavalieri Estimators with Missing Data\n\nSometimes in practice the value of $f$ cannot be determined at some locations. We study the variance of an estimator of $\\Theta$ based on interpolation to approximate the missing values $f(y_{k})$ . The sample locations are given by the process $\\Phi=\\{y_{k}\\}_{k\\in\\mathbb{Z}}$ with intensity $\\mu>0$ . We define $\\tilde{\\Theta}=\\frac{1}{\\mu}\\sum_{k\\in\\mathbb{Z}}\\omega_{k}f(y_{k})$, where $\\omega_{k}$ are random weights defined as $\\omega_{k}=\\mathbb{1}\\{U_{k}>p\\}\\left\\{1+\\frac{1}{2}\\operatorname*{max}(l\\geqslant0:U_{k+1}\\leqslant p,\\dots,U_{k+l}\\leqslant p)\\right.\\left.+\\frac{1}{2}\\operatorname*{max}(l\\geqslant0:U_{k-1}\\leqslant p,\\dots,U_{k-l}\\leqslant p)\\right\\},$ where $(U_{k})_{k\\in\\mathbb{Z}}$ is a sequence of independent and identically distributed uniform random variables on [0, 1], independent of $\\Phi$ , and $p>0$ is the probability that the value of $f$ cannot be determined at $y_{k}$ . If $f(y_{k})$ cannot be determined, then the value is replaced by the average of the nearest observation to the left and right of $y_{k}$ ."
  },
  {
    "qid": "statistic-posneg-990-0-3",
    "gold_answer": "TRUE. Explanation: The context states that model (4) introduces greater variability into the θj's, and this variability increases with j. This is a characteristic of model (4), though the paper notes that this behavior may not be reasonable for the type of ordinal data being considered.",
    "question": "Does the variability in θj increase with j in model (4)?",
    "question_context": "Regression Analysis of Ordinal Data with Variability of Classification\n\nThis paper examines the effect of assuming common classification intervals when in fact there is variability among observations. This is done in the context of the proportional hazards regression model for ordinal data discussed by MeCullagh (1980). A class of models is proposed based on the introduction of a variability of classification into the proportional hazards model. The behaviour of the proportional hazards model in the presence of such variability is examined and the application of the new class of models to four examples is discussed."
  },
  {
    "qid": "statistic-posneg-2084-1-0",
    "gold_answer": "TRUE. The context explicitly states this result in Theorem 11, where the bilinear form is expressed as a quadratic form with specific matrices $\\mathbf{A}$ and $\\mathbf{V}$. The derivation relies on the properties of these matrices and the normal distribution assumptions for $\\mathbf{Y}_{k}$ and $\\mathbf{Y}_{l}$.",
    "question": "Is the statement 'The rth cumulant of a bilinear form $\\mathbf{Y}_{k}^{\\prime}\\mathbf{Y}_{l}$ is given by $\\kappa_{r}(\\mathbf{Y}_{k}^{\\prime}\\mathbf{Y}_{l})=\\frac{1}{2}(r-1)!\\mathrm{tr}(\\mathbf{A}\\mathbf{V})^{r}$ for $r=1,2,\\ldots$ and $k\\neq l$' correct based on the provided context?",
    "question_context": "Cumulants and Covariance Structures in Quadratic and Bilinear Forms\n\nThe paper discusses the properties of quadratic and bilinear forms under normal distribution assumptions, providing detailed derivations for their cumulants and covariance structures. Key formulas include the rth cumulant of quadratic forms, the representation of bilinear forms as quadratic forms, and various covariance expressions between different forms."
  },
  {
    "qid": "statistic-posneg-595-0-2",
    "gold_answer": "TRUE. Explanation: Proposition 2 states that under assumptions (H1)-(H3) and (A1), the mixture estimator $\\tilde{f}_n$ converges uniformly to $f_0$ in probability, provided $\\Delta_n \\to 0$ and $(\\sqrt{|\\log(h_n)|/n h_n^d} + h_n^2)|\\log(\\Delta_n)|^{1/\\beta} \\to 0$. This result holds irrespective of whether $f_0 \\in \\mathcal{P}$ or not, as long as the stated conditions are met.",
    "question": "Is it correct that the mixture estimator $\\tilde{f}_n(x) = \\hat{\\alpha}_n f_{\\hat{\\theta}_n}(x) + (1-\\hat{\\alpha}_n)\\hat{f}_n(x)$ is uniformly consistent regardless of whether the parametric model $\\mathcal{P}$ is valid?",
    "question_context": "Kernel Density Estimation and Tail Behavior Analysis\n\nThe paper discusses kernel density estimation under various tail behaviors of the underlying density function. It introduces assumptions (H1)-(H3) to ensure uniform consistency of the estimator. The key focus is on how the tail behavior of the density (Gaussian, exponential, polynomial) affects the bandwidth selection and the convergence rates."
  },
  {
    "qid": "statistic-posneg-572-0-0",
    "gold_answer": "TRUE. The probability ${\\bf P}(i>j) = \\frac{1}{1+\\exp(R_j - R_i)}$ is indeed a function of the difference $R_i - R_j$ only, as seen from the formula. This is consistent with the logistic function's property where the outcome depends on the difference between the two stimuli's sensory responses, not their absolute values. This aligns with the model's focus on pair comparisons, where the metric of $R$ itself is not required, only the relative differences matter.",
    "question": "In the context of taste testing, the probability that a subject will judge stimulus $S_i$ as saltier than $S_j$ is given by ${\\bf P}(i>j) = \\frac{1}{1+\\exp(R_j - R_i)}$. Is it correct to say that this probability is independent of the absolute values of $R_i$ and $R_j$, and depends only on their difference $R_i - R_j$?",
    "question_context": "Psychometric Scaling and Pair Comparison in Taste Testing\n\nThe Model <PARAGRAPH_SEPARATOR> Postulate (i) a sensory response-continuum for the intensity of sodium chloride, (i) a monotonic function relating $S_{;}$ , the concentration of the salt, to $R$ , the corresponding point on the continuum, and (i) a distribution of judgements, in replicate independent trials, of the location of a particular $R$ . We further assume (provided that $R$ is not near to the threshold of detectability) that this distribution is close to the normal (Gaussian) form, with $R$ as the parameter of location. The other parameter (of dispersion) will be a measure of the subject's precision of judgement. <PARAGRAPH_SEPARATOR> The question immediately arises: How can $R$ be given a metric? This is a tough one, but, fortunately, it can be side-stepped. So long as we restrict our attention to binary ranking (pair comparison) in this setting, the problem of the $R$ scale obligingly drops out of sight and significance. Consider a subject exposed to two different concentrations $S_{i}$ and $S_{j}$ (under code); then his response to the query, “Which is saltier?’ will be governed by his judgement of the interval between the quantities $R_{i}$ and $R_{j}$ .Imagine a series of such comparisons, with a common (fixed) $S_{j}$ and various $S_{i}$ .Then, granted certain statistical assumptions that need not be elaborated here, the probability that the answer will indicate $S_{i}$ is <PARAGRAPH_SEPARATOR> $${\\bf P}(i>j)=\\int_{-\\infty}^{R_{i}-R_{j}}\\$$ <PARAGRAPH_SEPARATOR> where $\\mathbf{f}(u)$ is some symmetrical near-normal function. An operationally convenient device is to make $\\mathbf{f}(u)$ the ‘squared hyperbolic secant' -see, for example, Gridgeman10 and Maxwell.18 We then have <PARAGRAPH_SEPARATOR> $$\\begin{array}{c}{{{\\bf P}(i>j)=\\underbrace{{\\displaystyle\\int_{-\\infty}^{R_{i}-R_{j}}}}_{=}\\frac{\\imath}{2}\\cdot{\\bf d}u}}\\ {{=\\displaystyle\\frac{1}{1+\\exp(R_{j}-R_{i})}}}\\end{array}$$ <PARAGRAPH_SEPARATOR> 106 <PARAGRAPH_SEPARATOR> from which it follows that <PARAGRAPH_SEPARATOR> $$\\mathbf{ln}\\bigg\\{\\frac{\\mathbf{P}}{1-\\mathbf{P}}\\bigg\\}=R_{i}-R_{j}$$ <PARAGRAPH_SEPARATOR> Uses of Pair Comparison <PARAGRAPH_SEPARATOR> The above relations underlie many taste experiments. Take, for instance, the man who is interested in assessing taste acuity, or who wishes to explore the function that connects stimulus and response. His first hypothesis might be the maligned Fechner relation, <PARAGRAPH_SEPARATOR> $$R=\\alpha+\\beta\\log S$$ <PARAGRAPH_SEPARATOR> from which it follows that <PARAGRAPH_SEPARATOR> $$R_{i}-R_{j}=\\beta\\log{(S_{i}/S_{j})}$$ <PARAGRAPH_SEPARATOR> Substitution of (5) in (3) gives a probabilistic model for the estimationof $\\beta$ , which is a parameter of discriminability functionally related to the jnd or just noticeable difference'. The only observations needed are frequency estimates of $\\mathbf{P}_{:}$ , obtained from pair comparisons, for various $\\boldsymbol{S}_{\\boldsymbol{\\imath}}$ . The estimation problem is easy if we follow Berkson's treatment? of the analogous bioassay problem (this entails a non-iterative leastsquares solution). A check of the fit of the data to the model presents no difficulties. <PARAGRAPH_SEPARATOR> As another example, take the consumer surveyor who has $\\mathcal{N}$ different varieties of a food product, and who would like to know whether they generate preference differences in a certain population. Having chosen a panel that fairly represents the population, he can ask the members to rank subsets of $\\pmb{n}$ $({\\bar{\\mathcal{N}}}\\geqslant n\\geqslant2)$ items in order of preference. There will be $\\binom{N}{n}$ such subsets, and $\\pmb{n}$ ! possible arrangements of each. When $\\pmb{\\mathscr{n}}=\\mathcal{N}$ the whole group is ranked in one operation-a task that is psychosensorily difficult if $\\mathcal{N}$ is large. At the other extreme of $n=2$ there are ${\\mathcal{N}}({\\mathcal{N}}-1)/2$ subsets of pair comparisons, and Bradley and Terry5 have devised a method of handling these based on equation (2). Using ‘rankits', defined as the expected values of the deviates for the ranked items in a sample from a normal population with zero mean and unit standard deviation, Bliss3 appropriately transforms preference frequencies derived from the ${\\mathcal{N}}({\\mathcal{N}}-1)/2$ pair comparisons. This is a simple procedure that gives satisfactory results. Fisher and Yates's Table XX supplies the rankits.9 <PARAGRAPH_SEPARATOR> Other methods of dealing with multiple pair comparisons are associated with the names of Thurstone,22 Mosteller,20 Kendall,16 Gulliksen,13 Bose,4 and Scheffe.21 Informative discussions of these, as well as practical assessments of their relative merits, will be found in papers by Bliss et al.,3 and by Jackson and Fleckenstein.15"
  },
  {
    "qid": "statistic-posneg-1009-0-2",
    "gold_answer": "TRUE. The zero-inflation probability p_i is modeled as a logistic function of x_i1, with coefficients γ₀ and γ₁. This means the probability of excess zeros varies with x_i1, as confirmed by the significant γ₁ estimate (4.397, p < 0.001) in Table 5.",
    "question": "Does the ZINB model's zero-inflation component (p_i) depend on the covariate x_i1, as implied by the formula p_i = exp(γ₀ + γ₁x_i1) / (1 + exp(γ₀ + γ₁x_i1))?",
    "question_context": "Zero-Inflated Negative Binomial (ZINB) Regression Model Estimation and Diagnostics\n\nTable 5 Maximum likelihood estimates from the ZINB regression model fitted to the final apple shoot data. <PARAGRAPH_SEPARATOR> Finally, we fitted a ZINB model without the covariate $x_{i2}$ (non-significant), i.e., <PARAGRAPH_SEPARATOR> $$ \\mu_{i}=\\exp(\\beta_{0}+\\beta_{1}x_{i1})~\\mathrm{and}~p_{i}=\\frac{\\exp(\\gamma_{0}+\\gamma_{1}x_{i1})}{1+\\exp(\\gamma_{0}+\\gamma_{1}x_{i1})}, $$ <PARAGRAPH_SEPARATOR> for $i=1,\\ldots,270$ . The ML estimates for the parameters of this model are given in Table 5. <PARAGRAPH_SEPARATOR> Notice that the photoperiod has a negative influence on the root count, i.e., we could say that when the shoots are produced under a $16\\mathrm{~h~}$ photoperiod, the predicted root count is reduced by $25.6\\%$ . <PARAGRAPH_SEPARATOR> On the other hand, in relation to the estimated proportion of zeros $(p)$ , we obtain <PARAGRAPH_SEPARATOR> $$ \\hat{p}=\\frac{1}{270}\\sum_{i=1}^{270}\\hat{p}_{i}=0.232 $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ \\hat{p}_{i}=\\frac{\\exp(-4.513+4.397x_{i1})}{1+\\exp(-4.513+4.397x_{i1})}. $$"
  },
  {
    "qid": "statistic-posneg-1915-0-1",
    "gold_answer": "TRUE. The thresholds α_jk,1 and α_jk,H_j-1 are fixed at preassigned values to resolve the indeterminacy issue associated with the scaling of the underlying continuous variable y*_ij. This is a standard identification constraint in models for ordinal data.",
    "question": "For ordinal variables in the mixture GLVM, the underlying continuous variable y*_ij is modeled with a normal distribution. Is it correct that the thresholds α_jk,h are fixed at preassigned values to identify the model?",
    "question_context": "Mixture of Generalized Latent Variable Models for Multiple Data Types\n\nThe paper introduces a mixture of generalized latent variable models (GLVMs) designed to handle multiple types of data, including continuous, ordinal, count, and nominal variables. The model incorporates latent variables and uses a Bayesian approach for parameter estimation, including MCMC methods. The paper details the model specification, identification strategies, and prior distributions for the parameters."
  },
  {
    "qid": "statistic-posneg-241-0-0",
    "gold_answer": "TRUE. The condition is correct as derived in Lemma 2 of the paper. Under Assumptions (H1), (H2), (III.1), (I.1), (I.2), (I.3), and with the scaling sequence δk,n,y satisfying 1≪δk,n,y≪mins∈{0,1,k}δ̃s,n,y, the tail ratio estimator ĉk,y converges to ck,y at the rate δk,n,y, ensuring δk,n,y|ck,y−ĉk,y|→ℙ0 as n→+∞.",
    "question": "Is the condition for the consistency of the tail ratio estimator given by δk,n,y|ck,y−ĉk,y|→ℙ0 as n→+∞ correct under the assumptions of the paper?",
    "question_context": "Estimation of Extreme Multivariate Expectiles with Functional Covariates\n\nThe paper presents a semi-parametric method for estimating functional multivariate L1-expectiles at extreme risk levels (functional L1-MEEs). It establishes the consistency with rate of the approximated loss function using empirical kernel-based estimators for the tail index, tail ratio, and upper tail dependence function. The method couples the loss function estimation with a gradient descent algorithm (e.g., BFGS-family) and provides consistency with rate for the approximated optimum problem solving functional L1-MEEs."
  },
  {
    "qid": "statistic-posneg-1565-2-3",
    "gold_answer": "TRUE. The context derives this term using Taylor's expansion and the dominated convergence theorem for a $(m,1)$-piecewise smooth $g$ with $m\\geqslant1$. The $t^{4}$ term is explicitly shown with the given coefficient.",
    "question": "Does the variance expression $\\mathrm{var}(\\tilde{\\Theta})$ for a $(m,1)$-piecewise smooth $g$ with $m\\geqslant1$ include a $t^{4}$ term with coefficient $g^{(3)}(0^{+})\\frac{3p(p^{2}+3p+1)}{2(1-p)^{4}}$?",
    "question_context": "Variance Estimation for Generalized Cavalieri Estimators\n\nThe paper discusses variance estimation for generalized Cavalieri estimators, focusing on the properties of the covariogram $g$ of a measurement function $f$. It provides proofs for several propositions involving Taylor expansions and dominated convergence theorems to derive asymptotic expressions for variance terms. The context includes detailed derivations of terms like $G(k,t)$ and their expansions around $t=0$, considering different smoothness conditions for $g$."
  },
  {
    "qid": "statistic-posneg-1351-0-0",
    "gold_answer": "TRUE. The function g(x) decreases from g(λ₊⁺) to zero on (λ₊, ∞). If ωᵢ² > 1/g(λ₊⁺), the equation ωᵢ²g(ρᵢ) = 1 will have a unique solution ρᵢ > λ₊ due to the intermediate value theorem and the strict monotonicity of g(x) on this interval. If ωᵢ² ≤ 1/g(λ₊⁺), no such solution exists above λ₊.",
    "question": "Is the condition ωᵢ² > 1/g(λ₊⁺) necessary for the existence of a unique solution ρᵢ > λ₊ to the equation ωᵢ²g(ρᵢ) = 1, where g(x) = x m(x) m̃(x)?",
    "question_context": "Spike MUSIC Algorithm and Eigenvalue Behavior in Fixed Rank Perturbations\n\nThe paper discusses the Spike MUSIC algorithm for estimating angles of arrival in signal processing, leveraging the behavior of eigenvalues in fixed rank perturbations. Key formulas include the definition of the localization function χ(φ) and its estimator χ̂ₙ(φ), alongside the Stieltjes Transform m(x) and its properties."
  },
  {
    "qid": "statistic-posneg-850-2-0",
    "gold_answer": "TRUE. The context explicitly states that the LPML is a cross-validated measure proposed by Geisser and Eddy (1979) for model comparison, and that larger LPML values indicate 'better support' or 'greater predictive ability'. This is derived from the sum of log(CPO) statistics, where CPO measures how well the observed response is supported by the model and remaining data.",
    "question": "Is the statement 'The LPML statistic is a cross-validated measure of how well the model predicts the observed data, and larger LPML indicates better support or greater predictive ability' consistent with the provided context?",
    "question_context": "Bayesian Model Comparison and Localization Precision in Circular Regression\n\nFinally, for the random-effects distribution, assume $\\Sigma^{-1}\\sim{\\mathrm{Wishart}}_{p}({\\mathrm{df}},\\Sigma_{0}),p$ $p=16$ , in the unconstrained, no-symmetry model. We consider the Wishart model parameterized so that $E(\\Sigma)=\\Sigma_{0}/(\\mathrm{df}-p-1)$ . Fixing $\\mathrm{df}=p+2$ and taking $\\pmb{\\Sigma}_{0}$ to be a prior likely value of $\\pmb{\\Sigma}$ gives a prior that is quite vague, yet has a finite mean. Noting that $\\begin{array}{r}{\\mathrm{cov}(\\boldsymbol{\\alpha}_{v})=2^{2}\\mathbf{I}_{p}}\\end{array}$ is diagonal with constant values, we take $\\pmb{\\Sigma}_{0}=\\mathbf{I}_{p}$ ; the variability of the individual level deviations from the population curves is expected to be half of the prior variability of the population curves themselves. <PARAGRAPH_SEPARATOR> # 4. Model selection <PARAGRAPH_SEPARATOR> Geisser and Eddy (1979) proposed the LPML for model comparison. The LPML is a cross-validated measure of how well the model predicts the observed data. Specifically, let the conditional predictive ordinate (CPO) statistic for the $k$ th replicate for subject $i$ at signal $j$ be <PARAGRAPH_SEPARATOR> $$ \\mathrm{CPO}_{i j k}=f(y_{i j k}|\\mathbf{y}_{-i j k}), $$ <PARAGRAPH_SEPARATOR> where $f(\\cdot|\\mathbf{y}_{-i j k})$ is the predictive density for a new response from subject $i$ given covariates and $\\mathbf{y}{-}i j k$ , which here consists of the remaining responses from subject $i$ as well as all responses from the other subjects. Then $f(y_{i j k}|\\mathbf{y}_{-i j k})$ is how well the observed response $y_{i j k}$ is supported through the model and remaining data. The LPML is simply the sum of $\\log(\\mathrm{CPO}_{i j k})$ statistics <PARAGRAPH_SEPARATOR> $$ \\mathrm{LPML}=\\sum_{i=1}^{30}\\sum_{j=1}^{24}\\sum_{k=1}^{3}\\log(\\mathrm{CPO}_{i j k}). $$ <PARAGRAPH_SEPARATOR> Larger LPML indicates ‘better support’ or ‘greater predictive ability’. The LPML statistic has increased in popularity among Bayesians owing to the relative ease with which the LPML is stably estimated from Markov chain Monte Carlo (MCMC) output (Gelfand and Dey, 1994). The ijkth CPO statistic is computed by using the identity <PARAGRAPH_SEPARATOR> $$ \\mathrm{CPO}_{i j k}^{-1}=E_{\\theta_{i j},\\rho_{i j}|\\mathbf{y}}[f(y_{i j k}|\\theta_{i j},\\rho_{i j})^{-1}], $$ <PARAGRAPH_SEPARATOR> where $[\\theta_{i j},\\rho_{i j}|\\mathbf{y}]$ is the posterior of $(\\theta_{i j},\\rho_{i j})$ given the full data set y, i.e. the estimated reciprocal of ${\\mathrm{CPO}}_{i j k}$ is the reciprocal of equation (3) averaged over the posterior MCMC values of $(\\theta_{i j},\\rho_{i j})$ . Spiegelhalter et al. (2002) proposed the deviance information criterion for Bayesian model comparison. However, Draper and Krnjajic (2007), section 4.1, have shown that the deviance information criterion can be viewed as an approximation to the LPML. <PARAGRAPH_SEPARATOR> A referee has brought up a potential problem with Gelfand and Dey’s (1994) estimate of the CPO statistics, namely that harmonic mean estimators can have very large, or infinite, variance. This often happens when the harmonic mean is used to estimate the marginal likelihood of the data, as the difference between the prior and posterior distributions is typically so great that only a few MCMC atoms have non-negligible mass in the importance sampling estimate. However, with CPO computation, the distributions to consider are the full posterior and the posterior computed leaving out one of the data values—these distributions are likely to be very close and have similar behaviour in the tails. We double-checked our LPML estimates by repeatedly fitting models and found them to be stable within one unit. <PARAGRAPH_SEPARATOR> The precision $\\rho$ in the wrapped Cauchy model (2) provides an estimate of the localization performance within each component of the mixture (3) but does not provide a measure of overall localization. We instead consider a measure derived from the mixture (3) itself that is a function of both the true and the confused signals, the mixing parameter $\\theta$ and the precision $\\rho$ , which reflects how concentrated a group’s ability to localize the signal is. <PARAGRAPH_SEPARATOR> The population localization distribution for group $g$ given all model parameters is specified through the mixture (3) by <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l r}&{}&{f_{g}(y|S_{j},\\alpha_{g})=\\displaystyle\\frac{\\exp\\left\\{m_{g}(S_{j})\\right\\}}{1+\\exp\\left\\{m_{g}(S_{j})\\right\\}}f\\bigg[y|C(S_{j}),\\displaystyle\\frac{\\exp\\left\\{p_{g}(S_{j})\\right\\}}{1+\\exp\\left\\{p_{g}(S_{j})\\right\\}}\\bigg]}\\ &{}&{+\\displaystyle\\frac{1}{1+\\exp\\left\\{m_{g}(S_{j})\\right\\}}f\\bigg[y|S_{j},\\displaystyle\\frac{\\exp\\left\\{p_{g}(S_{j})\\right\\}}{1+\\exp\\left\\{p_{g}(S_{j})\\right\\}}\\bigg],}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\alpha_{g}=(\\alpha_{1g},\\ldots,\\alpha_{p g})^{\\prime}$ . <PARAGRAPH_SEPARATOR> We consider a measure of localization precision based on the notion of excess mass (Müller and Sawitzki, 1991; Polonik, 1995; Fisher and Marron, 2001), related to highest density regions, and often used for testing the unimodality of densities. The excess mass approach seeks to find a set of $k$ connected regions with the smallest volume that concentrates a given amount of mass above a threshold. We borrow from this approach by finding a cut-off $\\lambda$ and associated region $R$ such that <PARAGRAPH_SEPARATOR> $$ 0.5{\\overline{{}}}=\\int_{x\\colon f(x)>\\lambda}\\left\\{f(x)-\\lambda\\right\\}{\\mathrm{}}{\\mathrm{}}{\\mathrm{}}\\mathrm{d}x=\\int_{R}\\left\\{f(x)-\\lambda\\right\\}{\\mathrm{}}{\\mathrm{}}\\mathrm{d}x $$ <PARAGRAPH_SEPARATOR> (see Fig. 2 in Müller and Sawitzki (1991)) and then take $L_{50}$ to be the length of the region $R$ , which is bounded above by $2\\pi$ . Fisher and Marron (2001), section 5, considered a related empirical measure for circular data based on smoothing with a von Mises kernel. <PARAGRAPH_SEPARATOR> $L_{50}$ is the length of the region from which we would have to move $50\\%$ of the probability mass to obtain a uniform density, which is the ‘most diffuse’ density one can consider over $[0,2\\pi]$ . Thus, $L_{50}$ has a nice interpretation and we use it over other common measures such as the interquartile range or highest posterior density intervals. Larger values of $L_{50}$ indicate highly diffuse and unfocused localization distributions that are perhaps more markedly bimodal, whereas small values indicate more concentrated localization. Credible intervals for the median $L_{50}$ and median differences in $L_{50}$ among groups are computed from equation (5) evaluated at the MCMC posterior samples and indicate groupwise differences in localization performance. We expect that aging and hearing impariment will be associated with increased $L_{50}$ , and that these effects may depend on signal position. <PARAGRAPH_SEPARATOR> All analyses were carried out in SAS(R) version 9.3; in particular all MCMC runs were implemented in the new SAS MCMC procedure; SAS code for duplicating analyses in this paper is available from"
  },
  {
    "qid": "statistic-posneg-391-3-0",
    "gold_answer": "TRUE. Explanation: The model $X(z)=I(\\alpha_{1}+z\\alpha_{2}+H\\alpha_{3}+z H\\alpha_{4}+V>0)$ implies that the effect of $Z$ on $X$ depends on the unobserved $H$ if $\\alpha_4 \\neq 0$. When $\\alpha_4 = 0$, the effect of $Z$ is homogeneous, ensuring monotonicity. If $\\alpha_4 \\neq 0$, the effect of $Z$ can vary with $H$, potentially leading to some individuals defying the monotonicity assumption (e.g., $X(1) < X(0)$ for some $H$). Violating monotonicity complicates the identification of local average treatment effects (LATE) because it introduces 'defiers,' making it impossible to isolate compliers for causal inference. This undermines the validity of instrumental variable methods that rely on the monotonicity assumption.",
    "question": "In the logistic structural model for nonignorable noncompliance, the monotonicity assumption $X(1) \\geqslant X(0)$ holds only if $\\alpha_4 = 0$. Is this statement true, and what is the economic implication of violating this assumption?",
    "question_context": "Identification of Causal Effects in Logistic Structural Models with Nonignorable Noncompliance\n\nIn Figure 3, we set $\\gamma=-1$ and $\\delta=1.208$ to give $E\\{X(0)\\}=0.023$ . These parameter values generate data for which no contamination might be expected to provide a good approximation. As expected, the local parameters LATE and LRR are very close to ATEX1Z1 and RRX1Z1, respectively, and to ATEX1 and RRX1 too. The LOR and DL are in this case identical to ORX1Z1 and ORX1. <PARAGRAPH_SEPARATOR> # 6.3 Mixed logistic model simulation <PARAGRAPH_SEPARATOR> Didelez and others (2010) consider a more complex logistic structural model for generating nonignorable noncompliance. Using our notation, this model is written <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{X(z)=I(\\alpha_{1}+z\\alpha_{2}+H\\alpha_{3}+z H\\alpha_{4}+V>0),}}\\ {{{}}}\\\\ {{Y(x)=I(\\beta_{1}+x\\beta_{2}+H\\beta_{3}+x H\\beta_{4}+U>0),}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $U$ and $V$ are independent standard logistically distributed random variables, and $H$ is unobserved. Equivalent expressions to (6.1–6.2) are $E\\{X(z)|H=h\\}=\\mathrm{expit}(\\alpha_{1}+z\\alpha_{2}+h\\alpha_{3}+z h\\alpha_{4})$ and $E\\{Y(x)|H=h\\}=\\mathrm{expit}(\\beta_{1}+x\\beta_{2}+h\\beta_{3}+x h\\beta_{4})$ , respectively. Both models contain interaction terms allowing the effect of latent $H$ to vary depending on $z$ and $x$ . There are heterogeneous treatment effects on the latent scale if $\\beta_{4}\\neq0$ , but this poses no problems as SMMs do not constrain treatment effects to be homogeneous, or indeed place any constraints on the form of treatment effect heterogeneity. More importantly, however, the monotonicity assumption $X\\left(1\\right)\\geqslant X\\left(0\\right)$ holds only if ${a_{4}}=0$ , and monotonicity is crucial for identification of local effects."
  },
  {
    "qid": "statistic-posneg-744-2-2",
    "gold_answer": "TRUE. The RMISE is approximated by averaging over N Monte Carlo samples, where each sample's error is calculated as the square root of the sum of squared differences between the estimated and true densities, weighted by the interval length d.",
    "question": "Is the RMISE approximation formula correctly given as the average over N Monte Carlo samples of the square root of the integrated squared error between the estimated and true densities?",
    "question_context": "Model Selection Criteria in Unimodal Density Estimation\n\nWe present a novel procedure for selecting the number of weights by examining the condition number of A. Since A is a normal matrix the condition number is evaluated by, $\\mathsf{C N}(m)=\\left\\lceil\\frac{\\lambda_{\\operatorname*{max}}(A)}{\\lambda_{\\operatorname*{min}}(A)}\\right\\rceil$ , where $\\lambda_{\\operatorname*{max}}(A)$ and $\\lambda_{\\mathrm{min}}(A)$ are the maximum and minimum eigenvalues of A respectively. We select $m$ by including the largest number of weights possible while still bounding $\\log_{10}\\mathrm{CN}(m)$ by $\\sqrt{n}$ . <PARAGRAPH_SEPARATOR> # 2.1.2. AIC and BIC <PARAGRAPH_SEPARATOR> We also present more traditional methods for selecting the number of weights, specifically, the Akaike information criterion (AIC) and Bayesian information criteria (BIC). In the realm of density estimation, these criteria are given by, <PARAGRAPH_SEPARATOR> $\\mathsf{A I C}(m)=-2\\sum_{i=1}^{n}\\log[\\widehat{f}_{m}(x_{i},\\widehat{\\omega}_{m})]+2(m-1)$, <PARAGRAPH_SEPARATOR> $\\mathrm{BIC}(m)=-2\\sum_{i=1}^{n}\\log[\\hat{f}_{m}(x_{i},\\widehat{\\omega}_{m})]+\\log(n)(m-1)$, <PARAGRAPH_SEPARATOR> where $m$ is the number of weights, $\\hat{f}_{m}$ is the estimated density using $m$ weights, $\\widehat{\\omega}_{m}$ is the vector of estimated weights, and $x_{i}$ for $i=1,\\ldots,n$ are the observations. The effective dimension of the weight vector $\\widehat{\\omega}_{m}$ is $m-1$ since the weights are constrained to sum to one. <PARAGRAPH_SEPARATOR> A variation of Theorem 3.1 in Babu et al. (2002) shows that $\\hat{f}_{m}$ will converge uniformly to $f$ for $2\\leq m\\leq(n/\\log n)$ as $n\\rightarrow\\infty$ , so we implement both the AIC and BIC methods by estimating the density with $m$ weights ranging from 2 to $\\lceil n/\\log n\\rceil$ . The AIC or BIC is calculated for each fit, then the density estimate with the lowest AIC or BIC is selected as the best estimate. <PARAGRAPH_SEPARATOR> # 2.2. Comparison of criterion for selecting the number of weights <PARAGRAPH_SEPARATOR> We compare these three methods of selecting $m$ by performing a small simulation study. We generate data from a mixture of Beta densities with a fixed vector of weights, which follows the required format of our density estimation model. We then generate density estimates using the AIC, BIC, and CN criterion to select m. Our goal is to find the method which provides the best density estimate in terms of a low root mean integrated squared error (RMISE) and selects the true number of weights of the Beta mixture. The RMISE is approximated as follows: <PARAGRAPH_SEPARATOR> $\\mathrm{RMISE}=\\mathrm{E}\\left[\\Vert\\hat{f}_{\\widehat{m}}-f\\Vert_{2}\\right]\\approx\\frac{1}{N}\\sum_{\\ell=1}^{N}\\sqrt{d\\sum_{j=1}^{J}\\left[\\hat{f}_{\\widehat{m}}^{(\\ell)}(x_{j})-f(x_{j})\\right]^{2}}$, <PARAGRAPH_SEPARATOR> where $J=100,\\operatorname*{Pr}[x_{1}\\leq X\\leq x_{J}]=0.999$ , and $d=(x_{j}-x_{j-1})=\\frac{x_{J}-x_{1}}{J-1}$ for all $j\\geq2$ . In the above expression, $\\hat{f}_{\\widehat{m}}^{(\\ell)}$ denotes the estimated density at the ℓth sample for $\\ell=1,\\ldots,N$ with $\\widehat{m}$ chosen by one of the three criteria. <PARAGRAPH_SEPARATOR> We examine a symmetric and right-skewed Beta mixture each with 7 weights (i.e. $m=7$ ). The weight vectors are fixed to be $\\omega_{1}~=~\\{0.05,0.1,0.2,0.3,0.2,0.1,0.05\\}$ for the symmetric distribution, and $\\omega_{2}=\\{0.1,0.4,0.25,0.15,$ 0.05, 0.03, 0.02} for the right-skewed distribution. We generate $N~=~1000$ Monte Carlo (MC) samples of size $n=$ 15, 30, 100, and 500 for each Beta mixture. Table 1 displays the MC estimated RMISE values as well as the average estimate of m across the MC samples. Statistically significant lower RMISE values, based on a paired $t$ -test, are in bold. <PARAGRAPH_SEPARATOR> The AIC method has the lowest average RMISE in most situations. Unfortunately, none of the methods have average number of weight estimates that are close to 7. The CN method appears to select the number of weights at around $\\sqrt{n}$ , which is much larger than 7 for the cases of $n=100$ and 500. Both the AIC and BIC underestimate $m$ for all sample sizes with the AIC having slightly larger estimates. Overall it appears the CN criterion is a better option for samples with fewer observations while the AIC performs the best for larger samples of size 100 and 500."
  },
  {
    "qid": "statistic-posneg-1058-1-3",
    "gold_answer": "FALSE. The paper states that the smoothing parameter γ_j is selected through generalized cross-validation (GCV), not AIC. GCV is used for automatic and fast tuning of the smoothing parameter to balance model fit and complexity.",
    "question": "The paper suggests that the choice of the smoothing parameter γ_j in the penalized splines is determined by minimizing the Akaike Information Criterion (AIC). Is this statement true?",
    "question_context": "Sparse Single Index Models for Multivariate Responses: Optimization via ADMM\n\nThe paper discusses the estimation of sparse single index models for multivariate responses using penalized splines and ADMM optimization. The model involves estimating link functions and a coefficient matrix with various structural constraints (e.g., sparsity, low-rank). The optimization problem is solved using an ADMM algorithm with updates for the coefficient matrix B, dummy variables C, and Lagrange multipliers M."
  },
  {
    "qid": "statistic-posneg-1904-1-2",
    "gold_answer": "TRUE. The shrinkage operator ST(w, t) is applied in the updates for η and α to enforce sparsity. For η, it is used with the MCP penalty to produce sparse estimates, and for α, it is similarly applied to encourage homogeneity by setting some α_{kk'}^j to zero, implying that the corresponding coefficients β_{kj} and β_{k'j} are equal.",
    "question": "The shrinkage operator ST(w, t) is used to enforce sparsity in the updates for η and α. True or False?",
    "question_context": "ADMM Algorithm for High-Dimensional Integrative Analysis with Homogeneity\n\nThe paper presents an ADMM algorithm for high-dimensional integrative analysis with homogeneity. The objective function includes a least squares term, penalty terms for sparsity and homogeneity, and constraints to enforce homogeneity across groups. The augmented Lagrangian is constructed with dual variables and penalty parameters. The ADMM algorithm iteratively updates the primal variables (β, η, α) and dual variables (ν, υ) until convergence. Key steps involve solving subproblems for β, η, and α, and updating the dual variables. The algorithm uses the Sherman-Morrison-Woodbury formula for efficient matrix inversion and includes a termination criterion based on primal residuals."
  },
  {
    "qid": "statistic-posneg-1762-1-2",
    "gold_answer": "FALSE. While T_NEW performs best in terms of size control, the text states that its empirical sizes are 'around the nominal size 5%' rather than exactly 5%. This indicates some variability, though it is the most accurate among the three tests compared.",
    "question": "Is it correct to conclude that T_NEW performs best in terms of size control because its empirical sizes are exactly 5% under all settings?",
    "question_context": "Heteroscedastic One-Way MANOVA: Performance Comparison of Test Statistics\n\nWithout loss of generality, we specify $m=p$ , ${\\pmb{\\mu}}_{1}={\\pmb{0}}$ and $\\pmb{\\mu}_{\\alpha}=\\pmb{\\mu}_{1}+(\\alpha-1)\\delta\\mathbf{h}$ where $\\mathbf{h}=\\mathbf{h}_{0}/\\lVert\\mathbf{h}_{0}\\rVert$ and $\\mathbf{h}_{0}=(1,\\ldots,p)^{\\top}$ for any given dimension $p$ . In fact, a small simulation study (not shown here) based on the simulation setups of Simulation 1 indicates that different choices of $\\mathbf{h}_{0}$ will not change our conclusions substantially. Note that here $\\mathbf{h}$ is a constant unit vector which specifies the direction of the mean vector difference $\\pmb{\\mu}_{\\alpha}-\\pmb{\\mu}_{1},\\alpha=2,\\dots,k.$ The tuning parameter $\\delta$ controls the amount of the mean vector difference. The empirical powers of the tests are expected to increase when the value of $\\delta$ increases. We consider three cases of dimension $p=50$ , 500, and 1000 throughout this section. <PARAGRAPH_SEPARATOR> # 3.1. Simulation 1 <PARAGRAPH_SEPARATOR> In this simulation study, we consider to compare the performance of $T_{Z G Z},T_{Z Z G}$ , and $T_{N E W}$ for the heteroscedastic one-way MANOVA problem (2) when $k=3$ . To generate the group covariance matrices, we adopt the following compound symmetric covariance matrices <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\pmb{\\Sigma}_{\\alpha}=\\sigma_{\\alpha}^{2}[(1-\\rho_{\\alpha})\\mathbf{I}_{p}+\\rho_{\\alpha}\\mathbf{J}_{p}],\\alpha=1,\\ldots,k,}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\mathbf{J}_{p}$ is the $p\\times p$ matrix of ones. It is obvious that the covariance matrix differences $\\pmb{\\Sigma}_{3}-\\pmb{\\Sigma}_{1}$ and $\\pmb{\\Sigma}_{2}-\\pmb{\\Sigma}_{1}$ are determined by the values of $\\sigma_{\\alpha}^{2}$ and $\\rho_{\\alpha},\\alpha=1,\\ldots,k$ In particular, $\\sigma_{\\alpha}^{2}$ controls the variance of the $\\alpha$ -th generated sample and for simplicity, we set $\\sigma_{\\alpha}^{2}=\\alpha$ for $\\alpha=1,\\ldots,k$ . The value of $\\rho_{\\alpha}$ controls the correlation of the $\\alpha$ -th generated sample and we set $\\rho_{\\alpha}=\\rho+0.02(\\alpha-1),\\alpha=1,\\ldots,k$ with three cases of $\\rho$ as $\\rho=0.1,0.5$ and 0.9 to represent three different degrees of correlation. Finally, we specify three cases of ${\\bf n}=(n_{1},n_{2},n_{3})$ as ${\\bf n}_{1}=(30,40,50)$ , ${\\bf{n}}_{2}=(60,80,100)$ and ${\\bf n}_{3}=$ (120, 160, 200). <PARAGRAPH_SEPARATOR> The empirical sizes of $T_{Z G Z},T_{Z Z G}$ , and $T_{N E W}$ with the last row displaying their ARE values associated with the three values of $\\rho$ are displayed in Table 1. Note that when the value of $\\rho$ is small (or large), the simulated data are less (or highly) correlated. The three values of $\\rho$ represent the three situations in which the simulated data are nearly uncorrelated, moderately correlated, and highly correlated. It is seen that under each setting, the empirical sizes of $T_{Z G Z},T_{Z Z G}$ , and $T_{N E W}$ are generally in descending order. However, in terms of size control, $T_{N E W}$ performs best and generally performs well regardless of whether the generated data are nearly uncorrelated, moderately correlated, or highly correlated since its empirical sizes under various settings are around the nominal size $5\\%$ and its ARE values are 6.56, 3.61, and 3.84 when $\\rho=0.1,0.5$ , and 0.9, respectively; $T_{Z Z G}$ performs second and is slightly liberal when $\\rho=0.1$ and 0.5 since its empirical sizes range from $4.80\\%$ to $7.11\\%$ and its ARE values are 23.73, 14.72, and 6.27 associated with the three values of $\\rho$ ; while $T_{Z G Z}$ performs worst among these three tests and it is generally liberal since its empirical sizes range from $5.85\\%$ to $7.70\\%$ and its ARE values are 35.72, 42.75, and 42.59 when $\\rho=0.1,0.5$ , and 0.9 respectively which are larger than those of $T_{Z Z G}$ and $T_{N E W}$ generally. These results are consistent with those claims made in Remarks 2 and 6. <PARAGRAPH_SEPARATOR> Table 2 Estimated approximate degrees of freedom of $T_{Z Z G}$ and $T_{N E W}$ under Model 1 in Simulation 1, denoted as $\\hat{d}^{0}$ and $\\hat{\\boldsymbol{d}}$ respectively."
  },
  {
    "qid": "statistic-posneg-1915-3-3",
    "gold_answer": "TRUE. The matrix Σ_{ωk} is defined with blocks involving Π_{0k}'Π_{0k}, -Π_{0k}'Γ_kΔ, -Δ'Γ_k'Π_{0k}, and Φ_k^{-1} + Δ'Γ_k'Γ_kΔ, reflecting the structured dependence in the latent variable model.",
    "question": "In the proposal distribution for ω_i, the covariance matrix Ω_{ωk}^{-1} is given by Σ_{ωk} + Λ_k' Ψ_ω Λ_k. Is it true that Σ_{ωk} includes terms involving Π_{0k} and Γ_k?",
    "question_context": "Bayesian Mixture Models with Latent Variables: Full Conditional Distributions and MH Algorithm\n\nThe paper presents a Bayesian approach to mixture models with latent variables, detailing the full conditional distributions and the Metropolis-Hastings (MH) algorithm for sampling. The context includes complex mathematical formulas for the full conditional distributions of parameters like Ω, ζ, μ_k, A_k, Ψ_k, Π_k, Φ_k, α_k, Y*, and δ. The MH algorithm is described with proposal distributions and acceptance probabilities for generating samples from these non-standard distributions."
  },
  {
    "qid": "statistic-posneg-794-0-0",
    "gold_answer": "FALSE. The paper only establishes the implication in one direction for the general case. The two-dimensional case shows equivalence ($\\Longleftrightarrow$), but the general case is presented as a one-directional implication ($\\Longrightarrow$). The authors do not claim or prove reversibility for $n>2$ geometric random variables in the provided context.",
    "question": "Is the implication $(p_{1},\\ldots,p_{n})\\overset{\\mathtt{p}}{\\succeq}(p_{1}^{*},\\ldots,p_{n}^{*})\\Longrightarrow\\sum_{i=1}^{n}X_{p_{i}}\\succeq_{\\mathrm{hr}}\\sum_{i=1}^{n}X_{p_{i}^{*}}$ reversible for the general case of $n$ geometric random variables?",
    "question_context": "Hazard Rate Ordering of Sums of Heterogeneous Geometric Random Variables\n\nIn this paper, we investigate the ordering properties of the convolutions of heterogeneous geometric random variables. Let $X_{p_{1}},\\dotsc,X_{p_{n}}$ and $X_{p_{1}^{*}},\\ldots,X_{p_{n}^{*}}$ be two sequences of independent geometric random variables with parameters $p_{1},\\ldots,p_{n}$ and $p_{1}^{*},\\ldots,p_{n}^{*}$, respectively. It is shown that for the two-dimensional case, $$\\begin{array}{r l}&{X_{p_{1}}+X_{p_{2}}\\succeq_{\\mathrm{hr}}X_{p_{1}^{*}}+X_{p_{2}^{*}}}\\ &{\\Longleftrightarrow X_{p_{1}}+X_{p_{2}}\\succeq_{\\mathrm{st}}X_{p_{1}^{*}}+X_{p_{2}^{*}}}\\ &{\\Longleftrightarrow(p_{1},p_{2})\\succeq(p_{1}^{*},p_{2}^{*}),}\\end{array}$$ and for the general case, $$(p_{1},\\ldots,p_{n})\\overset{\\mathtt{p}}{\\succeq}(p_{1}^{*},\\ldots,p_{n}^{*})\\Longrightarrow\\sum_{i=1}^{n}X_{p_{i}}\\succeq_{\\mathrm{hr}}\\sum_{i=1}^{n}X_{p_{i}^{*}},$$ where the formal definitions of related partial orderings will be given in Section 2."
  },
  {
    "qid": "statistic-posneg-512-0-0",
    "gold_answer": "TRUE. When β = 1, the formula simplifies to $$\\mathrm{hF}=\\frac{(1+1)\\cdot\\mathrm{hP}\\cdot\\mathrm{hR}}{(1\\cdot\\mathrm{hP}+\\mathrm{hR})} = \\frac{2\\cdot\\mathrm{hP}\\cdot\\mathrm{hR}}{\\mathrm{hP}+\\mathrm{hR}}$$, which is the harmonic mean of precision and recall, identical to the standard F1 score. This shows that the hierarchical F-score generalizes the F1 score by introducing a weight parameter β to balance the importance of precision and recall.",
    "question": "Is it true that the hierarchical F-score (hF) formula $$\\mathrm{hF}=\\frac{(1+\\beta^{2})\\cdot\\mathrm{hP}\\cdot\\mathrm{hR}}{(\\beta^{2}\\cdot\\mathrm{hP}+\\mathrm{hR})}$$ reduces to the standard F1 score when β = 1?",
    "question_context": "Hierarchical Confusion Matrix for Classification Performance Evaluation\n\nThe paper discusses hierarchical classification evaluation measures, focusing on hierarchical precision (hP), recall (hR), and F-score (hF). These measures are extensions of binary classification metrics, adapted for hierarchical structures. The hierarchical confusion matrix is introduced, defining true positives (TP_H), true negatives (TN_H), false positives (FP_H), and false negatives (FN_H) in the context of hierarchical classification. The measures are formalized for different problem types, including tree (T) and directed acyclic graph (DAG) structures."
  },
  {
    "qid": "statistic-posneg-1945-0-3",
    "gold_answer": "FALSE. The paper defines I(x) as returning 1 if x > 0 and 0 otherwise. This means I(x) does not return 1 for x = 0, only for strictly positive values of x.",
    "question": "The function I(x) defined in the paper is used to compute absolute differences. Is it correct to say that I(x) returns 1 for all x ≥ 0?",
    "question_context": "Fast Algorithm for Distance Covariance Computation\n\nThe paper introduces a fast algorithm for computing sample distance covariance, which measures dependence between two random variables. The squared distance covariance is defined as the weighted L2 distance between the joint characteristic function and the product of marginal characteristic functions. The paper proposes an O(n log n) algorithm to compute the sample distance covariance for univariate random variables, leveraging sorting and merge techniques to optimize computations."
  },
  {
    "qid": "statistic-posneg-493-4-0",
    "gold_answer": "TRUE. The simplification is correct because for logistic regression, the estimated probabilities are given by $\\widehat{\\mathrm{Pr}}(y_i=1) = \\frac{e^{\\hat{f}(\\mathbf{x}_i)}}{1 + e^{\\hat{f}(\\mathbf{x}_i)}}$ and $\\widehat{\\mathrm{Pr}}(y_i=0) = \\frac{1}{1 + e^{\\hat{f}(\\mathbf{x}_i)}}$. Substituting these into the original unbiased estimator $\\mathrm{A}\\widehat{\\mathrm{KLD}}$ and simplifying leads to the given expression for $\\widehat{\\mathrm{AKLD}}$.",
    "question": "The unbiased estimator of the average Kullback–Leibler distance (AKLD) for logistic regression simplifies to $$\\widehat{\\mathrm{AKLD}}=-\\frac{1}{252}\\sum_{i=1}^{252}\\left(y_{i}\\hat{f}(\\mathbf{x}_{i})-\\log(1+e^{\\hat{f}(\\mathbf{x}_{i})})\\right).$$ Is this simplification correct given the original definition of AKLD?",
    "question_context": "Bayesian Variable Selection and Model Averaging in High-Dimensional Multinomial Nonparametric Regression\n\nThe paper discusses a Bayesian approach to variable selection and model averaging in high-dimensional multinomial nonparametric regression. It compares its method with that of Wahba et al. (1995), who used penalized log-likelihood smoothing splines and an unbiased estimate of the average Kullback–Leibler distance (AKLD) to measure model fit. The paper presents several models for the logit transformation and discusses the estimation of AKLD and its unbiased estimator."
  },
  {
    "qid": "statistic-posneg-387-0-2",
    "gold_answer": "TRUE. The context specifies that the formula for $D$ is given for the limiting posterior distribution when the prior is noninformative and $\\pmb{n}(i) = 1$ for all $i$. In this specific case, the posterior distribution of $F(X_{n})$ is degenerate, and the area enclosed by the bands can be computed as described. This formula is not generally applicable to other prior specifications or sample configurations.",
    "question": "Is the area enclosed by the confidence bands given by $D = \\Sigma(X_{i} - X_{i-1})(v_{i} - u_{i-1})$ only applicable when the prior is noninformative and $\\pmb{n}(i) = 1$ for all $i$?",
    "question_context": "Nonparametric Bayesian Confidence Bands for Distribution Functions\n\nLet $X_{1}<\\ldots<X_{s}$ denote the distinct observations in a sample of size $\\pmb{n}$ from $\\mathcal{P}$ and let $\\pmb{n}(\\pmb{\\hat{\\imath}})$ denote the number of repetitions of $\\pmb{X}_{i}$ in the sample. Ferguson showed that the posterior distribution of $\\pmb{P}$ is again a Dirichlet process with parameter $a(.)+\\Sigma n(i)d(X_{i},.)$ ,where $d(\\pmb{X},\\pmb{A})=1$ if $\\pmb{X}$ belongs to $\\pmb{A}$ and equals zero otherwise. As pointed out by Ferguson, any analysis performed on the Dirichlet process will be valid both $\\pmb{a}$ priori and $\\pmb{a}$ posteriori. If $\\mathscr{X}$ consists of a finite set of elements the above result is equivalent to stating that the Dirichlet distribution is the conjugate prior to be used in the estimation of multinomial probabilities. If the jumps in $a(.)$ are all integers then the joint distribution of the cumulative cell probabilities is identical to the joint distribution of some of the order statistics of a uniform random sample as discussed by Wilks (1962, p. 236). This in fact is the limiting posterior probability distribution when $a(\\mathcal X)\\rightarrow0$ , that is, the case of a noninformative prior. <PARAGRAPH_SEPARATOR> # 2. CONFIDENCE BANDS <PARAGRAPH_SEPARATOR> Formulae have been derived by Breth (1978) to compute rectangle probabilities over the ordered Dirichlet distribution; that is for $0\\leqslant u_{1}\\leqslant\\ldots\\leqslant u_{m}<1$ $0<v_{1}\\leqslant\\ldots\\leqslant v_{m}\\leqslant1$ and ${\\pmb u}_{\\nless}<{\\pmb v}_{\\pmb q}$ for all i, <PARAGRAPH_SEPARATOR> $$ p_{m}(u,B,v;a)=\\mathrm{pr}\\{u_{i}\\leqslant P(B_{i})\\leqslant v_{i};\\mathrm{~i=1,...,m\\}. $$ <PARAGRAPH_SEPARATOR> A simulation alternative was also given. In the case when the $\\pmb{a}(\\pmb{B}_{i})$ are all integers, the above probability was shown to be equivalent to a rectangular probability over the uniform order statistics. Steck (197l) expressed this probability as a determinant. <PARAGRAPH_SEPARATOR> Let ${\\pmb{\\mathcal{X}}}=(-\\infty,\\infty)$ and for $-\\infty<t_{1}<...<t_{m}<\\infty$ define $B_{j}=(-\\infty,t_{j}]$ for ${\\mathfrak{j}}=1,...,{\\mathfrak{m}}$ and $B_{m+1}=\\mathcal{X}$ .For $j=1,...,m,P(B_{j})=F(t_{j})~$ where $\\pmb{F}$ is the random distribution function associated with the random probability measure $\\pmb{P}$ . Then (2·1) becomes <PARAGRAPH_SEPARATOR> $$ p_{m}(u,B,v;\\boldsymbol{a})=\\mathrm{pr}\\{u_{i}\\leqslant F(t_{i})\\leqslant v_{i};\\boldsymbol{\\mathsf{i}}=1,...,m\\}, $$ <PARAGRAPH_SEPARATOR> which was used to set fxed Bayesian confidence bands within which the random distribution function lies with determined probability. In the sequel when there is no possible source of confusion reference to $\\pmb{B}$ and $\\pmb{\\alpha}(.)$ will be deleted and (2·2) will be written as <PARAGRAPH_SEPARATOR> $$ p_{m}(u,v)=\\mathrm{pr}\\{u_{i}\\leqslant F(t_{i})\\leqslant v_{i};i,=1,...,m\\}. $$ <PARAGRAPH_SEPARATOR> In practice to construct such confidence bands a desired probability level $\\pmb{\\mathcal{P}}$ would be stipulated and it would be necessary to determine $m,~u,~v$ and $\\pmb{\\ell}$ such that $\\pmb{\\mathscr{p}}_{m}(\\pmb{u},\\pmb{v})=\\pmb{p}$ First consider the case when it is possible to choose $\\pmb{\\ell}$ s0 that for some $\\pmb{m}$ , $\\pmb{a}(B_{\\pmb{j}})=\\dot{\\pmb{j}}$ for $j=1,...,m+1$ . Then for $\\dot{\\pmb{\\mathscr{i}}}=1,...,\\pmb{\\mathscr{m}}$ , the random value $\\pmb{\\mathcal{F}}(\\pmb{\\ell}_{\\pmb{\\ell}})$ has the same distribution a8 the ith order statistic of a uniform random sample of size $\\mathbf{\\nabla}_{m}$ . For this $\\pmb{\\ell}$ if $\\pmb{\\mathscr{u}}$ and $\\pmb{v}$ are restricted to Kolmogorov bands the solution to $\\pmb{p}_{m}(\\pmb{u},\\pmb{v})=\\pmb{p}$ is easily obtained from existing tables of critical values of the Kolmogorov statistic; a detailed example of this is discussed by Breth (1978). For the noninformative prior it is always possible to obtain such bands for the limiting posterior distribution provided that ${\\pmb n}({\\dot{\\bullet}})=1$ for all i. One simply takes $m=n-1$ and $t_{\\ell}=X_{\\ell}$ for $\\pmb{i}=1,...,m$ <PARAGRAPH_SEPARATOR> In general it may not be possible to choose $\\pmb{B}$ 80 that $\\pmb{a}(\\pmb{B}_{\\pmb{j}})=\\dot{\\pmb{j}}$ for all $j$ or to obtain a $\\pmb{B}$ which gives a suitable approximation to this. In such cases tables of values for $\\pmb{\\mathscr{p}}_{m}(\\pmb{\\mathscr{u}},\\pmb{\\ v})=\\pmb{\\mathscr{p}}$ are not currently available. It would be possible to proceed by using the computational techniques previously mentioned by fixing $\\pmb{\\ell}$ and considering the effects on $\\mathcal{\\pmb{p}}_{m}(\\pmb{u},\\pmb{v})$ caused by systematic alterations in $\\pmb{\\mathscr{u}}$ and $\\pmb{v}$ . Alternatively, if $\\pmb{\\mathscr{u}}$ and $\\pmb{v}$ are fixed it would be possible to consider the effects on $\\pmb{\\mathcal{P}}_{m}(\\pmb{u},\\pmb{v})$ caused by systematic alterations to t. These primitive approaches would require considerable computer time. Alternative techniques need to be developed but lie beyond the scope of this paper. <PARAGRAPH_SEPARATOR> When such bands have been constructed there is no guarantee that they are in any sense optimal. In the Neyman-Pearson framework Steck (197l) has discussed problems associated with the optimality criterion of minimizing the are8 enclosed by the bands. In particular it was noted that when the distribution has infinite or semiinfinite support the area may be infnite. This type of problem also arises in the Bayesian framework. As an example where the area is finite consider bands for the limiting posterior distribution when the prior is noninformative, $\\pmb{n}(\\mathrm{i})=1$ for all $\\pmb{\\dot{\\upnu}}$ and ${\\pmb F}({\\bf0})={\\bf0}$ . In this case the limiting posterior distribution of ${\\pmb F}({\\pmb X}_{\\pmb n})$ is degenerate and defining $\\pmb{\\mathcal{X}}_{0}=\\mathbf{0}$ , the area enclosed by the bands is given by $D=\\Sigma(X_{i}-X_{i-1})(v_{i}-u_{i-1})$ . In theory $\\pmb{D}$ may be minimized by numerical methods subject to $\\mathscr{p}_{n-1}(u,v)=p$ . Note that a posteriori for the noninformative prior, $\\pmb{\\mathscr{t}}=(\\pmb{X}_{1},...,\\pmb{X}_{n-1})$ i8 fxed and there are various possible $\\pmb{\\mathscr{u}}$ and $\\pmb{v}$ which are solutions to $\\mathscr{p}_{\\mathfrak{n}-\\mathfrak{1}}(\\mathfrak{u},\\mathfrak{v})=\\mathscr{p}$ . When $\\pmb{n X}_{i}=\\dot{\\pmb{\\i}}$ for all $\\pmb{\\dot{\\upnu}}$ ,Steck in fact studied $\\pmb{D}$ for selected candidate bands and concluded that the Kolmogorov bands were not far from being optimal."
  },
  {
    "qid": "statistic-posneg-1702-1-0",
    "gold_answer": "FALSE. The joint prior distribution f(c, k0, k) = 1 / (K N_k) is not uniform because N_k, the number of models for a given number of components k, is increasing in k. This means models with fewer components receive more weight, making the prior informative rather than uniform.",
    "question": "Is the joint prior distribution of (c, k0, k) given by f(c, k0, k) = 1 / (K N_k) uniform across all possible models?",
    "question_context": "Bayesian Mixture Modeling with Reversible Jump MCMC\n\nThe paper discusses a Bayesian approach to mixture modeling using Reversible Jump Markov Chain Monte Carlo (RJMCMC). The model involves parameters such as the number of components (k), the number of distinct means (k0), component weights (p), distinct means (μ*), and component variances (σ²). The prior distributions for these parameters are specified, including uniform priors for k and k0, and Dirichlet and gamma distributions for p and σ⁻², respectively. The posterior distribution is derived, and the RJMCMC algorithm is outlined, which includes steps for updating k, k0, c, p, μ*, σ², z, and β."
  },
  {
    "qid": "statistic-posneg-13-0-0",
    "gold_answer": "FALSE. The condition βD_high - D_low > (1/2)(1-β)Y is not directly derived from the formula for 1+r_S < 1. The economic intuition in the paper's model suggests that a large deleveraging shock forces borrowers to cut spending significantly, but the condition provided does not accurately reflect the equilibrium condition for a negative natural rate.",
    "question": "Is the condition for a negative natural rate derived as βD_high - D_low > (1/2)(1-β)Y consistent with the formula for 1+r_S < 1? Briefly explain the economic intuition derived from the paper's model.",
    "question_context": "Central Limit Theorems for Non-Linear Functionals of Gaussian Fields\n\nThe paper models a deleveraging shock where the debt limit falls from D_high to D_low. In the flexible price endowment economy, the short-run natural rate is 1+r_S = ( (1/2)Y + D_low ) / ( β(1/2)Y + βD_high ). A negative rate (r_S < 0) occurs if βD_high - D_low > (1/2)(1-β)Y. In the sticky price model at the ZLB, output is Ŷ_S = Γ - [ χ_b / (1-χ_b(μ+κγ_D)) ] * D̂, leading to paradoxes like the Paradox of Flexibility."
  },
  {
    "qid": "statistic-posneg-2140-4-2",
    "gold_answer": "FALSE. The condition $\\sum_{t=0}^{\\infty}t^{\\zeta}|\\gamma_{X}(t)|<\\infty$ does not necessarily imply exponential decay of $\\gamma_{X}(t)$. It only requires that the autocovariance function decays sufficiently fast to ensure the sum converges, which can include polynomial decay rates depending on the value of $\\zeta$.",
    "question": "Does the condition $\\sum_{t=0}^{\\infty}t^{\\zeta}|\\gamma_{X}(t)|<\\infty$ imply that the autocovariance function $\\gamma_{X}(t)$ decays exponentially fast?",
    "question_context": "Random Projection-Based Test of Gaussianity\n\nDefine $H_{n}=(l_{n}/a_{n})^{1/2}$ for $n\\geq0$ and set $\\mathbf{H}=(H_{0},H_{1},...)^{T}$ . It can be easily checked that the distribution of $\\mathbf{H}$ is dissipative. The only point remaining is to show that the elements generated from this distribution belong to $l^{2}$ . This is discussed in what follows. <PARAGRAPH_SEPARATOR> Proposition 3.1. Let $\\mathbf{H}=(H_{n})_{n\\geq0}$ be a stochastic process constructed as described above. Then, $\\|\\mathbf H\\|=1$ , a.s. <PARAGRAPH_SEPARATOR> Using this distribution, we obtain the random projections as follows. Let ${\\bf h}=(h_{i})_{i\\in\\mathbb{N}}$ be a fixed realization of the random element $\\mathbf{H}$ . We assume that $\\mathbf{H}$ is independent of the process $\\mathbf{X}.$ . Let us consider the process $\\mathbf{Y}^{h}=\\mathbf{\\Phi}(Y_{t}^{h})_{t\\in\\mathbb{Z}}$ given by the projections of $(X^{(t)})_{t\\in\\mathbb{Z}}$ on the one-dimensional subspace generated by $\\mathbf{h}$ , i.e. <PARAGRAPH_SEPARATOR> $$ Y_{t}^{h}=\\sum_{i=0}^{\\infty}h_{i}X_{t-i}a_{i},\\quad t\\in\\mathbb{Z}. $$ <PARAGRAPH_SEPARATOR> Henceforth, when no ambiguity arises, the superscript $h$ is omitted to simplify notation. <PARAGRAPH_SEPARATOR> We will denote $\\gamma_{Y|\\mathbf{h}}(t):=\\mathbb{E}[(Y_{0}-\\mu_{Y|\\mathbf{h}})(Y_{t}-\\mu_{Y|\\mathbf{h}})|\\mathbf{h}],$ where $\\mu_{Y|\\mathbf{h}}:=\\mathbb{E}[Y_{0}|\\mathbf{h}]$ . The following proposition shows that the projected process inherits the properties of the original one. <PARAGRAPH_SEPARATOR> Proposition 3.2. Let $(X_{t})_{t\\in\\mathbb{Z}}$ be an ergodic and stationary process such that $\\begin{array}{r}{\\sum_{t=0}^{\\infty}t^{\\zeta}|\\gamma_{X}(t)|<\\infty}\\end{array}$ , with $\\zeta\\geq0$ . Then, conditionally on h, the process $(Y_{t})_{t\\in\\mathbb{Z}}$ defined in (3) is ergodic and stationary. Additionally, $\\mathbb{E}[|Y_{0}||\\mathbf{h}]$ and $\\textstyle\\sum_{t=0}^{\\infty}t^{\\zeta}|\\gamma_{Y|\\mathbf{h}}(t)|$ are finite."
  },
  {
    "qid": "statistic-posneg-848-0-1",
    "gold_answer": "TRUE. When I_10 ≤ I_01, it follows that I_01 I_10 = I_10, which implies ν = E(I_01 I_10) = E(I_10) = π_10. Substituting ν = π_10 into the expressions for the class probabilities ρ_A, ρ_B, ρ_S, and ρ_P yields the given formulas. This scenario represents a case of spurious synergism where the interaction between A and B is entirely due to the differing amounts of a single harmful agent, leading to the simplified expressions for the ρ's.",
    "question": "Does the condition I_10 ≤ I_01 imply that ν = π_10 and thus ρ_A = 0, ρ_B = π_01 - π_10, ρ_S = π_11 - π_01, ρ_P = π_10 - π_00?",
    "question_context": "Synergistic Interaction Analysis in Binary Exposure Factors\n\nThe paper discusses the analysis of synergistic interactions between two binary exposure factors, A and B, in causing an outcome D. It introduces six classes of indicator matrices (I_jk) based on Assumption 3, labeled as ϕ, Ω, A, B, S, P. The probabilities of these classes (ρ_ϕ, ..., ρ_P) are functions of the π_jk parameters and a fifth parameter ν = E(I_01 I_10). The maximum-entropy value ν* is proposed to estimate the unobservable ν, leading to the calculation of class probabilities under constraints. The paper also discusses the distinction between genuine and spurious synergism and provides formulas for conditional expectations and inequalities for ν."
  },
  {
    "qid": "statistic-posneg-1869-0-0",
    "gold_answer": "TRUE. The theorem explicitly states that the deviations $|\\widehat{S}_{0j}-s_{0}|$ and $|\\widehat{\\alpha}_{j}-\\alpha|$ are bounded above by $c_{10}\\operatorname*{max}\\left(a_{j}^{2}r_{j},a_{j}^{-2}\\right)$ and $c_{11}\\operatorname*{max}\\left(a_{j}^{2}r_{j},a_{j}^{-2}\\right)$, respectively, for some almost surely finite random variables $c_{10}$ and $c_{11}$. This implies that the convergence rates are indeed governed by the term $\\operatorname*{max}\\left(a_{j}^{2}r_{j},a_{j}^{-2}\\right)$.",
    "question": "Is it true that the convergence rates of $\\widehat{s}_{0j}$ and $\\widehat{\\alpha}_{j}$ to their true values $s_{0}$ and $\\alpha$ are determined by the term $\\operatorname*{max}\\left(a_{j}^{2}r_{j},a_{j}^{-2}\\right)$ as stated in Theorem 1?",
    "question_context": "Convergence of Parameter Estimates in Gegenbauer Process Analysis\n\nRemark 11. As only a finite number of $(\\frac{\\bar{\\delta}_{j\\cdot}^{(2)}}{c_{2}},\\frac{\\Delta\\bar{\\delta}_{j\\cdot}^{(2)}}{c_{3}})$ fall outside of $R_{\\mathbf{y}}$ , then there is $j_{0}\\in\\mathbb{N}$ such that $\\begin{array}{r}{(\\frac{\\bar{\\delta}_{j\\cdot}^{(2,a)}}{c_{2}},\\frac{\\Delta\\bar{\\delta}_{j\\cdot}^{(2,a)}}{c_{3}})=(\\frac{\\bar{\\delta}_{j\\cdot}^{(2)}}{c_{2}},\\frac{\\Delta\\bar{\\delta}_{j\\cdot}^{(2)}}{c_{3}})}\\end{array}$ for all $j\\geq j_{0}$ . Therefore, in this case, $\\begin{array}{r}{(\\frac{\\bar{\\delta}_{j\\cdot}^{(2,a)}}{c_{2}},\\frac{\\Delta\\bar{\\delta}_{j\\cdot}^{(2,a)}}{c_{3}})}\\end{array}$ and $(\\frac{\\bar{\\delta}_{j\\cdot}^{(2)}}{c_{2}},\\frac{\\Delta\\bar{\\delta}_{j\\cdot}^{(2)}}{c_{3}})$ have the same rate of convergence to $(s_{0}^{-4\\alpha},\\alpha s_{0}^{-4\\alpha-2})$ when $j\\to+\\infty$ . <PARAGRAPH_SEPARATOR> Theorem 1. Let the process $X(t)$ and the filter $\\psi(\\cdot)$ satisfy Assumptions 1 to 3. Let $(\\hat{s}_{0j},\\hat{\\alpha}_{j})$ be $a$ solution of the system of equations <PARAGRAPH_SEPARATOR> $$ \\left\\{\\begin{array}{l l}{\\hat{\\alpha}_{j}(\\hat{s}_{0j})^{-4\\hat{\\alpha}_{j}-2}=\\Delta\\bar{\\delta}_{j\\cdot}^{(2,a)}/c_{3},}\\\\ {\\hat{s}_{0j}^{-4\\hat{\\alpha}_{j}}=\\bar{\\delta}_{j\\cdot}^{(2,a)}/c_{2},}\\end{array}\\right. $$ <PARAGRAPH_SEPARATOR> where △(2.a) and 8(2.a) are the adjusted statistics. Then, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\widehat{S}_{0j}=e x p\\left(\\frac{1}{2}L a m b e r t W\\left(\\frac{\\ln{\\left(c_{2}/\\bar{\\delta}_{j,}^{(2,a)}\\right)}}{2q_{j}}\\right)\\right),}\\\\ {\\widehat{\\alpha}_{j}=q_{j}e x p\\left(L a m b e r t W\\left(\\frac{\\ln{\\left(c_{2}/\\bar{\\delta}_{j,}^{(2,a)}\\right)}}{2q_{j}}\\right)\\right),}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\begin{array}{r}{q_{j}=\\frac{c_{2}}{c_{3}}\\frac{\\Delta\\bar{\\delta}_{j\\cdot}^{(2,a)}}{\\bar{\\delta}_{j\\cdot}^{(2,a)}}}\\end{array}$ <PARAGRAPH_SEPARATOR> If $\\mathrm{\\Delta}_{\\mathrm{\\hat{\\Pi}}}\\mathrm{\\Delta}_{S_{0}}$ and $\\alpha$ are the true values of parameters and the assumptions of Proposition 2 hold true, then $\\widehat{s}_{0j}\\xrightarrow[]{a.s.}s_{0}$ and $\\widehat{\\alpha}_{j}\\xrightarrow[]{a.s.}\\alpha$ , when $j\\to+\\infty$ . Moreover, there are almost surely finite random variables $c_{10}$ and $c_{11}$ such that, for all $j\\in\\mathbb{N},$ , it holds <PARAGRAPH_SEPARATOR> $$ \\left|\\widehat{S}_{0j}-s_{0}\\right|\\leq c_{10}\\operatorname*{max}\\left(a_{j}^{2}r_{j},a_{j}^{-2}\\right) $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{|\\widehat{\\alpha}_{j}-\\alpha|\\leq c_{11}\\operatorname*{max}\\left(a_{j}^{2}r_{j},a_{j}^{-2}\\right).}\\end{array} $$"
  },
  {
    "qid": "statistic-posneg-103-3-1",
    "gold_answer": "FALSE. The Hilbert transform filter $G(f) = -i\\operatorname{sgn}(f)$ has a gain function $|G(f)| = 1$ for all frequencies $f \\neq 0$, meaning it does not alter the amplitude spectrum of the input signal $x(t)$. The transform only affects the phase of the signal, introducing a $-\\pi/2$ shift for positive frequencies and a $\\pi/2$ shift for negative frequencies, as stated in the context. The energy spectrum (amplitude squared) remains unchanged.",
    "question": "Does the Hilbert transform filter $G(f) = -i\\operatorname{sgn}(f)$ alter the amplitude spectrum of the input signal $x(t)$?",
    "question_context": "Hilbert Transform and Analytic Signal Properties\n\nThe Hilbert transform of a real-valued finite-energy function $x(t)$ is the real-valued function ${\\check{x}}(t)$ given by (Papoulis, 1991) $$ \\breve{x}(t)=x(t)\\ast\\frac{1}{\\pi t}=\\frac{1}{\\pi}\\int_{-\\infty}^{\\infty}\\frac{x(u)}{t-u}d u, $$ where $^*$ denotes convolution, so that ${\\check{x}}(t)$ is seen to be the convolution of $x(t)$ and $\\left(\\pi t\\right)^{-1}$ , and, since when $t=u$ the integral diverges, it is defined in terms of the Cauchy principal value. The Fourier transform $G(f)$ of $\\left(\\pi t\\right)^{-1}$ is given by $$ G(f)=-i\\operatorname{sgn}(f)={\\left\\{\\begin{array}{l l}{-i,}&{{\\mathrm{if~}}f>0,}\\ {0,}&{{\\mathrm{if~}}f=0,}\\ {i,}&{{\\mathrm{if~}}f<0.}\\end{array}\\right.} $$ The convolution of $x(t)$ with $\\left(\\pi t\\right)^{-1}$ is equivalent to filtering $x(t)$ . The transfer function, or frequency response function, of the filter is $G(f)$ and $G(f)=\\left|G(f)\\right|$ exp $\\{i\\theta(f)\\}$ , where $|G(.)|$ and $\\theta(.)$ are called, respectively, the gain function and phase function of the filter. We see that $G(f)$ is totally imaginary, so that $|G(f)|=1$ for all $f$ , and $$ \\theta(f)=\\left\\{{\\begin{array}{l l}{-\\pi/2,}&{{\\mathrm{if~}}f>0,}\\ {0,}&{{\\mathrm{if~}}f=0,}\\ {\\pi/2,}&{{\\mathrm{if~}}f<0.}\\end{array}}\\right. $$ Thus Hilbert transforming is equivalent to leaving the energy spectrum of the signal unchanged while changing the phase by $-\\pi/2$ for positive frequencies and $\\pi/2$ for negative frequencies."
  },
  {
    "qid": "statistic-posneg-233-2-0",
    "gold_answer": "TRUE. The baseline hazard $\\lambda_0(t) = \\beta_0^* - (\\beta_1^*)^2 t$ explicitly includes a negative time-dependent term $- (\\beta_1^*)^2 t$. Since $(\\beta_1^*)^2$ is always non-negative (as it is a squared term), the baseline hazard will decrease linearly over time as long as $\\beta_1^* \\neq 0$. This reflects the model's assumption that the baseline hazard is not constant but diminishes over time, with the rate of decrease determined by the magnitude of $\\beta_1^*$.",
    "question": "In the additive hazards model, the hazard function conditional on $A_i$ alone is given by $\\lambda(t|A_{i})=\\lambda_{0}(t)+\\beta_{2}A_{i}$, where $\\lambda_{0}(t)=\\beta_{0}^{*}-\\beta_{1}^{*2}t$ and $\\beta_{2}=\\beta_{2}^{*}+\\beta_{1}^{*}$. Is it correct to say that the baseline hazard $\\lambda_0(t)$ decreases over time if $\\beta_1^* \\neq 0$?",
    "question_context": "Semiparametric Estimation in Additive Hazards Model with Outcome-Dependent Visits\n\nIn this section we examine the finite sample performance of the proposed method for an additive hazards model. We consider a setting where models involving a single covariate $A$ are of interest, but there exists another observed covariate $V$ which is probably related both to $T$ and to the visit process. Martinussen and Vansteelandt (2013) discussed the collapsibility of additive hazards models, which facilitates the simulation scenarios we consider. We simulate the exposure variable $A_{i}$ from a Bernoulli(0.5) distribution. We then generate an auxiliary variable $V_{i}$ from $N(A_{i},1)$ . It is assumed that both $A_{i}$ and $V_{i}$ are risk factors of failure time $T_{i}$ and may also be predictors of patients’ visit times. We generate $T_{i}$ from an exponential distribution with hazard function given by $\\lambda(t|V_{i},A_{i})=\\beta_{0}^{*}+\\beta_{1}^{*}V_{i}+\\beta_{2}^{*}A_{i}$ . Following Martinussen and Vansteelandt (2013), it can be shown th|at the distribution of $T_{i}$ conditional on $A_{i}$ alone is still of additive hazards form with hazard function $\\lambda(t|A_{i})=\\lambda_{0}(t)+\\beta_{2}A_{i},$ where $\\lambda_{0}(t)=\\beta_{0}^{*}-\\beta_{1}^{*2}t$ , and $\\beta_{2}=\\beta_{2}^{*}+\\beta_{1}^{*}$ . We let $\\beta_{0}^{*}=0.5$ for nonnegative $\\beta_{2}$ and 0.8 for negative $\\beta_{2}$ , $\\beta_{1}^{*}=0.2$ , and $\\beta_{2}^{*}=-0.6,-0.4,-0.2,0$ , or 0.2 so that true values of $\\beta_{2}$ range from $-0.4$ to 0.4 with an increment of 0.2 to represent various scenarios. Model (15) gives the survivor function $S(t|A_{i})=S_{0}(t)\\exp(-\\beta_{2}A_{i}t),$ where $S_{0}(t)$ is treated nonparametrically and regression coefficient $\\beta_{2}$ is of primary interest for estimation and inference."
  },
  {
    "qid": "statistic-posneg-1923-1-0",
    "gold_answer": "TRUE. The derivation is correct: Starting from the likelihood $L_{k j i}^{h} = \\frac{(t_{k j i}^{h})^{k}}{1+t_{k j i}^{h}}$, taking the negative logarithm gives $-\\log L_{k j i}^{h} = -k \\log(t_{k j i}^{h}) + \\log(1 + t_{k j i}^{h})$. Substituting $t_{k j i}^{h} = \\exp(a+b X_{k j i}^{h})$, we get $-k(a+b X_{k j i}^{h}) + \\log\\{1 + \\exp(a+b X_{k j i}^{h})\\}$, which matches the given negative log-likelihood function.",
    "question": "Is the negative log-likelihood function $l_{k j i}^{h}(a,b) = -k(a+b X_{k j i}^{h}) + \\log\\{1+\\exp(a+b X_{k j i}^{h})\\}$ correctly derived from the likelihood function $L_{k j i}^{h}(\\Pi_{k}\\mid a,b;X_{k j i}) = \\frac{(t_{k j i}^{h})^{k}}{1+t_{k j i}^{h}}$ where $t_{k j i}^{h} = \\exp(a+b X_{k j i}^{h})$?",
    "question_context": "Feature Selection via Generalized Logit Model in Tiered Classifiers\n\nOther approaches to creating the new populations are also possible. For example, mixture models, and in particular, models for hierarchical mixtures of experts (Jacobs et al. 1991; Jordan & Jacobs 1994), are attractive. Huang et al. (2012) have suggested bidirectional classifiers as alternative approaches to improving classification performance in the presence of clusters. <PARAGRAPH_SEPARATOR> # 2·4. Feature selection using the logit model and generalized correlation <PARAGRAPH_SEPARATOR> We discuss a popular feature-selection method here, since we shall use it in $\\S3$ for numerical studies and in $\\S4$ to develop theoretical properties of tiered classifiers. Recall, from $\\S2\\cdot1$ , that we have access to a training sample $\\mathcal{X}_{k}$ of data vectors $X_{k j}=(X_{k j1},\\ldots,X_{k j p})^{\\mathrm{T}}$ , for $j=1,\\ldots,n_{k}$ , drawn from $\\Pi_{k}$ for $k=0$ , 1. Let $h$ denote a general function, put Xkhji = h(Xkji ), and consider a generalized logit model for the probability that $X_{k j}$ came from $\\Pi_{\\ell}$ , given the value of $X_{k j i}$ : <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathrm{pr}(X_{k j}\\in\\Pi_{0}\\vert X_{k j i})=\\{1+\\exp(a+b X_{k j i}^{h})\\}^{-1},}\\ &{\\mathrm{pr}(X_{k j}\\in\\Pi_{1}\\vert X_{k j i})=1-\\mathrm{pr}(X_{k j}\\in\\Pi_{0}\\vert X_{k j i}),}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $a$ and $b$ are scalars. In this notation the likelihood that $X_{k j}$ came from $\\Pi_{\\ell}$ , given the value of $X_{k j i}$ , is modelled as <PARAGRAPH_SEPARATOR> $$ L_{k j i}^{h}(\\boldsymbol{\\Pi}_{\\ell}\\mid\\boldsymbol{a},\\boldsymbol{b};X_{k j i})=\\left(\\frac{t_{k j i}^{h}}{1+t_{k j i}^{h}}\\right)^{\\ell}\\left(\\frac{1}{1+t_{k j i}^{h}}\\right)^{1-\\ell}=\\frac{(t_{k j i}^{h})^{\\ell}}{1+t_{k j i}^{h}}, $$ <PARAGRAPH_SEPARATOR> where $t_{k j i}^{h}=t_{k j i}^{h}(a,b,X_{k j i})\\equiv\\exp(a+b X_{k j i}^{h})$ and $\\ell=0$ or 1. Taking $\\ell=k$ we see that the negative loglikelihood is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{l_{k j i}^{h}(a,b)=-\\log L_{k j i}^{h}(\\Pi_{k}\\mid a,b;X_{k j i})}\\ &{\\qquad=-k(a+b X_{k j i}^{h})+\\log\\{1+\\exp(a+b X_{k j i}^{h})\\},}\\end{array} $$ <PARAGRAPH_SEPARATOR> and its version for the set of all training data is <PARAGRAPH_SEPARATOR> $$ l_{i}^{h}(a,b)=\\frac{1}{n_{0}+n_{1}}\\sum_{k=0}^{1}\\sum_{j=1}^{n_{k}}l_{k j i}^{h}(a,b). $$ <PARAGRAPH_SEPARATOR> Define $(\\hat{a}_{i}^{h},\\hat{b}_{i}^{h})$ to be the value of $(a,b)$ that minimizes $l_{i}^{h}(a,b)$ , and put <PARAGRAPH_SEPARATOR> $$ \\hat{l}_{i}=\\operatorname*{min}_{h\\in\\mathcal{H}}l_{i}^{h}(\\hat{a}_{i}^{h},\\hat{b}_{i}^{h}), $$ <PARAGRAPH_SEPARATOR> where $\\mathcal{H}$ is a class of functions. If we take $\\mathcal{H}$ to be the class of linear functions, then we obtain the same result as if we were to put $h(x)\\equiv x$ throughout, and to remove from (1) the operation of minimization. More generally, $\\mathcal{H}$ could be the class of polynomials of given degree $d\\geqslant1$ , or a class of spline functions. <PARAGRAPH_SEPARATOR> Order the values of $\\hat{l}_{i}$ as $\\hat{l}_{11}\\leqslant\\cdots\\leqslant\\hat{l}_{\\hat{1}_{p}}$ , where $\\hat{\\mathbf{1}}_{1},\\ldots,\\hat{\\mathbf{1}}_{p}$ is a permutation of $1,\\ldots,p$ . This particular permutation order is the one indicated by a generalized logit model for likelihood. It suggests that the feature with index $\\hat{\\bf1}_{1}$ plays the greatest role in determining the population from which $X$ came, the feature with index $\\hat{\\mathbf{1}}_{2}$ plays the next greatest role, and so on. <PARAGRAPH_SEPARATOR> # 3. EMPIRICAL STUDIES AND DISCUSSION"
  },
  {
    "qid": "statistic-posneg-610-0-0",
    "gold_answer": "FALSE. The bound $(r+1)\\binom{l-1}{2(2r-l)+4}(2r)^{2(2r-l)+4}$ is derived under the assumption that $l\\geqslant\\frac{8}{5}r+\\frac{9}{5}$ and $l\\leqslant2r+1$, but it is not directly correct for the sum $\\sum_{i=0}^{2(2r-l)+4}\\binom{l-1}{i}(2r)^{i}$. The correct bound should consider all terms in the sum, not just the largest term, and the combinatorial factor $(r+1)$ is not justified in the context provided. The assumption $l\\geqslant\\frac{8}{5}r+\\frac{9}{5}$ ensures that the largest term is the $2(2r-l)+4$ term, but the bound should be more carefully derived to account for all terms in the sum.",
    "question": "Is the bound $(r+1)\\binom{l-1}{2(2r-l)+4}(2r)^{2(2r-l)+4}$ derived under the assumption $l\\geqslant\\frac{8}{5}r+\\frac{9}{5}$ and $l\\leqslant2r+1$ correct for the sum $\\sum_{i=0}^{2(2r-l)+4}\\binom{l-1}{i}(2r)^{i}$?",
    "question_context": "Eigenvector Limit Theorems in Random Matrices\n\nThe above arguments also apply to rows. <PARAGRAPH_SEPARATOR> We have then a maximum of $2(2r-l)+4$ (whenthe $2r+1\\operatorname{th}$ elementis both a row and column innovation) type 3 elements of (c) type for which an ambiguity can occur. <PARAGRAPH_SEPARATOR> With $i$ ranging along the number of ambiguities we therefore have for a bound on the factor contributed by type 3 elements of (c) type the quantity <PARAGRAPH_SEPARATOR> $$ \\sum_{i=0}^{2(2r-l)+4}\\binom{l-1}{i}(2r)^{i}. $$ <PARAGRAPH_SEPARATOR> Under  the  assumption $l\\geqslant\\frac{8}{5}r+\\frac{9}{5}$ the largest term in (4.49) is the $2(2r-l)+4$ term $(l\\geqslant\\frac{8}{5}r+\\frac{9}{5}\\Leftrightarrow2(2r-l)+4\\leqslant(l-1)/2)$ and  since $l\\leqslant2r+1$ we can bound (4.49) by <PARAGRAPH_SEPARATOR> $$ (r+1)\\binom{l-1}{2(2r-l)+4}(2r)^{2(2r-l)+4}. $$ <PARAGRAPH_SEPARATOR> Combining the above we get <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\quad_{4}\\zeta2(2r+1)(2r-1)M-1}\\ &{\\quad_{5}\\left(\\frac{\\hat{J}^{2}}{r_{2}}\\right)\\left(\\frac{\\hat{J}^{2}-r^{2}}{r_{3}-r_{2}}\\right)\\left(\\frac{4r}{r_{-}\\hat{J}}-1\\right)\\times4\\times r^{2}}\\ &{\\quad_{5}\\left((2r-2)\\left(r-r_{+}\\right)\\right)}\\ &{\\quad_{\\times(2r)^{2(3)-\\frac{r_{+}}{r_{+}}-1}\\times(2r)^{(2)(3r-r-r_{+})}\\times(r+1)}\\left(\\frac{\\hat{J}^{2}-r^{2}}{2(2r-r_{-})+4}\\right)}\\ &{\\quad_{\\times(2r)^{(3r-r_{+})}\\times4}}\\ &{=2(r+1)(2r-1)(r_{-})^{r_{-}}\\left(\\frac{\\hat{J}^{2}-r^{2}}{r_{-}-r_{-}^{2}}\\right)}\\ &{\\quad_{7}\\times\\frac{(2r-1)(4r-r_{+}+2)\\hat{J}^{2}}{(4r-2)+2(1+r_{-})(2r-r_{-})}(2r)^{(3r-r-r_{+})}\\times}\\ &{\\quad_{7}\\times2(2r+1)(r_{+})\\left(\\frac{r_{-}}{r_{-}\\hat{J}}\\right)^{r_{+}}}\\ &{\\quad_{8}\\left(\\frac{\\hat{J}^{2}-r^{2}}{r_{+}-r_{2}}\\right)\\left(2r-(r_{-})\\right)^{(2r-r_{+})/2}\\left(-\\hat{J}^{2}-(2r-r_{-})\\right)^{r_{-}}}\\ &{\\quad_{8}\\left(\\frac{\\hat{J}^{2}-r^{2}}{r_{+}-r_{+}}\\right)\\left(2r^{(3r-r_{+})}\\cdots\\right)}\\end{array} $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\leqslant2(2r+1)(r+1)\\left({\\displaystyle\\sum_{r_{0}}^{2r}}\\right)^{2}(4r)^{4(2r-l)+7}(2r)^{7(2r-l)+19}}}\\ {{\\leqslant\\left({\\displaystyle\\sum_{r_{0}}^{2r}}\\right)^{2}r^{22(2r-l)+56}}}\\end{array} $$ <PARAGRAPH_SEPARATOR> valid for $r\\geqslant6$ <PARAGRAPH_SEPARATOR> Therefore using (4.36) we have for $l\\geqslant{\\textstyle\\frac{8}{5}}r+{\\textstyle\\frac{9}{5}},r\\geqslant6$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{\\beta_{l}\\leqslant s^{l-j}r^{22(2r-l)+56}\\displaystyle\\sum_{r_{0}=j}^{l-1}\\left(\\displaystyle\\sum_{r_{0}}^{2r}\\right)^{2}\\left(\\displaystyle\\frac{n}{s}\\right)^{r_{0}}}}\\ {{\\leqslant s^{l-j}r^{22(2r-l)+56}\\left(1+\\left(\\displaystyle\\frac{n}{s}\\right)^{1/2}\\right)^{4r},}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where the relation between $j$ and the sum under consideration is given in Table II. Thus, (4.38) is established with $\\xi_{1}=22$ $\\xi_{2}=56$"
  },
  {
    "qid": "statistic-posneg-1655-0-0",
    "gold_answer": "TRUE. The normalization ${\\cal Z}_{N}(t)=\\{X_{N}(t)-a N\\}/\\sqrt{N}$ is indeed a standard condition for weak convergence of sequences of birth and death processes to an Ornstein-Uhlenbeck process, as discussed in the context. This normalization ensures that the centered and scaled process converges to a Gaussian process with appropriate mean and variance, which is characteristic of the Ornstein-Uhlenbeck process.",
    "question": "Is the condition for weak convergence of the sequence of birth and death processes $\\{X_{N}(t)\\}$ to an Ornstein-Uhlenbeck process given by the normalization ${\\cal Z}_{N}(t)=\\{X_{N}(t)-a N\\}/\\sqrt{N}$?",
    "question_context": "Central Limit Analogues for Markov Population Processes\n\nIN this paper the main discussion is concerned with obtaining asymptotic results for sequences of birth and death processes which are similar to the central limit theorem for sequences of random variables. The motivation is the need to obtain useful approximations to the distributions of sample paths of processes which arise as models for population growth, but for which Kolmogorov differential equations are intractable. <PARAGRAPH_SEPARATOR> In the first section, univariate processes are considered, and conditions are given for the weak convergence of <PARAGRAPH_SEPARATOR> $$ {\\cal Z}_{N}(t)=\\{X_{N}(t)-a N\\}/\\sqrt{N}, $$ <PARAGRAPH_SEPARATOR> where $\\{X_{N}(t),N=1,2,\\ldots\\}$ is a sequence of birth and death processes, to those of an Ornstein-Uhlenbeck process as $N{\\rightarrow}{\\infty}$ . Some examples are given for purpose of illustration."
  },
  {
    "qid": "statistic-posneg-106-0-1",
    "gold_answer": "TRUE. The paper explicitly states that the $W_{p}$ statistic is more efficient than the sign test for all distributions considered. This is supported by the calculated asymptotic relative efficiencies (ARE) across different distributions, including bivariate normal, Farlie-Gumbel-Morgenstern (FGM) with uniform, normal, and exponential marginals, and bivariate exponential. The ARE values consistently favor $W_{p}$ over the sign test.",
    "question": "Does the modified Wilcoxon rank sum test $W_{P}$ always have higher Pitman efficiency than the sign test for all distributions considered in the paper?",
    "question_context": "Modified Wilcoxon Rank Sum Test for Paired Data\n\nFor the general location problem, $F_{X}(x)=F_{Y}(\\dot{x}-\\theta)$ , the standard procedures are the paired $\\pmb{t}$ test, the sign test and the Wilcoxon signed rank test. All three of these procedures involve only the $_{n}$ differences $X_{i}-Y_{i}$ Asymptotically nonparametric procedures using the individual $2\\pi$ observations were developed by Hollander, Pledger $\\&$ Lin (1974) and Raviv (1978). In the present paper, another test will be proposed, the modifed Wilcoxon rank sum test, denoted by $W_{P}$ <PARAGRAPH_SEPARATOR> # 2. THE MODIFIED WILCOXON RANK SUM TEST <PARAGRAPH_SEPARATOR> Let $X_{(1)}<...<X_{(\\mathfrak{n})}$ be the order statistics of the $X$ sample. Let $R_{i}(i=1,\\ldots,n)$ be the rank of $X_{(i)}$ in the combined sample. Define <PARAGRAPH_SEPARATOR> $$ Z_{n}={\\sqrt{(2n+1)\\left[{\\frac{1}{n}}\\sum_{i=1}^{n}a_{n}{\\bigg(}{\\frac{R_{i}}{2n+1}}{\\bigg)}-\\int_{0}^{1}a(u)d F_{X}\\{H^{-1}(u)\\}\\right]}}, $$ <PARAGRAPH_SEPARATOR> where $a_{\\mathfrak{n}}(u)~(0<u<1)$ is a score function, such that $a_{n}(u)$ converges to $\\pmb{a}(\\pmb{u})$ a8 ${\\pmb n}\\rightarrow\\infty$ , <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{H(y)=\\frac{1}{2}F_{X}(y)+\\frac{1}{2}F_{Y}(y),\\quad H^{-1}(u)=\\operatorname*{inf}\\left\\{y\\colon H(y)\\geqslant u\\right\\}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> With $a_{n}(u)=u$ . the Wilcoxon score, and $F_{X}=F_{Y},Z_{n}$ is asymptotically normal with asymptotic mean equal to 0 and asymptotic variance given by $\\dot{\\sigma}^{2}=(1-\\dot{\\rho}_{G})/12$ ,where $\\rho_{G}=\\operatorname{corr}\\left\\{F_{X}(X_{1}),\\bar{F}_{Y}(Y_{1})\\right\\}$ <PARAGRAPH_SEPARATOR> Let $\\hat{\\rho}_{G}$ be Spearman's coefficient of rank correlation, that is, <PARAGRAPH_SEPARATOR> $$ \\hat{\\rho}_{G}=\\frac{12}{n(n^{2}-1)}\\sum_{i=1}^{n}S_{i}T_{i}-\\frac{3(n+1)}{n-1}, $$ <PARAGRAPH_SEPARATOR> where $S_{i}$ is the rank of $X_{i}$ in the $X$ sample and $\\pmb{T}_{i}$ is the rank of $Y_{i}$ in the $Y$ sample.Since $\\hat{\\rho}_{G}$ is a consistent estimator of $\\rho_{G}$ ,we have that $\\dot{\\sigma}_{n}^{2}=(1-\\hat{\\rho}_{G})/12$ is & consistent estimator of $\\sigma^{2}$ . The modifed Wilcoxon rank sum statistic under pairing will be defined as <PARAGRAPH_SEPARATOR> $$ W_{P}=\\sqrt{(2n+1)\\left\\{\\frac{1}{n(2n+1)}\\sum_{i=1}^{n}R_{i}-\\frac{1}{2}\\right\\}}\\Bigg/\\sigma_{n}. $$ <PARAGRAPH_SEPARATOR> The consistency of $\\sigma_{n}$ yields the asymptotic standard normality of $W_{P}$ . At significance leve! $\\pmb{\\alpha}$ $H_{0}\\colon F_{X}=F_{Y}$ will be rejected in favour of $H_{a}\\colon F_{X}\\leqslant F_{Y}$ , $F_{X}$ 丰 $F_{Y}$ when $W_{P}\\geqslant Z_{1-\\alpha},$ the ${\\mathbf{l}}-{\\pmb{\\alpha}}$ percentile of the standard normal. Since $W_{P}$ is asymptotically normal with asymptotic mean and variance not depending on F, the above procedure is asymptotically distribution free. <PARAGRAPH_SEPARATOR> An important special case of the above hypotheses is the location problem, that is, $F_{X}(x)={\\bar{F}}_{Y}(x-\\theta)$ ,,for all $_{x}$ where $\\theta>0$ .The consistency of $W_{P}$ for this setof alternatives is seen by examining <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\mathrm{pr}_{\\theta}\\left(W_{p}\\geqslant Z_{1-\\alpha}\\right)=\\operatorname*{lim}_{n\\to\\infty}\\mathrm{pr}_{\\theta}\\big[Y_{n}\\geqslant\\big\\{\\sqrt{\\left(2n+1\\right)(\\frac{1}{2}-\\mu)+\\sigma_{n}Z_{1-\\alpha}}\\big\\}/\\sigma\\big], $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ Y_{n}=\\sqrt{(2n+1)\\left[\\Sigma\\:R_{i}/\\{n(2n+1)\\}-\\mu\\right]/\\sigma},\\quad\\mu=\\frac{1}{4}+\\frac{1}{2}\\int F_{Y}(x)\\:d F_{X}(x), $$ <PARAGRAPH_SEPARATOR> and $\\sigma^{2}$ is the asymptotic variance of $\\mathbf{Z}_{\\mathfrak{n}}$ <PARAGRAPH_SEPARATOR> # 3. PITMAN ASYMPTOTIC RELATIVE EFFICIENCY <PARAGRAPH_SEPARATOR> The effcacy of Raviv's test is the same as the modifed Wilcoxon rank sum test, namely $\\sqrt{6\\lceil f_{X}^{\\bar{2}}(x)d x/(1-\\rho_{G})^{\\frac{1}{2}}}$ . Thus, the Pitman effciency of the two tests is always one. However, in order to calculate the Pitman efficiencies of the paired $t$ test, $\\pmb{t}$ , the sign test, $\\boldsymbol{s}$ , and Wilcoxon signed rank test, $W^{+}$ , relative to the $W_{P}$ test, the underlying bivariate distribution function must be specified. <PARAGRAPH_SEPARATOR> Let $F$ be bivariate normal with mean $(\\mu+\\theta,\\mu)$ and covariance matrix <PARAGRAPH_SEPARATOR> $$ \\sigma^{2}{\\binom{1}{\\rho}}.\\mathbf{\\Pi}_{1}^{\\rho}). $$ <PARAGRAPH_SEPARATOR> The asymptotic relative efficiencies are as follows: <PARAGRAPH_SEPARATOR> $$ \\operatorname{ARE}{\\big(}W_{p},W^{+}{\\big)}=(1-\\rho){\\Bigg/}{\\Bigg\\{}1-{\\frac{6}{\\pi}}\\sin^{-1}(0{\\cdot}5\\rho){\\Bigg\\}}=h(\\rho), $$ <PARAGRAPH_SEPARATOR> $$ \\mathrm{\\bf~ARE}\\left(W_{p},S\\right)=\\frac{3}{2}h(\\rho),\\quad\\mathrm{\\bf~ARE}\\left(W_{p},t\\right)=\\frac{3}{\\pi}h(\\rho). $$ <PARAGRAPH_SEPARATOR> $\\pmb{F}$ is a Farlie-Gumbel-Morgenstern distribution function $x y\\{1+\\alpha(1-x)(1-y)\\}$ <PARAGRAPH_SEPARATOR> $$ {\\bf A R E}\\left(W_{p},S\\right)=\\frac{81}{2(3-\\alpha)\\left(3+\\alpha\\right)^{2}},\\quad{\\mathrm{ARE}}\\left(W_{p},W^{+}\\right)=\\frac{3(105)^{2}}{8(3-\\alpha)\\left(2\\alpha^{2}+7\\alpha+105\\right)}. $$ <PARAGRAPH_SEPARATOR> When the marginal distributions are normal, FGM $(N)$ , the effciencies are derived as <PARAGRAPH_SEPARATOR> $$ \\operatorname{aRE}{(W_{p},S)}={\\frac{9\\pi^{2}}{2(3-\\alpha)\\left\\{\\pi(1+\\alpha)-2x\\cos^{-1}{(\\frac{1}{3})}\\right\\}}},\\quad\\operatorname{ARE}{(W_{p},t)}={\\frac{9(\\pi-\\alpha)}{\\pi^{2}(3-c)}}. $$ <PARAGRAPH_SEPARATOR> When the marginal distributions are exponential, FGM $\\langle E\\rangle$ ,the efficiencies are <PARAGRAPH_SEPARATOR> $$ \\mathrm{\\bf~ARE}(W_{p},S)=\\frac{81}{2(3-\\alpha)(3+\\alpha)^{2}},\\quad\\mathrm{\\bf~ARE}(W_{p},W^{+})=\\frac{2(3)^{7}}{(3-\\alpha)(\\alpha^{2}+6\\alpha+27)^{2}}, $$ <PARAGRAPH_SEPARATOR> $$ \\operatorname{ARE}{(W_{p},t)}=9(4-\\alpha)/4(3-\\alpha). $$ <PARAGRAPH_SEPARATOR> The bivariate exponential distribution considered here is the Marshall-Olkin random shock model (Marshall $\\&$ Olkin, 1967). This distribution places mass on the line $X=Y$ and hence the distribution of $D=X=Y$ has a singularity at O. The efficacies of the sign test and the Wilcoxon signed rank test thus cannot be computed using standard techniques. However, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\mathrm{{ARE}}(W_{p},t)=\\{3(4\\lambda_{\\perp}+3\\lambda_{3})\\}/(4\\lambda_{1}+2\\lambda_{3}).}\\end{array} $$ <PARAGRAPH_SEPARATOR> The minimum and maximum value of the asymptotic relative efficiencies are contained in Table 1. The $W_{p}$ statistic is more effcient than the sign test for all distributions considered. The signed rank test appears to have lower efficiency than $W_{p}$ especially for distributions with nonsymmetric marginals. The relative efficiency of $\\boldsymbol{\\W_{p}}$ tothepaired $\\pmb{t}$ test when sampling from a bivariate normal distribution is at least 0·87. <PARAGRAPH_SEPARATOR> Table 1. Minimum and maximum asymptotic relative effciencies, ARE $(W_{p},.)$ <PARAGRAPH_SEPARATOR> <html><body><table><tr><td rowspan=\"2\">Test</td><td colspan=\"5\">Distribution</td></tr><tr><td>BVN</td><td>FGM (U)</td><td>FGM (N)</td><td>FGM (E)</td><td>BVEXP</td></tr><tr><td>Pairedttest</td><td>087 (p = 1)</td><td>一</td><td>0·94 (α=-1)</td><td>2·81 (α=-1)</td><td>3</td></tr><tr><td rowspan=\"3\">Sign test</td><td>0·97 (p = -052)</td><td></td><td>0·98 (α = 1)</td><td>3·38(α=1)</td><td>4·5</td></tr><tr><td>1·36 (p = 1)</td><td>1·27 (α = 1)</td><td>1·47 (α = 0459)</td><td>1.27 (α = 1)</td><td></td></tr><tr><td>1.52 (p = -0-52)</td><td>2.53(α=-1)</td><td>1.83 (α = - l)</td><td>2.53 (α=-l)</td><td></td></tr><tr><td>Wilcoxon</td><td>0.91 (p = 1)</td><td>1-07 (α=1)</td><td></td><td>1·89(α=087)</td><td></td></tr><tr><td>signed rank</td><td>101(p= -0-52)</td><td>1·16 (α = -1)</td><td></td><td>2·29 (α = -1)</td><td></td></tr><tr><td>RavivR test</td><td></td><td></td><td>1</td><td></td><td>1</td></tr></table></body></html>"
  },
  {
    "qid": "statistic-posneg-1849-2-0",
    "gold_answer": "TRUE. The independence assumptions (12)–(13) make $\\gamma_{2}(t)$ behave as a nugget effect, adding a term $\\mathrm{Var}(\\gamma_{2}(t_{0}))$ to the prediction error variance. Since $\\operatorname{Var}(\\gamma_{2}(\\cdot))$ is derived from the variance of the underlying field $Y$, which is larger than the prediction errors $\\hat{Y}-Y$, the prediction error variance is overestimated at non-lattice locations. This overestimation does not occur at lattice locations where $\\mathrm{Var}(\\gamma_{2}(t_{0}))=0$.",
    "question": "Is the prediction error variance at non-lattice locations overestimated due to the independence assumptions (12)–(13) of $\\gamma_{2}(t)$?",
    "question_context": "Fast Kriging Prediction with Gaussian Markov Random Fields\n\nThis makes $\\gamma_{1}(\\cdot)$ a continuous Gaussian process that equals the GMRF $\\tilde{\\gamma}$ at grid locations. Elsewhere $\\gamma_{1}(\\cdot)$ is the weighted mean value of the four closest lattice points. The variance of $\\gamma_{1}(\\cdot)$ is tuned to $\\gamma(\\cdot)$ at lattice locations (Eq. (8)), and thus the variance of $\\gamma_{1}(\\cdot)$ will be lower than that of $\\gamma(\\cdot)$ within cells. The addition of $\\gamma_{2}(\\cdot)$ in (10) gives the stipulated variances (though not covariances) even for locations within cells.<PARAGRAPH_SEPARATOR># 3.2. Prediction<PARAGRAPH_SEPARATOR>This section contains an algorithm for efficient use of the suggested model when kriging large data sets. Let $\\pmb{K}=(k_{i j})$ be the $n\\times N$ matrix defined as $k_{i j}=k({t_{i}},\\tilde{{t}}_{j})$ . Then from (7)–(13) we have<PARAGRAPH_SEPARATOR>$$\\pmb{\\Sigma}=\\pmb{K}\\pmb{Q}^{-1}\\pmb{K}^{\\mathrm{T}}+\\pmb{D}.$$<PARAGRAPH_SEPARATOR>Here ${\\pmb Q}=({\\pmb I}_{N}-{\\pmb W})\\tau^{-2}$ is the precision matrix of $\\tilde{\\gamma}$ and $\\pmb{D}$ is a (diagonal) matrix with the variance of $\\gamma_{2}(t)+\\varepsilon(t)$ at the locations where the observations are collected. Thus, from (14) it follows that<PARAGRAPH_SEPARATOR>$$D=\\sigma_{\\gamma}^{2}I_{n}-\\mathrm{diag}(K\\tilde{\\Sigma}K^{\\mathrm{T}})+\\sigma_{\\varepsilon}^{2}I_{n},$$<PARAGRAPH_SEPARATOR>where $\\tilde{\\Sigma}=(\\mathbf{Cov}(\\gamma(\\tilde{t}_{i}),\\gamma(\\tilde{t}_{j})))_{i,j=1:N}.$ . In order to calculate $\\tilde{K\\Sigma K^{\\mathrm{T}}}$ only the diagonals corresponding to adjacent lattice locations must be known in $\\tilde{\\Sigma}$ .<PARAGRAPH_SEPARATOR>For an arbitrary point $\\pmb{t}_{0}$ , where we want to predict $Y$ , define the vector of $k$ -weights for the point, ${\\pmb k}=(k_{1},\\ldots,k_{N})^{\\mathrm{T}}$ , where $k_{i}=k(\\pmb{t}_{0},\\tilde{\\pmb{t}}_{i})$ , to obtain<PARAGRAPH_SEPARATOR>$${\\pmb{\\omega}}={\\pmb{K}}{\\pmb{Q}}^{-1}{\\pmb{k}}.$$<PARAGRAPH_SEPARATOR>In all equations it is assumed that the locations where predictions are wanted do not coincide with any location with an observation used in the calculations unless such a location is also a lattice location. As the index $t$ is continuous, this restriction does not affect the usefulness of the method.<PARAGRAPH_SEPARATOR>Should approximations other than (12)–(14) of $\\gamma_{2}(t)$ be used, Eqs. (17)–(19) would change.<PARAGRAPH_SEPARATOR>Insertion of expressions (17)–(19) into (3) gives the interpolation weights for our model. While direct calculation does not give any substantial computational gain, the following method of computation will considerably ease the computational burden of calculating the kriging predictions for large data sets.<PARAGRAPH_SEPARATOR>(1) Initially predict $Y(\\cdot)$ on all grid points. Let therefore<PARAGRAPH_SEPARATOR>and $\\hat{\\mathbf{\\calY}}=(\\hat{Y}(\\tilde{t}_{1}),\\dots,\\hat{Y}(\\tilde{t}_{N}))^{\\mathrm{T}}$ .<PARAGRAPH_SEPARATOR>Using Eqs. (2) and (3) for $\\pmb{t}_{0}=\\tilde{\\pmb{t}}_{1},\\dots,\\tilde{\\pmb{t}}_{N}$ we conclude<PARAGRAPH_SEPARATOR>$$\\hat{\\pmb{Y}}=\\left(\\pmb{\\Omega}+\\mathbf{1}_{n}\\left(\\frac{\\mathbf{1}_{N}^{\\mathrm{T}}-\\mathbf{1}_{n}^{\\mathrm{T}}\\pmb{\\Sigma}^{-1}\\pmb{\\Omega}}{\\mathbf{1}_{n}^{\\mathrm{T}}\\pmb{\\Sigma}^{-1}\\mathbf{1}_{n}}\\right)\\right)^{\\mathrm{T}}\\pmb{\\Sigma}^{-1}\\pmb{Z}.$$<PARAGRAPH_SEPARATOR>(2) At an arbitrary point $\\pmb{t}_{0}$<PARAGRAPH_SEPARATOR>$$\\hat{Y}(t_{0})=k^{\\mathrm{T}}\\hat{Y},$$<PARAGRAPH_SEPARATOR>see Appendix A for details.<PARAGRAPH_SEPARATOR>From (20) and (21) we get $\\hat{Y}(\\pmb{t}_{0})$ at any point $\\scriptstyle t_{0}$ . It is therefore enough to seek computationally effective expressions for $\\pmb{\\Sigma}^{-1}\\pmb{\\Omega}$ and $\\pmb{\\Sigma}^{-1}\\mathbf{1}_{n}$ .<PARAGRAPH_SEPARATOR>The definition of the intermediate-scale variation gives<PARAGRAPH_SEPARATOR>$$\\Omega=K Q^{-1}$$<PARAGRAPH_SEPARATOR>that together with (17) yields<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}&{\\Sigma^{-1}\\Omega=\\Sigma^{-1}\\Omega(Q+K^{\\operatorname{T}}D^{-1}K)(Q+K^{\\operatorname{T}}D^{-1}K)^{-1}}\\ &{\\qquad=\\Sigma^{-1}\\Sigma D^{-1}K\\cdot(Q+K^{\\operatorname{T}}D^{-1}K)^{-1}}\\ &{\\qquad=D^{-1}K(Q+K^{\\operatorname{T}}D^{-1}K)^{-1}.}\\end{array}$$<PARAGRAPH_SEPARATOR>Further, since the row-sums of $K$ equal 1,<PARAGRAPH_SEPARATOR>$$\\Sigma^{-1}\\mathbf{1}_{n}=\\Sigma^{-1}K\\mathbf{1}_{N}=\\Sigma^{-1}\\pmb{\\Omega}\\pmb{Q}\\mathbf{1}_{N}.$$<PARAGRAPH_SEPARATOR>From (23) and (24) we can thus conclude that to calculate (20), it suffices to invert a band limited $N\\times N$ matrix $Q+K^{\\mathrm{T}}D^{-1}K$ . When $K$ interpolates only grid points within the Markov neighbourhood (as for nearest neighbour or bilinear interpolation), the addition of $\\hat{K^{\\mathrm{T}}D^{-1}K}$ adds no extra bandwidth to the GMRF precision matrix $\\boldsymbol{\\mathscr{Q}}$ , and thus no extra computational cost.<PARAGRAPH_SEPARATOR># 3.3. Prediction error<PARAGRAPH_SEPARATOR>To calculate the prediction error variance for the Markov approach define<PARAGRAPH_SEPARATOR>$$C_{r}(t_{1},t_{2})=\\operatorname{Cov}(r(t_{1}),r(t_{2}))$$<PARAGRAPH_SEPARATOR>to be the covariance function of the prediction errors. Let $\\tilde{\\omega}_{i}=\\mathrm{Cov}(\\pmb{Z},Y(\\tilde{t}_{i}))$ be the $i$ th column of $\\pmb{\\Omega}$ , and generalize (4) to get<PARAGRAPH_SEPARATOR>$$C_{r}(\\tilde{t}_{i},\\tilde{t}_{j})=C_{\\gamma}(\\tilde{t}_{i},\\tilde{t}_{j})-\\tilde{\\omega}_{i}^{\\mathrm{T}}\\Sigma^{-1}\\tilde{\\omega}_{j}+\\frac{(\\mathbf{1}_{n}^{\\mathrm{T}}\\Sigma^{-1}\\tilde{\\omega}_{i}-\\mathbf{1})(\\mathbf{1}_{n}^{\\mathrm{T}}\\Sigma^{-1}\\tilde{\\omega}_{j}-\\mathbf{1})}{\\mathbf{1}_{n}^{\\mathrm{T}}\\Sigma^{-1}\\mathbf{1}_{n}}.$$<PARAGRAPH_SEPARATOR>As $\\mathrm{Var}(\\gamma_{2}(\\tilde{\\pmb{t}}_{j}))=0$ , it follows that $C_{\\gamma}(\\tilde{t}_{i},\\tilde{t}_{j})=C_{\\gamma_{1}}(\\tilde{t}_{i},\\tilde{t}_{j})=\\mathrm{Cov}(\\tilde{\\gamma}_{i},\\tilde{\\gamma}_{j})$ . Thus after some manipulations given in Appendix B, the covariance matrix $C_{\\tilde{r}}=(C_{r}(\\tilde{t}_{i},\\tilde{t}_{j}))_{i j}$ is given by<PARAGRAPH_SEPARATOR>$$C_{\\tilde{r}}=(Q+K^{\\mathrm{T}}{\\cal D}^{-1}K)^{-1}+\\frac{(\\mathbf{1}_{n}^{\\mathrm{T}}{\\boldsymbol\\Sigma}^{-1}\\boldsymbol\\Omega-\\mathbf{1}_{N}^{\\mathrm{T}})^{\\mathrm{T}}(\\mathbf{1}_{n}^{\\mathrm{T}}{\\boldsymbol\\Sigma}^{-1}\\boldsymbol\\Omega-\\mathbf{1}_{N}^{\\mathrm{T}})}{\\mathbf{1}_{n}^{\\mathrm{T}}{\\boldsymbol\\Sigma}^{-1}\\mathbf{1}_{n}}.$$<PARAGRAPH_SEPARATOR>For the conditional prediction error the last term of (25) vanishes.<PARAGRAPH_SEPARATOR>At nonlattice locations use $\\tilde{r}=(r(\\tilde{t}_{1}),\\dots,r(\\tilde{t}_{N}))$ and the independence assumptions (12) and (13) to obtain<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}&{r(t_{0})=Y(t_{0})-\\hat{Y}(t_{0})=\\mu+\\gamma(t_{0})-k^{\\mathrm{T}}\\hat{Y}=\\mu+\\gamma_{1}(t_{0})+\\gamma_{2}(t_{0})-k^{\\mathrm{T}}\\hat{Y}}\\ &{\\qquad=k^{\\mathrm{T}}\\mu\\mathbf{1}_{N}+k^{\\mathrm{T}}\\tilde{\\gamma}+\\gamma_{2}(t_{0})-k^{\\mathrm{T}}\\hat{Y}=\\gamma_{2}(t_{0})+k^{\\mathrm{T}}(\\mu\\mathbf{1}_{N}+\\tilde{\\gamma}-\\hat{Y})}\\ &{\\qquad=\\gamma_{2}(t_{0})+k^{\\mathrm{T}}\\tilde{r}.}\\end{array}$$<PARAGRAPH_SEPARATOR>Thus<PARAGRAPH_SEPARATOR>$$\\sigma_{r}^{2}(t_{0})=k^{\\operatorname{T}}C_{\\tilde{r}}k+\\operatorname{Var}(\\gamma_{2}(t_{0})).$$<PARAGRAPH_SEPARATOR>Using an interpolation scheme where only grid points within a Markov neighbourhood are used, only terms of $C(r(\\bar{\\tilde{t}}_{i}),r(\\tilde{t}_{j}))$ where $\\tilde{\\pmb{t}}_{i}\\sim\\tilde{\\pmb{t}}_{j}$ must be calculated to obtain the first term of (27). To do this we use the efficient algorithm of Rue and Martino (2007) implemented in GMRFLib, where marginal variances and covariances for neighbours are calculated with $N\\log(N)^{2}$ operations.<PARAGRAPH_SEPARATOR># 3.3.1. Alternative prediction error variances for the bilinear interpolation<PARAGRAPH_SEPARATOR>The independence assumptions (12)–(13) make $\\gamma_{2}(\\pmb{t})$ behave as a nugget effect. This gives a term $\\mathrm{Var}(\\gamma_{2}(\\pmb{t}_{0}))$ in the prediction error variance (27).<PARAGRAPH_SEPARATOR>In many practical problems, predictions on a grid are wanted. Since at lattice locations $\\mathrm{Var}(\\gamma_{2}(t_{0}))=0$ , the additional variance term does not affect the prediction error variance as long as the GMRF is defined on that same grid. Thus, for these problems, bilinear interpolation is only used to incorporate data (that are typically not collected on a grid). Quality of predictions and prediction error variances on the grid are both better by this method than if each data point is simply assigned to the closest grid point as in nearest neighbour interpolation, see results in Section 4. However, should predictions and prediction error variances be calculated even at nonlattice locations, a deficiency with assumptions (12)–(13) is that the prediction error variance will be overestimated. This is because $\\operatorname{Var}(\\gamma_{2}(\\cdot))$ is defined directly from the variance of the underlying field Y, which has larger variance than the prediction errors $\\hat{Y}-Y$ . As the prediction error variance at lattice locations is well tuned to what it would be with the full Gaussian model, it follows that the prediction error variance will be overestimated at locations within cells.<PARAGRAPH_SEPARATOR>However, if high-quality prediction error variances are wanted at nonlattice locations, a heuristically sound alternative could be to calculate the prediction error variance as the weighted average of the prediction error variances at the grid points surrounding $\\scriptstyle t_{0}$ ,<PARAGRAPH_SEPARATOR>$$\\boldsymbol{\\sigma}_{r}^{2}(t_{0})=\\sum_{j=1}^{N}k_{j}\\boldsymbol{\\sigma}_{r}^{2}(\\tilde{t}_{j})=\\boldsymbol{k}^{\\mathrm{T}}\\boldsymbol{V}_{\\tilde{r}},$$<PARAGRAPH_SEPARATOR>where $V_{\\tilde{r}}{=}\\mathrm{diag}(C_{\\tilde{r}})$ is a diagonal matrix containing the prediction error variances on the grid. This alternative, although quite ad hoc, stems from prediction error variances being well tuned at grid points to those of a Gaussian model, and in the same way prediction error variance being continuous, just as with a continuous Gaussian model.<PARAGRAPH_SEPARATOR>For approximations with nearest neighbour interpolation where the weight $k_{j}=1$ for the closest grid point $\\tilde{\\pmb{t}}_{j}$ , the used approximation (14) produces $\\mathrm{Var}(\\gamma_{2}(t))=0$ everywhere, and thus (27) and (28) are equal.<PARAGRAPH_SEPARATOR># 4. Simulation study"
  },
  {
    "qid": "statistic-posneg-25-0-3",
    "gold_answer": "TRUE. The formula $F_{n}(x)=N(x)/n$ where $N(x)$ is the number of $X_{j}\\leqslant x$ for $1\\leqslant j\\leqslant n$ shows that $F_{n}(x)$ is the proportion of sample observations less than or equal to $x$.",
    "question": "Is the empirical distribution function $F_{n}(x)$ defined as the proportion of sample observations less than or equal to $x$?",
    "question_context": "Empirical Characteristic Function and Hypothesis Testing\n\nThe empirical characteristic function is considered as a tool for large sample testing of a hypothesis that can be characterized in terms of the characteristic function. Two test statistics based upon the empirical characteristic function are proposed. The limiting distributions of these test statistics are obtained and methods are suggested for using these limiting distributions to calculate critical regions. Given a sequence of stochastic processes $Z_{n}$ and a function $\\pmb{\\mu}$ such that $\\pmb{n}^{1/2}(Z_{n}-\\mu)$ converges weakly to a zero mean Gaussian process, consider the statistics $S_{n}=\\operatorname*{sup}_{t\\in[a~b]}|Z_{n}(t)|$, and $T_{n}=\\int\\mid Z_{n}(t)\\mid^{2}d G(t)$ where $G$ is a distribution function. Statistics of these types arise naturally in the use of the empirical characteristic function for large sample hypothesis testing. Let $X_{1},X_{2},...,X_{n}$ be independent and identically distributed random variables. The empirical distribution function is $F_{n}(x)=N(x)/n,x\\in\\mathbb{R}$ where $N(x)$ is the number of $X_{j}\\leqslant x$ for $1\\leqslant j\\leqslant n$. The empirical characteristic function (e.c.f.) is defined by $c_{n}(t)=\\int e^{i t x}d F_{n}(x)={\\frac{1}{n}}\\sum_{j=1}^{n}e^{i t X_{j}},\\qquad t\\in\\mathbb{R}$."
  },
  {
    "qid": "statistic-posneg-988-0-3",
    "gold_answer": "FALSE. The Chandrasekhar recursions are computationally efficient because they avoid updating the large matrix P_t at each step, unlike the Kalman filter. Instead, they update smaller auxiliary matrices (K_t, M_t, L_t), reducing the computational burden.",
    "question": "The Chandrasekhar recursions for evaluating the Gaussian likelihood require updating a large matrix P_t at each time step, making them computationally intensive. True or False?",
    "question_context": "Structured VARMA Models and Exact Maximum Likelihood Estimation\n\nThe paper discusses structured VARMA models, focusing on echelon forms based on Kronecker indices and scalar component models (SCM). It covers I(1) VARMA models, their transformation to stationary VARMA models, and the evaluation of exact likelihood using recurrence equations. The paper also compares exact maximum likelihood estimation with conditional methods."
  },
  {
    "qid": "statistic-posneg-744-1-1",
    "gold_answer": "FALSE. While the initial estimate $\\tilde{\\omega}_{k}$ is consistent for estimating the weights, it does not guarantee that the resulting density estimate $f_{m}(x,\\tilde{\\omega})$ is unimodal. The unimodality condition (c) on the weights is not automatically satisfied by this initial estimate, and thus additional constraints or optimization (as described in the quadratic programming problem) are required to ensure unimodality.",
    "question": "Is it true that the initial estimate $\\tilde{\\omega}_{k}=F_{n}\\left(\\frac{k}{m-1}\\right)-F_{n}\\left(\\frac{k-1}{m-1}\\right)$ always results in a unimodal density estimate $f_{m}(x,\\tilde{\\omega})$?",
    "question_context": "Unimodal Density Estimation Using Bernstein Polynomials\n\nWe begin by assuming $X_{i}\\stackrel{i i d}{\\sim}f(\\cdot)$ , for $i=1$ , 2, . . . , n, and it is known that $f(\\cdot)$ is a unimodal continuous density function. In other words, we assume that there exists an , such that is non-decreasing on and is non-increasing on $(x^{*},\\infty)$ . Our goal is to construct an estimate of $\\overleftrightarrow{f}(\\cdot),\\overhat{f}(\\cdot)$ , which satisfies the following conditions: (i) ${\\hat{f}}(x)\\geq0$ , for all $x\\in\\mathbb{R}$ , (ii) $\\textstyle\\int{\\hat{f}}(x)d x=1$ , (iii) ${\\hat{f}}(x)$ is unimodal, i.e. there exists an $\\hat{x}^{*}\\in\\mathbb{R}$ , such that ${\\hat{f}}(x)$ is non-decreasing on $(-\\infty,\\hat{x}^{*})$ and ${\\hat{f}}(x)$ is non-increasing on $(\\hat{x}^{*},\\infty)$. It is also desirable that the proposed density estimate, $\\hat{f}(\\cdot)$ , satisfies a set of asymptotic properties (e.g. consistency). Without loss of generality, we first consider densities with support [0, 1], and then extend our methodology to more general supports. We begin by considering a Bernstein polynomial of order $m-1$ to estimate $f(x)$, $B_{m}(x,f)=\\sum_{k=1}^{m}f\\left(\\frac{k-1}{m-1}\\right)\\binom{m-1}{k-1}x^{k-1}(1-x)^{m-k}$. However, it is clear that $B_{m}(x,f)$ , defined in (2), is not a proper density function. We therefore consider a re-scaled version of $B_{m}(x,f)$, $f_{m}(x)=\\sum_{k=1}^{m}f\\left(\\frac{k-1}{m-1}\\right)m\\left({m-1\\atop k-1}\\right)x^{k-1}(1-x)^{m-k}\\left/\\sum_{k=1}^{m}f\\left(\\frac{k-1}{m-1}\\right),\\right.$, which is a proper density function for any value $m$ . This motivates us to consider the following class of density estimates, $f_{m}(x,\\omega)=\\sum_{k=1}^{m}\\omega_{k}f_{b}(x;k,m-k+1)$, where $\\boldsymbol{\\omega}=(\\omega_{1},\\omega_{2},\\dots,\\omega_{m})^{T}$ is a vector of weights of size $m$ , and $f_{b}(\\cdot)$ is the Beta density function with shape parameters $k$ and $m-k+1$ . Notice that $f_{m}(x,\\omega)$ will obtain all the desired properties of $\\hat{f}(\\cdot)$ if the vector of weights, $\\omega$ , satisfies the following constraints: (a) $\\omega_{k}\\geq0$ , for $k=1,\\ldots,m,$ (b) $\\textstyle\\sum_{k=1}^{m}\\omega_{k}=1$ , and (c) $\\omega_{1}\\leq\\omega_{2}\\leq\\cdot\\cdot\\cdot\\leq\\omega_{k^{*}}\\geq\\omega_{k^{*}+1}\\geq\\cdot\\cdot\\cdot\\geq\\omega_{m}$ , for some $k^{*}\\in\\{1,2,\\ldots,m\\}$ , i.e. the $\\omega_{\\boldsymbol{k}}\\mathbf{\\dot{s}}$ are non-decreasing for $k\\leq k^{*}$ and non-increasing for $k\\geq k^{*}$."
  },
  {
    "qid": "statistic-posneg-176-0-2",
    "gold_answer": "FALSE. The condition $h^{\\prime\\prime}(\\xi)=0$ and $\\rho h^{\\prime}(\\xi)\\leqslant0$ is necessary but not sufficient on its own to conclude that the distribution is ${\\mathrm{TP}}_{2}$. The theorem states that the distribution is ${\\mathrm{TP}}_{2}$ if and only if for each value of $\\xi>0$, the conditions $h^{\\prime\\prime}(\\xi)=0$ and $\\rho h^{\\prime}(\\xi)\\leqslant0$ hold. However, other conditions involving $h^{\\prime\\prime}(\\xi)$ and $K_{\\rho}(t)$ must also be considered, as detailed in the proof.",
    "question": "In Theorem 3.1, is the condition $h^{\\prime\\prime}(\\xi)=0$ and $\\rho h^{\\prime}(\\xi)\\leqslant0$ sufficient to conclude that the bivariate elliptically symmetric distribution is ${\\mathrm{TP}}_{2}$?",
    "question_context": "Positive Dependence Properties of Elliptically Symmetric Distributions\n\nTHEOREM 2.2  Let $\\pmb{X}=(X_{1},...,X_{p})^{\\prime}$ have  joint  p.d.f. $f({\\pmb x})=g({\\pmb x}^{\\prime}{\\pmb x}),$ $-\\infty<\\pmb{x}<\\infty$ ,， where $g(u)>0$ for all $u\\geqslant0$ and is twice dijfferentiable. Define $W_{i}=|X_{i}|$ $i=1,...,p$ .Then $h(w)$ , the joint $p.d.f.$ of $W_{1},...,W_{p}$ ,is $T P_{2}$ in pairs $\\pmb{j}\\pmb{f}$ and only $i f,$ ln $g(u)$ is convex for $\\pmb{u}\\geqslant0$"
  },
  {
    "qid": "statistic-posneg-1979-6-0",
    "gold_answer": "FALSE. The theoretical K-function for the inhomogeneous Thomas process should account for both the Poisson process component (πr²) and the clustering component induced by the offspring points. The given formula correctly includes the Poisson term (πr²) and the clustering term (1-exp(-r²/(4ω²)))/κ, where κ controls the intensity of the cluster centers and ω is the scale of dispersion for offspring points around each center. However, the formula's validity depends on the specific assumptions of the Thomas process model, including the Gaussian dispersion of offspring points. The consistency with clustering parameters is thus model-specific and not universally applicable to all point processes.",
    "question": "Is the theoretical K-function for the inhomogeneous Thomas process given by $K(r;\\kappa,\\omega)=\\pi r^{2}+(1-\\exp(-r^{2}/(4\\omega^{2})))/\\kappa$ consistent with the model's clustering parameters?",
    "question_context": "Minimum Contrast Estimation in Point Process Models\n\nThe pair correlation function $g$ and the $K$-function in some sense describe the ‘normalized’ second order properties of a point process, cf. (5) and (40). For many Cox processes, $g$ or $K$ has a closed form expression depending on the ‘clustering parameters’ of the model. Examples include log Gaussian Cox processes (see section titled ‘Log Gaussian Cox processes’) and inhomogeneous Neyman–Scott processes with random intensity functions of the form (12) where $k$ is a radially symmetric Gaussian density or a uniform density on a disc. Clustering parameter estimates may then be obtained using so-called minimum contrast estimation. That is, using an estimating function given in terms of a discrepancy between the theoretical expression for $g$ or $K$ and a non- or semi-parametric estimate $\\hat{g}$ or $\\hat{K}$ , e.g. (41) where $\\hat{\rho}$ could be a parametric estimate obtained from (47). This is illustrated in example 13 for the $K$ function. Minimum contrast estimation based on the $g$-function is considered in Møller et al. (1998). Asymptotic properties of minimum contrast estimates are derived in the case of stationary processes in Heinrich (1992) and Guan & Sherman (2007). <PARAGRAPH_SEPARATOR> Alternatively, we may consider an estimating function based on the second order product density $\rho_{\theta}^{(2)}(u,v)$ (Waagepetersen, 2007): <PARAGRAPH_SEPARATOR> $$ \\psi_{2}(\theta)=\\sum_{u,v\\in\\mathbf{x}}^{\neq}\\mathrm{d}\\log\rho_{\theta}^{(2)}(u,v)/\\mathrm{d}\theta-\\int_{W^{2}}\\left(\\mathrm{d}\\log\rho_{\theta}^{(2)}(u,v)/\\mathrm{d}\theta\right)\rho_{\theta}^{(2)}(u,v)\\mathrm{d}u\\mathrm{d}v. $$ <PARAGRAPH_SEPARATOR> This is the score of a limit of composite log likelihood functions based on Bernouilli observations $N_{i j}=\\mathbf{1}[N(C_{i})>0,N(C_{j})>0]$ , $i\neq j$ . Unbiasedness of $\\psi_{2}(\theta)=0$ follows from Campbell’s theorem (4). The integral in (48) typically must be evaluated using numerical integration. In the stationary case, Guan (2006) considers a related unbiased estimating function, where the integral in (48) is replaced by the number of pairs of distinct points times $\textstyle\\log\\int_{W^{2}}\rho_{\theta}^{(2)}(u,v)\\mathrm{d}u\\mathrm{d}v$ . <PARAGRAPH_SEPARATOR> Example 13 (Minimum contrast estimation of clustering parameters for tropical rainforest trees). The solid curve in Fig. 9 shows an estimate of the $K$-function for the tropical rainforest trees obtained using (41) with $\\hat{\boldsymbol\rho}$ given by the estimated parametric intensity function from example 11. For the inhomogeneous Thomas process, a minimum contrast estimate $(\\hat{\\kappa},\\hat{\\omega})=(8\\times10^{-5},20)$ is obtained by minimizing <PARAGRAPH_SEPARATOR> $$ \\int_{0}^{100}(\\hat{K}(r)^{1/4}-K(r;\\kappa,\\omega)^{1/4})^{2}~\\mathrm{d}r $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ K(r;\\kappa,\\omega)=\\pi r^{2}+(1-\\exp(-r^{2}/(4\\omega^{2})))/\\kappa $$ <PARAGRAPH_SEPARATOR> is the theoretical expression for the $K$-function. For the log Gaussian Cox process, we calculate instead the theoretical $K$-function <PARAGRAPH_SEPARATOR> $$ K(r;\\sigma,\\alpha)=2\\pi\\int_{0}^{r}s\\exp(\\sigma^{2}\\exp(-s/\\alpha))\\mathrm{d}s $$ <PARAGRAPH_SEPARATOR> using numerical integration, and obtain the minimum contrast estimate $(\\hat{\\sigma},\\hat{\\alpha})=(1.33,34.7)$ The estimated theoretical $K$-functions are shown in Fig. 9. <PARAGRAPH_SEPARATOR> Minimum contrast estimation is computationally very easy. A disadvantage is the need to choose certain tuning parameters like the upper limit 100 and the exponent $1/4$ in the integral (49). Typically, these parameters are chosen on an ad-hoc basis. <PARAGRAPH_SEPARATOR> Example 14 (Simultaneous estimation of parameters for tropical rainforest trees). To estimate the parameters $(\\tilde{\\beta}_{1},\\beta_{2},\\beta_{3})$ and $(\\kappa,\\omega)$ for the inhomogeneous Thomas process (see example 11) simultaneously, we apply the estimating function $\\psi_{2}$ (48). We solve $\\psi_{2}(\\theta)=0$ using a grid search for $\\omega$ combined with Newton-Raphson for the remaining parameters (NewtonRaphson for all the parameters jointly turns out to be numerically unstable). We then search for an approximate solution with respect to $\\omega$ within a finite set of $\\omega$-values. The resulting estimates of $(\\tilde{\\beta}_{1},\\beta_{2},\\beta_{3})$ and $(\\kappa,\\omega)$ are respectively $(-5.001,0.021,5.735)$ and $(7\\times10^{-5},30)$ . The estimate of $\\omega$ differs considerably from the minimum contrast estimate in example 13, while the remaining estimates are quite similar to those obtained previously for the inhomogeneous Thomas process in examples 11 and 13. The numerical computation of $\\psi_{2}$ and its derivatives is quite time consuming, and the whole process of solving $\\psi_{2}(\\theta)=0$ takes about 75 minutes."
  },
  {
    "qid": "statistic-posneg-1727-0-0",
    "gold_answer": "TRUE. The term $\\omega^{\\alpha}$ in the conditional likelihood $L_{i}(\\phi|\\omega_{i})$ captures the association between recurrent events and death. Here, $\\omega_{i}$ represents the shared frailty term, which accounts for unobserved heterogeneity affecting both recurrent events and death. The exponent $\\alpha$ modulates the strength and direction of this association. If $\\alpha = 1$, the frailty effect is the same for both recurrent events and death, indicating a strong dependence. If $\\alpha \\neq 1$, the frailty effect differs, allowing for a more flexible dependence structure. Thus, the term $\\omega^{\\alpha}$ explicitly models the dependence between recurrent events and death through the shared frailty.",
    "question": "In the joint frailty model, the marginal likelihood $L_{i}(\\phi)$ integrates out the random effects $\\omega_{i}$ to account for unobserved heterogeneity. Is it correct to say that the term $\\omega^{\\alpha}$ in the conditional likelihood $L_{i}(\\phi|\\omega_{i})$ specifically models the dependence between recurrent events and death?",
    "question_context": "Joint Frailty Models for Recurrent Events and Death\n\nWe denote $T_{i j}$ the $j$ th follow-up time for subject $i$ and $\\delta_{i j}$ the failure indicator for the recurrent events. Similarly, we define $T_{i}^{*}=\\operatorname*{min}(D_{i},C_{i})$ the last follow-up time for subject $i$ and the death indicator $\\delta_{i j}^{*}=I(D_{i}<C_{i})$ . <PARAGRAPH_SEPARATOR> The marginal contribution to the likelihood $L_{i}(r_{0}(\\cdot),\\lambda_{0}(\\cdot),\\pmb{\\beta},\\alpha,\\theta)=L_{i}(\\phi)$ for subject $i$ and for $j=1,\\ldots,n_{i}$ is $\\begin{array}{r}{L_{i}\\left(\\phi\\right)=\\int_{\\omega}L_{i}\\left(\\phi|\\omega\\right)f(\\omega)\\mathrm{d}\\omega}\\end{array}$ <PARAGRAPH_SEPARATOR> 1) The conditional distribution of the survival times given $\\omega_{i}$ is the product of the individual contributions: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{L_{i}(\\phi|\\omega_{i})=\\displaystyle\\prod_{j=1}^{n_{i}}\\left[\\mathrm{d}R_{i}(T_{i j}|\\omega_{i})^{\\delta_{i j}}\\times\\exp\\left(-\\omega\\displaystyle\\sum_{j=1}^{n_{i}}\\int_{T_{i j-1}}^{T_{i j}}\\mathrm{d}R_{i}(t)\\right)\\right]}\\ {\\times\\mathrm{d}\\Lambda_{i}(T_{i}^{*}|\\omega_{i})^{\\delta_{i}^{*}}\\times\\exp\\left(-\\omega^{\\alpha}\\displaystyle\\int_{0}^{\\infty}Y_{i}(t)\\mathrm{d}\\Lambda_{i}(t)\\right).}\\end{array} $$ <PARAGRAPH_SEPARATOR> 2) The probability density function for the random effects ω is f (ω) = ω(1/θ −1) /eθx)pθ(1/−θω/θ) . <PARAGRAPH_SEPARATOR> 3) Using the previous expressions, the $i$ th marginal contribution to the likelihood is obtained by integrating out the random effects: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l r}{\\lefteqn{L_{i}(\\phi)=\\frac{\\prod_{j=1}^{n_{i}}\\mathrm{d}R_{i}(T_{i j})^{\\delta_{i j}}\\times\\mathrm{d}\\Lambda_{i}(T_{i}^{*})^{\\delta_{i}^{*}}}{\\Gamma(1/\\theta)\\theta^{1/\\theta}}\\times\\int_{0}^{\\infty}\\omega^{\\left(N_{i}^{R}(T_{i}^{*})+\\alpha\\delta_{i}^{*}+\\frac{1}{\\theta}-1\\right)}}} \\ &{}&{\\times\\exp\\left(-\\omega\\displaystyle\\sum_{j=1}^{n_{i}}\\int_{T_{i j-1}}^{T_{i j}}\\mathrm{d}R_{i}(t)-\\omega^{\\alpha}\\displaystyle\\int_{0}^{\\infty}Y_{i}(t)\\mathrm{d}\\Lambda_{i}(t)-\\frac{\\omega}{\\theta}\\right)\\mathrm{d}\\omega.}\\end{array} $$ <PARAGRAPH_SEPARATOR> We then obtain the expression (2.2) of the full log-likelihood by using <PARAGRAPH_SEPARATOR> $$ l(\\phi)=\\log\\prod_{i=1}^{N}L_{i}(\\phi). $$"
  },
  {
    "qid": "statistic-posneg-2123-0-2",
    "gold_answer": "FALSE. While the context states that the estimators are asymptotically normally distributed and efficient for each $k \\geq 2$, it also mentions that numerical problems can occur if the sample coefficient of variation is greater or equal to 1, leading to the likelihood equations not having a solution. In such cases, the estimators may not be efficient, and the data may need to be fitted using an exponential or Laplace distribution instead.",
    "question": "Is it correct to say that the estimators $\\hat{\\mu}_{k}, \\hat{\\sigma}_{k},$ and $\\hat{\\boldsymbol{\\theta}}_{k}$ are efficient for all $k \\geq 2$ based on the context provided?",
    "question_context": "Efficiency of Doubly Truncated Normal Distribution Estimators\n\nThe resulting estimators will be denoted as ${\\hat{\\mu}},{\\hat{\\sigma}}$ and $\\hat{\\boldsymbol{\\theta}}$ . Notice that, as a consequence of proposition 1 and the results of Gong & Samaniego (1981), for each $k\\geq2$ the estimators $\\hat{\\mu}_{k},\\hat{\\sigma}_{k}$ and $\\hat{\\boldsymbol{\\theta}}_{k}$ , are asymptotically normally distributed and efficient. Therefore, this algorithm guarantees a sequence of efficient estimators so that the likelihood is increased with each iteration. <PARAGRAPH_SEPARATOR> Sometimes numerical problems can occur in step 2 of the algorithm. If the sample coefficient of variation of the transformed sample $|x_{1}~-~\\hat{\\mu}_{k}|,|x_{2}~-~\\hat{\\mu}_{k}|,...,|x_{n}~-~\\hat{\\mu}_{k}|$ is greater or equal to 1, then the likelihood equations of the singly truncated normal distribution do not have a solution (see Castillo, 1994; Castillo & Puig, 1999). When this occurs, the maximum of the likelihood function is attained at the border of the domain of the parameters, i.e. data are fitted by using the exponential distribution. From the point of view of the DTN distribution it corresponds to the limiting situation of the Laplace distribution mentioned above. When this problem arises it is better to fit the data using the Laplace distribution. <PARAGRAPH_SEPARATOR> The asymptotic variances of the estimators can be easily calculated from (6), obtaining the following expressions: <PARAGRAPH_SEPARATOR> $$ V(\\hat{\\mu})=\\frac{\\sigma^{2}}{n(1~+~\\theta c(\\theta))},~V(\\hat{\\theta})=\\frac{2~+~\\theta^{2}-\\theta c(\\theta)}{n\\delta(\\theta)}, $$ <PARAGRAPH_SEPARATOR> $$ V(\\hat{\\sigma})=\\frac{\\sigma^{2}(1+\\theta c(\\theta)-c^{2}(\\theta))}{n\\delta(\\theta)}, $$ <PARAGRAPH_SEPARATOR> where $\\delta(\\theta)=c^{3}(\\theta)\\theta-c^{2}(\\theta)(2\\theta^{2}+3)+c(\\theta)\\theta(\\theta^{2}+3)+2.$ In the next section, we give an example of the application of this algorithm. <PARAGRAPH_SEPARATOR> # 3.2. An example <PARAGRAPH_SEPARATOR> Figure 2 shows the daily currency exchange interbank rates US Dollar/Euro in a certain period of time."
  },
  {
    "qid": "statistic-posneg-252-0-0",
    "gold_answer": "FALSE. The paper claims that κ_r can be evaluated using the derivatives of log A_1 and log λ_1 at t=0, but it also requires the condition that the derivatives of the logarithmic term involving A_ℜ, A_ℑ, λ_ℜ, and λ_ℑ must vanish at t=0. This additional condition is crucial for the validity of the cumulant evaluation method described.",
    "question": "Is it true that the cumulants κ_r can be evaluated using the derivatives of log A_1 and log λ_1 at t=0, as stated in the paper?",
    "question_context": "Cumulant Evaluation for Probability Distributions on a Line\n\nThe paper discusses the evaluation of cumulants for probability distributions arising from points on a line with multiple characters. The moment-generating function (m.g.f.) is given by a recurrence relation, and the cumulants are derived using the roots of the characteristic equation and their derivatives."
  },
  {
    "qid": "statistic-posneg-2046-4-3",
    "gold_answer": "FALSE. Explanation: MRSB is a relative measure, as it compares each model's VaR predictions to the mean of all models' predictions. Its value depends on the specific set of models included; adding or removing models alters the mean and thus the MRSB. It is not an absolute measure of efficiency.",
    "question": "The mean relative scaled bias (MRSB) measures the deviation of a model's scaled VaR predictions from the average across all models. Is it correct to say that MRSB is an absolute measure of efficiency, unaffected by the choice of models included in the comparison?",
    "question_context": "Value-at-Risk (VaR) Forecasting Accuracy and Testing\n\nThe paper discusses the testing of VaR prediction models using likelihood ratio tests for unconditional coverage, independence of violations, and conditional coverage. It introduces the Boolean sequence of VaR violations, the empirical downfall probability, and the mean relative scaled bias (MRSB) for comparing model efficiency."
  },
  {
    "qid": "statistic-posneg-1684-1-2",
    "gold_answer": "TRUE. While increasing the number of columns in the design matrix can weaken the beta-min condition for sign recovery by TBP, it also decreases the upper bound on the sparsity index of $\\beta^{0}$. Therefore, it is not always beneficial to add more columns, as it can negatively impact the sparsity of the solution.",
    "question": "Is it false that increasing the number of columns in the design matrix always weakens the beta-min condition for sign recovery by TBP?",
    "question_context": "Model Selection with Lasso-Zero: Sign Recovery and False Discovery Trade-offs\n\nThe following result provides a lower bound for the probability of correct sign recovery. <PARAGRAPH_SEPARATOR> Theorem 1. Under the linear model (1) and Assumptions (A)– (D), there exist universal constants $c,c^{\\prime},c^{\\prime\\prime}>0$ such that for any $\\rho\\in(0,1)$ , <PARAGRAPH_SEPARATOR> provided that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{s^{0}<\\displaystyle\\frac{c^{\\prime\\prime}\\nu^{2}}{(1+\\rho^{-1})^{2}}\\displaystyle\\frac{n}{\\log p},}\\ {\\left\\vert\\beta^{0}\\right\\vert_{\\mathrm{min}}>C_{\\rho}\\displaystyle\\frac{2\\sqrt{2}\\sigma\\sqrt{p}}{\\nu(\\sqrt{p/n}-1)},}\\end{array} $$ <PARAGRAPH_SEPARATOR> awchheireev $\\begin{array}{l}{{C_{\\rho}={\\frac{2(3+\\rho)}{1-\\rho}}}}\\end{array}$ 2(13+ρρ ) . Consequently, if p/n → C > 1, TBP ency with $\\beta_{\\mathrm{min}}^{0}~=~\\Omega(\\sqrt{n})$ $s^{0}=$ $O(n/\\log n)$ . <PARAGRAPH_SEPARATOR> The proof is provided in Appendix A. Its main argument is that, no matter what $\\mathrm{sign}(\\beta_{S^{0}}^{0})$ is, TBP recovers the signs of $\\beta^{0}$ under some beta-min condition and the stable nullspace property: <PARAGRAPH_SEPARATOR> $$ \\beta\\in\\ker X\\implies\\left\\lVert\\beta_{S^{0}}\\right\\rVert_{1}\\leq\\rho\\left\\lVert\\beta_{\\overline{{S^{0}}}}\\right\\rVert_{1},\\qquad\\rho\\in(0,1). $$ <PARAGRAPH_SEPARATOR> Let us compare this to the theory of the Lasso. It is known (Zhao and Yu 2006) that the Lasso achieves sign consistency under the $\\theta$ -irrepresentable condition <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\left\\lVert X\\frac{T}{S^{0}}X_{S^{0}}(X_{S^{0}}^{T}X_{S^{0}})^{-1}\\mathrm{sign}(\\beta_{S^{0}}^{0})\\right\\rVert_{\\infty}<\\theta,}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\theta\\in\\quad(0,1)$ . For sign consistency independently of $\\mathrm{sign}(\\beta_{S^{0}}^{0})$ , the $\\theta$ -uniform irrepresentable condition— requiring (10) for all $\\mathrm{sign}(\\beta_{S^{0}}^{0})$ —should be assumed. It is known (see Bühlmann and van de Geer 2011, Theorem 7.2) that the $\\theta$ -uniform irrepresentable condition is stronger than the $(L,S^{0})$ -compatibility condition for any $L\\in(1,\\theta^{-1})$ , which states that there exists a $\\phi>0$ such that <PARAGRAPH_SEPARATOR> $$ \\big\\|\\beta_{\\overline{{S^{0}}}}\\big\\|_{1}\\leq L\\big\\|\\beta_{S^{0}}\\big\\|_{1}\\implies\\big\\|\\beta_{S^{0}}\\big\\|_{1}^{2}\\leq\\frac{\\big|S^{0}\\big|}{n\\phi^{2}}\\|X\\beta\\|_{2}^{2}. $$ <PARAGRAPH_SEPARATOR> Clearly, (11) implies (9) for $\\rho=L^{-1}$ , so the $\\theta$ -uniform irrepresentable condition implies the stable nullspace property with any constant $\\rho~\\in~(\\theta,1)$ . Hence, TBP achieves sign recovery under a weaker condition on $X$ and $S^{0}$ compared to the Lasso."
  },
  {
    "qid": "statistic-posneg-240-0-1",
    "gold_answer": "FALSE. Explanation: The paper explicitly states that it shows the hazard rate is increasing in the bivariate normal case without the condition on the correlation coefficient imposed by Johnson and Kotz (1975). This means the hazard rate is increasing regardless of whether the correlation is positive, negative, or zero. The result is then extended to higher dimensions, indicating that the increasing property of the multivariate normal hazard rate is more general and not dependent on the sign of the correlation.",
    "question": "According to the paper, the bivariate normal hazard rate is increasing only if the correlation coefficient is positive.",
    "question_context": "Multivariate Normal Hazard Rate: Definition and Properties\n\nIt is well known that the hazard rate of a univariate normal distribution is increasing. Johnson and Kotz (1975) presented a vector definition of multivariate hazard rates and associated definitions of increasing and decreasing multivariate distributions. They showed that in a bivariate normal case, the bivariate hazard is increasing provided that the correlation coefficient is positive. <PARAGRAPH_SEPARATOR> In this paper, we first show that in the bivariate normal case, the hazard rate is increasing without the condition on the correlation coefficient imposed by Johnson and Kotz. The result is then extended to the trivariate normal case and finally to the general $m$ dimensional multivariate case. Before proceeding further, we present some definitions and results which are used in establishing the results. <PARAGRAPH_SEPARATOR> 1. Definition 1. The joint multivariate hazard rate of $m$ jointly absolutely continuous random variables $X_{1},X_{2},...,X_{m}$ is defined as the vector <PARAGRAPH_SEPARATOR> $$ h(\\mathbf{x})=\\left(-\\left({\\frac{\\hat{\\partial}}{\\partial x_{1}}}\\right)\\cdots-\\left({\\frac{\\hat{\\partial}}{\\partial x_{m}}}\\right)\\right)\\ln G(\\mathbf{x})=-\\mathrm{grad}\\ln G(\\mathbf{x}) $$ <PARAGRAPH_SEPARATOR> where $G(\\mathbf{x})=P(X_{i}>x_{i},i=1,2,...,m)$ is the joint survival function. For convenience, we shall write $-(\\partial/\\partial x_{j})$ ln $G(x)=h_{j}(\\mathbf{x})$ . Note that $h_{j}(\\mathbf{x})$ is the <PARAGRAPH_SEPARATOR> Received March 2, 1995; revised February 3, 1997. <PARAGRAPH_SEPARATOR> Key words and phrases: Multivariate normal distribution, multivariate increasing hazard rate, hazard gradient."
  },
  {
    "qid": "statistic-posneg-1974-0-0",
    "gold_answer": "TRUE. The log-Normal prior's mean M represents the population-level central tendency for the continuous parameters, while the Dirichlet prior's mean Π (scaled by N_π) governs the expected nucleotide frequencies. N_π acts as a concentration parameter: higher values reduce variability across patients/genes, tightening the distribution around Π. This hierarchical structure pools information across individuals, improving estimation efficiency while allowing for patient/gene-specific deviations.",
    "question": "In the Bayesian HPM, the prior for continuous parameters (α_ij, κ1_ij, κ2_ij, μ_ij) is specified as log-Normal(M, L), and π_ij is Dirichlet(N_π × π). Is it correct to interpret M and Π as population-level means, with N_π controlling variability across patients and genes?",
    "question_context": "Bayesian Hierarchical Phylogenetic Model for Evolutionary Interaction Testing\n\nThe paper describes a two-way Bayesian hierarchical phylogenetic model (HPM) to test for evolutionary interactions between gene regions in HIV patients undergoing ENF therapy. The model estimates patient-specific and gene-specific parameters (e.g., tree topology, branch lengths, nucleotide substitution rates) using hierarchical priors. The key formulas include the log-Normal and Dirichlet priors for continuous parameters and a structured precision matrix for testing independence vs. dependence hypotheses in evolutionary tree outcomes."
  },
  {
    "qid": "statistic-posneg-1149-0-0",
    "gold_answer": "TRUE. The hazard function in the Cox Proportional Hazards Model is indeed defined as h(y_i|β) = h_0(y_i) * exp(x_i^T β), where h_0(y_i) is the baseline hazard function and exp(x_i^T β) is the exponential link function of covariates.",
    "question": "In the Cox Proportional Hazards Model, the hazard function is defined as the product of an unspecified baseline hazard function and an exponential link function of covariates. Is this statement true based on the provided context?",
    "question_context": "Cox Proportional Hazards Model and Fine-Gray Model for Survival Analysis\n\nThe context discusses the Cox Proportional Hazards Model and the Fine-Gray Model for survival analysis, including their formulations, parameter estimation, and computational challenges. It also covers statistical regularization techniques and the use of Cyclic Coordinate Descent for maximum likelihood estimation."
  },
  {
    "qid": "statistic-posneg-1505-0-0",
    "gold_answer": "FALSE. While the process generates a sample from the distribution of $y$, the explanation omits that $x$ must also be defined (either as a constant or another random variable). The context mentions B1 contains an exponential distribution, but the formula $\\alpha + {\\bf B}1 \\times {\\bf A}1$ suggests $x$ (B1) is random, not fixed. Thus, the simulation correctly accounts for both random components ($\\beta$ in A1 and $x$ in B1) to estimate the distribution of $y$.",
    "question": "In PREDICT, the formula $y = \\alpha + \\beta x$ with $\\beta \\sim N(\\mu, \\sigma)$ is evaluated by generating 100 random samples from the distribution of $\\beta$ and computing $y$ for each sample. Does this process correctly simulate the distribution of $y$ given the model assumptions?",
    "question_context": "Monte Carlo Simulation in Spreadsheet Software for Statistical Modeling\n\nPREDICT is a spreadsheet package, superficially very like other spreadsheets, so that users of LOTUS 1-2-3 would recognize much of it immediately. As in any spreadsheet the worksheet consists of a rectangular array of cells, rows indexed by numbers and columns by letters. Each cell may contain text (for labelling and titles), individual numbers or details of a calculation using data in other cells. Changing the details in one cell results in the others that depend on it being updated. The spreadsheet thus provides a clear and convenient way of laying out (possibly very complex) calculations and evaluating the effect of changing one or more components. These basic facilities are surrounded by utilities to allow, for example, input of data from external files, graphical display of results or saving worksheets for later use. <PARAGRAPH_SEPARATOR> PREDICT differs from standard spreadsheets in that there are two other types of content that a cell may have. A cell can be defined to contain a probability distribution or a list of values. This allows calculations involving random components to be performed. If a cell is defined to contain a distribution then when the spreadsheet is evaluated a random number from that distribution is generated and used in any calculations referring to that cell. Typically the results for single realizations are not of interest, but the distributions of results generated by the distributions of input values are: hence the need for cells containing lists of numbers. The spreadsheet is evaluated many times. Each evaluation requires simulation of data from the ‘input' distributions and results in an answer in the ‘output’ cells. The set of output values is collected in the list section of the cell, building up a simulated output distribution. <PARAGRAPH_SEPARATOR> A simple example considers the problem of finding the distribution of <PARAGRAPH_SEPARATOR> $$ y=\\alpha+\\beta x $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{\\alpha=\\mathtt{c o n s t a n t},}\\ {\\beta\\sim N(\\mu,\\sigma)}\\end{array} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> This would be set up in PREDICT by defining cell A1 to contain a normal distribution of required mean and variance, B1 to contain the exponential distribution and C1 to contain a list calculated as $\\alpha+{\\bf B}1\\times{\\bf A}1$ . The worksheet is then evaluated say 100 times. The result in the list section of C1 will be a sample of 100 observations from the distribution Oof $y$ <PARAGRAPH_SEPARATOR> PREDICT was designed for evaluating models related to business decision-making but has much wider application as described here. This review was prepared for Applied Statistics so the general facilities rather than those specific to financial applications are considered."
  },
  {
    "qid": "statistic-posneg-191-1-0",
    "gold_answer": "FALSE. The context states that $\\lambda_{n}$ converges almost surely to the solution $\\lambda_{0}$ of the given equations, but it does not explicitly confirm that this always holds under the described conditions. The mention of 'mild regularity conditions' and 'reasonable requirements' suggests that additional unspecified conditions might be necessary for the convergence, making the statement as presented potentially incomplete or overly general.",
    "question": "Is the statement 'Under the given regularity conditions, the estimator $\\lambda_{n}$ converges almost surely to $\\lambda_{0}$' true based on the provided context and equations?",
    "question_context": "Robust Estimation of Autoregressive-Moving Average Parameters\n\nUnder mild regularity conditions on $W(y,u)$ and $s(u)$ and reasonable requirements on $y_{t}$ , it is possible to show that $\\lambda_{n}=(\\beta_{n}^{\\prime},\\sigma_{n})^{\\prime}$ converges almost surely to the solution $\\lambda_{0}=(\\beta_{0}^{\\prime},\\sigma_{0})^{\\prime}$ of $$\\begin{array}{c}{{D(\\beta)E[W\\{A(\\beta)Y_{\\iota}/\\tau_{0},\\dot{u_{\\iota}}(\\beta)/\\sigma\\}Y_{\\iota}]=0,}}\\ {{E[S\\{|u_{\\iota}(\\beta)|/\\sigma\\}]=0}}\\end{array}$$ which has minimum $\\pmb{\\sigma}.$ Here, $\\tau_{0}$ is the almost sure limit of $\\tau_{n}$ . Details of the proof can be found in an unpublished report available from the author."
  },
  {
    "qid": "statistic-posneg-1236-0-2",
    "gold_answer": "TRUE. The PRIDE model incorporates individual random effects γ ∼ N(0, κ−1I) to account for overdispersion, as described in the context.",
    "question": "The PRIDE model for overdispersed count data includes individual random effects γ for each observation, which are assumed to follow a normal distribution with mean 0 and variance κ−1I. True or False?",
    "question_context": "P-spline Smoothing for Spatial Data with CAR Structure\n\nThe paper discusses the use of P-splines for smoothing spatial data, incorporating conditionally autoregressive (CAR) structures to model both global spatial trends and local regional effects. The models are formulated within a mixed model framework, allowing for flexible smoothing and handling of overdispersion in count data."
  },
  {
    "qid": "statistic-posneg-1448-7-0",
    "gold_answer": "TRUE. The condition $|\\alpha| > \\beta$ for all $\\alpha \\in \\mathbf{Spec}(B)$ ensures that the Fourier transform $\\hat{\\mu}(x)$ decays exponentially as $|x|_2 \\to \\infty$. This exponential decay allows the integral $w \\to \\int e^{-i\\langle w, y \\rangle} \\hat{\\mu}(y) \\lambda(dy)$ to converge for all $w \\in \\mathbb{C}^d$, thus extending $f$ to an entire function. The proof leverages the exponential bound $|\\hat{\\mu}(x)| \\leq \\exp\\{-b|x|_2^{1/c}\\}$ for $|x|_2 > a$, where $c \\in (1/2, 1)$, ensuring integrability and analyticity.",
    "question": "Is the condition $|\\alpha| > \\beta$ for all $\\alpha \\in \\mathbf{Spec}(B)$ sufficient to guarantee that the density $f$ of $\\mu$ can be extended to an entire function on $\\mathbb{C}^d$?",
    "question_context": "Analytic Extension of Probability Density Functions in Complex Domains\n\n(i) f $|\\alpha|>\\beta$ for all ${\\mathfrak{x}}\\in\\mathbf{Spec}(B)$ then the density $f$ of $\\mu$ can be extended to an entire function $(o n\\mathbb{C}^{d})$ <PARAGRAPH_SEPARATOR> (ii)If every ${\\mathfrak{\\alpha}}\\in\\mathbf{Spec}(B)$ with $|{\\pmb{\\alpha}}|=\\beta$ is a simple root of the minimal polynomial then $f$ can be extended to an analytic function in some strip $S_{\\delta}:=\\left\\{\\left(z_{1},...,z_{d}\\right)\\in\\mathbb{C}^{d}:\\left|\\mathrm{Im}~z_{j}\\right|<\\delta\\right.$ for $j=1,...,d\\}$ $(\\delta>0)$ <PARAGRAPH_SEPARATOR> Proof. (i) There exists some $c\\in]\\frac{1}{2},1[$ such that $\\beta^{c}<|\\alpha|$ for all ${\\mathfrak{a}}\\in\\mathbf{Spec}(B)$. Taking into account the proof of Theorem 2.1 in [9] there exist a, $b>0$ such that $\\vert\\hat{\\mu}(x)\\vert\\leqslant1$ if $|x|_{2}\\leqslant a$ and $|\\hat{\\mu}(x)|\\leqslant\\exp\\bigl\\{-b|x|_{2}^{1/c}\\bigr\\}$ if $|x|_{2}>a$ (where $\\hat{\\mu}$ denotes the Fourier transform of $\\pmb{\\mu}$ 0. Let $\\varepsilon:=1/c-1>0$ Fix $\\delta>1$ and let $x\\in V$ with $i x\\in S_{\\delta/d}$. Then we have for all $y\\in V$ such that $|y|_{2}>a$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{e^{\\langle x,y\\rangle}|\\hat{\\mu}(y)|\\leqslant\\exp\\{\\delta|y|_{2}-b|y|_{2}^{1+\\varepsilon}\\}}}\\ {{=\\exp\\{-|y|_{2}(b|y|_{2}^{\\varepsilon}-\\delta)\\}.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Hence <PARAGRAPH_SEPARATOR> $$ e^{\\langle x,y\\rangle}|\\hat{\\mu}(y)|\\leqslant\\exp\\{-|y|_{2}\\} $$ <PARAGRAPH_SEPARATOR> for all $y\\in V$ such that $|y|_{2}>\\operatorname*{max}\\{a,((\\delta-1)/b)^{1/\\varepsilon})$.Thus the function $y\\to e^{\\langle x,y\\rangle}\\hat{\\mu}(y)$ on $V$ is $\\lambda$ -integrable. Application of [2, Satz 49.5] to the measure with $\\lambda$ -density $\\hat{\\mu}$ yields: <PARAGRAPH_SEPARATOR> $$ w\\to\\int e^{-i\\langle w,y\\rangle}\\hat{\\mu}(y)\\lambda(d y) $$ <PARAGRAPH_SEPARATOR> is an analytic function $F$ on $S_{\\delta/d}$. But in view of the inversion formula of Fourier analysis we have $F(x)=f(x)$ for all $x\\in V.$ Since $\\delta>1$ was arbitrary the first assertion is proved. <PARAGRAPH_SEPARATOR> (ii) Let there exist some $\\alpha\\in\\mathbf{Spec}(B)$ such that $|{\\pmb{\\alpha}}|=\\beta$. Let $B^{t}$ denote the transpose of $\\pmb{B}$ and let $\\pmb{\\mathscr{A}}:=\\beta(\\pmb{\\mathscr{B}}^{t})^{-1}$. By assumption $\\rho(A)=1$ and every $\\alpha\\in\\mathbf{Spec}(A)$ with $|\\pmb{\\alpha}|=1$ is a simple root of the minimal polynomial. Hence there exists some $c>0$ such that $\\|A^{n}\\|\\leqslant c$ for all $\\pmb{n}\\in\\mathbb{N}$ [11, p. 43, Exercise 6]. <PARAGRAPH_SEPARATOR> Let Z := {xe V: |xl2 ≤1 <↓B-'x|2} and u := - In |μl. Then there exists some $\\varepsilon>0$ suchthat $u(x)\\geqslant\\varepsilon$ for all $x\\in Z$ (cf. [9, p. 295]). Moreover we have $u((B^{t})^{n}x)=\\beta^{n}u(x)$ for all $x\\in Z$ and $\\pmb{n\\in\\mathbb{Z}}$ Consequently we obtain for all $x\\in Z$ and $\\pmb{n}\\in\\mathbb{N}$ <PARAGRAPH_SEPARATOR> $$ {\\frac{u((B^{t})^{-n}x)}{|(B^{t})^{-n}x|_{2}}}{=}{\\frac{\\beta^{-n}u(x)}{|(B^{t})^{-n}x|_{2}}}{=}{\\frac{u(x)}{|A^{n}x|_{2}}}\\geqslant{\\frac{u(x)}{\\|A^{n}\\||x|_{2}}}\\geqslant{\\frac{\\varepsilon}{c}}{=}:b. $$ <PARAGRAPH_SEPARATOR> Hence $u(y)\\geqslant b|y|_{2}$ for all $\\begin{array}{r}{y\\in\\bigcup_{n\\geqslant1}\\left(B^{\\prime}\\right)^{-n}Z.}\\end{array}$. On the other hand we have $|y|_{2}\\leqslant1$ for all $\\scriptstyle y\\in\\bigcup_{n\\leqslant0}(B^{\\iota})^{-n}Z$ (since $\\left\\|B\\right\\|<1)$. Since $\\textstyle\\bigcup_{n\\in\\mathbb{Z}}(B^{\\prime})^{n}Z=V^{*}$ [9, Corollary 1.1] we obtain $|\\hat{\\mu}(x)|\\leqslant1$ if $|x|_{2}\\leqslant1$ and $|\\hat{\\mu}(x)|\\leqslant$ $\\exp\\left\\{-b\\left|x\\right|_{2}\\right\\}$ if $|x|_{2}>1$ <PARAGRAPH_SEPARATOR> Let $\\delta\\in]0,b[$ and $x\\in V$ such that $i x\\in S_{\\delta/d}$. Then we have for all $y\\in V$ such that $|y|_{2}>1$ <PARAGRAPH_SEPARATOR> $$ e^{\\langle x,y\\rangle}|{\\hat{\\mu}}(y)|\\leqslant\\exp\\{-(b-\\delta)\\mid y\\mid_{2}\\}. $$ <PARAGRAPH_SEPARATOR> Now we can proceed as in the proof of (i). Hence $f$ admits an analytic extension to the strip $S_{b/d}$ <PARAGRAPH_SEPARATOR> Remarks. 1. The case (i) of the proposition arises if and only if $\\mu$ admits an absolute moment of order 1, ie., iff $\\int|x|_{2}\\mu(d x)$ isfinite [9, Theorem 3.1]. <PARAGRAPH_SEPARATOR> 2. On the real line this proposition is due to V. M. Kruglov [8, Theorem 3]. It should be observed that in this case the matrix $\\pmb{B}$ can be identified with a real number $B\\in[\\beta,\\sqrt{\\beta}[$.Then ${\\boldsymbol{\\beta}}={\\boldsymbol{B}}^{\\alpha}$ withsome $\\pmb{\\alpha}\\in[1,2[$.Hence if $\\alpha>1$ then $f$ can be extended to an entire function; if $\\pmb{\\alpha}=1$ then $f$ can be extended to an analytic function in some strip $\\left\\{z\\in\\mathbb{C}\\colon|\\operatorname{Im}z|<\\delta\\right\\}$"
  },
  {
    "qid": "statistic-posneg-1776-0-1",
    "gold_answer": "FALSE. The paper clarifies that AGHQ with $m=3$ points does not provide the same error rate as second-order Laplace approximation. Instead, it shows that AGHQ requires at least $m=4$ points per dimension to achieve an error rate of $O(p^{-2})$, which is comparable to second-order Laplace approximation.",
    "question": "Does the paper claim that adaptive Gauss-Hermite quadrature with $m=3$ points per dimension provides the same error rate as second-order Laplace approximation?",
    "question_context": "Error Rate Analysis of Adaptive Gauss-Hermite Quadrature in Latent Variable Models\n\nThe paper discusses the error rate of adaptive Gauss-Hermite quadrature (AGHQ) approximation in latent variable models. It corrects the previously derived error rate by Liu & Pierce (1994), showing that the correct error rate for AGHQ is $O(p^{-\\lfloor(m+2)/3\\rfloor})$ rather than $O(p^{-\\lfloor m/3+1\\rfloor})$. The paper also compares AGHQ with Laplace approximation, demonstrating that AGHQ with $m=4,5,6$ points per dimension has an error rate of $O(p^{-2})$, similar to second-order Laplace approximation."
  },
  {
    "qid": "statistic-posneg-332-1-3",
    "gold_answer": "FALSE. The inequality is incorrectly stated. The correct inequality from the paper is $0\\cdot0075>P({H_{1}};{H_{0}})>0\\cdot0029$, but the direction of the inequalities should be reversed to match the context. The paper actually provides $0\\cdot0075 < P({H_{1}};{H_{0}}) < 0\\cdot0029$ as a typographical error, which is inconsistent with the numerical results.",
    "question": "Does the inequality $0\\cdot0075>P({H_{1}};{H_{0}})>0\\cdot0029$ hold for the given example where $A=95$ and $B=0{\\cdot}0505$?",
    "question_context": "Sequential Probability Ratio Test (SPRT) Error Bounds and Monotonicity Conditions\n\nThe paper discusses the Sequential Probability Ratio Test (SPRT) for normally distributed variates, focusing on error bounds and monotonicity conditions of certain functions. Key formulas include the inequalities for the product of likelihood ratios, the transformation of the SPRT into a sum-based test, and conditions for the monotonicity of functions like $E_{0}(y)$ and $E_{h}(y)$. The paper also provides approximations for type I and type II errors and demonstrates their application with specific values for $A$ and $B$."
  },
  {
    "qid": "statistic-posneg-1567-0-1",
    "gold_answer": "TRUE. The paper shows that Φ+(w) depends on k = (n′−3)/2, K = (N′−3)/2, and ρ², where n′ and N′ are the sample sizes of the two independent samples, and ρ is the correlation coefficient. The distribution function is expressed in terms of these parameters, and the probability integrals P+ and P− are derived based on them.",
    "question": "Does the distribution function Φ(w) depend on the sample sizes and the correlation coefficient ρ?",
    "question_context": "Distribution of the Ratio of Covariance Estimates in Normal Bivariate Populations\n\nThe paper discusses the distribution function for the ratio of covariance estimates from two independent random samples drawn from normal bivariate populations. The problem is generalized to derive the distribution function for the ratio of sums of products, which is a more convenient statistic than the covariance ratio itself. The mathematical proof involves transforming variables and integrating over ranges to obtain the distribution functions for positive and negative ratios."
  },
  {
    "qid": "statistic-posneg-1897-2-0",
    "gold_answer": "TRUE. The term $J(\\eta)=\\prod_{i<j}^{p}(\\eta_{i}-\\eta_{j})$ is the Vandermonde determinant, which arises in the joint density of ordered eigenvalues. It accounts for the fact that the eigenvalues are ordered ($\\eta_{1}>\\cdots>\\eta_{p}$) and are not independent. This term ensures that the density is zero whenever any two eigenvalues are equal, reflecting the repulsion between eigenvalues of a Wishart matrix. The presence of this term is crucial for correctly characterizing the joint distribution of the eigenvalues under the null hypothesis of sphericity.",
    "question": "The joint density function $f(\\eta)$ for the eigenvalues of a Wishart matrix under the null hypothesis of sphericity includes the term $J(\\eta)=\\prod_{i<j}^{p}(\\eta_{i}-\\eta_{j})$. Is it correct to say that this term accounts for the ordering constraint $\\eta_{1}>\\cdots>\\eta_{p}$ and the non-independence of the eigenvalues?",
    "question_context": "Importance Sampling for p-value Computations in Multivariate Tests Based on Wishart Eigenvalues\n\nDistributions based on the eigenvalues of a Wishart matrix can be approximated with the same methodology. The importance distribution in this setting is the distribution of the order statistics from a random sample of gamma variates. Consider, for example, the hypothesis of sphericity, $\\Sigma=\\sigma^{2}\\mathbf{I}_{\\mathbf{p}}$. Three different tests can be considered each of whose null distribution is determined by the ordered eigenvalues of a Wishart ${\\bf\\Sigma}_{p}(n,{\\bf I_{p}})$ random matrix. Denoting the eigenvalues as $\\eta^{\\mathbf{T}}=(\\eta_{1},\\ldots,\\eta_{p})$ where $\\eta_{1}>\\cdots>\\eta_{p}$, the statistics corresponding to the three tests are the likelihood ratio test statistic $(L=p^{p}\\prod_{i=1}^{p}(\\eta_{i}/\\sum_{j=1}^{p}\\eta_{j}))$, the ratio of the smallest eigenvalue to the largest eigenvalue $(L_{1}=\\eta_{p}/\\eta_{1})$, and the statistic for the locally best invariant test $(L_{2}=\\sum_{i=1}^{p}\\eta_{i}^{2}/(\\sum_{i=1}^{p}\\eta_{i})^{2})$ (Muirhead 1982, p. 353). Under the null hypothesis $\\Sigma=\\sigma^{2}\\mathbf{I_{p}}$, with $n\\geq p$, the joint distribution of $\\eta$ has density $$f(\\eta)=c_{3}\\left\\{\\prod_{i=1}^{p}\\eta_{i}^{\\frac{1}{2}(n-p-1)}\\right\\}\\left\\{\\exp(-\\frac{1}{2}\\sum_{i=1}^{p}\\eta_{i})\\right\\}J(\\eta),$$ where $$c_{3}=2^{-\\frac{1}{2}p n}\\pi^{\\frac{1}{2}p^{2}}\\left\\{\\Gamma_{p}\\left(\\frac{1}{2}n\\right)\\Gamma_{p}\\left(\\frac{1}{2}p\\right)\\right\\}^{-1},$$ and $$J(\\eta)=\\prod_{i<j}^{p}(\\eta_{i}-\\eta_{j})$$ (Muirhead 1982, p. 107). P-values for the three tests are expectations of functions of $\\eta$ under the distribution above and can be approximated using the same importance sampling procedure described in Section 2 with the original importance distribution now having the shape of the joint density of order statistics from p independently and identically distributed x(n-p+1) variates. As before, it is possible to modify the original importance distribution."
  },
  {
    "qid": "statistic-posneg-1510-9-0",
    "gold_answer": "FALSE. The variance convergence rate of $O(n_{\\mathrm{gen}}^{-3}(\\log n_{\\mathrm{gen}})^{p-1})$ for the RQMC estimator $\\hat{\\mu}_{n_{\\mathrm{gen}}}^{\\mathrm{NN}}$ is valid only when the mixed partial derivatives of the composite function $h$ up to order $p$ are continuous. This is explicitly stated in Corollary A.1, which requires the existence and continuity of these derivatives to achieve the stated convergence rate. Discontinuities would violate the smoothness conditions necessary for the theorem to hold.",
    "question": "Is the variance convergence rate of $O(n_{\\mathrm{gen}}^{-3}(\\log n_{\\mathrm{gen}})^{p-1})$ for the RQMC estimator $\\hat{\\mu}_{n_{\\mathrm{gen}}}^{\\mathrm{NN}}$ only valid when the mixed partial derivatives of the composite function $h$ up to order $p$ are discontinuous?",
    "question_context": "Variance Convergence Rate of RQMC Estimators Using Scrambled Nets\n\nFor RQMC estimators $\\frac{1}{n_{\\mathrm{gen}}}\\sum_{i=1}^{n_{\\mathrm{gen}}}g(\\tilde{\\pmb{\\nu}})$ based on scrambled nets, Owen (1997) initially derived a convergence rate for the variance of the estimators under a certain smoothness condition on $g$ , where $g:$ $[0,1]^{P}\\rightarrow\\mathbb{R}$ . Owen (2008) then generalized his earlier result to allow a weaker smoothness condition for a larger class of scrambled nets. Specifically, if $\\tilde{P}_{n_{\\mathrm{gen}}}=\\{\\tilde{\\nu}_{1},\\dots,\\tilde{\\nu}_{n_{\\mathrm{gen}}}\\}$ is a so-called relaxed scrambled $(\\lambda,q,m,p)$ -net in base $^b$ with bounded gain coefficients—for example, Sobol’ sequences randomized using nested uniform sampling belong to this class—then we have the following result as a direct consequence of Owen (2008). <PARAGRAPH_SEPARATOR> Theorem A.1 (Owen 2008). If all the mixed partial derivatives (up to order $\\boldsymbol{p}$ ) of $g$ exist and are continuous, then <PARAGRAPH_SEPARATOR> $$ \\mathrm{var}\\biggl(\\frac{1}{n_{\\mathrm{gen}}}\\sum_{i=1}^{n_{\\mathrm{gen}}}g(\\tilde{\\nu}_{i})\\biggr)=O(n_{\\mathrm{gen}}^{-3}(\\log n_{\\mathrm{gen}})^{p-1}). $$ <PARAGRAPH_SEPARATOR> Proof. See Owen (2008, Theorem 3). <PARAGRAPH_SEPARATOR> Now, for the GMMN RQMC estimator, $\\hat{\\mu}_{n_{\\mathrm{gen}}}^{\\mathrm{NN}}$ $=$ $\\frac{1}{n_{\\mathrm{gen}}}\\sum_{i=1}^{n_{\\mathrm{gen}}}\\Psi(q(\\tilde{\\nu}_{i}))=\\frac{1}{n_{\\mathrm{gen}}}\\sum_{i=1}^{n_{\\mathrm{gen}}}h(\\tilde{\\nu}_{i})$ , the corollary below naturally follows from Theorem A.1 with some added analysis of the composite function $h$ . <PARAGRAPH_SEPARATOR> Corollary A.1. If all the mixed partial derivatives (up to order $\\boldsymbol{p}$ ) of h=yoq=WofoFz exist and are continuous, then var $(\\hat{\\mu}_{n_{\\mathrm{gen}}}^{\\mathrm{NN}})=$ $O(n_{\\mathrm{gen}}^{-3}(\\log n_{\\mathrm{gen}})^{p-1})$ ."
  },
  {
    "qid": "statistic-posneg-1636-0-2",
    "gold_answer": "TRUE. The context reports that the Wang et al. (2017) method took 0.04 seconds per replication, while the exponentially tilted empirical likelihood method took 0.25 seconds (for n=100). This is because the normal approximation relies on a simpler asymptotic likelihood (θ̂∣θ∼N(θ,V̂)) and avoids the computational cost of empirical likelihood optimization.",
    "question": "Is the normal approximation method of Wang et al. (2017) faster to compute than the Bayesian exponentially tilted empirical likelihood method?",
    "question_context": "Bayesian Inference under Unequal Probability Sampling with Exponentially Tilted Empirical Likelihood\n\nIn this simulation, we examine estimation of the population mean of binary outcomes in a design setting. In the notation of §2.3, Z=Y, u(Z,θ)=Y−θ and θ0=EP0(Y). The design variables Wi (i=1,…,n) are independent and identically distributed according to the beta distribution Be(1.5, 3.5), and the outcomes Yi∣Wi∼Ber(Wi) so that θ0=0.3. The selection variables Ri (i=1,…,n) are independent and identically distributed according to Ri∣πi∼Ber(πi), where logit(πi)=Wi. Thus, Yi and the selection probability πi are positively correlated, and the selection must be adjusted for in order to estimate θ0. The data available for analysis are Di=(RiYi,Ri,Riπi) (i=1,…,n), so that the design variables are excluded. Following the approach in §2.3, the M-estimator θ̂n is the Hájek estimator which solves ∑i=1ng(Di,θ)=∑i=1nRiπi(Yi−θ)=0. We use g to define the exponentially tilted empirical likelihood Ln(θ), which we combine with three different priors for θ: θ∼Be(0.5,0.5), θ∼Un(0,1) and θ∼Be(1.5,3.5). The first is Jeffrey’s prior. The mean of the Be(1.5,3.5) prior is equal to θ0, so we consider this prior informative, while the first two are considered noninformative. We compare this approach with the proposal in Wang et al. (2017, §2). In a survey inference context, Wang et al. (2017) suggested using a Bayesian approach with an approximate normal likelihood θ̂∣θ∼N(θ,V̂), where θ̂ is a consistent and asymptotically normal estimator of θ0 and V̂ is a robust estimator of the variance of θ̂. The estimator θ̂̂ acts as a summary statistic for the data, such that the posterior is p(θ∣θ̂)∝p(θ̂∣θ)p(θ), where p(θ̂∣θ) is defined by the normal model above and p(θ) is a prior for θ. We choose θ̂ to be the Hájek estimator defined above and estimate its variance with the nonparametric bootstrap. We use the same priors as defined above. Table 1 compares the frequentist estimator θ̂n with the Bayesian methods. Each setting was replicated 2000 times. The Bayes point estimators are the posterior means. Coverage rates were computed based on central 95% credible regions. The Bayesian computation was carried out using importance sampling with 5000 particles for each replication, and the tolerance for computing the exponentially tilted empirical likelihood was 10−4. On a standard quad-core laptop computer, a single replication of the Bayesian exponentially tilted empirical likelihood method with n=100 took 0.25 seconds to run, while the method of Wang et al. (2017) took 0.04 seconds. The Bayesian exponentially tilted empirical likelihood estimators generally have a higher magnitude of bias than the normal approximation when a noninformative prior is used, but have lower bias with the informative prior. This reflects the fact that the exponentially tilted empirical likelihood is less informative than the normal likelihood, resulting in higher shrinkage towards the prior mean. In the case of the two noninformative priors, this causes an upward bias towards the prior mean 0.5. This conservative characteristic leads to the Bayesian exponentially tilted empirical likelihood approach having superior performance in terms of root mean squared error and coverage rate across almost all settings, particularly with the smaller sample sizes for which the normal approximation is less accurate."
  },
  {
    "qid": "statistic-posneg-2065-7-1",
    "gold_answer": "TRUE. The paper explicitly states that it can be easily verified that (6) and (7) lead to the same result as given in (3) when $\\pmb{m}=2$. This consistency between the exact formula and the integral form confirms the validity of the derivations for this specific case.",
    "question": "Does the exact formula (3) for $P_{k}(Q)$ simplify to the same result as the integral form (6) and (7) when $\\pmb{m}=2$?",
    "question_context": "Probability Integral of the Largest Variance Ratio in ANOVA\n\nThe paper discusses the probability integral of the largest variance ratio in the analysis of variance, denoted by $P_{k}(Q)$, which represents the probability of the ratio $\\pmb{\\vartheta}_{\\pmb k}/\\pmb{\\vartheta}_{0}$ being less than or equal to $Q$. Various formulas and approximations are provided for calculating $P_{k}(Q)$, including exact and approximate methods, with a focus on cases where $\\pmb{m}=1$ and $\\pmb{m}=2$. The paper also discusses the construction of tables for these probabilities using Hermite functions and numerical differentiation techniques."
  },
  {
    "qid": "statistic-posneg-2033-0-3",
    "gold_answer": "FALSE. The logistic STAR model has computational advantages over the normal STAR model, as its parameters can be estimated using analytical derivatives. The logistic distribution also approximates the normal distribution closely, making it a practical choice for applications where the normal model might be theoretically appropriate but computationally intensive.",
    "question": "Is the logistic STAR model computationally more cumbersome than the normal STAR model?",
    "question_context": "Testing Linearity in Smooth Transition Autoregressive (STAR) Models\n\nThe present paper offers a remedy by introducing a set of tests which can be used to test linearity in a whole class of sTAR models. Under the null hypothesis of linearity our test statistics possess an asymptotic $\\chi^{2}$ distribution, and they seem to have reasonable power even when the true model is sETAR. The power of one of these tests crucially depends on whether the two regimes have the same intercept or not. We also briefly present a member of the STAR model class called the logistic sTAR model. It is close to the normal sTAR model that Chan $\\pmb{\\&}$ Tong (1986b) simulate but has distinct computational advantages over the latter. Our simulation experiments with logistic STAR and sETAR models indicate that the new tests compare very favourably with the CusuM test which Petruccelli & Davies (1986) recently suggested for testing linearity against the sETAR model."
  },
  {
    "qid": "statistic-posneg-220-0-1",
    "gold_answer": "TRUE. The Cauchy functional equation $\\Psi_2(x+y)=\\Psi_2(x)\\Psi_2(y)$ for continuous functions has the exponential function $\\Psi_2(x)=e^{cx}$ as its unique solution (up to a multiplicative constant). This is a standard result in functional equations, and the continuity of $\\Psi_2$ (inherited from the continuity of $h$) ensures this solution holds.",
    "question": "In the proof, the function $\\Psi_2(x)$ is derived to satisfy the Cauchy equation $\\Psi_2(x+y)=\\Psi_2(x)\\Psi_2(y)$. Does this imply that $\\Psi_2(x)$ must be an exponential function?",
    "question_context": "Characterization of the Normal Distribution via Chi-Square Statistic\n\nIt is known that if the statistic $Y=\\sum_{j=1}^{n}{(X_{j}+a_{j})^{2}}$ is drawn from a population which is distributed $N(0,\\sigma)$ then the distribution of $Y$ depends on $\\sum_{j=1}^{n}a_{j}^{2}$ only. Kagan and Shalaevski [2] have shown that if the random variables $X_{1},X_{2},...,X_{n}$ are independent and identically distributed and the distribution of $Y$ depends only on $\\sum_{j=1}^{n}a_{j}^{2}$ , then each $X_{j}$ is distributed $N(0,\\sigma)$. It is shown below that if the random vectors $(X_{1},...,X_{m})$ and $(X_{m+1},...,X_{n})$ are independent and the distribution of $Y$ depends only on $\\sum_{j=1}^{n}a_{j}^{2}$ then all $X_{j}$ are independent and distributed $N(0,\\sigma)$. <PARAGRAPH_SEPARATOR> THEOREM. Let $(X_{1},...,X_{m})$ and $(X_{m+1},...,X_{n})$ ， $1\\leqslant m<n$ be independent and let $Y=\\sum_{j=1}^{n}{(X_{j}+a_{j})^{2}}$ have a distribution which depends only on $\\sum_{j=1}^{n}a_{j}^{2}$ $a_{j}\\in\\mathcal{R}$ . Then all $X_{j}$ are independent and distributed normally with zero means and common variance."
  },
  {
    "qid": "statistic-posneg-198-0-2",
    "gold_answer": "FALSE. The degree of contamination is $\\hat{\\eta}_{1}=7.49$ for Group 1 and $\\hat{\\eta}_{2}=49.33$ for Group 2. This means that while Group 1 has more outliers (lower $\\hat{\\alpha}_{1}=0.89$), the single outlier in Group 2 is much more severe, as reflected by the significantly higher $\\hat{\\eta}_{2}$ value.",
    "question": "Is the degree of contamination ($\\hat{\\eta}$) higher for Group 1 than for Group 2, indicating more severe outliers in Group 1?",
    "question_context": "Matrix-Variate Contaminated Normal Mixtures: Correlation and Contamination Analysis\n\nWe now report the estimated parameters of the MVCNM model to analyze the differences among the two estimated groups. In the following, Group 1 identifies the families of bachelor’s degrees whereas Group 2 refers to the families of master’s degrees. Figure 4 displays the line plots of the estimated means for both groups over the three years but separately for the three variables. <PARAGRAPH_SEPARATOR> As we can see, the two groups are on average quite different when looking at the “Credits” and “Continue” variables, whereas they are bit more similar in the “Re-enroll” variable. Additionally, Group 1 is always worse than Group 2, suggesting that students of bachelor’s degrees face more difficulties than the (more experienced) master’s degrees students. We also observe that the mean of the “Credits” variable is increasing over time for Group 2 and slowly decreasing for Group 1. Conversely, with respect to the “Continue” variable, both groups show a similar average pattern, that is, a small increase in 2016 followed by a reduction in 2017. Lastly, when the “Re-enroll” variable is considered, we note that after an increase in 2016, the mean for Group 2 decreases, whereas it is gradually growing for Group 1. <PARAGRAPH_SEPARATOR> To evaluate the correlations of the variables with each other and over time, for the two groups, we now report the correlation matrices related to $\\pmb{\\Sigma}_{g}$ and $\\Psi_{g}$ , that is, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\mathbf{R}\\left(\\pmb{\\Sigma}_{1}\\right)=\\left(\\begin{array}{c c c}{1.00}&{0.44}&{-0.13}\\\\ {0.44}&{1.00}&{0.03}\\\\ {-0.13}&{0.03}&{1.00}\\end{array}\\right),}\\end{array} $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{{\\bf R}\\left({\\pmb{\\Sigma}}_{2}\\right)=\\left(\\begin{array}{c c c}{1.00}&{0.14}&{-0.17}\\\\ {0.14}&{1.00}&{0.10}\\\\ {-0.17}&{0.10}&{1.00}\\end{array}\\right),}\\end{array} $$ <PARAGRAPH_SEPARATOR> $$ \\mathbf{R}\\left(\\Psi_{1}\\right)=\\left(\\begin{array}{c c c}{1.00}&{0.98}&{0.97}\\\\ {0.98}&{1.00}&{0.98}\\\\ {0.97}&{0.98}&{1.00}\\end{array}\\right) $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\mathbf{R}\\left(\\Psi_{2}\\right)=\\left(\\begin{array}{c c c}{1.00}&{0.91}&{0.89}\\\\ {0.91}&{1.00}&{0.90}\\\\ {0.89}&{0.90}&{1.00}\\end{array}\\right). $$ <PARAGRAPH_SEPARATOR> When ${\\bf R}\\left({\\pmb{\\Sigma}}_{1}\\right)$ and $\\mathbf{R}\\left(\\pmb{\\Sigma}_{2}\\right)$ are considered, notice that the correlation between the “Credits” and “Continue” variables are quite different between the two groups. In both cases, and as it might be reasonable to expect, these correlations are positive, but Group 1 has a higher value. On the contrary, both groups have similar negative correlations between the “Credits” and “Re-enroll” variables. Finally, “Continue” and “Re-enroll” are practically uncorrelated for Group 1, and weakly positively correlated for Group 2. With respect to $\\mathbf{R}\\left(\\Psi_{1}\\right)$ and R $(\\Psi_{2})$ , there are high correlations between time points in both groups. <PARAGRAPH_SEPARATOR> The estimated proportion of typical points and degree of contamination for the first group are $\\hat{\\alpha}_{1}=0.89$ and $\\hat{\\eta}_{1}=7.49$ , respectively, whereas for the second group they are $\\hat{\\alpha}_{2}=0.98$ and ${\\hat{\\eta}}_{2}~=~49.33$ . Therefore, while there are more outlying observations in Group 1, the single atypical observation found in Group 2 is more severe, as reflected by the much greater value of the inflation parameter. These aspects can be better understood by looking at Table 3, where the degree families marked as atypical are reported, along with their estimated probabilities to be typical points. <PARAGRAPH_SEPARATOR> As we can see, the only study program flagged as being atypical for the second group, that is, “Geophysical sciences”, has a very small $\\hat{\\nu}_{i g}$ value. This means that the corresponding"
  },
  {
    "qid": "statistic-posneg-1457-0-4",
    "gold_answer": "FALSE. The sup-L₁ metric is stronger than the integrated L₁ metric because it requires uniform convergence over all z ∈ ℝᵏ, whereas the integrated L₁ metric averages over z with respect to a weight function v(z).",
    "question": "Is the integrated L₁ metric stronger than the sup-L₁ metric for assessing posterior consistency in the DPMM?",
    "question_context": "Dirichlet Process Mixture Model for Time Series Analysis\n\nThe paper introduces a Dirichlet Process Mixture Model (DPMM) for modeling ergodic time series. The model assumes a nonparametric mixture of normal kernels for the conditional density, with a specific autoregressive (AR) link function involving the hyperbolic tangent function. The DP prior is placed on the mixing distribution, allowing for flexible shapes. Posterior inference is conducted via an MCMC scheme, and sufficient conditions for posterior consistency are established in two different topologies."
  },
  {
    "qid": "statistic-posneg-1145-0-1",
    "gold_answer": "FALSE. The paper explicitly states that simulation experiments and real data analyses show BCV bandwidth matrix selectors tend to perform less well than both AMSE and SAMSE plug-in selectors, which is consistent with the theoretical results.",
    "question": "Does the paper claim that the BCV bandwidth matrix selectors perform better than AMSE and SAMSE plug-in selectors in finite samples?",
    "question_context": "Convergence Rates for Bandwidth Matrix Selectors in Kernel Density Estimation\n\nThe proof of Theorem 2 proceeds via a pair of lemmas. Lemma 2 states that under the conditions of Theorem 2, the asymptotic bias of the bandwidth matrix selector is of order O(n^{-2/(d+4)}). Lemma 3 states that the asymptotic variance is of order O(n^{-d/(d+4)}). The proof involves analyzing the bias and variance of the BCV-AMISE difference, which is expressed in terms of the difference between the estimated and true fourth-order derivatives of the density."
  },
  {
    "qid": "statistic-posneg-877-4-0",
    "gold_answer": "TRUE. The paper explicitly states that for any positive vector $\\pmb{a}$, the transformation $\\Delta({\\pmb a}){\\pmb X}$ preserves the multivariate matrix–Weibull distribution property. This is a special property of this distribution, which is derived from its construction via the MPH* framework and the transformation functions $g_j(y) = y^{1/\\beta_j}$. The positive scaling factors $a^{(j)}$ interact multiplicatively with the underlying Weibull-distributed components, maintaining the matrix–Weibull structure due to the closure properties under scaling of Weibull distributions and the linear algebraic structure of the MPH* representation.",
    "question": "Is the statement 'For a vector $\\pmb{a}=(a^{(1)},\\dots,a^{(d)})$ with $a^{(j)}>0,j=1,\\dots,d$, the transformation $\\Delta({\\pmb a}){\\pmb X}$ results in a multivariate matrix–Weibull distribution' consistent with the properties of the multivariate matrix–Weibull distribution described in the paper?",
    "question_context": "Multivariate Matrix–Weibull Distribution: Properties and Parameter Estimation\n\nLet $\\pmb{X}=(\\mathbf{g}_{1}(Y^{(1)})$ , … , $g_{d}(Y^{(d)}))^{\\prime}$ , where ${\\pmb Y}\\sim\\mathrm{{MPH}}^{*}({\\pmb\\pi},{\\pmb T},{\\pmb R})$ and $g_{j}(y)=y^{1/\\beta_{j}},\\beta_{j}>0,j=1,\\dots,d,$ then we say that $\\pmb{X}$ has a multivariate matrix–Weibull distribution. Some special properties of this type of distribution are: <PARAGRAPH_SEPARATOR> 1. Marginal distributions are matrix–Weibull distributed.   2. For a vector $\\pmb{a}=(a^{(1)},\\dots,a^{(d)})$ , with $a^{(j)}>0,j=1$ , … , $d,\\Delta({\\pmb a}){\\pmb X}$ is multivariate matrix–Weibull distributed. <PARAGRAPH_SEPARATOR> For the bivariate case we get <PARAGRAPH_SEPARATOR> $$ f_{X}(x^{(1)},x^{(2)})=\\alpha\\mathrm{e}^{T_{11}(x^{(1)})^{\\beta_{1}}}T_{12}\\mathrm{e}^{T_{22}(x^{(2)})^{\\beta_{2}}}(-T_{22})e~\\beta_{1}(x^{(1)})^{\\beta_{1}-1}\\beta_{2}(x^{(2)})^{\\beta_{2}-1}. $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\overline{{F}}_{X}(x^{(1)},x^{(2)})=\\alpha(-T_{11})^{-1}\\mathrm{e}^{T_{11}(x^{(1)})^{\\beta_{1}}}T_{12}\\mathrm{e}^{T_{22}(x^{(2)})^{\\beta_{2}}}e.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Remark 4. The Marshall–Olkin Weibull distribution (see Hanagal (1996b)) is a particular case of this distribution. <PARAGRAPH_SEPARATOR> # 4.4.1 Parameter estimation <PARAGRAPH_SEPARATOR> In contrast to Section 4.3.1, all transformations are parameter–dependent, and the fitting procedures of the previous subsection are not applicable. However, we can apply Algorithm 4 in the bivariate case. <PARAGRAPH_SEPARATOR> Example 8 (Bivariate Matrix–Weibull). We generate an i.i.d. sample of size 5000 of a bivariate random vector with matrix–Weibull marginals with parameters <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\pmb{\\pi}_{1}=(0.8,0.2),}\\ &{\\pmb{T}_{1}=\\left(\\begin{array}{c c}{-1}&{0.5}\\\\{0}&{-0.5}\\end{array}\\right),}\\ &{\\pmb{\\beta}_{1}=0.4}\\end{array} $$ <PARAGRAPH_SEPARATOR> for the first marginal and <PARAGRAPH_SEPARATOR> $$ \\pmb{\\pi}_{2}=(0.5,0.25,0.25). $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\begin{array}{c c c}{\\displaystyle T_{2}=\\left(\\begin{array}{c c c}{-1}&{1}&{0}\\\\{0}&{-0.5}&{0.5}\\\\{0}&{0}&{-0.1}\\end{array}\\right),}\\ {\\displaystyle\\beta_{2}=0.6,}\\end{array}}\\end{array} $$ <PARAGRAPH_SEPARATOR> for the second marginal, and a Gaussian copula with parameter $\\rho=0.5$ . While any copula, or also simply a bivariate matrix–Weibull based on a $\\mathrm{{\\bfMPH}}^{*}$ construction could be used, we choose the Gaussian copula here to illustrate that the algorithm is able to work with any type of dependence structure. This distribution has theoretical mean $\\mathbb{E}(X)=(18.7997,86.3711)^{\\prime}$ . The sample has numerical values $\\hat{\\mathbb{E}}(\\mathbf{X})=(18.7690,88.1637)^{\\prime}$ and $\\hat{\\rho}_{\\tau}=0.3431$ . <PARAGRAPH_SEPARATOR> We fit a bivariate matrix–Weibull distribution with $p_{1}=p_{2}=3$ using Algorithm 4 with 1500 steps (with a running time of 3930 s for a step-length of $10^{-5}$ ), getting the following parameters: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\hat{\\alpha}=(0.2101,0,0.7899,0,0,0,0),}\\ {\\hat{\\left(\\begin{array}{c c c c c c c}{-4.2507}&{0.5527}&{1.2916}&{0}&{0}&{2.4064}\\\\{0}&{-0.3069}&{0}&{0}&{0.3069}&{0}\\\\{0.0089}&{0.2575}&{-0.6903}&{0.4238}&{0}&{0}\\\\{0}&{0}&{0}&{-0.0946}&{0}&{0.0946}\\\\{0}&{0}&{0}&{0.0360}&{-0.0360}&{0}\\\\{0}&{0}&{0}&{0.0026}&{0}&{-0.2542}\\end{array}\\right)}\\end{array}, $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\hat{R}=\\left(\\begin{array}{l l}{1}&{0}\\\\{1}&{0}\\\\{1}&{0}\\\\{1}&{0}\\\\{0}&{1}\\\\{0}&{1}\\\\{0}&{1}\\\\{0}&{1}\\end{array}\\right),}\\end{array} $$ <PARAGRAPH_SEPARATOR> $$ \\beta_{1}=0.4689,\\quad\\beta_{2}=0.7340. $$ <PARAGRAPH_SEPARATOR> One sees that the algorithm estimates the shape parameters of the matrix–Weibull marginals reasonably well. The fitted distribution has mean $\\mathbb{E}(X)=(18.6988,88.1166)^{\\prime}$ , and from simulated data we get $\\hat{\\rho}_{\\tau}=0.3236$ . The QQ and contour plots are given in Figure 16 and Figure 17, respectively. The log-likelihood of the fitted bivariate matrix–Weibull is −39, 748, which is to be compared with the log-likelihood −39, 687.19 using the original distribution (Figure 18). <PARAGRAPH_SEPARATOR> Remark 5. In all examples of this section the marginals were assumed to be of the same type (both matrix–Pareto or both matrix–Weibull). We would like to mention that the generality of Algorithm 4 also allows to fit models with marginals of different types (e.g., one marginal matrix–Pareto and the other matrix–Weibull). <PARAGRAPH_SEPARATOR> # 5 CONCLUSION"
  },
  {
    "qid": "statistic-posneg-777-0-1",
    "gold_answer": "FALSE. Proposition 1(ii) shows that sequential bagging requires $|q(x_0) - 1/2|$ to be of order exceeding $O(|1-\\rho|)$, while direct bagging requires it to be of order exceeding $O(|1-\\rho|^{1/2})$. Thus, sequential bagging actually requires a smaller distance for consistency compared to direct bagging as $\\rho \\uparrow 1$.",
    "question": "Does the condition for consistency with the Bayes classifier require a larger distance $|q(x_0) - 1/2|$ for sequential bagging compared to direct bagging as $\\rho \\uparrow 1$?",
    "question_context": "Sequential Bagging for Nearest Neighbor Classification\n\nFor a formal investigation we consider application of sequential bagging to the nearest neighbour classifier, which provides a natural platform for analysing the theoretical effects of bagging. Breiman (1996) remarked that the $n$ out of $n$ bagged nearest neighbour classifier does not converge to the Bayes classifier. Hall & Samworth (2005) showed that the above deficiency can be remedied by changing the bootstrap resample size from $n$ to some smaller value $m$ . Samworth (2012) derived a formula for the optimal m which minimizes the asymptotic regret of the bagged classifier. We shall focus on the $m$ out of $n$ bootstrap and investigate the effects of sequential bagging on nearest neighbour classifiers. Similar to Hall & Samworth (2005), we assume that each bagging round is based on infinitely many bootstrap resamples. <PARAGRAPH_SEPARATOR> Consider for simplicity a two-class problem with $k=2$ , although our findings can readily be generalized to cases where $k>2$ . A nearest neighbour classifier, trained on a learning set $\\mathcal{L}$ , assigns a point $x_{0}$ to class 1 or class 2 according to whether $x_{0}$ is closest to an observed data point from class 1 or class 2, respectively. For any $x\\in{\\mathcal{X}}$ , let $\\{(X_{(i)}(x),Y_{(i)}(x)):i=1,\\dots,n\\}$ be a permutation of $\\mathcal{L}$ such that $X_{(i)}(x)$ is the $i$ th nearest to $x$ among $\\{X_{1},\\ldots,X_{n}\\}$ . The $m$ out of $n$ bagged nearest neighbour classifier, which we denote by $T_{\\mathrm{BAG},m}$ , assigns $x_{0}$ to class 1 if $\\begin{array}{r}{\\Pi_{\\mathrm{BAG},m}\\equiv\\sum_{i=1}^{n}V_{n,m,i}\\mathbb{I}\\{Y_{(i)}(x_{0})=1\\}>1/2}\\end{array}$ , and to class 2 otherwise, <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{V_{n,m,i}=\\{1-(i-1)/n\\}^{m}-(1-i/n)^{m}\\quad(i=1,\\ldots,n)}\\end{array} $$ <PARAGRAPH_SEPARATOR> if resampling is done with replacement, or <PARAGRAPH_SEPARATOR> $$ V_{n,m,i}={\\binom{n-i}{m-1}}{\\binom{n}{m}}^{-1}\\mathbb{I}(i\\leqslant n-m+1)\\quad(i=1,\\dots,n) $$ <PARAGRAPH_SEPARATOR> if resampling is done without replacement. Note that $T_{\\mathrm{BAG},m}$ has the same first-order asymptotic properties whether $V_{n,m,i}$ is given by (1) or (2). <PARAGRAPH_SEPARATOR> If proper densities, $f$ and $g$ say, are defined on $\\mathcal{X}$ for classes 1 and 2 with prior probabilities $\\pi_{f}$ and $\\pi_{g}=1-\\pi_{f}$ , respectively, then the Bayes rule assigns $x_{0}$ to class 1 if <PARAGRAPH_SEPARATOR> $$ q(x_{0})=\\pi_{f}f(x_{0})/\\{\\pi_{f}f(x_{0})+\\pi_{g}g(x_{0})\\}>1/2. $$ <PARAGRAPH_SEPARATOR> Hall & Samworth (2005) showed that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\operatorname{pr}\\{T_{\\mathrm{BAG},m}(\\mathcal{L},x_{0})=1\\}}\\end{array}$ converges to the Bayes rule $\\mathbb{I}\\{q(x_{0})>1/2\\}$ as $\\mathrm{lim}_{n\\to\\infty}m/n=\\ell\\downarrow0$ . Thus, $T_{\\mathrm{BAG},m}$ can be made consistent with the Bayes classifier provided that $m$ is chosen of order $o(n)$ . <PARAGRAPH_SEPARATOR> Application of sequential bagging leads to a classifier $T_{\\mathrm{SBAG},M,m}$ , which assigns $x_{0}$ to class 1 if and only if $\\begin{array}{r}{\\Pi_{\\mathrm{SBAG},M,m}\\equiv\\dot{\\sum}_{k=1}^{n-1}\\sum_{j=1}^{n}{V_{n-1,M,k}V_{n,m,j}}\\mathbb{I}\\{Y_{(k+1)}(X_{(j)}(x_{0}))=1\\}>1/2.}\\end{array}$ It suffices for our purpose to consider the case where the resampling fractions $M/n$ and $m/n$ converge to the same limit $\\ell$ . In the Appendix we prove the following proposition, which compares the asymptotic properties of $T_{\\mathrm{SBAG},M,m}$ and $T_{\\mathrm{BAG},m}$ under different resampling fractions $\\ell$ and with a fixed $x_{0}\\in\\mathcal{X}$ . Following Hall & Samworth (2005), we take $\\rho=\\exp(-\\ell)$ for with-replacement bagging and $\\rho=1-\\ell$ for without-replacement bagging. <PARAGRAPH_SEPARATOR> PROPOSITION 1. Assume that $f$ and $g$ are continuous and that $q\\left(x_{0}\\right)\\in\\left(0,1\\right)$ . Then: <PARAGRAPH_SEPARATOR> (i) $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\operatorname{var}(\\Pi_{\\mathrm{SBAG},M,m})/\\operatorname{var}(\\Pi_{\\mathrm{BAG},m})=(1-\\rho)/(1+\\rho)f o}\\end{array}$ r any $\\rho\\in(0,1)$ ; (ii) for $q(x_{0})\\pm1/2$ we have, as $\\rho\\uparrow1$ , <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\mathrm{pr}\\{T^{*}(\\mathcal{L},x_{0})=1\\}=\\mathbb{I}\\{q(x_{0})>1/2\\}+\\{q(x_{0})-1/2\\}^{-2}O(|1-\\rho|^{\\alpha}), $$ <PARAGRAPH_SEPARATOR> where $\\alpha=1$ or 2 according to whether $T^{*}=T_{\\mathrm{BAG},m}$ or $T_{\\mathrm{SBAG},M,m}$ , respectively. <PARAGRAPH_SEPARATOR> Proposition 1(i) shows that majority voting based on sequential bagging is less variable than that based on direct $m$ out of $\\cdot_{n}$ bagging, irrespective of the value of $\\rho$ . Indeed, as $\\rho\\to1$ , i.e., as $\\ell\\to0$ , $\\Pi_{\\mathrm{SBAG},M,m}$ has a variance which is negligibly small compared to that of $\\Pi_{\\mathrm{BAG},m}$ . Proposition 1(ii) implies that, for a fixed $q(x_{0})\\neq1/2$ , both $T_{\\mathrm{SBAG},M,m}$ and $T_{\\mathrm{BAG},m}$ converge to the Bayes classifier if $M$ and $m$ are chosen of order $o(n)$ . The true gain of sequential bagging is brought out when $q(x_{0})$ lies in a shrinking neighbourhood of $1/2$ , where classification is most difficult. Here we see that $T_{\\mathrm{BAG},m}$ is consistent with the Bayes rule whenever $q(x_{0})$ differs from $1/2$ by a distance of order greater than $O(|1-\\rho|^{1/2})$ as $\\rho\\uparrow1$ . In the case of sequential bagging, consistency with the Bayes rule is assured whenever the distance $|q(x_{0})-1/2|$ has order exceeding $O(|1-\\rho|)$ , a condition less stringent than that required by $T_{\\mathrm{BAG},m}$ . It can be shown, by considering higher-order cumulants in expansions of Gram–Charlier type, that $(1-\\rho)^{-1/2}\\{\\Pi_{\\mathrm{BAG},m}-$ $q(x_{0})\\}$ and $(1-\\rho)^{-1}\\{\\Pi_{\\mathrm{SBAG},M,m}-q(x_{0})\\}$ converge weakly to some nondegenerate limits as $\\rho\\uparrow1$ , so that the orders given above for the critical distances are indeed sharp. <PARAGRAPH_SEPARATOR> The results of Proposition 1 cannot be revealed by considering the single case $\\rho=1$ , that is, $M,m=$ $o(n)$ , as has been the convention in formal validation of the $m$ out of $n$ bootstrap. A shift of focus from $\\rho=1$ to a more liberal regime $\\rho\\uparrow1$ helps to alleviate the risk posed by backtracking from a limiting case to finite-sample applications, thus providing a practically more informative comparison between sequential and $m$ out of $n$ baggings."
  },
  {
    "qid": "statistic-posneg-820-2-0",
    "gold_answer": "TRUE. The formula explicitly shows that the conditional probability of return at time $t$ given dropout at time $k$ and no return between $k$ and $t$ depends only on $X$ and $Y_k$, not on subsequent outcomes $Y_{k+1},...,Y_t$. This is the essence of the independent return assumption, which simplifies the missing data mechanism by decoupling the return probability from unobserved future outcomes.",
    "question": "Is the statement 'The independent return assumption implies that the probability of return at time $t$ depends only on the most recent observed outcome $Y_k$ and not on any subsequent outcomes' true based on the given formula and context?",
    "question_context": "Missing Data Mechanisms and Estimator Consistency in Longitudinal Studies\n\nCorollary. If the conditions of Theorem 2 are satisfied, then (i) $N^{-1}\\sum_{i=1}^{N}Y_{i t}^{\\mathrm{est}}$ is an unbiased estimator of $E(Y_{t})$ and (ii) if a linear regression model for $Y$ with any or all of $X$ and $t$ as covariates is fitted to the imputed dataset using least squares, the parameter estimators are asymptotically unbiased. <PARAGRAPH_SEPARATOR> Returning to Example 1, it can be shown that $N^{-1}\\sum_{i=1}^{N}Y_{i2}^{\\mathrm{est}}$ tends to 0.585 as $N\\to\\infty$ , and so is an inconsistent estimator of $E(Y_{2})=0$ . <PARAGRAPH_SEPARATOR> A&G say that Equation (7) will hold if <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{P(R_{0,t}=1|\\mathcal{R}_{0,k-1},R_{0,k}=1,R_{0,k+1}=R_{0,k+2}=...=R_{0,t-1}=0,X,Y_{k},Y_{k+1},...,Y_{t})}\\ {=P(R_{0,t}=1|\\mathcal{R}_{0,k-1},R_{0,k}=1,R_{0,k+1}=R_{0,k+2}=...=R_{0,t-1}=0,X,Y_{k})}\\end{array} $$ <PARAGRAPH_SEPARATOR> for all $k\\leq t-2$ , and imply that it is unlikely to hold otherwise. Equation (8) can be interpreted as meaning that the conditional probability of return at time $t$ given dropout immediately after time $k$ , no return between times $k$ and $t$ , the baseline covariates $X$ , the most recently observed outcome $Y_{k}$ and subsequent outcomes does not depend on these subsequent outcomes (or, more informally, as ‘return after a dropout is independent of outcomes occurring since that dropout’). We shall call Equation (8) the ‘independent return’ assumption."
  },
  {
    "qid": "statistic-posneg-279-1-0",
    "gold_answer": "FALSE. Mathematical Support: The formula k_i = δ̂_i/2 is derived from the optimality condition for the CUSUM statistic when the mean shift δ is known. However, in practice, δ is unknown and estimated as δ̂_i, which introduces estimation error. Economic Implication: While setting k_i = δ̂_i/2 aims to optimize detection for the estimated shift, the reliance on real-time estimates (δ̂_i) can lead to suboptimal performance if the estimates are inaccurate. The adjustment balances sensitivity to different shift sizes but may not achieve the theoretical optimality due to estimation uncertainty.",
    "question": "In the ACUSUM control chart, the reference value k_i is set to half the predicted mean shift δ̂_i to optimize the CUSUM statistic. Is this claim mathematically supported by the formula k_i = δ̂_i/2, and what is the economic implication of this adjustment?",
    "question_context": "Adaptive CUSUM Control Chart with Variable Sampling Intervals\n\nThe paper discusses the design and properties of an Adaptive CUSUM (ACUSUM) control chart with variable sampling intervals (VSI). The ACUSUM chart uses an EWMA operator to estimate the mean shift δ in real-time, adjusting the reference value k_i = δ̂_i/2 for optimal detection. The VSI feature allows the sampling interval to vary based on the current CUSUM statistic, improving detection efficiency. The paper presents various formulas for the CUSUM statistic, EWMA estimator, and control limits, and discusses the impact of initial parameters on the chart's performance."
  },
  {
    "qid": "statistic-posneg-1318-0-4",
    "gold_answer": "FALSE. The estimator $\\widehat{\\mathbf{R}}$ constructed using the inverse bridge functions $F^{-1}$ is not guaranteed to be positive definite. While the individual pairwise estimates $\\widehat{r}_{jk} = F^{-1}(\\widehat{\\tau}_{jk})$ are consistent for the true latent correlations, the resulting matrix $\\widehat{\\mathbf{R}}$ may not be positive definite, especially in high dimensions or with small sample sizes. Additional steps, such as eigenvalue adjustment or projection onto the space of positive definite matrices, may be required to ensure positive definiteness.",
    "question": "Is the latent correlation estimator $\\widehat{\\mathbf{R}}$ constructed using the inverse bridge functions $F^{-1}$ for all combinations of continuous, binary, and truncated variables guaranteed to be positive definite?",
    "question_context": "Latent Correlation Estimation via Bridge Functions in Mixed Gaussian Copula Models\n\nThe mixed latent Gaussian copula model jointly models $\\mathbf{W}=$ $({\\bf W}_{1},{\\bf W}_{2},{\\bf W}_{3})\\sim\\mathrm{NPN}({\\bf0},{\\pmb\\Sigma},f)$ such that $X_{1j}=W_{1j},X_{2j}=$ $I(W_{2j}>c_{2j})$ and $W_{3j}=I(W_{3j}>c_{3j})W_{3j}$ . <PARAGRAPH_SEPARATOR> The latent correlation matrix $\\pmb{\\Sigma}$ is the key parameter in Gaussian copula models. Estimation of latent correlations is achieved via the bridge function $F$ such that $\\mathbb{E}(\\widehat{\\tau}_{j k})~=~F(\\sigma_{j k})$ , where $\\sigma_{j k}$ is the latent correlation between variables $j$ and $k,$ and $\\widehat{\\tau}_{j k}$ is the corresponding sample Kendall’s $\\tau$ . Given observed $\\mathbf{x}_{j},\\dot{\\mathbf{x}_{k}}\\in\\mathbb{R}^{n}$ , <PARAGRAPH_SEPARATOR> $$ \\widehat{\\tau}_{j k}=\\widehat{\\tau}(\\mathbf{x}_{j},\\mathbf{x}_{k})=\\frac{2}{n(n-1)}\\sum_{1\\leq i<i^{\\prime}\\leq n}\\mathrm{sign}(x_{i j}-x_{i^{\\prime}j})\\mathrm{sign}(x_{i k}-x_{i^{\\prime}k}), $$ <PARAGRAPH_SEPARATOR> where $n$ is the sample size. Using $F$ , one can construct $\\widehat{\\sigma}_{j k}~=~{\\cal F}^{-1}(\\widehat{\\tau}_{j k})$ with the corresponding estimator $\\widehat{\\pmb\\Sigma}$ being consistent for $\\pmb{\\Sigma}$ (Fan et al. 2017; Quan, Booth, and Wells 2018; Yoon, Carroll, and Gaynanova 2020). The explicit form of $F$ has been derived for all combinations of continuous(C)/binary(B)/truncated(T) variables (Fan et al. 2017; Yoon, Carroll, and Gaynanova 2020). We summarize these results below, and use CC, BC, TC, etc. to denote corresponding combinations."
  },
  {
    "qid": "statistic-posneg-1284-0-4",
    "gold_answer": "TRUE. Theorem 4.2 in the paper establishes that X_{k:n} ≤_{st} Y_{k:n} if and only if λ ≤ (∑_{i=1}^{n+1-k} λ_{(i)}) / (n+1-k), where λ_{(i)} are the ordered hazard rates. This provides the best upper bound for the survival function of the kth order statistic from heterogeneous exponentials.",
    "question": "Is the survival function of the kth order statistic from heterogeneous independent exponential random variables always bounded above by the survival function of the same order statistic from i.i.d. exponential random variables with λ = (∑_{i=1}^{n+1-k} λ_{(i)}) / (n+1-k)?",
    "question_context": "Stochastic Ordering and Hazard Rate Bounds for Mixtures of Exponential Order Statistics\n\nThe paper discusses stochastic ordering and hazard rate bounds for mixtures of exponential order statistics. It presents various formulas and theorems to compare the survival and hazard rate functions of order statistics from heterogeneous independent exponential random variables with those from i.i.d. exponential random variables. Key formulas include the survival function of the kth order statistic, the hazard rate function, and conditions for stochastic and hazard rate ordering."
  },
  {
    "qid": "statistic-posneg-1426-0-1",
    "gold_answer": "FALSE. Shao (1993) actually suggests the opposite: as the sample size $n$ increases, the number of groups $V$ should decrease, and the number of observations $k$ left out in each fold should increase faster than $n$. This is contrary to the behavior of leave-one-out cross-validation, where the number of left-out observations remains fixed at 1 regardless of $n$. The rationale is that for large $n$, leaving out a larger proportion of data (increasing $k$) can lead to more stable model selection, especially in the context of variable subset selection.",
    "question": "Is it correct that V-fold cross-validation requires the number of groups V to increase as the sample size n increases, according to Shao (1993)?",
    "question_context": "Cross-Validation Methods for Tuning Parameter Estimation\n\nAlternatively, extrapolation may also be handled by defining extra B-splines outside the range of the training data as in Currie et al. (2003), but I will not use this approach in the present paper. <PARAGRAPH_SEPARATOR> # 3.5. Estimating tuning parameters by permuted leave-k-out cross-validation <PARAGRAPH_SEPARATOR> Alternative methods for choosing tuning parameters includes Akaikes information criterion, (ordinary) cross-validation or generalized cross-validation, see for instance Eilers and Marx (1996) or Koenker and Mizera (2004), but here we restrict ourselves to (ordinary) cross-validation. The most popular variant is leave-one-out cross-validation, i.e. the tuning parameters are estimated by minimizing <PARAGRAPH_SEPARATOR> $$ \\sum_{i=1}^{n}\\{y_{i}-{\\hat{\\mu}}_{(-i)}(\\mathbf{x}_{i})\\}^{2}, $$ <PARAGRAPH_SEPARATOR> where $\\hat{\\mu}_{(-i)}$ is the regression function estimated without the ith observation. In linear models, this can equivalently be calculated as <PARAGRAPH_SEPARATOR> $$ \\sum_{i=1}^{n}\\frac{\\{y_{i}-\\hat{\\mu}(\\mathbf{x}_{i})\\}^{2}}{(1-h_{i i})^{2}}, $$ <PARAGRAPH_SEPARATOR> i.e. without any need to recalculate ${\\hat{\\mu}}n$ times. Here $h_{i i}$ is the ith diagonal element of the so-called hat matrix $\\hat{\\bf H}$ (see Appendix A). This expression is used in Marx and Eilers (1998), since P-splines are linear in the B-basis. But (12) does not cover extrapolation, i.e. $\\mathbf{X}_{i}$ lying outside the range of $\\mathbf{X}_{(-i)}$ , as discussed in Section 3.4. If we assume that (11) is calculated with extrapolation performed as described in the previous section, (12) will not be exactly equivalent to (11). <PARAGRAPH_SEPARATOR> The so-called V-fold cross-validation is a common alternative to leave-one-out crossvalidation. It saves computer time, and several authors (e.g. Breiman and Spector, 1992) have noted that it often yields better results than the leave-one-out variant. The training data are divided into $\\mathrm{v}$ groups with approximately $n/V$ observations in each group, and the response values in each group are predicted by the model estimated from the remaining $(V-1)$ groups. Denote the groups by $v$ , $(v{=}1,\\ldots,V)$ ), and let the subscript $(-v)$ symbolize estimates without the $v$ th group. The criterion to be minimized is then <PARAGRAPH_SEPARATOR> $$ \\sum_{v=1}^{V}\\sum_{i\\in v}\\{y_{i}-\\hat{\\mu}_{(-v)}(\\mathbf{x}_{i})\\}^{2}. $$ <PARAGRAPH_SEPARATOR> In $V.$ -fold cross-validation about $k\\approx n/V$ observations are left out for prediction, and the term leave- $\\cdot k$ -out may also be used to emphasize that more than one observation may be left out. In the context of variable subset selection in a linear model, Shao (1993) showed that when $n\\rightarrow\\infty$ , the proportion to be left out should go to 1, the opposite of what happens in leave-one-out cross-validation! Thus $V$ should decrease when $n$ increases, whereas the number $k$ of observations to be left out should increase faster than $n$ . <PARAGRAPH_SEPARATOR> There is no unique way to divide the training data into $V$ groups. Assume that one divides the data into $V$ groups by putting the first $k$ observations in the first group, the next $k$ observations in the second group etcetera. Then, if the data were permuted, the group assignment would be different, and the cross-validated sum of squares (13) would differ. An obvious extension of leave- $\\cdot k$ -out cross-validation is then to perform this several times for various permutations of the data, and taking the average over (13). In this way all response values $y_{i}$ will be predicted by more than one estimated model, and the variance of the optimization criterion will be reduced. I will call this permuted leave- $\\cdot k$ -out cross-validation, see Shao (1993) and Pan (1999) who suggested a similar method called bootstrap-smoothed cross-validation. <PARAGRAPH_SEPARATOR> # 4. Simulation experiment"
  },
  {
    "qid": "statistic-posneg-539-1-1",
    "gold_answer": "TRUE. The context describes the CPO as the cross-validation posterior density conditional on the rest of the observations, and it is computed as the inverse of the expected value of the inverse likelihood. The provided formula is a practical estimation of this concept, where the expectation is approximated by the average over B iterations of the inverse likelihood. This matches the theoretical definition given in the context.",
    "question": "Does the formula $$ \\mathsf{C P O}_{i,j}=\\left[\\frac{1}{B}\\sum_{b=1}^{B}\\left(\\frac{1}{\\mathsf{P r}(T_{i,j}\\in(L_{i,j},R_{i,j}]|\\theta,\\omega_{i},\\mathbf{x}_{i,j})}\\right)\\right]^{-1} $$ correctly represent the CPO as described in the context?",
    "question_context": "Semiparametric Spatial Model for Interval-Censored Data: LPML and CPO Comparison\n\nTable 2 LPML comparison result in the simulation study. <PARAGRAPH_SEPARATOR> <html><body><table><tr><td></td><td>Combo 1</td><td>Combo2</td><td>Combo3</td></tr><tr><td>Model3>Model 2</td><td>84%</td><td>79%</td><td>89%</td></tr><tr><td></td><td>Combo4</td><td>Combo5</td><td>Combo6</td></tr><tr><td>Model3>Model 2</td><td>85%</td><td>70%</td><td>81%</td></tr></table></body></html> <PARAGRAPH_SEPARATOR> based on all observations: <PARAGRAPH_SEPARATOR> $$ \\mathrm{LPML}=\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}\\log\\mathrm{CPO}_{i,j}. $$ <PARAGRAPH_SEPARATOR> A larger value of LPML implies a better model. The conditional predictive ordinate (CPO) in (10) is defined as the crossvalidation posterior density conditional on the rest of the observations (Dey et al., 1997). Sinha et al. (1999) showed that a ${\\mathrm{CPO}}_{i,j}$ can be computed as <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\mathbb{C P O}_{i,j}=\\big\\{\\mathtt{E}\\left[\\operatorname*{Pr}(T_{i,j}\\in(L_{i,j},R_{i,j}]|\\theta,\\omega_{i},\\mathbf{x}_{i,j})^{-1}\\right]\\big\\}^{-1},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\mathrm{Pr}(T_{i,j}\\in(L_{i,j},R_{i,j}]|\\boldsymbol{\\mathcal{O}},\\omega_{i},\\mathbf{x}_{i,j})$ is the individual likelihood evaluated after taking the expectation of the joint posterior of $\\Theta$ and $\\omega_{i}$ . In practice, (11) can be estimated as <PARAGRAPH_SEPARATOR> $$ \\mathsf{C P O}_{i,j}=\\left[\\frac{1}{B}\\sum_{b=1}^{B}\\left(\\frac{1}{\\mathsf{P r}(T_{i,j}\\in(L_{i,j},R_{i,j}]|\\theta,\\omega_{i},\\mathbf{x}_{i,j})}\\right)\\right]^{-1}, $$ <PARAGRAPH_SEPARATOR> where index $b$ is the bth iteration and $B$ is the total number of iterations. Here we need to pay attention because, when the central limit theory is violated, one should consider an alternative leave-one-out cross-validation (LOO) (Vehtari and Gelman, 2014; Epifani et al., 2008). In this study, we use LPML for model comparison. <PARAGRAPH_SEPARATOR> Table 2 shows the LPML comparison results of Model 3 versus Model 2. The percentage of LPML difference (Model 3 − Model 2) greater than zero out of 100 replicates is reported. Between Model 3 and Model 2, the largest percentage is $89\\%$ provided by Combo 3 and the smallest percentage is $70\\%$ provided by Combo 5. The results in Table 2 indicate that Model 3 performs consistently better than Model 2. The comparison between Model 1 and the other two models is not necessary due to its obvious benefit for estimating the constant coefficient and deficiency in capturing the time-varying coefficient trend. Moreover, the LMPL of Model 1 is not comparable to the other two models since it has a different technique in modeling the baseline hazard function. <PARAGRAPH_SEPARATOR> To investigate the spatial pattern of frailties, we examined the true values in the simulation and the posterior means of frailty estimates from all three models. Fig. 2 shows the plots of Combo 6, which has both constant and time-varying coefficients. Since the plots from the six combinations are similar, we only include one here as an example. Note that Model 1 and Model 3 generate a similar spatial frailty pattern to the true values. However, Model 2 fails to capture some large frailties in the southeastern corner."
  },
  {
    "qid": "statistic-posneg-773-0-0",
    "gold_answer": "FALSE. The function C(S) is designed to reflect the signal position S around the medial plane to generate the confused position C(S). However, the condition sin(S) ≥ 0 corresponds to S ∈ [0, π], and sin(S) < 0 corresponds to S ∈ (π, 2π). For S ∈ (π, 2π), the reflection should be C(S) = 3π - S, which ensures C(S) ∈ [π, 2π]. But for S ∈ [0, π], C(S) = π - S ensures C(S) ∈ [0, π]. Therefore, the statement is false because it does not correctly describe the mapping for all S ∈ [0, 2π]. The correct mapping ensures C(S) ∈ [0, 2π] as required.",
    "question": "The function C(S) = π - S when sin(S) ≥ 0 and C(S) = 3π - S when sin(S) < 0 correctly maps the signal position S to its confused position C(S) within the range [0, 2π]. Is this statement true based on the context?",
    "question_context": "Circular Regression Model for Sound Localization Judgments\n\nSubjects were placed inside a double-walled, sound-isolated chamber. Subjects were seated facing $0^{\\circ}$ inside a battery of 24 speakers, each placed at $15^{\\circ}$ intervals, arranged in a horizontal circle around the subject. A signal was emitted from one of the 24 speakers, and the subject was then asked to identify—i.e. to point, indicating an angle in radians—where the sound came from, which was termed a ‘judgement’. All the speakers emitted three replicates of the signal, one at a time and in random order. So each subject identified three replicate sounds from each of 24 speakers, yielding 72 repeated measures. Because of time and budgetary constraints this type of experiment commonly tests relatively few subjects but elicits many responses from each subject under several test conditions. The experiment and data entry interface are illustrated in appendix B of the on-line supplementary material. <PARAGRAPH_SEPARATOR> An example of a sample of group YNH judgements for a low frequency $(250{-}500\\mathrm{Hz})$ ), narrowband signal emitted at $165^{\\circ}$ (rear left) is shown in Fig. 2(a). By design, localization judgements are on a circle and thus require statistical methods that are appropriate for the modular nature of the response. Judgements are also concentrated at two distinct modes: near the signal position (S) and near the front of the listener at 0.26 rad (C). The bimodality that is evident in Fig. 2(a) is the result of FB confusion that was described above and in Fig. 1, and is ubiquitous in horizontal localization studies. The collection of judgements against all signal positions by each subject group is shown in the remaining panels of Fig. 2. The vertical reference line in each plot separates the left-hand side $(0{-}\\pi)$ from the right-hand side $(\\pi{-}2\\pi)$ of the listener. The full line with unit slope indicates perfect correspondence between judgement and signal. The broken line corresponds to the confused position, which is the signal position reflected around the medial plane of a person (which divides a listener’s front and back) according to <PARAGRAPH_SEPARATOR> ![](images/9eb9f50fd0010122428b550ed86874ec00bbed3bce26c7a3bb1ef550af2dcdc2.jpg)  Fig. 2. Observed judgements of a low frequency narrowband noise (the listener faces in the direction of $\\ '>\\ '$ in (a)): (a) all judgements to the $165^{\\circ}$ signal; (b) group YNH; (c) group ONH; (d) group OHI <PARAGRAPH_SEPARATOR> $$ C(S)=\\left\\{\\begin{array}{l l}{{\\pi-S,}}&{{\\qquad\\sin(S)\\geqslant0,}}\\ {{3\\pi-S,}}&{{\\qquad\\sin(S)<0.}}\\end{array}\\right. $$ <PARAGRAPH_SEPARATOR> For example, a signal emitted at $\\pi/4$ has a confused position at $3\\pi/4$ , and so forth. This definition of $C(S)$ guarantees that $C(S)\\in[0,2\\pi]$ , which is necessary for the model that is proposed below. Fig. 2(b) for group YNH subjects shows that FB confusion events are relatively rare when the signal comes from the front of the listener but are more common when the signal comes from behind. Older subjects, hearing impaired or otherwise, appear to have more confusions in response to signals from the front or the rear. It is also apparent that the response distribution of group YNH subjects is more concentrated around either the signal or confused positions than for older subjects. <PARAGRAPH_SEPARATOR> Also note that the judgement distribution to the left of the vertical reference line in Fig. 2 is nearly identical to the distribution to the right of the vertical line. This suggests that judgements of signals that are emitted to the left of the listener are similar to judgements of signals that are emitted to the right of the listener. Symmetric localization performance is theoretically expected because the human head is bilaterally symmetric. Whether or not this type of symmetry actually exists is an open question. Binaural deficits, unilateral hearing impairment or reverberant listening environments might induce asymmetry. More detailed inferences about types of symmetry and differences in localization performance beyond graphical explorations are necessary, and these require statistical analysis. <PARAGRAPH_SEPARATOR> The underlying localization distribution constitutes the data-generating mechanism for the observed judgements shown in Fig. 2. The goal is to contrast formally the localization distributions between group YNH, ONH and OHI subjects over the range of signals. Contrasts and inferences are derived from a statistical model of the underlying localization distribution. Several characteristics of localization experiments, along with observations on localization data, can motivate specific models. These are as follows:"
  },
  {
    "qid": "statistic-posneg-145-1-0",
    "gold_answer": "FALSE. The context states that the expansion was used for values like N=81 and N=101, and it mentions that the method provides seven-figure accuracy over a considerably larger range of N. This indicates that the expansion is not limited to very small values of N.",
    "question": "Is the asymptotic expansion formula for $\\int_{0}^{\\theta}\\cos^{\\theta}\\theta d\\theta$ valid only for very small values of N?",
    "question_context": "Asymptotic Expansion of the Integral of Cosine to the Power of Theta\n\nA table of numerical examples, designed to show the range of the new expansion, is given below. Various methods of checking were employed. Some are the examples of the earlier paper. Others, for lower values of ${\\pmb{\\mathscr{N}}}_{:}$ were checked (a) by expansion in terms of multiple angles, or $(b)$ from the manuscript of the Tables of the Incomplete Beta-Function. It would appear that seven-figure accuracy is obtainable by a computation which is easier than the previous one, and over a considerably larger range of ${\\pmb{\\mathscr{N}}}.$ The full expansion (4) was used for $\\pmb{N}\\pmb{-9}$ Let $\\pmb{N=81}$ .In the case of $\\scriptstyle{\\pmb{N}}={\\pmb{101}}$ the term in $\\frac{1}{N^{4}}$ was dropped while for the last sample the term in $\\frac{1}{N^{5}}\\cdots$ also neglected. The standard deviation (the unit being sin 9) is given. <PARAGRAPH_SEPARATOR> It may be noted that the mathematician who desires not the probability integral, but merely $\\int_{0}^{0}\\cos^{\\nu}\\theta d\\theta_{i}$ can readily compute it from the Table by the formula <PARAGRAPH_SEPARATOR> $$ \\int_{0}^{\\theta}\\cos^{\\theta}\\theta d\\theta=\\sqrt{\\frac{2\\pi}{N}}\\left\\{\\phi_{0}\\left(z\\right)-\\frac{1}{N}\\phi_{1}\\left(z\\right)+\\frac{1}{N^{\\prime}}\\phi_{2}\\left(z\\right)-\\frac{1}{N^{3}}\\phi_{3}\\left(z\\right)+\\frac{1}{N^{\\prime}}\\phi_{4}\\left(z\\right)\\right\\}, $$ <PARAGRAPH_SEPARATOR> where $x=2\\sqrt{N}\\tan\\frac{1}{2}\\theta$"
  },
  {
    "qid": "statistic-posneg-81-1-1",
    "gold_answer": "TRUE. The likelihood ratio test statistic involves both tr(Ψ̂⁻¹Φ) and |Ψ̂⁻¹Φ|, which are the trace and determinant of the matrix product of the inverse of the null hypothesis estimator (Ψ̂) and the alternative hypothesis estimator (Φ).",
    "question": "Does the likelihood ratio test statistic involve both the trace and determinant of the matrix product of the inverse of the null hypothesis estimator and the alternative hypothesis estimator?",
    "question_context": "Likelihood Ratio Test for Spatial Pattern Reconstruction\n\nThe smoothing procedure is formulated by using the generalized linear mixed models framework. This is achieved by assuming that the spatial patterns are normal random-effect variables whose covariance matrix depends on the ‘neighbourhood structure’ of the spatial locations. The estimation of spatial and temporal patterns is carried out by using a modified non-linear iterative partial least squares algorithm.<PARAGRAPH_SEPARATOR>An aspect of the methodology that needs further development is inference. Although the spatial and temporal patterns that were shown in the previous section seem convincing, a formal statistical test to decide how many pairs of patterns are required needs to be developed. One possibility is by using a likelihood ratio test, although computationally it will be demanding. Let us assume that we want to test whether the first $k$ pairs of patterns are adequate for describing relationships between the two fields. Let<PARAGRAPH_SEPARATOR>$$z_{j}=\\bigg({\\frac{x_{j}}{y_{j}}}\\bigg),\\qquadj=1,2,\\ldots,n,$$<PARAGRAPH_SEPARATOR>where $x_{j}$ . $(p\\times1)$ and $y_{j}\\left(q\\times1\\right)$ are vectors of observations from $X$ - and $Y$ -fields at time $j$ . The covariance matrix of this vector is given by<PARAGRAPH_SEPARATOR>$$\\Psi=\\left(\\frac{\\Sigma_{x x}|\\Sigma}{\\Sigma^{\\prime}|\\Sigma_{y y}}\\right).$$<PARAGRAPH_SEPARATOR>Here, $\\Sigma_{x x}$ and $\\Sigma_{y y}$ are the within-field covariance matrices of $x_{j}$ and $y_{j}$ , and $\\Sigma$ is the crosscovariance matrix of the two fields.<PARAGRAPH_SEPARATOR>If we ‘reconstruct’ the two fields by using the first $k$ pairs only, then the covariance matrix $\\Sigma_{0}$ of the reconstructed fields is the theoretical covariance of these vectors:<PARAGRAPH_SEPARATOR>$$\\begin{array}{l}{{\\displaystyle x_{j}^{*}=\\sum_{i=1}^{k}x_{j}^{\\prime}a_{i}^{*}a_{i}^{\\prime*}\\Omega_{x},}}\\ {{\\displaystyle y_{j}^{*}=\\sum_{i=1}^{k}y_{j}^{\\prime}b_{i}^{*}b_{i}^{\\prime*}\\Omega_{y}.}}\\end{array}$$<PARAGRAPH_SEPARATOR>Our null hypothesis is $H_{0}:\\Sigma=\\Sigma_{0}$ versus $H_{1}:\\Sigma\\neq\\Sigma_{0}$ . Under the null hypothesis the maximum likelihood estimator of $\\Psi$ is given by<PARAGRAPH_SEPARATOR>$$\\hat{\\Psi}=\\Bigg(\\frac{S_{x x}^{0}\\big|\\Sigma_{0}}{\\Sigma_{0}\\big|S_{y y}^{0}\\big)}\\Bigg),$$<PARAGRAPH_SEPARATOR>where $S_{x x}^{0}$ and $S_{y y}^{0}$ are the maximum likelihood estimators of the within-field covariance matrices under the null hypothesis. The maximum likelihood estimator of $\\Psi$ under $H_{1}$ is given by<PARAGRAPH_SEPARATOR>$$\\Phi={\\left(\\frac{S_{x x}}{S^{\\prime}\\left|S_{y y}\\right.}\\right)}.$$<PARAGRAPH_SEPARATOR>The likelihood ratio test statistic involves $\\operatorname{tr}({\\hat{\\Psi}}^{-1}\\Phi)$ and $|{\\hat{\\Psi}}^{-1}\\Phi|$ (Mardia et al., 1979). The statistic is distributed as $\\chi^{2}$ with degree of freedom<PARAGRAPH_SEPARATOR>$$\\begin{array}{l}{{\\mathrm{df}=p q-(k p+k q+k+4)-k(k+1)}}\\ {{\\mathrm{~}=(p-k)(q-k)-4.}}\\end{array}$$<PARAGRAPH_SEPARATOR>Unfortunately, evaluations of $\\mathrm{tr}(\\hat{\\Psi}^{-1}S)$ and $|\\hat{\\Psi}^{-1}S|$ are computationally demanding for large data sets."
  },
  {
    "qid": "statistic-posneg-712-0-1",
    "gold_answer": "FALSE. The paper explicitly mentions that the Wolters method provides more accurate density estimates for smaller sample sizes (15 and 30), but the proposed method is superior for larger samples (100 and 500), except for densities on the $[-1,1]$ support. Thus, the Wolters method does not outperform the proposed method across all sample sizes.",
    "question": "Does the Wolters method consistently outperform the proposed method across all sample sizes, including $n=100$ and $n=500$?",
    "question_context": "Density Estimation Performance Under Contamination\n\nFigs. 1–3 display plots of the average density estimates over the 1000 MC samples for each sample size for both methods. The plots are consistent with the numeric results such that the Wolters method provides more accurate density estimates for sample sizes of 15 and 30 but our method is superior for the larger samples of 100 and 500, with the exception of the densities on the $[-1,1]$ support. Both methods adeptly estimated the location of the mode for all proposed cases. <PARAGRAPH_SEPARATOR> # 3.6. Outliers <PARAGRAPH_SEPARATOR> One concern is how well our method performs when the data is contaminated with outliers. We explore this by generating data from a $\\mathcal{N}(0,1)$ density mixed with a $\\mathcal{N}(4,0.5)$ density. We set the mixture proportion to $95\\%$ , resulting in the following normal mixture, <PARAGRAPH_SEPARATOR> $$ p\\mathcal{N}(0,1)+(1-p)\\mathcal{N}(4,0.5), $$ <PARAGRAPH_SEPARATOR> where $p\\sim$ bernoulli(0.95). <PARAGRAPH_SEPARATOR> We generate 1000 MC samples from the mixture distribution with sizes $n=15$ , 30, and 100. Fig. 4 displays the average density estimates across the 1000 MC samples. Assuming the data is truly from a $\\mathcal{N}(0,1)$ distribution, the $5\\%$ contamination does not completely compromise the density estimate. For the $n=100$ sample, the density estimate still properly locates the mode at 0 and there is only a slightly heavier tail on the side of the contaminating distribution. <PARAGRAPH_SEPARATOR> # 4. Real data examples"
  },
  {
    "qid": "statistic-posneg-1438-0-0",
    "gold_answer": "FALSE. The likelihood displacement is correctly defined as $LD(\\omega) = 2\\{L(\\widehat{\\pmb\\theta}) - L(\\widehat{\\pmb\\theta}_{\\omega})\\}$, but the question incorrectly states that it is used to find influential observations in MLEs. In reality, it measures the influence of perturbations on the parameter estimates, not directly the observations.",
    "question": "Is the likelihood displacement $LD(\\omega)$ correctly defined as $2\\{L(\\widehat{\\pmb\\theta}) - L(\\widehat{\\pmb\\theta}_{\\omega})\\}$ for identifying influential observations in MLEs?",
    "question_context": "Influence Diagnostics in Nonlinear Mixed-Effects Models\n\nThe diagnostic technique developed in Cook (1986) is a widely used tool to check the model assumptions and conduct diagnostic studies. The author proposes looking at the likelihood displacement $L D(\\omega)=2\\{L(\\widehat{\\pmb\\theta})-L(\\widehat{\\pmb\\theta}_{\\omega})\\}$ to find possible influential observations in the MLEs, where $\\begin{array}{r}{L(\\pmb{\\dot{\\theta}})=\\sum_{i}L_{i}(\\pmb{\\theta})}\\end{array}$ is the log-likelihood function abnd $\\omega$ isb an $s\\times1$ vector of perturbation restricted in an open set $\\pmb{\\Omega}\\subset\\mathbb{R}^{s}$ . A vect or with no perturbation is defined as ${\\pmb{\\omega}}_{0}\\in{\\pmb{\\Omega}}$ in which $L D(\\omega_{0})=0$ , i.e., $L(\\pmb{\\theta}_{\\omega_{0}})=L(\\pmb{\\theta})$ . In his seminal paper, Cook shows that the normal curvature at the unit direction ℓ has the following form $C_{\\ell}(\\pmb\\theta)=2|\\pmb\\ell^{\\top}\\pmb{\\varDelta}^{\\top}(\\ddot{\\pmb L}_{\\theta\\theta})^{-1}\\pmb{\\Delta}\\ell|$ , where $\\pmb{\\varDelta}=\\partial^{2}L(\\pmb{\\theta}|\\omega)/\\partial\\pmb{\\theta}\\partial\\omega^{\\top}$ , both $\\pmb{\\varDelta}$ and $\\stackrel{\\cdot\\cdot}{L}_{\\theta\\theta}$ are evaluated at ${\\pmb\\theta}=\\widehat{\\pmb\\theta}$ and $\\omega=\\omega_{0}$ . Thus, $C_{\\mathbf{{d}}_{\\mathrm{{max}}}}$ is twice the largest eigenvalue of $\\pmb{{\\cal B}}=-\\pmb{\\varDelta}^{\\top}\\ddot{\\pmb{L}}_{\\theta\\theta}^{-1}\\pmb{\\varDelta}$ and $\\pmb{d}_{\\mathrm{{max}}}$ is the corresponding eigenvector. T hbe index plot of $\\pmb{d}_{\\mathrm{max}}$ may reveal how to perturb the model (or data) to obtain large changes in the estimate of $\\pmb\\theta$ For more detailed information, we refer the reader to the work of Russo et al. (2009) and the references therein."
  },
  {
    "qid": "statistic-posneg-2079-4-1",
    "gold_answer": "TRUE. The context specifies that for $x > 0$, $[Y|X=x]$ is an exponential random variable with failure rate $\\alpha/x$. The joint density function $f_{\\alpha}(x,y)$ is constructed such that the conditional distribution of $Y$ given $X = x$ is indeed exponential with rate parameter $\\alpha/x$, as evidenced by the term $\\exp\\{-\\alpha y/x\\}$ in the joint density.",
    "question": "Does the joint density function $f_{\\alpha}(x,y) = \\frac{\\alpha^{2}}{x} \\exp\\{-\\alpha(x + y/x)\\}$ imply that $Y$ given $X = x$ follows an exponential distribution with rate parameter $\\alpha/x$?",
    "question_context": "Stochastic Ordering of Random Vectors with Exponential Conditional Distributions\n\nRegarding condition (2.4), in addition to the above examples, it is also satisfied by the Plackett copula (see [20, page 91]) with a shape parameter greater than unity (this is shown in [20, page 197]). Furthermore, Nelsen [20, Exercise 5.34 in page 205] provides conditions on Archimedean copulas that correspond to random vectors that satisfy (2.4). <PARAGRAPH_SEPARATOR> Conditions (3.1) or (3.15) are assumed throughout all the results in Section 3. So, naturally one may wonder how restrictive these conditions are. <PARAGRAPH_SEPARATOR> It turns out that examples of random vectors that satisfy (3.1) are easy to construct. For example, let $X$ be an exponential random variable with failure rate $\\alpha>0$ , and, for $x>0$ , let $[Y\\big|X=x]$ be an exponential random variable with failure rate $\\alpha/x$ . Then the joint density function of $(X,Y)$ is given by <PARAGRAPH_SEPARATOR> $$ f_{\\alpha}(x,y)={\\frac{\\alpha^{2}}{x}}\\exp\\{-\\alpha(x+y/x)\\},\\quad(x,y)\\in(0,\\infty)^{2}. $$ <PARAGRAPH_SEPARATOR> Let $(S,T)$ be another random vector with density function $f_{\\alpha^{\\prime}}$ . It is easy to see that if $\\alpha\\ge\\alpha^{\\prime}>0$ then $(X,Y)\\leq_{s s t}(S,T)$ . <PARAGRAPH_SEPARATOR> We note that constructions of the type above are useful for modeling. For example, in the reliability application in Section 4.1 the above procedure can be used to model the relationship between a lifetime of a component, and the cost that is incurred upon its failure. <PARAGRAPH_SEPARATOR> Examples of random vectors that satisfy (3.15) (or, equivalently, (3.17)) abound in the literature. For example, Karlin and Rinott [14, Theorems 2.3 and 5.2] give conditions for ordering vectors of absolute values of dependent normal random variables with respect to $\\le_{\\mathrm{lr}}$ , that is, (3.15). Belzunce et al. [7] describe circumstances under which (3.15) holds. In particular, they note that some mixed multivariate distribution functions that arise from proportional hazard models are ordered as in (3.15)."
  },
  {
    "qid": "statistic-posneg-1058-1-1",
    "gold_answer": "FALSE. The indicator function ι_S(c) enforces the identifiability constraint that the Euclidean norm of c must be 1 (not 2) and the first element of c must be non-negative. This constraint is crucial for ensuring the identifiability of the model parameters.",
    "question": "The indicator function ι_S(c) enforces the identifiability constraint that the Euclidean norm of c must be 2 and the first element of c must be non-negative. Is this statement accurate based on the provided context?",
    "question_context": "Sparse Single Index Models for Multivariate Responses: Optimization via ADMM\n\nThe paper discusses the estimation of sparse single index models for multivariate responses using penalized splines and ADMM optimization. The model involves estimating link functions and a coefficient matrix with various structural constraints (e.g., sparsity, low-rank). The optimization problem is solved using an ADMM algorithm with updates for the coefficient matrix B, dummy variables C, and Lagrange multipliers M."
  },
  {
    "qid": "statistic-posneg-2062-0-1",
    "gold_answer": "FALSE. The estimator $\\bar{\\sigma}$ is described as a pseudo-likelihood estimator, obtained by replacing $\\mu$ with $\\bar{\\mu}$ in the likelihood function and then maximizing with respect to $\\sigma$. While it is asymptotically efficient, it is not the MLE, as the MLE would require joint maximization over both $\\mu$ and $\\sigma$.",
    "question": "Does the estimator $\\bar{\\sigma}$ given by the formula $$\\bar{\\sigma}=\\frac{1}{2}\\bigg(\\Delta(\\bar{\\mu})+\\sqrt{\\Delta(\\bar{\\mu})^{2}+4(\\bar{x}-\\bar{\\mu})^{2}+4(n-1)s^{2}/n}\\bigg)$$ coincide with the MLE of $\\sigma$ in the given model?",
    "question_context": "Efficient Estimation in Location-Scale Family Models\n\nThe characterizations of Remark 1, as far as we know, are new. In fact, for a fixed value of $\\theta$ , the estimator $\\bar{\\mu}=\\bar{\\mu}(\\theta)=w(\\theta)\\bar{x}+(1-w(\\theta))\\tilde{x}$ is unbiased and asymptotically efficient but in general it does not coincide with the MLE of $\\mu$ , as will be seen in the following example. It is important to remark that $\\bar{\\mu}$ is much more simple and easy to calculate than MLE.<PARAGRAPH_SEPARATOR># 2.1. An example<PARAGRAPH_SEPARATOR>Consider the location and scale family obtained by fixing $\\theta=1$ . Now the density function is<PARAGRAPH_SEPARATOR>$$g(x;\\mu,\\sigma)=\\frac{0.762567}{\\sigma}\\mathrm{exp}\\left(-\\frac{\\left|x-\\mu\\right|}{\\sigma}-\\frac{\\left(x-\\mu\\right)^{2}}{2\\sigma^{2}}\\right),$$<PARAGRAPH_SEPARATOR>and the corresponding unbiased and asymptotically efficient estimator of the location parameter $\\mu$ will be $\\Bar{\\mu}=0.39602\\Bar{x}+0.60398\\Tilde{x}$ . This estimator does not coincide with the MLE. For instance, given the sample of size $n=7$ , 1.251, )0.446, )0.297, )0.953, 1.377, )0.158, 0.579, the MLE of the location parameter is $\\hat{\\mu}=0.01030$ and $\\bar{\\mu}=-0.01888$ .<PARAGRAPH_SEPARATOR>The parameter of scale $\\sigma$ can be estimated by replacing $\\mu$ by $\\bar{\\mu}$ in the corresponding likelihood function of the model (12) and maximizing with respect to $\\sigma$ . It gives the following simple estimator,<PARAGRAPH_SEPARATOR>$$\\bar{\\sigma}=\\frac{1}{2}\\bigg(\\Delta(\\bar{\\mu})+\\sqrt{\\Delta(\\bar{\\mu})^{2}+4(\\bar{x}-\\bar{\\mu})^{2}+4(n-1)s^{2}/n}\\bigg),$$<PARAGRAPH_SEPARATOR>where $s^{2}$ is the sample variance and $\\begin{array}{r}{\\Delta(\\bar{\\mu})=\\sum_{i=1}^{n}|x_{i}-\\bar{\\mu}|/n}\\end{array}$ . This pseudo-likelihood estimator is also asymptotically efficient, as can be d educed from Gong $\\&$ Samaniego (1981). For the above example of size $n=7$ it gives the value $\\bar{\\sigma}=1.28429$ .<PARAGRAPH_SEPARATOR>In the preceding example we have studied the model (8) where $\\theta=1$ , but in practical situations it can be difficult to choose a fixed value for $\\theta$ . In the next section, we proceed to study the model (8) as a three-parameter family of distributions.<PARAGRAPH_SEPARATOR># 3. The double truncated normal distribution"
  },
  {
    "qid": "statistic-posneg-733-0-0",
    "gold_answer": "FALSE. The standard error formula provided is incorrect for the context. The correct standard error for the estimated unaccounted population $\\hat{n}_{U}^{(\\mathrm{X})}$ should account for the propagation of error through the linear transformation $\\hat{n}_{U}^{(\\mathrm{X})}=n_{\\mathrm{total}}^{(\\mathrm{X})}\\cdot\\hat{p}_{\\mathrm{age}\\geq10}^{(\\mathrm{X})}-n_{\\mathrm{A}}^{(X)}$. The given formula only computes the standard error for the binomial proportion $\\hat{p}_{\\mathrm{age}\\geq10}^{(\\mathrm{X})}$ multiplied by $n_{\\mathrm{total}}^{(\\mathrm{X})}$, ignoring the subtraction of $n_{\\mathrm{A}}^{(X)}$. Since $n_{\\mathrm{A}}^{(X)}$ is a known constant (from table NT13), it does not contribute to the standard error. The correct standard error should be $\\mathrm{SE}\\left(\\hat{n}_{U}^{\\mathrm{(X)}}\\right)=n_{\\mathrm{total}}^{\\mathrm{(X)}}\\cdot\\sqrt{\\hat{P}_{\\mathrm{age}\\ge10}\\cdot\\left(1-\\hat{P}_{\\mathrm{age}\\ge10}\\right)/n_{\\mathrm{sample}}^{\\mathrm{(X)}}}$, where $n_{\\mathrm{sample}}^{\\mathrm{(X)}}$ is the size of the 1% microsample used to estimate $\\hat{p}_{\\mathrm{age}\\geq10}^{(\\mathrm{X})}$.",
    "question": "Is the standard error formula $\\mathrm{SE}\\left(\\hat{n}_{U}^{\\mathrm{(X)}}\\right)=\\sqrt{\\hat{P}_{\\mathrm{age}\\ge10}\\cdot\\left(1-\\hat{P}_{\\mathrm{age}\\ge10}\\right)n_{\\mathrm{total}}^{\\mathrm{(X)}}}$ correctly derived from the binomial distribution assumption for the proportion of individuals aged 10 and over?",
    "question_context": "Estimation of Unaccounted Population in Historical Census Data\n\nTo reproduce these charts using modern plotting methods, some statistical archaeology is necessary—we must reconstruct the data underlying the charts, using sources available nearly 150 years later. The National Historical Geographic Information System (NHGIS) (Minnesota Population Center 2016) provides a wealth of tabulated data, and most of the data needed to reproduce plate $\\#32$ is available directly from table NT13 on “Employed Population by Occupation by Age by $\\operatorname{Sex}^{\\gg}$ from the 1870 Census: Religious Bodies, Occupation & Government Data $(1870\\_\\mathrm{sROG})$ . Unfortunately, the size of the unemployed population is not published with this information. To get an estimate for the size of the unemployed population in each state, we make use of the $1\\%$ microsample of the 1870 census provided by the Integrated Public Use Microdata Series (IPUMS-USA) through the Minnesota Population Center (Ruggles et al. 2015). <PARAGRAPH_SEPARATOR> Using the microsample, we obtain counts of the male and female population above 10 as well as state population numbers, which allows us to get estimates of the state-level population totals for individuals above the age of 10 for each state by extrapolating from the sample proportions. This information is then used to calculate the size of the unaccommodated populations in each set of charts—either those who are not gainfully employed (plate $\\#32$ ) or those who are not accommodated religiously (plate $\\#31$ ): <PARAGRAPH_SEPARATOR> Let $n_{U}^{(\\mathrm{X)}}$ be the—unfortunately unknown—size of the unaccounted population of state $X$ . Then we have the relationship <PARAGRAPH_SEPARATOR> $$ n_{U}^{(\\mathrm{X)}}=n_{\\mathrm{age\\geq10}}^{(\\mathrm{X)}}-n_{\\mathrm{A}}^{(X)}, $$ <PARAGRAPH_SEPARATOR> where $n_{\\mathrm{age}\\geq10}^{(\\mathrm{X})}$ is the number of over 10 year olds in the state, and n(AX) is the size of the population accounted for (i.e., gainfully employed or going to school, or those who are religiously accommodated). n(AX) is known for each state from table NT13. We get an estimate for the unknown number of over 10 year olds in each state nage≥10 from: <PARAGRAPH_SEPARATOR> $$ n_{\\mathrm{age}\\geq10}^{\\mathrm{(X)}}=n_{\\mathrm{total}}^{\\mathrm{(X)}}\\cdot p_{\\mathrm{age}\\geq10}^{\\mathrm{(X)}}, $$ <PARAGRAPH_SEPARATOR> where state $X$ $\\hat{P}_{\\mathrm{age}\\geq10}^{\\mathrm{(X)}}\\in[0,1]$ who is over 10 years of age. While this proportion is also is the proportion of the population in not known, we can estimate it from the $1\\%$ microsample as the <PARAGRAPH_SEPARATOR> ![](images/92dad01adea2aecea9dc40bcd912718d9f838e6320f057e1cf113ac04702168b.jpg)  Figure 2. Plate #31 from the Statistical Atlas of 1870: church accommodation by state and denomination for the population of over 10 years of age <PARAGRAPH_SEPARATOR> ratio of individuals over 10 and the total number of individuals in the state. <PARAGRAPH_SEPARATOR> $$ \\hat{p}_{\\mathsf{a g e}\\geq10}^{\\mathrm{(X)}}=\\frac{\\sharp\\mathrm{individuals~age~10~and~over~in~state~}X}{\\sharp\\mathrm{individuals~in~state~}X}. $$ <PARAGRAPH_SEPARATOR> Piecing this result back into (1), we get both estimates for the size of the unaccounted population as well as a standard error for it <PARAGRAPH_SEPARATOR> $$ \\hat{n}_{U}^{(\\mathrm{X})}=n_{\\mathrm{total}}^{(\\mathrm{X})}\\cdot\\hat{p}_{\\mathrm{age}\\geq10}^{(\\mathrm{X})}-n_{\\mathrm{A}}^{(X)}, $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\mathrm{SE}\\left(\\hat{n}_{U}^{\\mathrm{(X)}}\\right)=\\sqrt{\\hat{P}_{\\mathrm{age}\\ge10}\\cdot\\left(1-\\hat{P}_{\\mathrm{age}\\ge10}\\right)n_{\\mathrm{total}}^{\\mathrm{(X)}}}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Estimates for the number of unaccounted women and men over 10 years of age in each state can be achieved similarly. We also <PARAGRAPH_SEPARATOR> use these results to get estimates and standard errors for the size of the religiously unaccommodated population over 10 years of age, used to recreate plate 31. <PARAGRAPH_SEPARATOR> # 2.1. Plate 32: Gender Ratio in Agriculture, Trade, Service, Manufacturing, and Schools"
  },
  {
    "qid": "statistic-posneg-742-0-0",
    "gold_answer": "FALSE. The EM algorithm for IPH distributions increases the likelihood in each iteration, but it only guarantees convergence to a local maximum, not necessarily the global maximum. The likelihood surface can have multiple local maxima, and the algorithm's convergence point depends on the initial parameter values. This is a common characteristic of EM algorithms in general, not just for IPH distributions.",
    "question": "Is it true that the EM algorithm for IPH distributions guarantees convergence to the global maximum of the likelihood function?",
    "question_context": "IPH Distribution and EM Algorithm for Parameter Estimation\n\nThe paper discusses Inhomogeneous Phase-Type (IPH) distributions, which generalize Phase-Type (PH) distributions by allowing time-inhomogeneous Markov jump processes. The key formulas include the density and distribution functions for IPH distributions, and the EM algorithm for parameter estimation in transformed PH distributions. The EM algorithm iteratively updates parameters to maximize the likelihood, with specific steps for transforming data and updating parameters."
  },
  {
    "qid": "statistic-posneg-1460-1-0",
    "gold_answer": "TRUE. The paper states in Theorem 3.2 that under assumptions A1 to A3, the weighted M-estimator converges almost surely to the true parameter $\\beta$. Assumption A1 ensures the loss function $\\rho$ is bounded and strictly increasing, A2 guarantees the weight function $w$ is non-negative and bounded with a support of positive probability, and A3 ensures the covariates are not degenerate. Together, these conditions ensure the estimator's consistency.",
    "question": "The weighted M-estimator defined by ${\\widehat{\\pmb{\\beta}}}=\\arg\\operatorname*{min}_{\\bf{b}}\\sum_{i=1}^{n}\\phi(Y_{i},{\\bf X}_{i}^{\\prime}{\\bf b})w({\\bf X}_{i})$ is consistent under the logistic regression model (3) if assumptions A1 to A3 hold. Is this statement true based on the paper's results?",
    "question_context": "Robust Testing in Logistic Regression with Weighted M-Estimators\n\nThis paper discusses robust estimators for logistic regression, focusing on weighted M-estimators that downweight outliers in the explanatory variables. The estimators are defined by minimizing a robust loss function with a bias correction term. The asymptotic properties of these estimators are derived, including consistency and normality, and a Wald-type test statistic is proposed for hypothesis testing."
  },
  {
    "qid": "statistic-posneg-1336-0-0",
    "gold_answer": "TRUE. The context explicitly states that under heavy-tailed error distributions like Cauchy, FLL has better performance in terms of RMSE and variable selection compared to FGL. Specifically, FGL tends to select more insignificant variables (higher false positives) and fails to select nonzero predictors in the case of Cauchy errors, leading to much larger RMSEs. FLL, on the other hand, demonstrates robustness to heavy-tailed errors.",
    "question": "Is the claim that the functional LAD-Lasso method (FLL) outperforms the functional group lasso method (FGL) in terms of RMSE and variable selection under heavy-tailed error distributions (e.g., Cauchy) supported by the simulation results described in the context?",
    "question_context": "Robust Shrinkage Estimation and Selection for Functional Data\n\nTo show the robustness of our proposal, we compare the results under square loss function to those of our proposal which uses LAD loss. In our simulations, ‘‘L.’’ and ‘‘G.’’ denote the proposed functional LAD-Lasso method (FLL) in this paper and the method using least square loss function with group lasso penalty (FGL), respectively. <PARAGRAPH_SEPARATOR> Taking into account computation efficiency of CPV-SIC and its good performance in Example 1 as shown in Section 4.2, we adopt the CPV-SIC criterion to select the tuning parameters for both FLL and FGL. That is, both methods select $L_{j}^{\\prime}s$ by CPV criterion. Simulation results for other criteria of tuning parameter selection are similar and omitted for conciseness. <PARAGRAPH_SEPARATOR> Denoted by $\\mathbf{L}_{c p v}$ the L selected by CPV method. Given $\\mathbf{L}_{c p v}$ , SIC criterion for square loss function is defined as follows (He et al., 2002; Huang et al., 2014) <PARAGRAPH_SEPARATOR> $$ \\mathrm{SIC}(\\lambda)=\\ln\\left(\\frac{1}{2n}\\sum_{i=1}^{n}\\Bigl(Y_{i}-\\sum_{j=1}^{p}\\sum_{k=1}^{L_{j,c p v}}\\hat{\\xi}_{i,j k}\\hat{v}_{j k}\\Bigr)^{2}\\right)+\\frac{\\ln n}{2n}(m_{\\lambda}+1), $$ <PARAGRAPH_SEPARATOR> where $\\begin{array}{r}{m_{\\lambda}=\\sum_{j:\\tilde{\\upsilon}_{\\lambda,j}\\neq0}L_{j,c p\\nu}}\\end{array}$ with $\\tilde{\\boldsymbol{v}}_{\\lambda,j}=(\\hat{\\boldsymbol{v}}_{j k},1\\le k\\le L_{j,c p\\nu})^{T}$ <PARAGRAPH_SEPARATOR> We first c ompare the RMSE of FLL and FGL. By Table 7, the RMSEs of FGL are slightly smaller than FLL under normal distribution. However, as the distribution tail of error term becomes heavy, the FLL will have better performance in terms of RMSE. Now we turn to the variable selection of these two methods. Clearly, both methods can select the nonzero variables, but the FGL is opt to select more insignificant variables into the model, which results in much larger value in false positive (FP). Particularly, when the error is from Cauchy distribution, which is heavy tailed, we see from Table 7 that FLL is much better than FGL in terms of both RMSE and variable selection. It demonstrates FLL is insensitive to heavy-tailed error. Table 8 presents the simulation results for Example 2. With normal errors, FGL is slightly better than FLL. When the error follows $t_{3}$ whose tail is heavier than normal, FLL outperforms FGL in terms of RMSE and slightly worse than FGL in terms of FP. In case of Cauchy error, FGL cannot select nonzero predictors, and consequently RMSEs are much large. Note that the penalty term in (16) involves a term $m_{\\lambda}+1$ , where $m_{\\lambda}$ is the total number of FPCs selected. Compared with Example 1, more FPCs (about 10) are selected in Example 2. Thus compared to Example 1, the penalty term in Example 2 is much larger, and FGL tends to select less variables into the model. <PARAGRAPH_SEPARATOR> # 4.4. Real data"
  },
  {
    "qid": "statistic-posneg-1140-0-2",
    "gold_answer": "TRUE. The integer partition $\\operatorname{IP}5(5)=2+2+1$ corresponds to the sum of the number of elements in each column of the shape 5. In Young tableaux, the shape is often described by an integer partition where each term represents the number of cells in the corresponding column. For shape 5, the partition 2+2+1 indicates that the first column has 2 cells, the second column has 2 cells, and the third column has 1 cell, which is consistent with the description provided in the context.",
    "question": "Does the integer partition $\\operatorname{IP}5(5)=2+2+1$ correctly represent the sum of the number of elements by column in the shape 5?",
    "question_context": "Standard Young Tableaux and Permutation Analysis\n\nShape 1 corresponds to the permutation $\\pi(1)\\mathit{\\Theta}=5\\mathit{\\Theta}$ , $\\pi(2)=4$ , $\\pi(3)=3$ , $\\pi(4)\\:=\\:2$ , $\\pi(5)=1$ $\\mathit{l d}_{5}=5\\$ and it is associated to the integer partition of $n=5$ given by $\\operatorname{IP}1(5)=5$ (the sum of the number of elements in the first column of the shape 1). The shape 5 is associated to the integer partition of $n=5$ , $\\begin{array}{r}{\\operatorname{IP}5(5)=2+2+1.}\\end{array}$ , where each term of IP5(5) (from left to right) is the sum of the number of elements by column in the shape 5.<PARAGRAPH_SEPARATOR>Given a permutation $\\pi$ , the size of the longest increasing subsequence for $\\pi$ is the size of the first row in the shape of the Tableau corresponding to the permutation. The next results allow to compute the number of permutations of $n$ numbers such that $l_{n}(\\pi)=k$ which is the number of standard Young tableaux with a shape such that the first row has size $k$ .<PARAGRAPH_SEPARATOR>Theorem A.1 (Frame et al. [6]). Given a shape W , the number of standard Young tableaux with shape W , containing the integers $\\left\\{1,\\ldots,n\\right\\}$ is<PARAGRAPH_SEPARATOR>$$N(W)={\\frac{n!}{\\displaystyle\\prod_{j=1}^{n}h_{j}}},$$<PARAGRAPH_SEPARATOR>where the $h_{j},j=1,\\ldots,n$ are the hook numbers for each cell of the Tableau.<PARAGRAPH_SEPARATOR>Table 10 Empirical power at level $\\alpha=0.01$ for distribution D2. For each case stand out the best power, in bold letter."
  },
  {
    "qid": "statistic-posneg-882-0-1",
    "gold_answer": "FALSE. While the almost sure convergence to finite limits suggests that the elements of $\\nabla^{2}A(d^{*})$ are bounded in the limit, it does not directly imply that the random Hessian matrix is bounded in probability ($O_{p}(1)$) for finite sample sizes. The almost sure convergence is a stronger condition than boundedness in probability, but the claim requires additional justification for finite samples, such as uniform integrability or moment conditions, to ensure that the elements do not exhibit extreme behavior with non-negligible probability.",
    "question": "Is the claim that the random Hessian matrix $\\nabla^{2}A(d^{*})$ is bounded in probability, i.e., each element is $O_{p}(1)$, supported by the fact that the elements of $\\nabla^{2}A(d^{*})$ converge almost surely to finite limits as shown in the formula $[\\nabla^{2}F(d^{u})]_{t-1,j-1}\\xrightarrow{\\scriptscriptstyle{\\mathrm{a.s.}}}\\frac{-(E_{\\pi_{1}}v)(E_{\\pi_{1}}u)}{\\big(\\sum_{l=1}^{k}d_{l}E_{\\pi_{1}}u\\big)^{2}}$?",
    "question_context": "Asymptotic Normality in Importance Sampling with Multiple Markov Chains\n\nIn this article, we have shown that if $\\Phi_{1},\\ldots,\\Phi_{k}$ are $k$ geometrically ergodic Markov chains with invariant densities $\\pi_{h_{1}},\\ldots,\\pi_{h_{k}}$ , respectively, and if the density $\\pi_{h}$ is similar to at least one of $\\pi_{h_{1}},\\ldots,\\pi_{h_{k}}$ , then the scope of the $k$ MCMC algorithms can be extended to the honest exploration of $\\pi_{h}$ . The methodology can be used in two ways. On occasion, $\\pi_{h}$ has a structure that is fundamentally different from that of $\\pi_{h_{1}},\\ldots,\\pi_{h_{k}}$ , and our approach enables us to circumvent the construction and analysis of a new MCMC algorithm; this is the case for the application in Section 3.1, in which $k=1$ . In other situations, all the $\\pi_{h}$ ’s come from the same parametric family, and the issue is not the need to develop new MCMC algorithms, but rather how to handle many $\\pi_{h}$ ’s simultaneously; this is the case for the application in Section 4.1."
  },
  {
    "qid": "statistic-posneg-1269-0-2",
    "gold_answer": "TRUE. A J-shaped beta distribution (with $y < 1$) has an infinite ordinate at $p=1$, indicating a high concentration of households with near-certain infection probability. This matches the observed excess of chains where both susceptibles succumbed immediately (type $1\\rightarrow2\\rightarrow0$ in Table 1). The model captures this heterogeneity by allowing some households to have extremely high transmission rates.",
    "question": "Is the J-shaped beta distribution (with $y < 1$) consistent with the observed excess of households where both susceptibles were immediately infected?",
    "question_context": "Beta-Distributed Infection Probability in Household Epidemics\n\nWe will now consider households of three individuals, containing two susceptibles besides the primary case. Let us suppose that the probability of a susceptible contracting the disease on exposure to an infectious case is ${\\pmb{\\mathscr{P}}}={\\pmb{1}}-{\\pmb q}.$ . Then the frst generation of secondary cases, following the primary case, will take the values 0, 1 or 2, with frequencies $q^{2}$ 2pq and $\\pmb{p^{2}}$ . For the values 0 and 2 the epidemic within the household comes to an end; in the first case there is no further source of infection, and in the second there are no more susceptibles. On the other hand, a single secondary case may be followed in the second generation by 0 or 1 tertiary case, with frequencies $\\pmb q$ and $\\pmb{\\mathscr{p}}$ . The expected and observed numbers of households falling into the four classes are shown in Table 1, which also includes some suitable material taken from the Providence measles experience already referred to (Wilson et al. 1939), together with an obvious notation for the different kinds of chain. Wilson et al. analysed their material in several different ways, both including and excluding children under 7 months or over 10 years old. Moreover, some of their tables are really relevant to families containing more than three children. We have decided for the present purpose to confine ourselves to the data on households of three, containing just two further susceptibles, apart from the primary case, at risk between the ages of 7 months and 10 years. This seemed to be the most reasonable way of selecting what might be expected to be a fairly homogeneous group of data. In analysing their data Wilson et al. estimated the parameter $\\pmb{\\mathcal{P}}$ by equating the expected size of the total epidemic to the observed value. This is not necessarily very efficient. They then examined the fit to the four classes shown in Table 1. A more satisfactory procedure is to estimate $\\pmb{\\mathcal{P}}$ by maximum likelihood using all four classes shown in the table. It is easy to see that this efficient estimate is\n\n<PARAGRAPH_SEPARATOR>\n\nTable 1. Greenwood chain for households of three\n\n<PARAGRAPH_SEPARATOR>\n\n<html><body><table><tr><td>Type of chain</td><td>Expected no.of households</td><td>Observed no.of households</td><td>Providence measles data (see text)</td><td>Fitted values</td></tr><tr><td>1→0 1→1→0 1→1→1-→0 1→2→0 Total</td><td>2npq2 2np²g np8 n</td><td>D 6 C d n</td><td>34 25 36 239 334</td><td>14·9 23.5 87.7 207.9 334·0</td></tr></table></body></html>\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\widehat{\\pmb{\\mathscr{p}}}=(b+2c+2d)/(2a+3b+3c+2d),\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nwith large sample variance\n\n<PARAGRAPH_SEPARATOR>\n\nApplying formula (1) to the data shown in Table 1, we find ${\\pmb{\\mathscr{p}}}={\\pmb{0}}{\\cdot}{\\pmb{789}}$ . The corresponding expectations appear in the right-hand column. The goodness-of-fit $x^{2}$ is 59·8, which for 2 degrees of freedom gives $P<0{\\cdot}001$ . Thus the Greenwood model, which for groups of three is unaffected by the modification suggested by Lidwell $\\&$ Sommerville,is quite inappropriate as it stands. Comparing the actual observations with their expectations in Table I shows that there are more families than expected, in which the epidemic either does not go beyond the primary case or else involves the remaining two susceptibles immediately. This fits in with Greenwood's (1949) suggestion that the chance of infection may vary from family to family. Suppose we now assume that this source of variation in $\\pmb{\\mathcal{P}}$ between households is represented by the $\\beta$ distribution\n\n<PARAGRAPH_SEPARATOR>\n\n$$\nd F=\\frac{1}{B(x,y)}p^{x-1}(1-p)^{y-1}d p,\\quad(0\\leqslant p\\leqslant1).\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nAll we have to do is to average the expectations for each kind of chain over all households and exhibit them as functions of $\\pmb{x}$ and ${\\pmb y}.$ . As the individual frequencies in Greenwood or Lidwell chains can always be represented in terms of quantities like $\\pmb{p}^{\\pmb{\\mathscr{n}}}\\pmb{q}^{\\pmb{\\mathscr{s}}}$ it isconvenient to work out the expectation of this quantity. We have\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\begin{array}{c}{{E\\displaystyle{p^{r}q^{s}}=\\int_{0}^{1}p^{x+r-1}q^{y+s-1}d p/B(x,y)}}\\ {{=B(x+r,y+s)/B(x,y).}}\\end{array}\n$$\n\n<PARAGRAPH_SEPARATOR>\n\n# Therefore\n\n<PARAGRAPH_SEPARATOR>\n\n$$\nE{p^{r}q^{s}}=x(x+1)\\dots(x+r-1)y(y+1)\\dots(y+s-1)/(x+y)(x+y+1)\\dots(x+y+r+s-1).\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nIn particular\n\n<PARAGRAPH_SEPARATOR>\n\nand\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\begin{array}{l}{{E{p}^{r}}=x(x+1)\\ldots(x+r-1)/(x+y)(x+y+1)\\ldots(x+y+r-1),}\\ {{E q}^{s}=y(y+1)\\ldots(y+s-1)/(x+y)(x+y+1)\\ldots(x+y+s-1).}\\end{array}\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nIf we apply these results to the simple expectations shown in Table 1 we obtain the derived set of values set out in Table 2, which we shall proceed to analyse by the familiar technique of maximum-likelihood scoring. The maximum-likelihood scores for $\\pmb{\\mathscr{x}}$ and $\\pmb{y}$ areclearly\n\n<PARAGRAPH_SEPARATOR>\n\nand\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\begin{array}{l}{\\displaystyle\\mathcal{S}_{x}=\\left(\\frac{b+c+d}{x}+\\frac{c+d}{x+1}\\right)-\\left(\\frac{n}{x+y}+\\frac{n}{x+y+1}+\\frac{b+c}{x+y+2}\\right),}\\ {\\displaystyle\\mathcal{S}_{y}=\\left(\\frac{a+b+c}{y}+\\frac{a+b}{y+1}\\right)-\\left(\\frac{n}{x+y}+\\frac{n}{x+y+1}+\\frac{b+c}{x+y+2}\\right).}\\end{array}\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nTable 2. Modifed chain for household of three with variable $\\pmb{\\mathscr{p}}$\n\n<PARAGRAPH_SEPARATOR>\n\n<html><body><table><tr><td>Typeofchain</td><td>Expected no. of households</td><td>Observed no.of households</td><td>Providence measles data (see text)</td><td>Fitted values</td></tr><tr><td>1→0 0←←l 1→1→1→0 1→2→0 Total</td><td>ny(y+ 1)/(x+y)(x+y+ 1) 2nxy(y+1)/(x+y)(x+y+1)(x+y+2) 2nyx(x+1)/(x+y)(x+y+1)(x+y+2) nc(x+1)/(x+y)(x+y+1) n</td><td>C d n</td><td>34 25 36 239 334</td><td>34·9 22.7 37.6 238.8 334·0</td></tr></table></body></html>\n\n<PARAGRAPH_SEPARATOR>\n\nThe expected amounts of information are\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\begin{array}{r l}&{I_{x x}=\\cfrac{n(x+2y+1)}{x(x+y)(x+y+1)}+\\cfrac{n x(x+3y+2)}{(x+1)(x+y)(x+y+1)(x+y+2)}-J,}\\ &{I_{x y}=\\cfrac{1}{\\cfrac{\\alpha}{y(x+y)(x+y+1)}+\\cfrac{n y(3x+y+2)}{(y+1)(x+y)(x+y+1)(x+y+2)}}J,}\\ &{J=\\cfrac{n}{(x+y)^{2}}+\\cfrac{n}{(x+y+1)^{2}}+\\cfrac{2n x y}{(x+y)(x+y+1)(x+y+2)^{2}}.}\\end{array}\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nwhere\n\n<PARAGRAPH_SEPARATOR>\n\nIn many cases it will be easier to use the observed amounts of information, which can be derived from the three different expressions in brackets making up the two scores in (7) simply by replacing the denominators by their squares.\n\n<PARAGRAPH_SEPARATOR>\n\nAn easy way to obtain two rough estimates of $\\pmb{x}$ and $\\pmb{y}$ to commence the iterative process is to amalgamate the second and third classes in Table 2, and then equate the observations to their expectations. This gives\n\n<PARAGRAPH_SEPARATOR>\n\nand\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\begin{array}{r l}&{\\check{x}=(b+c)(b+c+2d)/\\{4a d-(b+c)^{2}\\},\\bigr]}\\ &{\\check{y}=(b+c)(2a+b+c)/\\{4a d-(b+c)^{2}\\}.\\bigr]}\\end{array}\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nIf we now use these results on the Providence measles data, we obtain first the initial estimates from equation (9), giving\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\breve{x}=1{\\cdot}142\\quad\\mathrm{and}\\quad\\breve{y}=0{\\cdot}273,\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nfor which values the scores are $S_{x}=-1{\\cdot}1253$ and $\\mathcal{S}_{y}=+2{\\cdot}1255$ . The information and covariance matrices are respectively\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\{I\\}=\\left[\\begin{array}{c c}{63\\cdot8027}&{-226\\cdot696}\\ {-226\\cdot696}&{1075\\cdot635}\\end{array}\\right]\\enspace\\enspace\\mathrm{and}\\enspace\\enspace\\{V\\}=10^{-2}\\times\\left[\\begin{array}{c c}{6\\cdot2401}&{+1\\cdot3151}\\ {+1\\cdot3151}&{0\\cdot3701}\\end{array}\\right].\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nThe adjustments to be made to $\\check{\\pmb{x}}$ and $\\check{\\pmb{y}}$ are therefore -- 0-042 and - 0.007 giving 1·100 and 0.266, respectively. A further stage of iteration makes a slight change to\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\hat{x}_{3}=1{\\cdot}092\\pm0{\\cdot}250\\quad\\mathrm{~and~}\\quad\\hat{y}_{3}=0{\\cdot}264\\pm0{\\cdot}061,\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nwhere we have followed the estimates by their standard errors derived from the covariance matrix, and the subscripts refer to the size of the household. The expected values are given in the right-hand column of Table 2. The fit to the data is now very good indeed, the $x^{2}$ with one degree of freedom being about 0·32. It is interesting to observe that since $\\pmb{\\mathscr{y}}$ is less than unity-the $\\beta$ -distribution is J-shaped with an infinite ordinate at $\\pmb{\\mathscr{p}}=\\mathbf{1}$ . This is in keeping with the excess of families observed in Table 1, in which both susceptibles succumbed immediately to infection by the primary case."
  },
  {
    "qid": "statistic-posneg-242-0-1",
    "gold_answer": "FALSE. The inverse regression equations are derived by inverting the coefficient matrix of the direct regression equations. However, the provided inverse equations X1=2.1311y1−1.1829y2 and X2=−0.6070y1+0.5484y2 are not the correct inverses of the direct equations. The correct inverse should be derived by solving the system of equations Y1=1.2166x1+2.6240x2 and Y2=1.3465x1+4.7276x2 for x1 and x2, which would yield different coefficients. The provided inverse equations are inconsistent with the direct equations.",
    "question": "Is the inverse regression equation X1=2.1311y1−1.1829y2 correctly derived from the direct regression equations Y1=1.2166x1+2.6240x2 and Y2=1.3465x1+4.7276x2?",
    "question_context": "Inverse Regression Estimation in Simultaneous Equations\n\nFisher et al. (1955) fitted quadratic regression equations to their data, but as we found that the quadratic terms were significant only at the 5% level for optical densities at 560mμ(y2), we have ignored these terms and fitted only linear regressions. The analyses of variance and covariance of y1 and y2 are shown in Table 1, and the B, T and v matrices and their inverses in Table 2. Thus we see from Table 2 that the direct regression equations are Y1=1.2166x1+2.6240x2, Y2=1.3465x1+4.7276x2 and the inverse equations are X1=2.1311y1-1.1829y2, X2=-0.6070y1+0.5484y2 in agreement with the results of Fisher et al. (1955). The direct equations are less useful than the inverse ones. Since in this example the numbers of dependent and independent variables are equal, no test for consistency is possible, but we can derive fiducial limits for the values of x1 and x2 corresponding to observed values y1 and y2. Table 3. Matrix products required in estimating variances M-1=B-1T-1B'-1, 10%(Q-1 =B'-1VB-1). Since the inverse regression coefficients, as well as the direct coefficients, are likely to be well determined, we may calculate their approximate standard errors. Table 3 gives the matrices B−1T−1B′−1 and B′−1=B−1 required in these calculations. Then, for example, the variance of ∇b21 is obtained using the second diagonal term of B−1T−1B′−1 and the first diagonal term of B′−1VB−1, 9.183×10−6×8699/26=0.003072, so that the standard error of ∇b21 is 0·055. The standard errors of the coefficients may be set [0.091 0.034; 0.055 0.021]. For general purposes, of course, the covariances as well as the variances of the regression coefficients will be of interest. In determining the approximate variance of an estimate Xi, since the regression is through the origin rather than the point of means, the actual values of the yj rather than departures from means are used, and the term 1/n is omitted from the variance estimates, in equation (19). Thus, approximately, V(X1)=10−6×8699/26(1+24.99y12−30.06y1y2+9.18y22) =10−6×8699/26(1+4.396X12−2.637X1X2+4.396X22) with similar results for cov (X1,X2) and V(X2)."
  },
  {
    "qid": "statistic-posneg-1430-4-2",
    "gold_answer": "FALSE. The upper bound $\\beta_{n}\\leqslant(1+o(1))[\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c})]$ is derived under the condition that $f$ satisfies either (3.9) or (3.10), which are not restricted to densities with compact support. The examples provided in the context include both compactly supported and non-compactly supported densities.",
    "question": "Is the upper bound for $\\beta_{n}$ given by Theorem 3.7 valid only when $f$ has compact support?",
    "question_context": "Expected Number of Nonempty Bins Analysis\n\nCorollary 3.6. If $\\beta_{n}=o(n)$ , then $n\\delta_{n}^{d}\\rightarrow\\infty$ . <PARAGRAPH_SEPARATOR> Proof. Let $\\beta_{n}=o(n)$ . Then, by Theorem 3.2, both $(n\\delta_{n}^{d})^{-1}\\lambda(A_{n})\\to0$ , and $\\lambda_{f}(A_{n}^{c})\\to0$ . Suppose that $n\\delta_{n}^{d}\\not\\to\\infty$ . Then there is a $C>0$ such that for a subsequence (still denoted by $n\\:\\delta_{n}^{d},$ ) we have $n\\delta_{n}^{d}\\leqslant C$ . It follows that $\\lambda(A_{n})\\to0$ , and, by the absolute continuity of the measure $\\lambda_{f}$ , also $\\lambda_{f}(A_{n})$ $\\rightarrow0$ . But then $1=\\lambda_{f}(A_{n})+\\lambda_{f}(A_{n}^{c})\\to0$ , a contradiction. K <PARAGRAPH_SEPARATOR> It is possible to prove an upper bound of the same type as the lower bound formula (3.4) provided that $f$ satisfies a regularity condition. Let $B=\\left\\{x\\in\\mathbb{R}^{d}\\vert\\Vert x\\Vert\\leqslant1\\right\\}$ be the closed unit ball and denote $\\dot{\\rho_{n}}=\\sqrt{d}\\delta_{n}$ . Also, for two sets $S$ and $T.$ , let $S+T=\\left\\{x+y\\mid x\\in S,y\\in T\\right\\}$ . For our upper bound result we need $f$ to satisfy either <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\frac{\\delta_{n}^{-d}\\lambda((A_{n}+\\rho_{n}B)\\cap A_{n}^{c})}{\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c})}{=}0, $$ <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\frac{n\\lambda_{f}((A_{n}^{c}+\\rho_{n}B)\\cap A_{n})}{\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c})}{=}0. $$ <PARAGRAPH_SEPARATOR> Note that the denominator $\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c})$ is always positive: if $\\lambda(A_{n})=0$ , then $\\lambda_{f}(A_{n}^{c})=\\int_{A_{n}^{c}}f=\\int_{\\mathbb{R}^{d}}f=1$ . It turns out that the densities often used in practice actually satisfy a stronger condition such as (cf. (3.9)) <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\frac{\\lambda((A_{n}+\\rho_{n}B)\\cap A_{n}^{c})}{\\lambda(A_{n})}=0. $$ <PARAGRAPH_SEPARATOR> We give examples in the next subsection. The set $(A_{n}+\\rho_{n}B)\\cap A_{n}^{c}$ is a \\`\\`shell'' of thickness $\\rho_{n}$ around the set $A_{n}$ . The condition (3.11) means that, compared to the volume of $A_{n}$ itself, the volume of the shell should become negligible as $n\\to\\infty$ . <PARAGRAPH_SEPARATOR> Theorem 3.7. $I f f$ satisfies either (3.9) or (3.10), then <PARAGRAPH_SEPARATOR> $$ \\beta_{n}\\leqslant(1+o(1))[\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c})]. $$ <PARAGRAPH_SEPARATOR> Proof. Suppose that the condition (3.9) holds. First we find an upper bound for $1-(1-\\lambda_{f}(B_{n\\mu}))^{n}$ . Trivially, $1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\leqslant1$ , for all $n$ and $\\mu$ . Also, for all $n$ and $\\mu$ , <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{1-(1-\\lambda_{f}(B_{n\\mu}))^{n}=\\lambda_{f}(B_{n\\mu})\\displaystyle\\frac{1-(1-\\lambda_{f}(B_{n\\mu}))^{n}}{1-(1-\\lambda_{f}(B_{n\\mu}))}}} {{}} {{=\\lambda_{f}(B_{n\\mu})\\displaystyle\\sum_{k=0}^{n-1}(1-\\lambda_{f}(B_{n\\mu}))^{k}\\leqslant n\\lambda_{f}(B_{n\\mu}),}}\\end{array} $$ <PARAGRAPH_SEPARATOR> because $(1-\\lambda_{f}(B_{n\\mu}))^{k}\\leqslant1$ for all $k$ . Thus, <PARAGRAPH_SEPARATOR> $$ 1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\leqslant1\\land(n\\lambda_{f}(B_{n\\mu})) $$ <PARAGRAPH_SEPARATOR> for all $n$ and $\\mu$ . Define <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{R_{n}=\\big\\{\\mu\\in\\mathbb{Z}^{d}|B_{n\\mu}\\subset A_{n}\\big\\},} &{} &{S_{n}=\\big\\{\\mu\\in\\mathbb{Z}^{d}|B_{n\\mu}\\subset A_{n}^{c}\\big\\},\\qquadT_{n}=(R_{n}\\cup S_{n})^{c}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Then $R_{n},S_{n},T_{n}$ are disjoint and $\\mathbb{Z}^{d}=R_{n}\\cup S_{n}\\cup T_{n}$ . It follows from (3.13) that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{\\lefteqn{\\beta_{n}=\\sum_{\\mu}\\big[1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\big]}} &{=\\displaystyle{\\sum_{\\mu\\in\\mathcal{R}_{n}\\cup T_{n}}\\big[1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\big]+\\sum_{\\mu\\in S_{n}}\\big[1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\big]}} &{\\leqslant|R_{n}\\cap T_{n}|+n\\displaystyle{\\sum_{\\mu\\in S_{n}}\\lambda_{f}(B_{n\\mu})}} &{=\\delta_{n}^{-d}\\displaystyle{\\sum_{\\mu\\in\\mathcal{R}_{n}\\cup T_{n}}\\lambda(B_{n\\mu})+n\\displaystyle{\\sum_{\\mu\\in S_{n}}\\lambda_{f}(B_{n\\mu})}}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Here <PARAGRAPH_SEPARATOR> $$ \\sum_{\\mu\\in{\\cal R}_{n}\\cup{\\cal T}_{n}}\\lambda(B_{n\\mu})=\\lambda(A_{n})+\\sum_{\\mu\\in{\\cal T}_{n}}\\lambda(B_{n\\mu}\\cap A_{n}^{c}) $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\sum_{\\mu\\in S_{n}}\\lambda_{f}(B_{n\\mu})\\leqslant\\lambda_{f}(A_{n}^{c}). $$ <PARAGRAPH_SEPARATOR> Thus, by the estimate (3.14), <PARAGRAPH_SEPARATOR> $$ \\beta_{n}\\leqslant\\delta_{n}^{-d}\\lambda(A_{n})+\\delta_{n}^{-d}\\sum_{\\mu\\in{\\cal T}_{n}}\\lambda(B_{n\\mu}\\cap A_{n}^{c})+n\\lambda_{f}(A_{n}^{c}). $$ <PARAGRAPH_SEPARATOR> Let $\\mu\\in T_{n}$ and suppose that $y\\in B_{n\\mu}\\cap A_{n}^{c}$ . Since $B_{n\\mu}\\cap A_{n}\\neq\\emptyset$ , we can pick an $x\\in B_{n\\mu}\\cap A_{n}$ and write $y=x+(y-x)$ with $\\|y-x\\|\\leqslant\\sqrt{d}\\delta_{n}\\overset{\\cdot}{=}\\rho_{n}$ . Therefore, $B_{n\\mu}\\cap A_{n}^{c}\\subset(A_{n}+\\rho_{n}B)\\cap A_{n}^{c}$ , where $B$ is the closed unit ball. We then have <PARAGRAPH_SEPARATOR> $$ \\sum_{\\mu\\in T_{n}}\\lambda(B_{n\\mu}\\cap A_{n}^{c})\\leqslant\\lambda((A_{n}+\\rho_{n}B)\\cap A_{n}^{c}) $$ <PARAGRAPH_SEPARATOR> which together with (3.15) and the condition (3.9) implies (3.12). <PARAGRAPH_SEPARATOR> The proof is analogous for the condition (3.10). Only now the split of (3.14) in the $\\mu$ -sum is done into the sets $R_{n}$ and $S_{n}\\cup T_{n}$ . K <PARAGRAPH_SEPARATOR> # 3.2. Examples <PARAGRAPH_SEPARATOR> We will present examples of densities that satisfy (3.11) and also give some concrete upper and lower bounds for $\\beta_{n}$ . In all examples we assume that $n\\delta_{n}^{d}\\to\\infty$ (cf. Corollary 3.6) <PARAGRAPH_SEPARATOR> Example 1. Let $f$ have compact support, that is, pos $f$ is bounded. We have <PARAGRAPH_SEPARATOR> $$ \\frac{\\lambda((A_{n}+\\rho_{n}B)\\cap A_{n}^{c})}{\\lambda(A_{n})}\\leqslant\\frac{\\lambda((\\mathrm{pos}f+\\rho_{n}B)\\cap A_{n}^{c})}{\\lambda(A_{n})}. $$ <PARAGRAPH_SEPARATOR> On the other hand, it is easy to see that <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\lambda((\\mathrm{pos}f+\\rho_{n}B)\\cap A_{n}^{c})=\\lambda(\\overline{{\\mathrm{pos}f}}\\setminus\\mathrm{pos}f). $$ <PARAGRAPH_SEPARATOR> Also, <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\lambda(A_{n})=\\lambda(\\operatorname{pos}f). $$ <PARAGRAPH_SEPARATOR> It follows that (3.11) holds if $\\lambda(\\overline{{\\mathrm{pos}f}}\\backslash\\mathrm{pos}f)=0$ , a condition satisfied by all reasonable densities. Now, by Theorem 3.7, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\beta_{n}\\leqslant(1+o(1))[\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c})]} &{\\quad=(1+o(1))[\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c}\\cap\\mathrm{pos}f)]} &{\\quad\\leqslant(1+o(1))\\delta_{n}^{-d}[\\lambda(A_{n})+\\lambda(A_{n}^{c}\\cap\\mathrm{pos}f)]=(1+o(1))\\delta_{n}^{-d}(\\mathrm{pos}f),}\\end{array} $$ <PARAGRAPH_SEPARATOR> where the second inequality follows from (ii) of Lemma 3.3. Combining this with Theorem 3.2 and using (3.16) we get <PARAGRAPH_SEPARATOR> $$ (1+o(1))(1-e^{-1})\\lambda(\\mathrm{pos}f)\\delta_{n}^{-d}\\leqslant\\beta_{n}\\leqslant\\left(1+o(1)\\right)\\lambda(\\mathrm{pos}f)\\delta_{n}^{-d}. $$"
  },
  {
    "qid": "statistic-posneg-1807-1-1",
    "gold_answer": "TRUE. The Holmberg size is defined as \\( N_{i}^{*} = \\left\\lfloor\\frac{1}{\\pi_{i}}\right\rfloor + \\epsilon_{i} \\), where \\( \\epsilon_{i} \\) are independent Bernoulli random variables with \\( P(\\epsilon_{i} = u | \\pmb{s}) = r_{i}^{u}(1 - r_{i})^{1 - u} \\), ensuring integer-valued replications for constructing the pseudo-population.",
    "question": "Does the Holmberg size involve randomization through Bernoulli random variables to ensure integer-valued replications?",
    "question_context": "Resampling Methods in Unequal Probability Sampling\n\nThe paper discusses resampling methods in unequal probability sampling, focusing on the construction of pseudo-populations and the choice of drawing probabilities. It introduces the Holmberg size and Horvitz-Thompson size for determining the number of replications of sample units, and explores various approximations for drawing probabilities to ensure asymptotically correct resampling schemes."
  },
  {
    "qid": "statistic-posneg-1302-0-0",
    "gold_answer": "FALSE. While it is true that trace-class operators are Hilbert-Schmidt operators, the converse is not necessarily true. The context states that $\\Sigma_X$ and $\\Sigma_Y$ are trace-class operators because their traces are finite, which is a stronger condition than being Hilbert-Schmidt.",
    "question": "Is it true that the covariance operators $\\Sigma_X$ and $\\Sigma_Y$ are trace-class operators because they are Hilbert-Schmidt operators?",
    "question_context": "Kernel Canonical Correlation Analysis: Convergence Rates and Lower Bounds\n\nDefine the two covariance operators by $\\langle f,\\Sigma_{X}f\\rangle_{\\mathcal{H}_{X}}=\\mathsf{V a r}\\{f(x)\\},\\forall f\\in\\mathcal{H}_{x}$ and $\\langle g,\\Sigma_{Y}g\\rangle_{\\mathcal{H}_{y}}=\\mathsf{V a r}\\{g(y)\\},\\forall g\\in\\mathcal{H}_{y}$ . Since $\\begin{array}{r}{\\mathrm{Var}\\{f(x)\\}=\\mathrm{E}\\langle f,K_{1}(.,x)-\\mathrm{E}_{x}K_{1}(.,x)\\rangle_{\\mathcal{H}_{x}}^{2}\\le\\|f\\|_{\\mathcal{H}_{x}}^{2}\\{\\mathrm{E}_{x}K_{1}(x,x)-\\|\\mathrm{E}_{x}K_{1}(.,x)\\|_{\\mathcal{H}_{x}}^{2}\\}<\\infty,}\\end{array}$ covariance operators are well-defined bounded operators. In fact we can write $\\Sigma_{X}=\\dot{\\operatorname{E}_{x}}[\\{K_{1}(.,x)-\\operatorname{E}_{x}K_{1}(.,x)\\}\\otimes\\{\\ddot{K}_{1}(.,x)-\\operatorname{E}_{x}K_{1}(.,x)\\}]$ and $\\varSigma_{Y}=\\mathbb{E}_{y}[\\{K_{2}(.,y)-$ $\\mathtt{E}_{y}K_{2}(.,y)\\}\\otimes\\{K_{2}(.,y)-\\mathtt{E}_{y}K_{2}(.,y)\\}]$ where for $f_{1},f_{2}\\in\\mathcal{H}_{x},f_{1}\\otimes f_{2}$ is the operator such that $(f_{1}\\otimes f_{2})h=\\langle f_{2},h\\rangle_{\\mathcal{H}_{x}}f_{1}$ , for example. Since $\\mathtt{E}_{x}K_{1}(x,x)<\\infty$ , we have $\\mathrm{tr}(\\Sigma_{X})=\\operatorname{E}_{x}\\langle K_{1}(.,x)-\\operatorname{E}_{x}K_{1}(.,x)$ , K1(., x) − ExK1(., x)⟩Hx < ∞ and thus $\\Sigma_{X}$ and $\\varSigma_{Y}$ are trace-class operators and in particular are Hilbert–Schmidt operators. Furthermore, we define the cross-covariance operators $\\Sigma_{X Y}=\\Sigma_{Y X}^{\\bar{\\top}}=\\mathsf E_{x,y}[\\{K_{1}(.,\\bar{x})-\\mathsf E_{x}K_{1}(.,x)\\}\\otimes\\{K_{2}(.,y)-\\mathsf E_{y}K_{2}(.,y)\\}]$ where $()^{\\top}$ denotes the adjoint operator. By the reproducing property, we can easily see $\\langle f,\\Sigma_{X Y}g\\rangle_{\\mathcal{H}_{x}}=\\mathsf{C o v}\\{f(x),g(y)\\}$ ."
  },
  {
    "qid": "statistic-posneg-2055-2-0",
    "gold_answer": "FALSE. The $p$-value of 0.138 is greater than the conventional significance level of 0.05, indicating that there is insufficient evidence to reject the null hypothesis. This means that the effects of age, triglycerides, and blood pressure treatment on coronary heart disease and stroke are not significantly different between the two diseases.",
    "question": "Is the hypothesis $H_{0}:\\beta_{21}=\\beta_{22}$, $\\beta_{31}=\\beta_{32}$, $\\beta_{41}=\\beta_{42}$ rejected based on the $p$-value of 0.138?",
    "question_context": "Case-Cohort Study Analysis: Comparing Effects of Serum Ferritin on Coronary Heart Disease and Stroke\n\nThe study aims to compare the effect of serum ferritin on coronary heart disease with its effect on stroke using case-cohort sampling to reduce costs. The full cohort consists of 1210 subjects with viable blood serum samples, including 174 subjects with only coronary heart disease, 75 with only stroke, and 43 with both diseases. The subcohort consists of 334 disease-free subjects, 61 subjects with only coronary heart disease, 36 with only stroke, and 19 with both diseases. The total number of assayed serum samples was 626. The study fits the model: $$\\lambda_{k}(t\\mid Z_{1},Z_{2},Z_{3},Z_{4})=\\lambda_{0k}(t)\\exp(\\beta_{1k}Z_{1}+\\beta_{2k}Z_{2}+\\beta_{3k}Z_{3}+\\beta_{4k}Z_{4})\\quad(k=1,2),$$ where $Z_{1},Z_{2},Z_{3}$, and $Z_{4}$ denote, respectively, the logarithm of serum ferritin level, age in years, level of triglycerides in millimoles per litre, and whether subjects had blood pressure treatment. The study then tests $H_{0}:\\beta_{21}=\\beta_{22}$, $\\beta_{31}=\\beta_{32}$, $\\beta_{41}=\\beta_{42}$ based on the proposed method, and the $p$-value turns out to be 0.138. Therefore, the final model is fitted as: $$\\lambda_{k}(t\\mid Z_{1},Z_{2},Z_{3},Z_{4})=\\lambda_{0k}(t)\\exp(\\beta_{1k}Z_{1}+\\beta_{2}Z_{2}+\\beta_{3}Z_{3}+\\beta_{4}Z_{4})\\quad(k=1,2).$$"
  },
  {
    "qid": "statistic-posneg-476-0-0",
    "gold_answer": "TRUE. The derivation is correct because the $Z_{i}$ variables are multivariate normal with common correlation $\\rho$ and unit variances. The integral form $P_{w}=\\int_{-\\infty}^{+\\infty}F^{n}\\{(h+\\rho^{\\dagger}y)/(1-\\rho)^{\\dagger}\\}f(y)d y$ leverages the properties of multivariate normal distributions and the common correlation structure to compute the exact probability that all $Z_{i}$ are less than $h$. This approach is consistent with Gupta (1963) and provides an exact solution, unlike Mitchell's approximation.",
    "question": "Is the exact probability $P_{w}$ correctly derived as $P_{w}=\\int_{-\\infty}^{+\\infty}F^{n}\\{(h+\\rho^{\\dagger}y)/(1-\\rho)^{\\dagger}\\}f(y)d y$ based on the multivariate normal properties of $Z_{i}$ and the common correlation $\\rho$?",
    "question_context": "Exact Probability of Being the Lowest Bidder in a Closed Auction\n\nMitchell (1977) formulates a model for obtaining the probability of being the lowest bidder in a closed auction and calculates this approximately. Mitchell's probabilities may be evaluated exactly. Mitchell's firm produces a cost estimate $X_{0}$ with a distribution $N(\\mu_{1}+m,\\sigma_{1}^{2})$ and his $\\pmb{n}$ competitors independently produce their cost estimates each from $N(\\mu_{2},\\sigma_{2}^{2})$. Define $P_{w}$ to be the probability that $X_{0}$ is the smallest of the $X$ 's. A computation was offered by Mitchell which was based on the assumption that the minimum of 5 standard normal variates has approximately a normal distribution. The exact value of $P_{w}$ may be obtained as follows: $$\\begin{array}{r l}{\\lefteqn{P_{w}=\\operatorname{Prob}\\left\\{X_{0}-X_{1}<0,X_{0}-X_{2}<0,...,X_{0}-X_{n}<0\\right\\}}\\ &{\\quad=\\operatorname{Prob}\\left\\{Z_{1}<h,Z_{2}<h,...,Z_{n}<h\\right\\},}\\end{array}$$ where $Z_{i}=\\{X_{0}-X_{i}-(\\mu_{1}+m-\\mu_{2})\\}/(\\sigma_{1}^{2}+\\sigma_{2}^{2})^{\\frac{1}{2}}$ and $h=(\\mu_{2}-m-\\mu_{1})/(\\sigma_{1}^{2}+\\sigma_{2}^{2})^{\\frac{1}{4}}$. The $Z_{i}$ s as defined above are multivariate normal random variables with common correlation $\\rho=\\sigma_{1}^{2}/(\\sigma_{1}^{2}+\\sigma_{2}^{2})$ and unit variances. I follow Gupta (1963) and state that $$P_{w}=\\int_{-\\infty}^{+\\infty}F^{n}\\{(h+\\rho^{\\dagger}y)/(1-\\rho)^{\\dagger}\\}f(y)d y,$$ where $\\pmb{F}$ and $f$ are the c.d.f. and p.d.f. of a standard normal variable. Gupta gives tables for computing these for different values of $\\rho$. Below, for $\\pmb{n}=5$ and $\\begin{array}{r}{(\\sigma_{1}/\\sigma_{2})=\\frac{1}{2}}\\end{array}$, I extend Mitchell's figures to 3 decimal places and compare these with the exact values obtainable from Gupta's tables."
  },
  {
    "qid": "statistic-posneg-1488-0-1",
    "gold_answer": "TRUE. Accurate prediction requires both conditions: high confidence in species identification ($P\\{O\\in S_{i}^{*}|K\\} \\approx 1$) and high certainty about the feature given the species ($P\\{F\\vert K\\cap S_{i}^{*}\\}$ near 0 or 1). The first ensures correct model selection, while the second ensures the selected model provides a definitive prediction for $F$. This aligns with the Bayesian prediction framework where model uncertainty and within-model prediction certainty jointly determine overall prediction accuracy.",
    "question": "Does the condition for accurate prediction require both $P\\{O\\in S_{i}^{*}|K\\}$ close to 1 and $P\\{F\\vert K\\cap S_{i}^{*}\\}$ close to zero or one, as stated in the context?",
    "question_context": "Bayesian Recognition and Prediction in Statistical Modeling\n\n$\\bullet$ Let $F$ be some future prediction about the present object $o$ $\\bullet$ Let $K$ be present knowledge of the object. $\\bullet$ Let $S_{1},S_{2},\\ldots,S_{n}$ be the remembered species. <PARAGRAPH_SEPARATOR> The knowledge $K$ would be generated by an object in species $S_{i}$ with probability $P(K|S_{i})$ Prior to knowing $K$ , an object belongs to species $S_{i}$ with probability $P(O\\in S_{i}\\}$ <PARAGRAPH_SEPARATOR> With these probabilities, we use Bayes theorem to derive Recognition probabilities: <PARAGRAPH_SEPARATOR> $$ P\\{O\\in S_{i}|K\\}=P(K|S_{i})P\\{O\\in S_{i}\\}/{\\sum_{i}}P(K|S_{i})P\\{O\\in S_{i}\\}. $$ <PARAGRAPH_SEPARATOR> Prediction probabilities: <PARAGRAPH_SEPARATOR> $$ P(F|K)=\\sum_{i}{P(F\\cap K|S_{i})P\\{O\\in S_{i}\\}}/{\\sum_{i}{P(K|S_{i})P\\{O\\in S_{i}\\}}}. $$ <PARAGRAPH_SEPARATOR> Recognition leads to accurate prediction when there is enough data known about the present object to confidently identify its species (that is $P\\{O\\in S_{i}^{*}|K\\}$ is close to 1 for some recognised species $S_{i}^{*}$ ), and when we are relatively sure about the feature $F$ given the identified object (that is, $P\\{F\\vert K\\cap S_{i}^{*}\\}$ is close to zero or one). <PARAGRAPH_SEPARATOR> Recognition as described above corresponds to a familiar scheme for statistical modelling. Our experience is expressed as a set of probability models $S_{i}$ eachof which specifies a probability distribution of observable quantities. The present object is some set of available observations. We recognise the object as coming from one of the models with various probabilities. We predict other properties of the object using the probability distributions of those properties based on the likely models given the present data."
  },
  {
    "qid": "statistic-posneg-2065-6-1",
    "gold_answer": "TRUE. The simplification $,P_{k}(Q)=\\sum_{r=0}^{k}(-1)^{r k}C_{r}\\bigg(1+\\frac{2r Q^{3}}{\\nu}\\bigg)^{-\\dagger\\nu}$ is explicitly stated to hold for the case $\\pmb{m}=\\pmb{2}$. For other values of $\\pmb{m}$, especially odd ones, the integral is not amenable to such simplifications and numerical calculations become difficult.",
    "question": "Does the probability integral $,P_{k}(Q)$ simplify to a sum involving binomial coefficients only when $\\pmb{m}=\\pmb{2}$?",
    "question_context": "Probability Integral of the Largest Variance Ratio in ANOVA\n\nLet $,P_{k}(Q)$ denote the probability of ${\\pmb\\vartheta}_{\\pmb k}/{\\pmb\\vartheta}_{0}$ being $\\leqslant Q$ . This is the same as the probability of $\\mathit{F}_{k}=\\mathit{s}_{k}^{2}/\\vartheta_{0}^{2}$ being $\\leqslant\\pmb{Q^{\\mathfrak{z}}}$ When $\\pmb{\\nu}\\rightarrow\\infty$ , this probability will be denoted by $P_{k}(Q)$. Finney (1941) showed that $,P_{k}(Q)=M^{k}(1+\\lambda)^{-1\\nu}$, where $M=\\Gamma(\\frac{1}{2}m)^{-1}{\\displaystyle\\int_{0}^{-\\frac{m}{\\nu}{\\cal Q}^{*}\\frac{\\partial}{\\partial\\lambda}}}u^{\\frac{\\partial}{\\partial{\\displaystyle u^{}-1}}{\\displaystyle e^{-u}}d u}$, and $\\lambda$ is put equal to zero in (l) after differentiation. He found that (l) was not amenable to numerical calculations if $\\pmb{m}$ is odd. The simplest case is $\\pmb{m}=\\pmb{2}$ for which the probability integral reduces to $,P_{k}(Q)=\\sum_{r=0}^{k}(-1)^{r k}C_{r}\\bigg(1+\\frac{2r Q^{3}}{\\nu}\\bigg)^{-\\dagger\\nu}$. Hartley (1938) had suggested that if we assume the $\\pmb{k}$ variance ratios to be independent, an assumption quite justifiable if $\\pmb{\\nu}$ is very large, ${\\mathcal{P}}_{k}(Q)$ can be obtained approximately from the formula ${\\mathcal{P}}_{k}(Q){\\simeq}\\{{\\boldsymbol{\\jmath}}_{1}P_{1}(Q)\\}^{k}$."
  },
  {
    "qid": "statistic-posneg-1073-0-0",
    "gold_answer": "TRUE. The equation is derived from setting the sum of the differences between the observed pseudo-values $\\hat{\\theta}_{i}(t)$ and the model-predicted values $g^{-1}(\\alpha(t) + \\beta^{\\mathrm{T}}\\mathbf{Z}_{i})$ to zero. This ensures that the estimated $\\alpha(t)$ aligns with the fixed $\\beta$ to minimize the discrepancy between observed and predicted values, following the generalized estimating equations (GEE) framework.",
    "question": "Is the equation $$\\sum_{i}(\\hat{\\theta}_{i}(t)-g^{-1}(\\alpha(t)+\\pmb{\\beta}^{\\mathrm{T}}\\mathbf{Z}_{i})=0$$ used to estimate $\\alpha(t)$ for a fixed $\\beta$ in the pseudo-value regression model?",
    "question_context": "Pseudo-Value Regression Modeling for State Occupation Probabilities\n\nThe pseudo-value approach discussed here has considerable appeal for direct regression modelling of state or transition probabilities. It allows a direct model for the covariate effect on these probabilities rather than an indirect modelling via regression models for the transition intensities. As presented here, separate models were set up for each state occupation probability and we saw in the example in the previous section that the effect of a covariate for one state may be influenced by its effect on other states. A possible extension of the models considered would be to study several state occupation probabilities together in a single multivariate model. This can easily be achieved by defining multivariate pseudo-observations for the relevant states in (7) and (9). <PARAGRAPH_SEPARATOR> The model as formulated here requires the selection of a grid of time points and although our analyses in section 3 suggested that the choice of time points may not be crucial it would be more satisfactory to define pseudo-values for all times. For both the Aalen–Johansen estimator or the difference of Kaplan–Meier estimators the estimate of the state occupation probability, $\\hat{\\theta}(t)$ , changes its value only when there is a transition in to or out of the state of interest. This means that if we let $T_{1},\\dots,T_{N}$ denote these transition times and let $\\hat{\\theta}_{i}(t)=\\hat{\\theta}_{i}(t-)+\\Delta\\hat{\\theta}_{i}(t)$ then $\\Delta\\hat{\\theta}_{i}(t)=0$ when $t$ is not in the set $\\mathcal{T}=\\{T_{1},...,T_{N}\\}$ . For $t$ in $\\tau$ we specify the model as <PARAGRAPH_SEPARATOR> $$ g(\\boldsymbol{\\theta}_{i}(t))=\\alpha(t)+\\beta^{\\mathrm{T}}\\mathbf{Z}_{i}, $$ <PARAGRAPH_SEPARATOR> where $\\beta$ is now a $p$ -vector of regression coefficients. <PARAGRAPH_SEPARATOR> An estimation approach which is an alternative to standard GEEs, inspired by the work of Lawless & Nadeau (1995) for analysis of repeated events, is as follows. Consider the usual GEE for $\\beta$ <PARAGRAPH_SEPARATOR> $$ \\mathbf{U}({\\boldsymbol{\\beta}})=\\sum_{i=1}^{n}\\sum_{t\\in T}{\\frac{\\partial}{\\partial{\\boldsymbol{\\beta}}}}g^{-1}(\\alpha(t)+{\\boldsymbol{\\beta}}^{\\mathrm{{T}}}\\mathbf{Z}_{i})\\mathbf{V}_{i}^{-1}({\\hat{\\boldsymbol{\\theta}}}_{i}(t)-g^{-1}(\\alpha(t)+{\\boldsymbol{\\beta}}^{\\mathrm{{T}}}\\mathbf{Z}_{i}))=\\mathbf{0}, $$ <PARAGRAPH_SEPARATOR> and for fixed $\\beta$ estimate $\\alpha(t),t\\in\\mathcal{T}$ by the equation: <PARAGRAPH_SEPARATOR> $$ \\sum_{i}(\\hat{\\theta}_{i}(t)-g^{-1}(\\alpha(t)+\\pmb{\\beta}^{\\mathrm{T}}\\mathbf{Z}_{i})=0. $$ <PARAGRAPH_SEPARATOR> For some choices of link function one may be able to solve (13) for $\\alpha(t)$ as a function of $\\beta$ and substitute the solution into (12) to yield a profile estimating equation. When this is not possible estimators of $\\alpha(t)$ and $\\beta$ may be obtained by iterating between (12) and (13). Note that equation (13) takes the form <PARAGRAPH_SEPARATOR> $$ {\\hat{\\ddot{\\theta}}}^{*}(t)={\\frac{1}{n}}\\sum_{i}g^{-1}(\\alpha(t)+\\beta^{\\Gamma}{\\bf Z}_{i}),\\quad{\\mathrm{where}}\\quad{\\hat{\\ddot{\\theta}}}^{*}(t)={\\frac{1}{n}}\\sum_{i}{\\hat{\\theta}}_{i}(t), $$ <PARAGRAPH_SEPARATOR> the average of the pseudo-values, is the jackknife estimator of the marginal mean $\\theta(t)$ (see, e.g. Miller, 1974). Estimators of the variance of the estimators of $\\alpha(t)$ and $\\beta$ can be obtained by standard GEE-based sandwich estimators or via a non-parametric bootstrap procedure. Large sample properties of these estimators do no longer follow by results of Liang & Zeger (1986). However, results of Lin et al. (2000) may be applicable. <PARAGRAPH_SEPARATOR> An alternative approach to estimation for this model is to use a GEE with a penalty function to smooth out $\\alpha(t)$ ; that is, we will find $\\alpha(t)$ and $\\beta$ to maximize <PARAGRAPH_SEPARATOR> $$ \\sum_{i}({\\hat{\\theta}}_{i}(t)-g^{-1}({\\boldsymbol{\\alpha}}(t)+\\beta^{\\mathrm{T}}{\\bf Z}_{i}))^{\\mathrm{T}}{\\bf V}_{i}^{-1}({\\hat{\\theta}}_{i}(t)-g^{-1}({\\boldsymbol{\\alpha}}(t)+\\beta^{\\mathrm{T}}{\\bf Z}_{i}))+q({\\boldsymbol{\\alpha}}(t),\\phi). $$ <PARAGRAPH_SEPARATOR> Here $q(\\alpha(\\cdot),\\phi)$ is the penalty function with a tuning constant $\\phi$ . For example, we may take $\\begin{array}{r}{q(\\alpha(\\cdot),\\phi)=\\sum_{j}\\phi(\\alpha(T_{j})-\\alpha(T_{j+1}))^{2}}\\end{array}$ . Both methods are currently being investigated."
  },
  {
    "qid": "statistic-posneg-250-0-2",
    "gold_answer": "FALSE. The context shows that CUSUMs with a head start perform worse when thresholds are set based on false alarm probabilities. For instance, when η=5000, the out-of-control hitting probabilities are 0.45 for ordinary CUSUMs versus only 0.06 for CUSUMs with a head start. This is because CUSUMs with a head start have a far higher threshold when set according to hitting probabilities.",
    "question": "Does setting the threshold based on false alarm probabilities (Pr{N(τ) ≤ 100|η=∞} = 0.01) instead of average run length improve the performance of CUSUMs with a head start in all cases?",
    "question_context": "Risk-adjusted Monitoring of Time-to-Event Models: Hazard Rate Alternatives and Head Start Impact\n\nThe last four rows of the table show that there is a substantial gain by using a head start when the monitoring starts in an out-of-control state, but when the shift happens later the head start actually leads to a slightly longer average run length. The latter is due to the slightly larger threshold for the CUSUM with a head start and the forgetting of the head start over time, see also Hawkins & Olwell (1998). <PARAGRAPH_SEPARATOR> Using thresholds based on false alarm probabilities instead of average run length leaves the relative relationships among the methods mainly the same. However, there is one important difference: the CUSUMs with a head start do far worse. For instance, suppose we base the threshold on the false alarm probability ${\\mathfrak{p r}}\\{N(\\tau)\\leqslant100|\\eta=\\infty\\}=0.01$ and consider the case with $\\rho=1{\\cdot}25$ and use ordinary time to event monitoring. Then if $\\eta=0$ , the out-of-control hitting probabilities are respectively 0·33 and 0·37 for CUSUMs without and with a head start. However, if $\\eta=5000$ , the out-of-control hitting probabilities are respectively 0·45 and 0·06 for CUSUMs without and with head start. The results are similar for other cases, and the explanation is, as can be seen in Fig. 1, that CUSUMs with a head start will have a far higher threshold than ordinary CUSUMs when the thresholds are set according to hitting probabilities. This feature is more or less extreme than observed here for other head start fractions than $S(0)=c/2$ . <PARAGRAPH_SEPARATOR> In monitoring based on time to event models, there are numerous options for choosing alternatives, and we briefly illustrate the importance of choosing appropriate alternatives by considering a model where the hazard rate in control is the following step function: <PARAGRAPH_SEPARATOR> $$ h_{0}(t)=I(t\\leqslant1)+2I(t>1). $$ <PARAGRAPH_SEPARATOR> Two types of alternatives are considered, one time-change alternative $h_{1,T C}=1{\\cdot}25h_{0}(1{\\cdot}25t)$ and one proportional alternative $h_{1,P r o p}=1\\cdot25h_{0}(t)$ . If data are generated from $h_{1,P r o p}$ , we get an average run length of 166 if the monitoring is using the alternative $h_{1,T C}$ as opposed to an average run length of 106 if the monitoring uses the correct alternative $h_{1,P r o p}$ . If in the same setting data are simulated from $h_{1,T C}$ , the average run length is respectively 76 and 52 for monitoring using the alternatives $h_{1,P r o p}$ and $h_{1,T C}$ . A similar or even more dramatic difference can be seen for other parameter values and other types of alternatives."
  },
  {
    "qid": "statistic-posneg-1524-1-0",
    "gold_answer": "TRUE. The Matérn covariance function is given by \\( C(h) = \\frac{2\\sigma^{2}}{\\Gamma(v)}\\left(\\frac{h}{2\\phi}\right)^{v}K_{v}\\left(\\frac{h}{\\phi}\right) \\). When \\( v = 0.5 \\), the modified Bessel function \\( K_{0.5} \\) simplifies, and the Matérn covariance reduces to the exponential model \\( C(h) = \\sigma^{2} \\exp\\left(-\\frac{h}{\\phi}\right) \\). This is a well-known property of the Matérn family, where specific values of \\( v \\) yield simpler covariance functions.",
    "question": "Is it true that the Matérn covariance function reduces to an exponential model when the smoothness parameter \\( v \\) is set to 0.5?",
    "question_context": "Comparison of Gaussian Geostatistical Models and Gaussian Markov Random Fields\n\nThe paper compares Gaussian Geostatistical Models (GGMs) and Gaussian Markov Random Fields (GMRFs) in terms of their spatial modeling approaches, spectral density approximation methods, and prediction errors. GGMs model spatial associations through parametric covariance functions, while GMRFs specify spatial associations through conditional distributions based on neighborhood structures. The paper introduces spectral methods to measure discrepancy between models and compares them with covariance function approximation methods."
  },
  {
    "qid": "statistic-posneg-1153-0-2",
    "gold_answer": "FALSE. While $\\|\\theta_{0}\\|=1$ addresses the scale ambiguity in $\\theta_{0}$, it does not resolve the rotational ambiguity between $\\beta_{0}$ and $\\theta_{0}$. Without the perpendicularity condition, one could rotate $\\beta_{0}$ and $\\theta_{0}$ in a way that preserves their linear combinations, leading to multiple equivalent parameterizations. Thus, both conditions ($\\|\\theta_{0}\\|=1$ and $\\beta_{0}^{\\mathrm{T}}\\theta_{0} = 0$) are necessary for full identifiability.",
    "question": "The condition $\\|\\theta_{0}\\|=1$ in the extended partially linear single-index model is sufficient to ensure the uniqueness of the parameter estimates without requiring $\\beta_{0}$ and $\\theta_{0}$ to be perpendicular. Is this claim valid?",
    "question_context": "Extended Partially Linear Single-Index Models: Structure and Applications\n\nThe paper introduces an extended version of a generalized partially linear single-index model, which takes the form $$y=\\beta_{0}^{\\mathrm{{T}}}X+\\phi(\\theta_{0}^{\\mathrm{{T}}}X)+\\varepsilon,$$ where $E(\\varepsilon|X)=0$ and $E\\varepsilon^{2}=\\sigma^{2}$, $\\beta_{0}$ and $\\theta_{0}$ are unknown vector parameters and $\\phi(.)$ is an unknown function. To ensure estimability, the authors assume that $\\beta_{0}$ and $\\theta_{0}$ are perpendicular to each other with $\\|\\theta_{0}\\|=1$ and its first nonzero element positive. The model includes many existing models as special cases, such as the single-index model and the partially linear model. The paper also discusses the application of the model to nonlinear time series data, exemplified by the analysis of annual sunspot numbers."
  },
  {
    "qid": "statistic-posneg-128-0-0",
    "gold_answer": "TRUE. The identifiability constraints E{ψ(U)}=1 and E{ϕ(U)}=1 are imposed to ensure that the multiplicative distortion functions ψ(U) and ϕ(U) average out to no distortion on average. This is a natural assumption to avoid systematic bias in the distortion effects, allowing the model to recover the true regression parameters γ0 and γ1 from the distorted observations.",
    "question": "In the covariate-adjusted regression model, the identifiability constraints E{ψ(U)}=1 and E{ϕ(U)}=1 ensure that the mean distorting effect corresponds to no distortion. Is this statement true based on the model assumptions?",
    "question_context": "Covariate-Adjusted Regression with Multiplicative Distortion\n\nWe address the problem of parameter estimation in multiple regression when the actual predictors and response are not observable. Instead, one observes contaminated versions of these variables, where the distortion is multiplicative, with a factor that is a smooth unknown function of an observed covariate. The simultaneous dependence of response and predictors on the same covariate may lead to artificial correlation and regression relationships which do not exist between the actual hidden predictor and response variables. An example is the fibrinogen data of Kaysen et al. (2003), where the regression of fibrinogen level on serum transferrin level in haemodialysis patients is of interest. Both observed response and predictor are known to depend on body mass index, defined as weight/height2, which thus has a confounding effect on the regression relationship. The theme of this paper is to explore such confounding in regression and to develop appropriate adjustment methods."
  },
  {
    "qid": "statistic-posneg-127-0-0",
    "gold_answer": "TRUE. The condition $E(\\tilde{Y}|X)=E(Y|X)$ explicitly states that the mean regression function of $Y$ given $X$ remains unchanged by the confounding variable $U$. This implies that the distortion introduced by $U$ averages out to zero, ensuring that the regression function is not affected by $U$ in expectation. The context further supports this by stating that 'the confounding of $Y$ by $U$ does not change the mean regression function'.",
    "question": "In the context of covariate-adjusted regression, the condition $E(\\tilde{Y}|X)=E(Y|X)$ implies that the confounding variable $U$ has no effect on the mean regression function of $Y$ given $X$. Is this statement true based on the provided context?",
    "question_context": "Covariate-Adjusted Regression and Confounding Variable Analysis\n\nCondition A5. As $n\\to\\infty$ , $n^{-1}X^{\\mathrm{T}}X\\to\\mathcal{X}$ in probability, where the limiting $\\left(p+1\\right)\\times\\left(p+1\\right)$ matrix $\\mathcal{X}$ is nonsingular. <PARAGRAPH_SEPARATOR> Condition A6. The function $h(u)=\\int x g(x,u)d x$ is uniformly Lipschitz, where $g(.,.)$ is the joint density function of $\\tilde{X}$ and $U.$ . <PARAGRAPH_SEPARATOR> These are mild conditions that are satisfied in most practical situations. Bounded covariates are standard in asymptotic theory for least squares regression, as are Conditions A2 and A5 (Lai et al., 1979). The identifiability conditions, Condition A4, are equivalent to <PARAGRAPH_SEPARATOR> $$ E(\\tilde{Y}|X)=E(Y|X),\\quad E(\\tilde{X}_{r}|X_{r})=X_{r}. $$ <PARAGRAPH_SEPARATOR> This means that the confounding of $Y$ by $U$ does not change the mean regression function, and the distorting effects of the confounding variable $U$ average out to 0."
  },
  {
    "qid": "statistic-posneg-1329-0-2",
    "gold_answer": "FALSE. The context provides two different definitions for W(x). In one instance, W(x) is defined as w₀ + w₁(x + x⁻¹) + w₂(x² + x⁻²) + ... + wₙ(xⁿ + x⁻ⁿ), which includes both x and x⁻¹ terms. However, in another instance, W(x) is defined as w₀ + w₁x + w₂x² + ... + wₙxⁿ, which is a standard polynomial in x only.",
    "question": "Is the function W(x) defined as a polynomial in both x and x⁻¹ in all instances within the context?",
    "question_context": "Cramér-Wold Factorization and Recursive Solutions in ARMA Processes\n\nThe context discusses auxiliary algorithms for solving specific equations related to polynomial functions in the context of ARMA processes. The key equations involve polynomial functions G(x), H(x), and W(x), and their relationships. The recursive procedure by Laurie (1980) is highlighted for its efficiency, requiring O(n) divisions compared to O(n²) in Wilson (1979). The equations include transformations and operations such as Y(J) ← X(J) - P*X(M+1-J) and H(x) = G(x) + W(x)W(x⁻¹)."
  },
  {
    "qid": "statistic-posneg-117-0-0",
    "gold_answer": "FALSE. While the elasso penalty generally results in higher Rcv values compared to hlasso, this is not always the case. For example, in the Fingerprint dataset, the hlasso has Rcv = 0.510, while the elasso has Rcv = 0.489, which is lower. This indicates that the elasso does not always lead to greater cumulative regularization compared to hlasso, depending on the dataset.",
    "question": "Is it true that the elasso penalty always results in a higher cumulative regularization rate (Rcv) compared to the hlasso penalty, based on the data in Table 1?",
    "question_context": "Adaptive Regularization in Boosting: Comparing hlasso and elasso Penalties\n\nTwo penalty terms were introduced for regularization purposes. Their main difference is that the elasso is not differentiable at zero, while the hlasso is. As a result, if $\\lambda$ is finite and the base learner’s accuracy is greater than 0.5, then the elasso’s adaptive learning rate can be zero, whereas the hlasso’s learning rate cannot. In the ensuing analysis, we experimentally investigate the performance of RSB under these two penalties.<PARAGRAPH_SEPARATOR>For the RSB algorithm with the adaptive learning rate, the amount of regularization is allowed to change from iteration to iteration. To quantify the overall amount of regularization, consider the following measure with $t\\in[0,1]$ :<PARAGRAPH_SEPARATOR>$$r_{\\lambda}(t)=\\frac{\\sum_{m=1}^{M}I\\{\\nu_{\\lambda}(h_{m})\\leq t\\}}{M}.$$<PARAGRAPH_SEPARATOR>This measure records the relative frequency that the adaptive learning rate shrinks the coefficients less than $t$ . For a fixed $t$ , $r_{\\lambda}(t)$ allows us to compare the rates of regularization for different penalties: the larger $r_{\\lambda}(t)$ , the more regularization up to t . In addition to $r_{\\lambda}(t)$ , consider Rλ = 01 rλ(t) dt = 1 − \u0003mM=1 Mνλ(hm) , which provides the area under the regularization rate curve, or equivalently the average amount of regularization in the ensemble. When comparing penalties, the larger the value of $R_{\\lambda}$ , the greater the regularization. In practice, one would typically estimate the value of $\\lambda$ iteratively using (3.10); in this case, we denote by $r_{c v}$ and $R_{c v}$ , respectively, the pointwise and cumulative cross-validated regularization rates. To illustrate these measures, RSB was applied to a number of datasets from the UCI repository (Asuncion and Newman 2007); a chemical fingerprint and solubility dataset from pharmaceutical applications (Leach and Gillet 2003; Culp, Michailidis, and Johnson 2010); and a text dataset using the bag of words for the abstracts in computer science documents (McCallum et al. 1999). The results are given in Table 1 (averaged over 50 replications).<PARAGRAPH_SEPARATOR>Table 1. $r_{c v}(0)$ and $R_{c v}$ for the ensemble averaged over 50 values.<PARAGRAPH_SEPARATOR><html><body><table><tr><td></td><td></td><td colspan=\"2\">hlasso</td><td colspan=\"2\">elasso</td></tr><tr><td>Datasets</td><td>(n, p)</td><td>rcv(0)</td><td>Rcv</td><td>rcv(0)</td><td>Rcv</td></tr><tr><td>Thyroid</td><td>(215,5)</td><td>0.000</td><td>0.646</td><td>0.471</td><td>0.664</td></tr><tr><td>Ion</td><td>(351, 34)</td><td>0.000</td><td>0.204</td><td>0.090</td><td>0.208</td></tr><tr><td>Fingerprint</td><td>(438,152)</td><td>0.000</td><td>0.510</td><td>0.395</td><td>0.489</td></tr><tr><td>Cancer</td><td>(699,9)</td><td>0.000</td><td>0.659</td><td>0.462</td><td>0.670</td></tr><tr><td>Pima</td><td>(768,8)</td><td>0.000</td><td>0.293</td><td>0.069</td><td>0.284</td></tr><tr><td>Text</td><td>(879,1543)</td><td>0.000</td><td>0.671</td><td>0.593</td><td>0.684</td></tr><tr><td>Credit</td><td>(1000,34)</td><td>0.000</td><td>0.683</td><td>0.403</td><td>0.692</td></tr><tr><td>Solubility</td><td>(5631,72)</td><td>0.000</td><td>0.453</td><td>0.263</td><td>0.459</td></tr></table></body></html>"
  },
  {
    "qid": "statistic-posneg-1889-0-0",
    "gold_answer": "FALSE. The context states that the preliminary test estimator $\\overline{\\sigma}^{\\mathtt{a}}$ is biased, but the magnitude of the bias is less than that of $\\hat{\\sigma}^{\\Im}$ or ${\\pmb{\\tilde{\\sigma}^{\\Im}}}$. This implies that $\\overline{\\sigma}^{\\mathtt{a}}$ is a better choice when bias reduction is a priority.",
    "question": "Is it true that the preliminary test estimator $\\overline{\\sigma}^{\\mathtt{a}}$ always results in a higher bias compared to $\\hat{\\sigma}^{\\Im}$ or ${\\pmb{\\tilde{\\sigma}^{\\Im}}}$?",
    "question_context": "Preliminary Test Estimator for Bias Reduction in Logistic Distribution\n\nIf bias is considered important by the experimenter, he could consider the preliminary test estimator which chooses between ${\\pmb\\vartheta}^{\\bullet}$ which is unbiased, and $\\pmb{w^{2}}$ which is unbiased when ${\\pmb{\\mu}}={\\pmb{\\mu}}_{0}$ i.e. <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\overline{{\\sigma}}^{\\mathtt{a}}=\\left\\{\\begin{array}{l l}{\\theta^{\\mathtt{a}}}&{(\\left|t\\right|>t_{\\mathbf{j}\\alpha,\\mathtt{s}-1}),}\\ {w^{\\mathtt{a}}}&{(\\left|t\\right|\\leqslant t_{\\mathbf{j}\\alpha,\\mathtt{s}-1}).}\\end{array}\\right.}\\end{array} $$ <PARAGRAPH_SEPARATOR> This estimator is also biased, but the magnitude of the bias is less than that of $\\hat{\\sigma}^{\\Im}$ or ${\\pmb{\\tilde{\\sigma}^{\\Im}}}$ . One would be better advised to use either ${\\hat{\\pmb{\\sigma}}}^{\\bullet}$ or ${\\tilde{\\sigma}}^{\\Im}$ or else use $\\pmb{\\vartheta}^{\\bullet}$ all of the time if bias is of special importance. <PARAGRAPH_SEPARATOR> # 3.REMARKS <PARAGRAPH_SEPARATOR> This investigation has shown that, in terms of squared error risk, the usual unbiased estimators should not necessarily be considered. Also, we have shown that one can improve upon the unique best mean squared error estimator of the form $\\mathbf{\\mathcal{C}\\mathcal{B}^{\\hat{\\Omega}}}$ . The basic intuitive reason that we are able to improve upon such estimators is that the preliminary test procedure enables one to utilize information from prior estimates."
  },
  {
    "qid": "statistic-posneg-1589-1-2",
    "gold_answer": "TRUE. Explanation: The context function in Example B is defined as $c(x_{-\\infty}^0) = 0$ if $x_0 = 0$, with no dependence on $x_{-\\infty}^{-1}$. This means that any sequence ending with 0 is lumped into the same context, demonstrating the variable-length memory property where the relevant past can be shorter than the maximum order $p$.",
    "question": "In Example B with $\\mathcal{X} = \\{0,1\\}$, the context function $c(x_{-\\infty}^0)$ assigns the same context to all sequences ending with $x_0 = 0$, regardless of the earlier history.",
    "question_context": "Variable Length Markov Chains in DNA Sequence Analysis\n\nA variable length Markov chain is a potentially high-order Markov chain, taking values in a finite categorical space $\\mathcal{X}$, with a natural parsimonious structure for the transition probabilities. The paper provides an example with DNA sequences where $X_t \\in \\{A,C,G,T\\}$ and models it using a stationary Markov chain of order 2 with specific transition probabilities that depend on a variable number of lagged values. The model is parsimonious, reducing the number of transition probability vectors needed compared to a full Markov chain of the same order."
  },
  {
    "qid": "statistic-posneg-1965-0-1",
    "gold_answer": "FALSE. The paper explicitly mentions that $A_{1,\\beta\\phi}$ and $A_{2,\\beta\\phi}$ are functions only of the total number of parameters $p$, the number of restrictions $q$, the precision parameter $\\phi$, and the function $d_{1}(.)$ through its second and third derivatives. They do not depend on the model matrix $X$, except through its rank, or on the link function.",
    "question": "Does the term $A_{1,\\beta\\phi}$ depend on the model matrix $X$ beyond its rank?",
    "question_context": "Finite-sample corrections for score tests in generalised linear models\n\nThis paper derives finite-sample corrections for score tests in generalised linear models when the dispersion parameter is unknown. The corrections can be used to design improved tests and to assess the chi-squared approximation in small samples."
  },
  {
    "qid": "statistic-posneg-428-0-3",
    "gold_answer": "FALSE. The term $(-1)^m \\frac{(t_i - v)^{2m-1}}{(2m-1)!}$ is correctly derived through the integration by parts and recursive application process. It is a valid component of the kernel function expression.",
    "question": "The final expression for $K_{\\lambda}(t_{i},v)$ includes a term $(-1)^m \\frac{(t_i - v)^{2m-1}}{(2m-1)!}$ that is incorrectly derived. True or False?",
    "question_context": "Spatially Adaptive Smoothing Splines with Piecewise-Constant Smoothing Parameters\n\nThe paper derives expressions for the kernel function $K_{\\lambda}(t_{i},v)$ used in spatially adaptive smoothing splines, where the smoothing parameter $\\lambda(u)$ is piecewise constant. The derivation involves integration by parts and recursive application to handle the piecewise nature of $\\lambda(u)$. The kernel function is expressed in terms of basis functions and their integrals over intervals defined by knots $\\tau_k$."
  },
  {
    "qid": "statistic-posneg-1380-0-0",
    "gold_answer": "FALSE. Explanation: According to the context, setting γ=0 corresponds to P2, which gives ordinary (generalized) additive models, not ridge regression. Setting λ=∞ corresponds to P1, which yields an alternative formulation of ridge regression. Therefore, the statement is false as it incorrectly combines the conditions for γ and λ to describe ridge regression.",
    "question": "The penalized fitting criterion in the model includes both first and second order difference penalties to prevent overfitting. Is it true that setting γ=0 and λ=∞ in the penalized fitting criterion results in ridge regression?",
    "question_context": "Penalized B-Splines for Generalized Additive Models\n\nMarx and Eilers (1998) proposed a variant of generalized additive models (Hastie and Tibshirani, 1990) based on penalized B-splines. I give a short review below for $p$ predictors. Eilers and Marx (1996) give a detailed description in the case with one predictor. <PARAGRAPH_SEPARATOR> A B-spline of degree $q$ consists of $(q+1)$ polynomial pieces, each of degree $q$ , joined at $q$ inner knots. At the joining points, derivatives up to order $(q-1)$ are continuous. The B-spline is positive on a domain spanned by $(q+2)$ knots, and elsewhere 0. The left-hand panel in Fig. 2 shows a B-spline of degree $q=3$ . The right-hand panel of the figure shows a set of $r=13$ equidistant B-splines in the range from 0 to 1, constructed by dividing the interval from 0 to 1 into $(r-q)=10$ equal intervals. At any given value between 0 and 1, $(q+1)$ B-splines are nonzero, and their sum is 1. <PARAGRAPH_SEPARATOR> The value $x_{j}$ of a predictor variable is transformed into $r$ new variables $b_{l}(\\boldsymbol{x}_{j}),l{=}1,\\ldots,r.$ where $b_{l}(x_{j})$ denotes the value at $x_{j}$ of the lth B-spline. The jth smooth function $s_{j}$ in (1) is then assumed to be satisfactorily represented by the linear combination <PARAGRAPH_SEPARATOR> $$ s_{j}(x_{j})=\\sum_{l=1}^{r}\\alpha_{j l}b_{l}(x_{j}). $$ <PARAGRAPH_SEPARATOR> This is repeated for all $p s\\cdot$ -functions, with the same number $r$ of B-splines used for each predictor variable. Then the full model becomes <PARAGRAPH_SEPARATOR> $$ \\mu(\\mathbf{x})=\\alpha_{0}+\\sum_{j=1}^{p}\\sum_{l=1}^{r}\\alpha_{j l}b_{l}(x_{j}), $$ <PARAGRAPH_SEPARATOR> now denoting the intercept by $\\boldsymbol{\\alpha}_{0}$ <PARAGRAPH_SEPARATOR> We now have a flexible functional form of $\\mu(\\mathbf{x})$ , but the model contains $r\\cdot p+1$ free or effective parameters, and when $r$ is large, there are possibly too many which may lead to severe over fitting. To overcome this, Marx and Eilers (1998) introduce difference penalties on adjacent estimated $\\alpha$ -coefficients. The 0th order difference of $\\boldsymbol{\\mathfrak{x}}_{j l}$ is the coefficient value itself, the first order difference is $(\\alpha_{j l}-\\alpha_{j l-1})$ , and the second order difference is $\\{(\\alpha_{j l}-$ $\\alpha_{j l-1})-(\\alpha_{j l-1}-\\alpha_{j l-2})\\}=(\\alpha_{j l}-2\\alpha_{j l-1}+\\alpha_{j l-2})$ . Higher order differences may be used as well, but here I will only consider differences up to order 2. The penalized fitting criterion is now <PARAGRAPH_SEPARATOR> Table 1 Effective number of parameters <PARAGRAPH_SEPARATOR> <html><body><table><tr><td></td><td>入</td></tr><tr><td>0</td><td></td></tr><tr><td>0</td><td></td></tr><tr><td>8</td><td></td></tr></table></body></html> <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n}\\{y_{i}-\\tilde{\\mu}(\\mathbf{x}_{i})\\}^{2}+\\gamma\\sum_{j=1}^{p}\\sum_{l=2}^{r}(\\tilde{\\alpha}_{j l}-\\tilde{\\alpha}_{j l-1})^{2}}\\ &{\\quad\\quad+\\lambda\\displaystyle\\sum_{j=1}^{p}\\sum_{l=3}^{r}(\\tilde{\\alpha}_{j l}-2\\tilde{\\alpha}_{j l-1}+\\tilde{\\alpha}_{j l-2})^{2}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> The penalizations on the first and second order differences approximate the corresponding penalizations on $\\tilde{s}_{j}^{\\prime}$ and $\\tilde{s}_{j}^{\\prime\\prime}$ in (6) (see Eilers and Marx, 1996). Using the same number $r$ of B-splines for all predictors corresponds to scaling the predictor variables to cover the same range as assumed in (6). <PARAGRAPH_SEPARATOR> This approach is called penalized B-splines or P-splines by Marx and Eilers (1998). They essentially consider penalizing with just one of the first or second (or higher) order differences. If $\\gamma=0$ , (10) corresponds to P2, and thus gives ordinary (generalized) additive models. If $\\lambda=\\infty$ , (10) corresponds to (5), and yields an alternative formulation of ridge regression. I denote this P1. <PARAGRAPH_SEPARATOR> The vector $\\hat{\\boldsymbol{\\alpha}}$ of coefficients that minimizes (10) is rather simple to compute, and is given in Appendix A, which also includes a formula to compute the effective number of parameters. Table 1 shows the effective number of parameters for some special cases. In this paper, $r=13$ for all methods."
  },
  {
    "qid": "statistic-posneg-1038-0-4",
    "gold_answer": "TRUE. The local influence analysis involves introducing small perturbations to the model (e.g., case-weight perturbation, explanatory variable perturbation) and examining the effect on the parameter estimates. The paper states: 'The idea of the local influence method is to investigate how much the estimates are affected by the corresponding perturbations.' This method helps identify observations that have a disproportionate influence on the model estimates.",
    "question": "The local influence analysis for the ZINB model involves perturbing the model in various ways and examining the effect on the parameter estimates. Is this method used to identify influential observations?",
    "question_context": "Zero-Inflated Negative Binomial (ZINB) Regression Model and Influence Diagnostics\n\nThe paper discusses the Zero-Inflated Negative Binomial (ZINB) regression model, which is used for modeling count data with overdispersion and excess zeros. The model's probability mass function, log-likelihood function, and estimation methods including Maximum Likelihood (ML) estimation, EM algorithm, jackknife, and bootstrap methods are presented. The paper also covers sensitivity analysis through global and local influence diagnostics."
  },
  {
    "qid": "statistic-posneg-206-0-1",
    "gold_answer": "FALSE. The interval includes 0 (the null value), and the p-value is stated to be 'just over 5%', meaning the effect is not statistically significant at the α=0.05 level. The point estimate (0.047) suggests a positive association, but the data do not provide sufficient evidence to reject the null hypothesis of no difference in risks.",
    "question": "Does the 95% confidence interval (-0.002, 0.096) for the additive hazards model's regression coefficient imply a statistically significant (α=0.05) effect of HLA-B27 status on joint damage risk?",
    "question_context": "Semiparametric Analysis of Time-to-Event Data with Interval Censoring\n\nOur analysis focuses on 407 patients who did not have clinical joint damage at their time of enrolment (340 with HLA-B27 negative and 67 with HLA-B27 positive). We study the time to the first presence of joint damage up to 15 years after enrolment for these patients. Among them, it is found that 192 (154 with HLA-B27 negative, 38 with HLA-B27 positive) developed joint damage by a later follow-up visit, and 215 still had no joint damage at the end of 15 years follow-up. Joint damage is only assessed at visits to the clinic, so the occurrence time of first joint damage is subject to interval censoring.<PARAGRAPH_SEPARATOR>These patients enrolled over a substantial period of time, which affects their follow-up length and number of visits. The B27 negative group had an average of 10 visits and the B27 positive group nine visits per person, with the average time between visits approximately 1.5 years. Thus, we choose the bandwidth for our kernel estimators of baseline survivor function to be 1 year. For estimating the IIV weights, visit gap times are fitted with model (A3) with cut-points selected according to a nonparametric estimate of the baseline intensity. Covariates included in the visit gap time model are gender, age*, psoriasis (PS) duration and PsA duration at the time of enrolment, family history of PS (yes/no), family history of PsA (yes/no), and the status for human leukocyte antigen HLA- $\\mathbf{\\nabla}\\cdot\\mathbf{B}27^{*}$ (positive/negative) as time-fixed variables and erythrocyte sedimentation rate* (ESR), total number of active (inflamed) joints, use of NSAIDs (nonsteroidal anti-inflammatory drugs), DMARDs* (disease-modifying antirheumatic drugs), or biologics* medications, year* (of most recent visit), and median length* (of past visit gaps) as time-varying variables. Time-varying covariate values are measured at the most recent visit time except for medication status (NSAIDs, DMARDs, and biologics) which could change between visits. Factors significant at $\\alpha=.05$ are annotated by *. The Nelson–Aalen (NA) estimate of the marginal visit intensity is employed to stabilize the IIV weights; the value of $\\hat{\\boldsymbol{s}}(t_{i j})$ was taken as the increment in the NA estimate at time $t_{i j}$ . The stabilized weights have first, second, and third quartiles 0.57, 1.65, and 3.33.<PARAGRAPH_SEPARATOR>We consider and compare here a variety of outcome models in the generalized linear family; the models are seen to agree quite closely, and we then compare the additive hazards models with nonparametric estimates. We first consider the linear transformation model (3) where the distribution function of $\\epsilon$ has the form<PARAGRAPH_SEPARATOR>$$F_{\\epsilon}(t)=\\left\\{\\begin{array}{l l}{1-\\{1+r\\exp(t)\\}^{-1/r}\\quad}&{\\mathrm{if~}r\\neq0}\\\\ {1-\\exp\\{-\\exp(t)\\}\\quad}&{\\mathrm{if~}r=0}\\end{array}.\\right.$$<PARAGRAPH_SEPARATOR>F I G U R E 3 Estimates of survivor functions $S(t|A),A=0$ or 1, based on additive hazards (AH) model, Cox model and proportional odds (PO) model; regression coefficient is assumed to be time-invariant. Solid curves denote that HLA-B27 is positive $(A=1)$ ) and dashed curves denote that HLA-B27 is negative $\\left(A=0\\right)$ )<PARAGRAPH_SEPARATOR>![](images/8c1aae363ff7fa01d57fac1fdc10345feb8ea1e22a3bbb2065902292d0d2380e.jpg)<PARAGRAPH_SEPARATOR>This family has been discussed by Dabrowska and Doksum (1988a), Chen et al. (2002), and Zeng and Lin (2006), among others; it covers the Cox model with $r=0$ and proportional odds (PO) model with $r=1$ , respectively. Figure 3 displays the fitted survivor functions with $r=0$ or 1 and compares them with the time-invariant coefficient additive hazards model (6). It is seen that all these models give very similar estimates of the survivor functions up to 6 years from enrolment. The Cox model agrees closely with the additive hazards model beyond 6 years; the PO model differs gradually from the other two after 6 years for the HLA-B27 positive group. However, there are only six patients who have the failure event occurring after 6 years for the HLA-B27 positive group. Each model results in estimates with large variance in that region. The additive hazards model gives the regression coefficient estimate 0.047 with a standard error of 0.025, giving an approximate $95\\%$ confidence interval $(-0.002,0.096)$ , so the $p$ -value for a test of no difference in the risks of first joint damage between patients with and without HLA-B27 is just over $5\\%$ ."
  },
  {
    "qid": "statistic-posneg-1090-0-3",
    "gold_answer": "FALSE. The asymptotic normality result does not require the Hilbert space $\\mathcal{H}$ to be finite-dimensional. The paper explicitly considers infinite-dimensional Hilbert spaces, as evidenced by the use of an orthonormal basis $\\{e_j : j = 1, \\dots, \\infty\\}$ and the summation over an infinite number of components in the bias and covariance terms.",
    "question": "Does the asymptotic normality result require the Hilbert space $\\mathcal{H}$ to be finite-dimensional?",
    "question_context": "Functional Nonparametric Regression with Functional Response and Predictor\n\nThe paper introduces a nonparametric regression model where both the response variable $\\mathcal{Y}$ and the predictor $\\mathcal{X}$ are functional. The estimator $\\widehat{r}_{h}(\\chi)$ is constructed using a kernel approach, and its asymptotic normality is established under certain conditions. The paper also discusses the importance of the semi-metric $d$ and the small ball probability $F_{\\chi}(h)$ in the behavior of the estimator."
  },
  {
    "qid": "statistic-posneg-1551-0-0",
    "gold_answer": "TRUE. The formula for $u_{n d}$ explicitly states that it is zero when $d_{D} > d_{T}$, which aligns with the condition that customers will not consider the dummy facility if it is beyond their maximum travel distance. The formula is: $$u_{n d}=\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{\\exp{\\left(-d_{D}^{2}/2\\sigma_{d}^{2}\\right)}}{2\\pi\\sigma_{d}^{2}\\left(1-\\exp{\\left(-d_{T}^{2}/2\\sigma_{d}^{2}\\right)}\\right)},}&{0\\leq d_{D}\\leq d_{T}}\\\\ {0,}&{o t h e r w i s e.}\\end{array}\\right.$$",
    "question": "In the BSIM, the utility term $u_{n d}$ for the dummy facility is set to zero if the distance $d_{D}$ exceeds the maximum travel distance $d_{T}$. Is this condition correctly reflected in the formula for $u_{n d}$?",
    "question_context": "Bayesian Spatial Interaction Model for Competitive Facility Location\n\nThe paper introduces a Bayesian spatial interaction model (BSIM) to analyze customer demand and optimize facility locations. The model uses truncated Gaussian distributions to represent customer utilities for visiting stores, incorporating socio-demographic characteristics and spatial distances. The optimal facility location problem is formulated as an integer nonlinear programming problem with budget constraints, aiming to maximize revenue or market share."
  },
  {
    "qid": "statistic-posneg-2140-7-0",
    "gold_answer": "TRUE. The condition ensures that the vector $\\mathbf{h}$ is constructed such that its norm is at least $1-\\delta$, where $\\delta$ is a very small positive number (e.g., $10^{-15}$). The term $h_{m}$ is then adjusted to make the norm exactly 1, i.e., $\\left\\|\\mathbf{h}\\right\\|=1$. This normalization is crucial for the random projection method to work correctly, as it ensures the projected data maintains certain statistical properties under the null hypothesis.",
    "question": "Is the condition $m=1+\\operatorname*{min}\\{\\operatorname*{min}\\{t:\\|(h_{0},\\ldots,h_{t})^{T}\\|\\ge1-\\delta\\},n-1\\}$ used to ensure that the vector $\\mathbf{h}$ has a norm close to 1?",
    "question_context": "Random Projection-Based Test of Gaussianity in Time Series Data\n\nOn the other hand, in the case of time series data, only the goodness of fit test for Gaussianity of a fixed finite marginal has been studied (Epps, 1987; Lobato and Velasco, 2004; Moulines and Choukri, 1996, for example). Notice that the regularity assumption has recently been weakened in Ghosh (2013) in order to work with long range dependence processes. Our work appears to be at the crossroad between all these works; that is, those on time series, on i.i.d. data in infinite dimension or on finite dimension. As a matter of fact, the random projection trick allows only a one dimensional marginal to be considered. Nevertheless, as the sample is not i.i.d., tools similar to those used in the multidimensional case appear. For example, we have to whiten empirically the sample in order to obtain an asymptotic $\\chi^{2}$ distribution (under the null hypothesis). Furthermore, the procedure based on the empirical characteristic function is a method related to the one developed in Csorgo (1986) although the sampled frequencies are, in our work, chosen randomly.<PARAGRAPH_SEPARATOR># 3.2. The test in practice<PARAGRAPH_SEPARATOR>Let $X_{0},\\ldots,X_{n}$ be the available measurements. To compute $\\mathbf{h}$ , let $\\delta>0$ be a fixed number (equal to $10^{-15}$ in the simulations that we carry out in Sections 4 and 5), and take $\\mathbf{h}=(h_{0},\\ldots,h_{m})^{T}$ with<PARAGRAPH_SEPARATOR>$$m=1+\\operatorname*{min}\\{\\operatorname*{min}\\{t:\\|(h_{0},\\ldots,h_{t})^{T}\\|\\ge1-\\delta\\},n-1\\},$$<PARAGRAPH_SEPARATOR>where $h_{0},\\ldots,h_{m-1}$ are drawn as follows and $h_{m}$ is such that $\\left\\|\\mathbf{h}\\right\\|=1$ . Then, define<PARAGRAPH_SEPARATOR>$$Y_{t}=\\sum_{i=0}^{\\operatorname*{min}(m,t)}h_{i}X_{t-i}a_{i},\\quad t=0,\\dots,n.$$<PARAGRAPH_SEPARATOR>To draw $h_{0},\\ldots,h_{m-1}$ , let us fix $\\alpha_{1}$ , $\\alpha_{2}>0$ . Then, we choose $(\\beta_{n})_{n\\in\\mathbb{N}}$ independent and identically distributed with beta distribution of parameters $\\alpha_{1}$ and $\\alpha_{2}$ . Further, we consider the probability distribution which selects a random point in $l^{2}$ according to the following iterative procedure:<PARAGRAPH_SEPARATOR>• $l_{0}=\\beta_{0}\\in[0,1]$ .   • Given $n\\geq1,l_{n}\\in[0,1-\\sum_{i=0}^{n-1}l_{i}]$ equal to $\\beta_{n}(1-\\textstyle\\sum_{i=0}^{n-1}l_{i})$ .<PARAGRAPH_SEPARATOR>Let us define $H_{n}=(l_{n}/a_{n})^{1/2}$ for $n\\in\\mathbb{N}$ with $a_{0}=1$ and $a_{n}=n^{-2}$ , $n\\geq1$ , and take $\\mathbf{H}=(H_{n})_{n\\in\\mathbb{N}}$ . Thus, ${\\bf h}=(h_{i})_{i\\in\\mathbb{N}}$ is a fixed realization of the random element $\\mathbf{H}$."
  },
  {
    "qid": "statistic-posneg-542-2-2",
    "gold_answer": "FALSE. While the covariance matrix Σ is expressed in terms of μ₃, μ₅, and the geometry of E (via Φ₂(E)), it also implicitly depends on the parameters of the random field ε(u) (κ, τ, α) through the moments μ₃ and μ₅. The paper derives Σ as Σ = (3μ₅)/(2πμ₃a₁a₂a₃) Φ₂(E), but μ₃ and μ₅ themselves are functions of κ, τ, and α.",
    "question": "Is it true that the covariance matrix Σ of the cover density f_K₀ depends only on the moments μ₃ and μ₅ of the random field ε(u) and the geometry of the ellipsoid E?",
    "question_context": "Stochastic Particle Model with Isotropic Random Field\n\nThe paper presents a stochastic model for star-shaped particles in ℝ³, where each particle K₀ is defined as K₀ = c₀ + M, with M being star-shaped with respect to the origin. The radial function R of M is modeled multiplicatively as R(u) = M(u)ε(u), where M is the radial function of a fixed ellipsoid E and ε(u) is an isotropic random field on the unit sphere 𝕊². The random field ε(u) is given by ε(u) = ∫𝕊² k(u,v)Z(dv), where Z is a Gamma Lévy basis and k(u,v) is a von Mises–Fisher kernel. The paper derives moments of the cover density and discusses parameter estimation for the model."
  },
  {
    "qid": "statistic-posneg-1057-0-0",
    "gold_answer": "FALSE. The adaptive selection of $\\gamma_{j}$ within the $\\mathcal{F}$ estimation step, while computationally efficient, sacrifices the convergence guarantees that would be provided by a standard block coordinate descent approach where $\\gamma_{j}$ is fixed. The paper explicitly notes that convergence guarantees are lost with this adaptive approach, though empirical observations suggest stabilization of $\\gamma_{j}$ selections after several iterations.",
    "question": "Is it true that the proposed two-step iterative procedure with adaptive selection of $\\gamma_{j}$ within the $\\mathcal{F}$ estimation step guarantees convergence due to its similarity to block coordinate descent?",
    "question_context": "Sparse Single Index Models for Multivariate Responses: Convergence and Tuning Parameter Selection\n\nThe two steps, estimating $\\mathcal{F}$ and estimating B, individually are well behaved procedures with convergence guarantees. A natural question is whether the overall two-step iterative procedure converges. If the tuning parameters $\\gamma_{j}$ in the $\\mathcal{F}$ estimation step are fixed, the overall two-step procedure is performing a version of block coordinate descent, which generates a sequence of parameter estimates with decreasing overall objective function values. We have proposed to incorporate the selection of $\\gamma_{j}$ within the $\\mathcal{F}$ estimation step to expedite computation, however. This choice translates to less computational time but at the cost of giving up potential convergence guarantees that could be proven for a block coordinate descent algorithm. Having said that, we have observed that in practice that the selection of γj’s tends to stabilize after a few complete rounds of the overall algorithm. We leave to future work investigating how the current overall algorithm might be modified to guarantee convergence."
  },
  {
    "qid": "statistic-posneg-1864-0-1",
    "gold_answer": "FALSE. Explanation: The penalty term λ²/2 I(||γ_i||₂≠0) is a nonconvex penalty that directly penalizes the presence of non-zero γ_i (an indicator function), unlike the Lasso penalty which uses the L1 norm (∑|γ_i|). The nonconvex penalty induces sparsity more aggressively but is not equivalent to the Lasso, which is convex and encourages shrinkage rather than exact sparsity.",
    "question": "The penalty term λ²/2 I(||γ_i||₂≠0) in the objective function is equivalent to the Lasso penalty. Is this statement correct?",
    "question_context": "Robust MAVE with Nonconvex Penalization for Dimension Reduction\n\nThe paper introduces a robust version of the Minimum Average Variance Estimation (MAVE) method using nonconvex penalization to handle outliers in dimension reduction. The method involves minimizing an objective function that includes a mean-shift parameter γ_i for each observation, which is penalized using an indicator function I(|γ_i|≠0) to induce sparsity. The problem is separable in each γ_i, leading to a trimmed least squares formulation when γ_i=0, and a penalty term λ²/2 when γ_i≠0. The conditions C1-C7 ensure the theoretical properties of the estimators, including stationarity, moment conditions, and smoothness of density functions."
  },
  {
    "qid": "statistic-posneg-791-0-1",
    "gold_answer": "TRUE. The condition $\\operatorname*{det}(I-\\varPhi(1)z-\\cdots-\\varPhi(p)z^{p})\\neq0$ for all complex $z$ with $|z|=1$ ensures that the VAR(p) process has a unique stationary solution. This condition guarantees that the process is stable and does not have unit roots, which is essential for stationarity.",
    "question": "Is it true that for a stationary VAR(p) process, the condition $\\operatorname*{det}(I-\\varPhi(1)z-\\cdots-\\varPhi(p)z^{p})\\neq0$ for all complex $z$ with $|z|=1$ is necessary and sufficient for the existence of a unique stationary solution?",
    "question_context": "Subset Vector Autoregressive (SVAR) Models and Forecasting\n\nVector autoregressive (VAR) processes constitute a widely used class of models for zero-mean stationary multivariate time series, i.e., series $\\{\\mathbf{X}_{t}=\\left(X_{t,1},\\ldots,X_{t,d}\\right)^{\\prime}$ ; $t=0,\\pm1,\\ldots\\}$ for which $\\mathbf{EX}_{t}=\\mathbf{0}$ ; and $\\mathbf{E}(\\mathbf{X}_{t+h}\\mathbf{X}_{t}^{\\prime})$ is finite and independent of $t$ for all integers $h$ : The matrix $\\varGamma(h)=\\mathbf E(\\mathbf X_{t+h}\\mathbf X_{t}^{\\prime})$ is called the autocovariance at lag $h$ of the process $\\left\\{{\\mathbf{X}}_{t}\\right\\}$ : The process $\\{\\mathbf{X}_{t},~t=0,\\pm1,\\ldots\\}$ ; is a $\\operatorname{VAR}(p)$ process if it is a stationary solution of the equations, $\\mathbf{X}_{t}=\\boldsymbol{\\varPhi}(1)\\mathbf{X}_{t-1}+\\cdots+\\boldsymbol{\\varPhi}(\\boldsymbol{p})\\mathbf{X}_{t-p}+\\mathbf{Z}_{t},$ where, $\\varPhi(1),\\ldots,\\varPhi(p)$ are $d\\times d$ constant matrices (the VAR coefficients), and $\\{\\mathbf{Z}_{t}\\}$ is a sequence of zero-mean uncorrelated random vectors, each with covariance matrix $\\boldsymbol{\\Sigma}$ : Such a process $\\{\\mathbf{Z}_{t}\\}$ is termed white-noise, and we write: $\\{\\mathbf{Z}_{t}\\}\\sim\\mathbf{W}\\mathbf{N}(\\mathbf{0},{\\boldsymbol{\\Sigma}})$ : A stationary solution exists (and is unique) if and only if $\\operatorname*{det}(I-\\varPhi(1)z-\\cdots-\\varPhi(p)z^{p})$ is non-zero for all complex $z$ with $|z|=1$ : If there is a positive integer $k_{m}$ and a proper subset $K=\\{k_{1},\\ldots,k_{m}\\}$ of $\\{1,2,...,k_{m}\\}$ such that $\\boldsymbol{\\varPhi}(i)=0$ for $i\\not\\in K$ ; and $\\boldsymbol{\\varPhi}(i)\\neq0$ for $i\\in K$ ; then $\\left\\{{\\mathbf{X}}_{t}\\right\\}$ is called a $\\operatorname{SVAR}(K)$ process (subset vector autoregressive process with lags in $K$ ) with coefficients $\\phi_{K}(i),i\\in K$ ; and white-noise covariance matrix $\\boldsymbol{\\Sigma}$ ; if it is a stationary solution of the equations, ${\\bf X}_{t}=\\sum_{i\\in K}\\phi_{K}(i){\\bf X}_{t-i}+{\\bf Z}_{t},\\quad\\{{\\bf Z}_{t}\\}\\sim{\\bf W N}(\\mathbf{0},\\boldsymbol{\\itSigma}).$ The fitting of SVAR models is appropriate when it is believed that the regression of $\\mathbf{X}_{t}$ on past values, say $\\mathbf{X}_{t-j}$ ; $j=1,\\ldots,k_{m}$ ; involves only a subset of the lagged values. For an arbitrary stationary process $\\left\\{{\\mathbf{X}}_{t}\\right\\}$ ; the best linear forecast of $\\mathbf{X}_{t}$ (i.e., the one with minimum mean squared error for each component) in terms of $\\{\\mathbf{X}_{t-i},i\\in K\\}$ can be expressed as $\\hat{\\mathbf{X}}_{t}^{(f)}(K)=\\sum_{i\\in K}\\phi_{K}^{(f)}(i)\\mathbf{X}_{t-i},$ with prediction error covariance matrix, $U_{K}^{(f)}\\equiv E[({\\bf X}_{t}-\\hat{{\\bf X}}_{t}^{(f)}(K))({\\bf X}_{t}-\\hat{{\\bf X}}_{t}^{(f)}(K))^{\\prime}]$ : The coefficients, $\\varPhi_{K}^{(f)}(i)$ ; $i\\in K$ ; and error covariance matrix, $U_{K}^{(f)}$ ; can be determined by solving the Yule–Walker (YW) equations, $\\begin{array}{l}{{\\displaystyle{\\cal T}(k)=\\sum_{i\\in{\\cal K}}\\phi_{\\cal K}^{(f)}(i){\\cal T}(k-i),\\quad k\\in{\\cal K},}}\\ {{\\displaystyle{\\cal U}_{\\cal K}^{(f)}={\\cal T}(0)-\\sum_{i\\in{\\cal K}}\\phi_{\\cal K}^{(f)}(i){\\cal T}(i)^{\\prime}}.}\\end{array}$ Remark 1.1. If $\\{\\mathbf{X}_{t}\\}$ satisfies (1) and $\\mathbf{Z}_{t}$ is uncorrelated with $\\left\\{{\\mathbf{X}}_{s},~s<t\\right\\}$ for each $t$ ; then $\\phi_{K}^{(f)}(i)=\\phi_{K}(i)$ ; $i\\in K$ ; and $U_{K}^{(f)}=\\Sigma$ : Remark 1.2. If $\\varGamma(\\cdot)$ is unknown, it can be estimated from the sample $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}$ as $\\hat{\\Gamma}(h)=\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{1}{n}\\sum_{t=1}^{n-h}\\mathbf{x}_{t+h}\\mathbf{x}_{t}^{\\prime}}&{\\mathrm{if}h\\geqslant0,}\\ {\\hat{\\Gamma}(-h)^{\\prime}}&{\\mathrm{if}h<0.}\\end{array}\\right.$ Eqs. (3) and (4) are then known as the empirical Yule–Walker equations for $\\varPhi_{K}^{(f)}(i)$ ; $i\\in K$ ; and $U_{K}^{(f)}$ : As well as the best linear forecast defined above, we need to introduce the best linear backward estimate of $\\mathbf{X}_{t}$ in terms of $\\{\\mathbf{X}_{t+i},i\\in K\\}$ ; i.e., $\\hat{\\mathbf{X}}_{t}^{(b)}(K)=\\sum_{i\\in K}\\Psi_{K}^{(b)}(i)\\mathbf{X}_{t+i},$ which is determined, together with the corresponding error covariance matrix, $V_{K}^{(b)}=E[({\\bf X}_{t}-\\hat{\\bf X}_{t}^{(b)}(K))({\\bf X}_{t}-\\hat{\\bf X}_{t}^{(b)}(K))^{\\prime}]$ ; by the backward Yule–Walker equations, ${\\cal{T}}(k)^{\\prime}=\\sum_{i\\in K}\\Psi_{K}^{(b)}(i){\\cal{T}}(k-i)^{\\prime},\\quad k\\in{\\cal{K}},$ $V_{K}^{(b)}={\\cal{T}}(0)-\\sum_{i\\in K}\\:\\Psi_{K}^{(b)}(i){\\cal{T}}(i).$ The problems of fitting a subset VAR model and of computing forecasts based on a specified set of lagged observations are closely related. This relationship will play a key role in the asymptotic development."
  },
  {
    "qid": "statistic-posneg-376-0-2",
    "gold_answer": "TRUE. The bootstrap p-value is indeed calculated as Ψ(r*_{W,Z} < r)/B, where B is the number of bootstrap resamples. This measures the proportion of resampled correlation coefficients under the null hypothesis that are more extreme (in the direction of the alternative) than the observed correlation r.",
    "question": "Is the bootstrap p-value for testing H₀: ρ = ρ₀ vs. Hₐ: ρ < ρ₀ calculated as the proportion of resampled correlation coefficients r*_{W,Z} that are less than the observed correlation r?",
    "question_context": "Bootstrap Hypothesis Testing for Correlation Coefficient\n\nThe paper discusses bootstrap hypothesis testing for the correlation coefficient ρ in a bivariate sample. The null hypothesis H₀: ρ = ρ₀ is tested against the alternative Hₐ: ρ < ρ₀. For ρ₀ ≠ 0, the data is rotated using a matrix B(ρ₀) to impose the null correlation on resamples. The bootstrap p-value is calculated based on resampled correlation coefficients r*_{W,Z}. The paper also discusses bootstrap power calculations by resampling from a distribution with correlation ρₐ."
  },
  {
    "qid": "statistic-posneg-72-0-0",
    "gold_answer": "FALSE. The paper explicitly states that the IIV weighting method is designed to adjust for dependent visit times (informative visit times) when the assumption of independent censoring is not satisfied. The consistency of the estimators is achieved under the sequentially missing at random (SMAR) condition (assumption 4) and the positivity condition, not by requiring independence between visit times and the failure time process. The IIV weights are constructed to account for the dependence structure.",
    "question": "Is the statement 'The IIV-weighted estimating equations lead to consistent estimators only if the visit times are independent of the failure time process' true? Explain based on the paper's assumptions and methodology.",
    "question_context": "Semiparametric Estimation for Additive Hazards Models with IIV Weighting\n\nThe paper discusses semiparametric regression models for failure time data, focusing on additive hazards models with time-invariant coefficients. The survivor function is specified by S(t|A_i) = S_0(t) exp(-A_i^T β t), where S_0(t) is the baseline survivor function. The methodology uses inverse-intensity-of-visit (IIV) weighting to adjust for dependent visit times. The estimating equations for the baseline survivor function and regression coefficients are provided, along with conditions for consistency and asymptotic normality of the estimators."
  },
  {
    "qid": "statistic-posneg-890-0-1",
    "gold_answer": "TRUE. As the difference $a$ increases, the probability $\\operatorname{pr}\\left(X_{1}-X_{2}>a\\right)$ decreases because the condition $X_{1}-X_{2}>a$ becomes less likely to be satisfied.",
    "question": "Does the probability $\\operatorname{pr}\\left(X_{1}-X_{2}>a\\right)$ decrease as the difference $a$ increases?",
    "question_context": "Comparison of Sequential Sampling Procedures for Binomial Populations\n\nIn this section we give finite sums for the exact distribution of the difference between two independent negative binomial random variables with general parameters. <PARAGRAPH_SEPARATOR> Consider two populations where the probability of success on a given trial for the ith population is $\\pmb{\\mathscr{p}}_{i}$ and probability of failure is $q_{i}=1-p_{i}(i=1,2)$ .Let $X_{i}$ represent the total number of trials required to obtain $\\boldsymbol{\\mathscr{r}}_{i}$ successes from the ith population. If $\\pmb{a}$ is a nonnegative integer, <PARAGRAPH_SEPARATOR> $$ \\operatorname{pr}\\left(X_{1}-X_{2}>a\\right)=\\sum_{n=r_{1}}^{\\infty}{\\binom{n-1}{r_{2}-1}}p_{2}^{r_{2}}q_{2}^{n-r_{2}}\\sum_{j=0}^{r_{1}-1}{\\binom{n+a}{j}}p_{1}^{j}q_{1}^{n+a-j}. $$ <PARAGRAPH_SEPARATOR> Writing $\\mathcal{n}^{(i)}X^{n-j}=d^{j}(X^{n}),$ we obtain <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\operatorname{pr}\\left(X_{1}-X_{2}>a\\right)=p_{2}^{r_{3}-1}\\overset{1}{\\underset{j=0}{\\sum}}\\frac{1}{j!}\\frac{p_{1}^{j}}{q_{2}^{r_{3}+a-j}}\\left[d^{j}\\langle X^{a+r_{3}}(1-X)^{-r_{3}}\\rangle\\right]}\\ &{\\qquad=\\overset{r_{1}-1}{\\underset{j=0}{\\sum}}\\overset{j}{\\underset{i=0}{\\sum}}\\binom{r_{2}+j-i-1}{r_{2}-1}\\binom{a+r_{2}}{i}p_{2}^{r_{3}}p_{1}^{j}q_{2}^{i-i}q_{1}^{a+r_{2}-i}(1-q_{1}q_{2})^{-(r_{3}+j-i)},}\\end{array} $$ <PARAGRAPH_SEPARATOR> using Leibnitz's formula for the differential product. If we make the substitution $u=r_{2}+j-i$ and use the well-known Incomplete Beta Function representation for the cumulative sum of binomial probabilities, we obtain <PARAGRAPH_SEPARATOR> $$ \\operatorname{pr}\\left(X_{1}-X_{2}>a\\right)=\\sum_{u=\\tau_{1}}^{r_{1}+r_{1}-1}\\binom{u-1}{r_{2}-1}p_{2}^{r_{3}}(p_{1}q_{2})^{u-r_{3}}(1-q_{1}q_{2})^{-u}J_{q_{1}}(a+u-r_{1}+1,r_{1}+r_{2}-u), $$ <PARAGRAPH_SEPARATOR> which is just a sum of products of point negative binomial probabilities and Incomplete Beta Functions."
  },
  {
    "qid": "statistic-posneg-1364-3-0",
    "gold_answer": "FALSE. The confidence set $C_{\\delta^{A^{\\bullet}}}$ uniformly dominates $C_{\\bar{X},\\sigma}^{0}$ only when $0 \\leqslant a \\leqslant a_{0}$, where $a_{0}$ is the largest value satisfying $G_{\\!}(a,c) \\geqslant 1$ and $H_{1}(a,c) \\geqslant 1$. For values of $a$ greater than $a_{0}$, the dominance is not guaranteed.",
    "question": "The confidence set $C_{\\delta^{A^{\\bullet}}}$ uniformly dominates the usual confidence set $C_{\\bar{X},\\sigma}^{0}$ for all values of $a$ greater than $a_{0}$.",
    "question_context": "Improved Confidence Sets Using Empirical Bayes Methods\n\nIn this section we consider various cases of the estimators and confidence sets constructed in Section 2.1. We pay particular attention to the type of prior information which may be useful in achieving the greatest possible improvement. <PARAGRAPH_SEPARATOR> ExAMPLE 1. The unbalanced one-way analysis of variance. In a oneway ANOVA model, it is assumed that there are $p$ treatments characterized by  the  levels $\\theta_{1},...,\\theta_{p}$ , and there are $n_{i}$ i.i.d. $N(\\theta_{i},\\sigma^{2})$ observations $X_{i1},...,X_{i n_{i}}$ . The variance, $\\sigma^{2}$ , is assumed to be known. <PARAGRAPH_SEPARATOR> By sufficiency, we can consider only procedures depending on ${\\bar{X}}_{i}=$ $\\textstyle(1/n_{i})\\sum_{i=1}^{n_{i}}X_{i j}$ $1\\leqslant i\\leqslant p$ The vector $\\bar{X}=(\\bar{X}_{1},...,\\bar{X}_{\\rho})^{\\prime}$ has a multivariate normal  distribution  with   mean $(\\theta_{1},...,\\theta_{p})^{\\prime}$ and covariance  matrix $\\varSigma=\\sigma^{2}D_{n}^{\\mathrm{~\\tiny~{~1~}~}}$ $D_{n}{=}\\mathrm{diag}(n_{1}{,}...,n_{p})$ . The usual confidence set is <PARAGRAPH_SEPARATOR> $$ C_{\\bar{X},\\sigma}^{0}=\\lbrace\\theta\\colon(\\theta-\\bar{X})^{\\prime}D_{n}(\\theta-\\bar{X})\\leqslant c^{2}\\sigma^{2}\\rbrace. $$ <PARAGRAPH_SEPARATOR> Corollary 2.1 provides better confidence sets, however. <PARAGRAPH_SEPARATOR> Consider the situation when prior information indicates $\\theta_{i}^{\\ddots}\\mathbf{s}$ are very likely to be close. Under the assumption that $\\theta_{i}=\\theta_{j}$ ,Vi, $j,$ the classical estimator (the uniformly minimum variance unbiased estimator and the maximum likelihood estimator) of the common value 0 is x=(1/N) Z?=1 $\\sum_{i=1}^{n_{i}}X_{i j}$ , where $N=\\Sigma n_{i}$ . In this situation, the reasonable estimator to use is one   which  shrinks $\\scriptstyle{\\overbar{X}}$ to an estimate of the common $\\theta$ If we take $A=D_{n}^{1/2}\\mathbf{11^{\\prime}}D_{n}^{1/2}/N_{!}$ we have $A^{*}=11^{\\prime}D_{n}/N.$ and <PARAGRAPH_SEPARATOR> $$ \\delta^{A^{*}}({\\bar{X}})={\\tilde{\\bar{x}}}\\mathbf{1}+\\left[1-{\\frac{a\\sigma^{2}}{\\sum_{i=1}^{p}n_{i}({\\bar{X}}_{i}-{\\bar{\\bar{x}}})^{2}}}\\right]^{+}({\\bar{X}}-{\\bar{\\bar{x}}}\\mathbf{1}), $$ <PARAGRAPH_SEPARATOR> a version of the positive-part Lindley estimator. Note that $A$ is a symmetric, idempotent matrix, and hence Corollary 2.1 shows that the confidenceset <PARAGRAPH_SEPARATOR> $$ C_{\\delta^{A^{\\bullet}}}=\\left\\{\\theta\\colon[\\theta-\\delta^{A^{\\bullet}}(\\bar{x})]^{\\prime}D_{n}[\\theta-\\delta^{A^{\\bullet}}(\\bar{X})]\\leqslant c^{2}\\sigma^{2}\\right\\} $$ <PARAGRAPH_SEPARATOR> uniformly dominates $C_{\\vec{X},\\sigma}^{0}$ provided $0\\leqslant a\\leqslant a_{0}$ , where $a_{0}$ is the largest value which  satisfies $G_{\\!}(a,c)\\geqslant1$ and $H_{1}(a,c)\\geqslant1$ For $a=0.1$ and 0.05, the values of $a_{0}$ are given in Table I."
  },
  {
    "qid": "statistic-posneg-790-0-3",
    "gold_answer": "TRUE. The model specifies that the random effects u_i are independently distributed as N_{16}(0_{16}, Σ), as shown in the formula for u_1, ..., u_30. This allows for flexible subject-level deviations in both the mixing and precision components.",
    "question": "Does the model assume that the random effects u_i are independently distributed with a 16-dimensional normal distribution with mean 0 and covariance matrix Σ?",
    "question_context": "Hierarchical Bayesian Model for Bimodal Angular Judgement Data\n\nWe require a method for evaluating localization performance differences between groups across the entire set of signal azimuths. A reasonable approach is to model the bimodal, angular judgement data via a regression model, with patient group and signal position as covariates, while accounting for the repeated measures aspect of the study design. In this paper, we take a hierarchical Bayesian approach to model judgements of all signals by using structured, correlated circular $B$ -splines across mixing and precision components of a localization performance model."
  },
  {
    "qid": "statistic-posneg-1078-0-3",
    "gold_answer": "FALSE. The Chandrasekhar recursions are computationally efficient because they avoid updating the large matrix P_t at each step, unlike the Kalman filter. Instead, they update smaller auxiliary matrices (K_t, M_t, L_t), reducing the computational burden.",
    "question": "The Chandrasekhar recursions for evaluating the Gaussian likelihood require updating a large matrix P_t at each time step, making them computationally intensive. True or False?",
    "question_context": "Structured VARMA Models and Exact Maximum Likelihood Estimation\n\nThe paper discusses structured VARMA models, focusing on echelon forms based on Kronecker indices and scalar component models (SCM). It covers I(1) VARMA models, their transformation to stationary VARMA models, and the evaluation of exact likelihood using recurrence equations. The paper also compares exact maximum likelihood estimation with conditional methods."
  },
  {
    "qid": "statistic-posneg-2065-9-0",
    "gold_answer": "TRUE. The calculated probability of 0.8258 (or 0.17 when considering the integral from $q$ to $\\infty$) is indeed much greater than the 0.05 significance level. This means that the observed $F_{1}$ value of 0.0018 is not statistically significant, and the 'negative interaction' is not significant based on this test.",
    "question": "Is the probability calculation ${}_{18}\\mathcal{p}_{6}(0\\cdot04)=a_{0}+a_{1}(1-8/\\nu)/\\nu=0\\cdot8258$ correctly interpreted to mean that the observed $F_{1}$ value is not significantly small at the 0.05 level?",
    "question_context": "Analysis of Variance: Testing the Smallest Variance Ratio\n\nThere is satisfactory agreement between the two methods. Indeed, as is to be expected, the agreement is much better for $q=0{\\cdot}01$ than for $q=0{\\cdot}10$ . The difference in the top row values in columns (2) and (3) is due to calculating the latter up to the term in $\\nu^{\\pmb{\\mathscr{A}}}$ using the incomplete beta-function expansion (10) of Part I. It will be seen that for this expansion, terms up to $\\nu^{-2}$ give a five decimal accuracy when $q=0{\\cdot}10$ and this improves to six decimals when $q=0{\\cdot}01$ <PARAGRAPH_SEPARATOR> We may, before concluding, add an illustration for the use of the test of the smallest variance ratio. This is also taken from Wishart's (1938) paper, where by the ordinary test of significance he gets variance for an interaction NP as significantly less than the error variance. He calls this a *negative interaction'. Applying Bartlett's test, Wishart found that this ' negative′ interaction was not significant. The values concerned are $\\scriptstyle\\vartheta_{1}^{\\tt Q}=0\\cdot01$ , $\\pmb{\\vartheta_{0}^{8}}=5{\\cdot}57$ , $\\pmb{k}=\\uppmb{6}$ , $\\pmb{m}=1$ , $\\nu=18$ . To apply the test for significance of the smallest variance ratio, we calculate F = 5.57 $F_{1}={\\frac{0\\cdot01}{5\\cdot57}}=0{\\cdot}0018$ giving $q=0{\\cdot}04$ . Using (26) and referring to Table 5, we get <PARAGRAPH_SEPARATOR> $$ {}_{18}\\mathcal{p}_{6}(0\\cdot04)=a_{0}+a_{1}(1-8/\\nu)/\\nu=0\\cdot8258. $$ <PARAGRAPH_SEPARATOR> The probability that $\\pmb{F_{1}}$ is less than or equal to the observed value is about 0·17, which is much greater than 0.05. The observed $\\pmb{F_{1}}$ is therefore not significantly small."
  },
  {
    "qid": "statistic-posneg-883-4-0",
    "gold_answer": "TRUE. The inequality is correctly derived by simplifying the ratio of the posterior densities and bounding the term $\\left[2 + \\left(\\frac{\\sigma_{e}^{2}}{\\sigma_{e}^{2}+3\\sigma_{\\theta}^{2}}\\right)^{2}\\right]^{1/2}$ by $\\sqrt{3}$. Since $C_{3} \\approx 0.96$, the exponent $(1-C_{3})/2$ is positive, ensuring that $(\\sigma_{\\theta}^{2})^{(1-C_{3})/2}$ is well-behaved. This bound, combined with Proposition 1, guarantees that $E_{\\pi_{s}}|f_{i}p_{r}/p_{s}|^{3} < \\infty$ for all $i=0,1,2,3$, making the importance sampling technique applicable.",
    "question": "Is the inequality $\\frac{p_{r}(\\mu,\\sigma_{\\theta}^{2},\\sigma_{e}^{2})}{p_{s}(\\mu,\\sigma_{\\theta}^{2},\\sigma_{e}^{2})} \\leq \\sqrt{3}(\\sigma_{\\theta}^{2})^{(1-C_{3})/2}$ correctly derived from the given formula, and does it ensure the finiteness of $E_{\\pi_{s}}|f_{i}p_{r}/p_{s}|^{3}$ for $i=0,1,2,3$?",
    "question_context": "Bayesian Importance Sampling with Multiple Markov Chains\n\nHere, we use the Bayesian model from the previous subsection to analyze a real dataset from Lyles, Kupper, and Rappaport (1997). Thirteen workers were randomly selected from a group within a boat manufacturing plant and each worker’s styrene exposure was measured on three separate occasions. The dataset was previously analyzed in Tan and Hobert (2009), which reproduces the data and also gives some summary statistics. The two posterior distributions for $(\\theta,\\mu,\\sigma_{\\theta}^{2},\\sigma_{e}^{2})$ under consideration are $\\pi_{s}$ , which is based on the standard diffuse prior, and $\\pi_{r}$ , which is based on the reference prior. Both posteriors are proper since $q=13$ . We will focus on the posterior expectations of three functions: $f_{1}(\\mu,\\sigma_{\\theta}^{2},\\sigma_{e}^{2})=\\sigma_{\\theta}^{2}$ , $f_{2}(\\mu,\\sigma_{\\theta}^{2},\\sigma_{e}^{2})=\\sigma_{e}^{2}$ , and $f_{3}(\\mu,\\sigma_{\\theta}^{2},\\sigma_{e}^{2})=\\sigma_{\\theta}^{2}/(\\sigma_{\\theta}^{2}+\\sigma_{e}^{2})$ . Note that $f_{3}$ is the correlation between observations on the same worker. <PARAGRAPH_SEPARATOR> Tan and Hobert (2009) used their minorization condition to simulate $R=40{,}000$ regenerations of the block Gibbs sampler for $\\pi_{s}$ . This simulation was used to produce estimates and standard errors for $E_{\\pi_{s}}f_{i}(\\mu,\\sigma_{\\theta}^{2},\\sigma_{e}^{2})$ , $i={1,2,3}$ , and their results are summarized in Table 1. Figure 1 gives trace plots that justify the choice $R=40,000.$ Now consider reusing Tan and Hobert’s (2009) block Gibbs output to produce estimates and standard errors for $E_{\\pi_{r}}f_{i}(\\mu,\\sigma_{\\theta}^{2},\\sigma_{e}^{2})$ , $i={1,2,3}$ . According to Corollary 1, our importance sampling results are applicable if we can find $\\epsilon>0$ such that $E_{\\pi_{s}}|f_{i}\\:p_{r}/p_{s}|^{2+\\epsilon}<\\infty$ for $i=0$ , 1, 2, 3, where $f_{0}\\equiv1$ . First, note that $|f_{i}p_{r}/p_{s}|=f_{i}p_{r}/p_{s}$ , since all the terms are positive. Now <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\frac{p_{r}\\left(\\mu,\\sigma_{\\theta}^{2},\\sigma_{e}^{2}\\right)}{p_{s}\\left(\\mu,\\sigma_{\\theta}^{2},\\sigma_{e}^{2}\\right)}=\\frac{\\left(\\sigma_{\\theta}^{2}\\right)^{-C_{3}/2}(\\sigma_{e}^{2})^{-1}\\left[2+\\left(\\sigma_{e}^{2}/\\left(\\sigma_{e}^{2}+3\\sigma_{\\theta}^{2}\\right)\\right)^{2}\\right]^{1/2}}{(\\sigma_{\\theta}^{2})^{-1/2}(\\sigma_{e}^{2})^{-1}}}\\ &{\\qquad=\\left(\\sigma_{\\theta}^{2}\\right)^{(1-C_{3})/2}\\left[2+\\left(\\frac{\\sigma_{e}^{2}}{\\sigma_{e}^{2}+3\\sigma_{\\theta}^{2}}\\right)^{2}\\right]^{1/2}\\leq\\sqrt{3}\\left(\\sigma_{\\theta}^{2}\\right)^{(1-C_{3})/2},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $C_{3}\\doteq0.96$ . This inequality together with Proposition 1 shows that $E_{\\pi_{s}}|f_{i}p_{r}/p_{s}|^{3}<$ $\\infty$ for $i=0,1,2,3$ . Hence, our importance sampling technique is applicable, and the results are given in Table 1. Again, Figure 1 justifies the use of $R=40{,}000$ . From Table 1, we see that the estimates under the two priors are very close, so the issues regarding choice of the prior raised by Bernardo (1996) are not a concern for this dataset."
  },
  {
    "qid": "statistic-posneg-768-6-1",
    "gold_answer": "FALSE. The bound shows that the error of the mixture estimator is a weighted average of the errors of the parametric and nonparametric estimators, not necessarily less than or equal to the maximum of the two. The weights $\\hat{\\alpha}_{n}$ and $(1-\\hat{\\alpha}_{n})$ determine the contribution of each estimator's error to the total error.",
    "question": "Does the bound $|\\widetilde{f}_{n}(x)-f_{0}(x)| \\leq \\hat{\\alpha}_{n}|f_{\\hat{\\theta}_{n}}(x)-f_{0}(x)| + (1-\\hat{\\alpha}_{n})|\\hat{f}_{n}(x)-f_{0}(x)|$ imply that the error of the mixture estimator $\\widetilde{f}_{n}(x)$ is always less than or equal to the maximum error of the parametric and nonparametric estimators?",
    "question_context": "Fitness Coefficient and Density Estimation in Parametric vs Nonparametric Models\n\nThe paper discusses the fitness coefficient $\\hat{\\alpha}_{n}$, which is a maximizer of the mixture likelihood function $L_{n}(\\alpha)$. This function is defined as a weighted sum of the log-likelihoods of a parametric model $f_{\\hat{\\theta}_{n}}(X_{i})$ and a nonparametric estimator $\\hat{f}_{n,i}^{\\mathrm{LR}}$. The fitness coefficient helps in determining the adequacy of the parametric model compared to the nonparametric one. The paper also provides bounds and convergence results for the estimators under different conditions."
  },
  {
    "qid": "statistic-posneg-824-0-0",
    "gold_answer": "FALSE. Explanation: The context explicitly states that Algorithm 3 is restricted to tail-independent models (\"The concrete structure of the intensity and reward matrix underlying Algorithm 3 restricts its application to tail-independent models\"). While the fitted distribution matches marginal tail indices ($\\lambda_{1}^{\\mathrm{(max)}}$, $\\lambda_{2}^{\\mathrm{(max)}}$) and moments well, the original Mardia distribution has $\\lambda_{U}=0$ (tail independence), and the fitted model's $\\hat{\\lambda}_{U}$ is not reported. The algorithm's restriction implies it cannot model tail dependence, so any claim about recovering tail dependence structure is incorrect. The evaluation focuses on marginal fits and likelihood, not tail dependence.",
    "question": "Is the statement 'The fitted matrix-Pareto distribution using Algorithm 3 accurately recovers the tail dependence structure of the original Mardia type I Pareto distribution, as evidenced by the close match between the theoretical $\\lambda_{U}=0$ and the estimated $\\hat{\\lambda}_{U}$ from the fitted model' correct based on the provided context?",
    "question_context": "Matrix-Pareto Distribution Fitting and Tail Dependence Analysis\n\nExample 5. (Mardia type I) We generated an i.i.d. sample of size 10, 000 from a (translated) Mardia type I Pareto distribution (see Mardia (1962)) with parameters $\\sigma_{1}=\\sigma_{2}=1$ and $\\alpha=2$ . This distribution has theoretical numerical values $\\mathbb{E}(\\mathbf{X})=(1,1)^{\\prime}$ , $\\lambda_{U}=0$ and $\\rho_{\\tau}=0.2$ . The simulated sample has numerical values $\\hat{\\mathbb{E}}(\\mathbf{X})=(0.9812,0.9712)^{\\prime}$ and $\\hat{\\rho}_{\\tau}=0.2049$ . <PARAGRAPH_SEPARATOR> We fitted a bivariate matrix–Pareto distribution using Algorithm 3 with $p_{1}=p_{2}=2$ (i.e., $p=4,$ ) and 2, 000 steps on the transformed data (with a running time of 530 s), getting the following parameters: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\hat{\\alpha}=(0.1666,0.8334,0,0),}\\end{array} $$ <PARAGRAPH_SEPARATOR> The tails of the marginals of the fitted distribution are determined by the real part of the eigenvalues with largest real part of the sub-intensity matrices of the marginal distributions, which are 𝜆(1max) = −1.9785 and 𝜆(2max) $\\lambda_{2}^{\\mathrm{(max)}}=-2.0035$ . These resemble well the ones of the original distribution. The fitted distribution has first moment $\\mathbb{E}(X)=(1.0164,0.9963)^{\\prime}$ . Moreover, we estimated $\\rho_{\\tau}$ via simulation, obtaining $\\hat{\\rho}_{\\tau}=0.1582$ . The QQ plots are available in Figure 9 and contour plots are depicted in Figure 10, from where it becomes clear that the algorithm recovers the structure of the data well. Again, the log-likelihood of the fitted bivariate matrix–Pareto (−15, 550.52) exceeds the log-likelihood using the original Mardia distribution (−16, 146.65) (Figure 11)."
  },
  {
    "qid": "statistic-posneg-1876-0-0",
    "gold_answer": "TRUE. The additive property holds because the likelihood ratio criteria λ₁, λ₂, and λ₃ decompose the total discrepancy λ₄ when testing the joint hypothesis H₄ (complete independence with specified marginal distributions). This decomposition reflects the partitioning of information in the contingency table, analogous to the decomposition of variance in ANOVA, and is valid under the asymptotic χ² distributions of these statistics.",
    "question": "Is the additive property of the likelihood ratio criterion, as shown by the equation λ₄ = λ₁ + λ₂ + λ₃, valid for testing hypotheses H₁, H₂, H₃, and H₄ in a contingency table?",
    "question_context": "Goodness of Fit and Likelihood Ratio Criteria in Contingency Tables\n\nthat the underlying distribution is normal, then the methods given by Dr. Watson in his Biometrika paper can be used for finding the best groups. If, however, the data will need transformation the question then arises as to whether a test of goodness of fit is to be carried out before or after the data are transformed for analysis. Although it may be simpler to transform the data and then test, this is really begging the question. When a statistician meets data of a type he has never seen before he has to find the distribution that best fits them before he can proceed at all.<PARAGRAPH_SEPARATOR>If a record is to be taken in categories it is not always automatic where the bounds of these categories are to be taken. Since those bounds are best that lose least information then, if the form of the underlying distribution is known, the methods set out by Dr. Watson could be used to discriminate between the various records. Thus it would seem that Dr. Watson's paper has some immediate applications to various problems arising in the analysis of experimental results.<PARAGRAPH_SEPARATOR>Mr. D. V. LINDLEY: The appearance of a paper on the problem of goodness of fit at a Research Section meeting tempts me to raise the old point of why the criterion<PARAGRAPH_SEPARATOR>$$(\\mathrm{observed}-\\mathrm{expected})^{2}/(\\mathrm{expected})$$<PARAGRAPH_SEPARATOR>should be used at all. Admittedly it has some optimum asymptotic properties, but so have a lot of other criteria including that obtained by the likelihood ratio method. It is known that the likelihood ratio method can give rise to ridiculous results, but in the class of situationswhere the $(O-E)^{2}/E$ criterion is used it would appear to give satisfactory answers. Furthermore it has some useful properties denied to the other criterion.<PARAGRAPH_SEPARATOR>Firstly it has an additive property. If we take a $k\\times l$ contingency table with $\\pmb{p}_{i j}$ a1 the probability of an event in the celi in the $i^{\\mathrm{th}}$ row and $j^{\\mathrm{th}}$ column, and if<PARAGRAPH_SEPARATOR>$$\\sum_{j=1}^{l}p_{i j}=p_{i}.,\\qquad\\sum_{i=1}^{k}p_{i j}=p.j,$$<PARAGRAPH_SEPARATOR>then for the four hypotheses<PARAGRAPH_SEPARATOR>$$\\begin{array}{l}{{H_{1}:p_{i\\cdot}=p_{i\\cdot}{\\left(0\\right)}}}\\\\ {{H_{2}:p_{\\cdot j}=p_{\\cdot j}{\\left(0\\right)}}}\\\\ {{H_{3}:p_{i j}=p_{i\\cdot}{p_{\\cdot j}}}}\\\\ {{H_{4}:p_{i j}=p_{i\\cdot}{\\left(0\\right)}p_{\\cdot j}{\\left(0\\right)}}}\\end{array}$$<PARAGRAPH_SEPARATOR>and with $\\lambda_{i}$ as the likelihood ratio criterion for testing $H_{i}$ $H_{3}$ is the usual hypothesis of independence in a contingency table), then<PARAGRAPH_SEPARATOR>$$\\lambda_{4}=\\lambda_{1}+\\lambda_{2}+\\lambda_{3}.$$<PARAGRAPH_SEPARATOR>The argument extends to $k\\times l\\times m$ and higher order contingency tables though the \"interaction\" terms do not have the asymptotic $\\mathbf{\\bar{\\chi}}^{2}$ distributionsthatthese $\\lambda_{i}$ have.Nevertheless a sort of analysis of variance technique can be employed. A key reference here is a paper by W. R. Garner and W. J. McGill, Psychometrika, 21 (1956), 219.<PARAGRAPH_SEPARATOR>In fact this quasi-analysis of variance is more properly thought of as analysis of information—and it is this connection that excites me the most. In the case of $\\mathbf{\\dot{\\calH}_{1}}$ above, if the second suffix is dropped for convenience, the likelihood ratio criterion is<PARAGRAPH_SEPARATOR>$$\\sum_{i=1}^{k}{\\frac{n_{i}}{n.}}\\ln{\\frac{n_{i}}{n.}}-\\sum_{i=1}^{k}{\\frac{n_{i}}{n.}}\\ln{p_{i}(0)},$$<PARAGRAPH_SEPARATOR>where $n_{i}$ is the observed number and<PARAGRAPH_SEPARATOR>$$n=\\sum_{i=1}^{k}n_{i}.$$<PARAGRAPH_SEPARATOR>If $p_{i}(0)=k^{-1}$ the second term may be ignored and the criterion is, apart from a minus sign, Shannon's information function. The null hypothesis chosen here corresponds to no information being given by the row classification. Extensions to more dimensions are immediate.<PARAGRAPH_SEPARATOR>It would be interesting to have some small-sample investigations into the relative powers of the two criteria. Anyone with an electronic computer to spare may like to investigate the problem; an analytic solution seems inconvenient.<PARAGRAPH_SEPARATOR>Dr. D. R. Cox: I should like to outline a very crude and approximate method of tackling one of Dr. Watson's problems, which may throw a little light on some of his numerical results. _ In fitting a normal curve, let $\\bar{x}_{u},s_{u}{}^{2}$ be the ungrouped mean and varianceand  let $h$ be the grouping interval. Assume that the maximum likelihood estimates from the grouped data are effectively the grouped mean and variance $\\bar{x}_{g},s_{g}{}^{2}$ Then<PARAGRAPH_SEPARATOR>$$\\begin{array}{c c}{{\\displaystyle V(\\bar{x}_{u})=\\sigma^{2}/n,}}&{{V(\\bar{x}_{g})=\\bigg(\\sigma^{2}+\\frac{1}{12}h^{2}\\bigg)/n,}}\\\\ {{\\displaystyle C(\\bar{x}_{u},\\bar{x}_{g})=\\sigma^{2}/n,E(\\bar{x}_{u}-\\bar{x}_{g})^{2}=\\frac{1}{12}\\frac{h^{2}}{n}.}}\\end{array}$$<PARAGRAPH_SEPARATOR>Now, so far as the distribution of $X^{2}$ isconcerned, ${\\bar{x}}_{g}$ is to be considered as an efficient estimateand $\\bar{x}_{u}$ as inefficient, the reverse of the usual situation. Hence the efficiency of estimationis<PARAGRAPH_SEPARATOR>$$1{\\Big/}{\\Big(}1+{\\frac{1}{12}}{\\frac{h^{2}}{\\sigma^{2}}}{\\Big)}$$<PARAGRAPH_SEPARATOR>and by the rule quoted by Cochran in his review article on the $\\chi^{2}$ test (Ann. Math. Statist., 23 (1952), 315) this inflates the expectation of $X^{2}$ by $\\mathbf{\\widetilde{12}}\\mathbf{\\varepsilon}h^{2}/\\sigma^{2}$ The contribution from the estimation of the variance is likewise approximately $\\textstyle{\\frac{1}{6}}h^{2}/\\sigma^{2}$ ; in deriving this it is assumed that Sheppard's correction in reverse is added to $s\\boldsymbol{u}^{2}$ before calculating expected frequencies, although this is important only with a sample size greater than about 300. Hence the increase in the expected value of $X^{2}$ is approximately $\\scriptstyle{\\frac{1}{4}}h^{2}/\\sigma^{2}$<PARAGRAPH_SEPARATOR>This result may be compared with some numerical values given by Dr. Watson in an earlier paper (Biometrika, 44 (1957), 336). It is reasonable to expect the approximation to work at all well only when the number of groups is not too small.<PARAGRAPH_SEPARATOR><html><body><table><tr><td>No.of</td><td></td><td>Exact Increment</td><td>Rough</td></tr><tr><td>Groups</td><td>h/o</td><td>toE(X2)</td><td>Formula</td></tr><tr><td>10</td><td>1</td><td>0·22</td><td>0.25</td></tr><tr><td>10</td><td>x21</td><td>0·13</td><td>0·06</td></tr><tr><td>6</td><td></td><td>0·26</td><td>0·25</td></tr><tr><td>6</td><td>12</td><td>0·52</td><td>0·06</td></tr></table></body></html>"
  },
  {
    "qid": "statistic-posneg-1170-0-3",
    "gold_answer": "FALSE. The approximation uses $u$ values corresponding to $k/(n-1)$, where $k = r_1 + r_2$, and these values are obtained by interpolation from a table. The paper emphasizes that the variance is largely independent of the individual $r_1$ and $r_2$ values, as long as $k$ is fixed.",
    "question": "Does the approximation $\\arcsin{(\\tilde{\\sigma})}=u\\sigma^{2}/(n-1{\\cdot}25)$ for censored samples require exact knowledge of the censoring pattern (i.e., specific $r_1$ and $r_2$ values)?",
    "question_context": "Variance Estimation in Normal Population from Doubly Censored Samples\n\nThe paper discusses the efficiency of the estimator $\tilde{\\sigma}$ for normal population standard deviation under various censoring conditions. It provides simplified equations for the estimator and its variance in uncensored samples, and introduces approximation procedures for variance estimation. The reciprocal of the variance of $\tilde{\\sigma}$ is shown to be nearly linear in sample size, leading to a simple approximation formula. Comparisons are made with the unbiased root mean square estimator, and methods for handling censored samples are detailed."
  },
  {
    "qid": "statistic-posneg-800-0-0",
    "gold_answer": "FALSE. While the NNWCLP method can include non-symmetric weights that the DDWCLP method ignores (as shown in the toy example with weights $w_{14}=1$ and $w_{34}=1$), it does not guarantee that it always includes more pairs. The selection depends on the spatial configuration of the points and the choice of $m$ (nearest neighbors) versus $k$ (distance threshold). The key advantage of NNWCLP is its ability to capture asymmetric relationships, not necessarily a higher count of pairs.",
    "question": "Is it true that the NNWCLP method always includes more pairwise log-likelihoods than the DDWCLP method when estimating parameters of the Tukey-hh RF?",
    "question_context": "Nearest-Neighbors Weighted Composite Likelihood Estimation for Tukey-hh Random Fields\n\nFor the rest of the paper, given an RF $Z=\\{Z(s),s\\in A\\}$ defined on $A\\subset\\mathbb{R}^{d}$ , with $\\mathbb{E}(Z(s))=\\mu(s)$ and $V a r(Z(s))=\\sigma^{2}$ , we denote by $\\rho_{Z}(d)=C o r r(Z(s_{i}),Z(s_{j}))$ its correlation function, where $d=s_{i}-s_{j}$ is the lag separation vector. <PARAGRAPH_SEPARATOR> For any set of distinct points $(s_{1},\\ldots,s_{n})^{T}$ , $\\pmb{s}_{i}\\in\\pmb{A}$ , $n\\in\\mathcal{N}$ , we denote by $\\begin{array}{r}{Z_{i j}=(Z(s_{i}),Z(s_{j}))^{T},}\\end{array}$ $i\\neq j$ and $Z_{i|j}=Z(s_{i})\\mid Z(s_{j})=z_{j}$ the bivariate random vector and the conditional random variable respectively and we denote by $Z=(Z(s_{1}),\\ldots,Z(s_{n}))^{T}$ the multivariate random vector. In addition, we denote with $f_{Z_{i j}},f_{Z_{i|j}}$ and $f_{Z}$ the associated probability density functions and we denote $f_{Z_{k}}$ as the marginal density function of $Z(s_{k})$ . Finally, we denote $Z^{*}$ as the standardized RF, 𝑖.𝑒., $Z^{*}(s):=(Z(s)-\\mu(s))/\\sigma$ . <PARAGRAPH_SEPARATOR> Following Varin et al. (2011) and Lindsay (1988) consider a random vector $Z$ with 𝑝𝑑𝑓 $f_{Z}(z;\\theta)$ for some unknown parameter vector $\\theta$ . Denote by $\\{B_{1},\\ldots,B_{K}\\}$ a set of marginal or conditional events with associated log-likelihood $l_{k}(z;\\theta)=l o g(f_{Z}(z\\in B_{k},\\theta))$ , $k=1,\\ldots,K$ . The log-CL is an objective function defined as a sum of $K$ sub-log-likelihoods <PARAGRAPH_SEPARATOR> $$ C L(\\theta)=\\sum_{k=1}^{K}l_{k}(z;\\theta)w_{k}, $$ <PARAGRAPH_SEPARATOR> where $w_{k}$ are suitable weights that do not depend on 𝜽. The maximum CL estimate is given by $\\hat{\\boldsymbol{\\theta}}=\\mathrm{argmax}_{\\boldsymbol{\\theta}}C L(\\boldsymbol{\\theta})$ . <PARAGRAPH_SEPARATOR> The WCLP estimation method (Bevilacqua and Gaetan, 2015) is a special case of (2) obtained by setting $B_{k}={Z}_{i j}$ or $B_{k}=Z_{i|j}$ . In the first case we obtain the pairwise marginal log-likelihood $l_{i j}(\\pmb\\theta)=l o g(f_{Z_{i j}}(z_{i j},\\pmb\\theta))$ and in the second case we obtain the pairwise conditional log-likelihood $l_{i|j}(\\theta)=l o g(f_{Z_{i j}}(z_{i j},\\theta)/f_{Z_{j}}(z_{j},\\theta))$ . The corresponding weighted composite log-likelihoods functions are given by: <PARAGRAPH_SEPARATOR> $$ w p l_{M}(\\theta)=\\sum_{i=1}^{n}\\sum_{j\\neq i}^{n}l_{i j}(\\theta)w_{i j},\\qquadw p l_{C}(\\theta)=\\sum_{i=1}^{n}\\sum_{j\\neq i}^{n}l_{i|j}(\\theta)w_{i j}, $$ <PARAGRAPH_SEPARATOR> and $\\hat{\\boldsymbol{\\theta}}_{a}=\\mathrm{argmax}_{\\boldsymbol{\\theta}}w\\boldsymbol{p l}_{a}(\\boldsymbol{\\theta}).$ , where $a=M,C$ is the associated estimator. Note that, assuming non-zero weights, the computational cost associated with both functions is of order $O(n^{2})$ . In general, a loss of statistical efficiency is expected for both cases with respect to the <PARAGRAPH_SEPARATOR> ML estimation and the role of the weights $w_{i j}$ is to minimize this loss. Using theory of optimal estimating equations (Heyde, 1997), it can be easily seen (Bevilacqua et al., 2012) that the optimal weights require the computation of the inverse of a $n(n-1)\\times n(n-1)$ matrix which is even computationally harder than the requirements for ML estimation. Some approximations of the optimal weights have been proposed in literature as for instance in Li and Sang (2018) and Pace et al. (2019). However the computation of this kind of weights can be computationally demanding for large $n$ . <PARAGRAPH_SEPARATOR> To avoid this computational problem different authors (Bai et al., 2014; Bevilacqua and Gaetan, 2015; Feng et al., 2014; Heagerty and Lele, 1998; Varin and Vidoni, 2005) have proposed the DDWCLP method that considers the weight function: <PARAGRAPH_SEPARATOR> $$ w_{i j}(k)=\\left\\{{1\\atop{\\mathrm{()}}}\\right.\\Vert s_{i}-s_{j}\\Vert<k $$ <PARAGRAPH_SEPARATOR> where $k\\in\\mathbb{R}^{+}$ is an arbitrary distance greater than the minimum distance of the location points. This kind of weights allows ruling out a certain percentage (depending on $k$ ) of the total number of pairs allowing a clear computational gain with respect to the non-zero weighted version. Additionally, it has been shown that this kind of weights improves the statistical efficiency of the method with respect to the use of constant weights (see for instance Joe and Lee (2009), Davis and Yau (2011) and Bevilacqua et al. (2012)). <PARAGRAPH_SEPARATOR> It should be outlined that the function (4) restricts the weights to be symmetric i.e. $w_{i j}(k)=w_{j i}(k)$ and this is a potential limitation, particularly when considering the conditional likelihood $l_{i|j}(\\boldsymbol{\\theta})$ which is not symmetric. <PARAGRAPH_SEPARATOR> Our proposal (NNWCLP) considers weights based on nearest neighbors that can be either symmetric or not symmetric. Specifically, let $N_{m}(\\mathbf{s}_{l})$ be the set of neighbors of order $m=1,2,\\ldots$ of the point $\\mathbf{s}_{l}\\in A$ . We propose the following weight function: <PARAGRAPH_SEPARATOR> $$ w_{i j}(m)=\\left\\{{1\\begin{array}{l l}{{\\mathbf{s}_{i}\\in N_{m}(\\mathbf{s}_{j})}}\\ {{0}}\\end{array}}\\right. $$ <PARAGRAPH_SEPARATOR> for $i,j=1,\\dots,n$ and $i\\neq j$ . By construction the weights $w_{i j}(m)$ can be either symmetric or not and to illustrate this we consider a simple toy example with four location sites $\\mathbf{s}_{1}=(0.15,0.75)^{T}$ , ${\\bf s}_{2}=(0.2,0.85)^{T}$ , $\\mathbf{s}_{3}=(0.3,0.7)^{T}$ and $\\mathbf{s}_{4}=(0.26,0.35)^{T}$ . The left part of Fig. 1 depicts the weights selected using the weight function (4) based on distances $w_{i j}(k)$ and setting $k=0.36$ . On the right part, the weights selected using the weight function (5) based on nearest neighbors $w_{i j}(m)$ with $m=2$ are depicted (the zero weights are ignored in both cases). It can be appreciated that the number of selected weights is the same in both cases and the two weight functions share most of the weights. However, the NNWCLP method includes the weights $w_{14}=1$ and $w_{34}=1$ while the DDWCLP method includes the symmetric weights $w_{34}=1$ and $w_{43}=1$ . <PARAGRAPH_SEPARATOR> This simple example shows that the proposed weight function can potentially include weights (and as a consequence pairwise or conditional log-likelihoods) that the method based on distances ignores. Thus, in principle, more information is considered when estimating with NNWCLP than with DDWCLP. An interesting question is whether this implies a gain in statistical efficiency. In Section 4, a simulation study shows that the proposed NNWCLP method actually outperforms the DDWCLP method. <PARAGRAPH_SEPARATOR> In addition, the proposed weight function is computationally convenient since kd-tree type algorithms (Elseberg et al., 2012; Bentley, 1975; Arya et al., 1998) can be exploited to drastically reduce the computational costs of the WCLP functions in Equation (3). Two preliminary steps are required before the optimization of the WCLP functions: 1) building a kd-tree that typically requires $O(n l o g(n))$ time complexity and $O(n)$ associated storage and 2) searching for $m$ nearest neighbors inside the kd-tree that has an $O(m l o g(n))$ time complexity. In our implementation in the $R$ package GeoModels these preliminary steps are performed, using the function GeoNeighIndex that exploits the function knn of the $R$ package 𝑛𝑎𝑏𝑜𝑟 (Elseberg et al., 2012). <PARAGRAPH_SEPARATOR> The final step involves the maximization of ${w p l}_{a}(\\theta)$ , $a=M,C$ functions in (3) that can be computed in $O(m n)$ time, summing up only the $l_{i j}(\\boldsymbol{\\theta})$ or $l_{i|j}(\\boldsymbol{\\theta})$ functions selected through the nearest neighbors weight function. <PARAGRAPH_SEPARATOR> The maximum XWCLP estimator, where $\\Chi=\\Nu\\Nu$ and DD is given by <PARAGRAPH_SEPARATOR> $$ \\widehat{\\theta}_{a}:=\\mathrm{argmax}_{\\theta}w p l_{a}(\\theta),\\quad a=M,C, $$ <PARAGRAPH_SEPARATOR> and, as in Bevilacqua and Gaetan (2015), under some mixing conditions of the Tukey-ℎℎ RF and under the assumption that the weight function is compactly supported as in (4) or (5), it can be shown that, under increasing domain asymptotics, $\\widehat{\\pmb{\\theta}}_{a}$ is consistent and asymptotically Gaussian with the asymptotic covariance matrix given by $G_{a}^{-1}(\\theta)$ the inverse of the Godambe information $G_{a}(\\theta):=H_{a}(\\theta)J_{a}(\\theta)^{-1}H_{a}(\\theta).$ , where $H_{a}(\\theta):=\\mathbb{E}[-\\nabla^{2}w p l_{a}(\\theta)]$ and $J_{a}(\\theta):=\\mathrm{Var}[\\nabla w p l_{a}(\\theta)]$ . Standard error estimation can be obtained considering the square root diagonal elements of $G_{a}^{-1}({\\widehat{\\pmb\\theta}})$ . Moreover, model selection can be performed by considering the information criterion, defined as <PARAGRAPH_SEPARATOR> $$ \\mathrm{PLIC}:=-2w p l_{a}(\\hat{\\theta})+2\\mathrm{tr}(H_{a}(\\hat{\\theta})G_{a}^{-1}(\\hat{\\theta})), $$ <PARAGRAPH_SEPARATOR> which is composite likelihood version of the Akaike information criterion (AIC) (Varin and Vidoni, 2005). Note that, the computation of standard errors and PLIC require evaluation of the matrices $H_{a}(\\hat{\\pmb\\theta})$ and $J_{a}({\\hat{\\pmb\\theta}})$ . However, the evaluation of $J_{a}({\\hat{\\boldsymbol{\\theta}}})$ is computationally unfeasible for large datasets and in this case subsampling techniques can be used to estimate $J_{a}(\\theta)$ as in Bevilacqua et al. (2012) and Heagerty and Lele (1998). A straightforward and more robust alternative that we adopt in Section 4.1 and in Section 5, is parametric bootstrap estimation of $G_{a}^{-1}(\\theta)$ . Since this technique is simulation based, fast methods of simulation of Gaussian RFs such as circulant embedding or turning bands methods (Emery et al., 2016) are required for large datasets."
  },
  {
    "qid": "statistic-posneg-611-0-0",
    "gold_answer": "TRUE. The paper explicitly states that under the assumptions of i.i.d. entries $\\{v_{i j}\\}$ with $E(v_{11})=0$, $E(v_{11}^{2})=1$, and $E(v_{11}^{4})=3$, the largest eigenvalue $\\lambda_{\\sf max}^{(n)}$ converges almost surely to $(1+\\sqrt{y})^{2}$ as $n \\rightarrow \\infty$. This result is derived from the empirical distribution of eigenvalues and their limiting behavior, supported by previous literature and the conditions specified in the paper.",
    "question": "Is it true that under the given conditions, the largest eigenvalue $\\lambda_{\\sf max}^{(n)}$ of the sample covariance matrix $M_{n}$ converges almost surely to $(1+\\sqrt{y})^{2}$ as $n \\rightarrow \\infty$?",
    "question_context": "Asymptotic Behavior of Eigenvectors in Sample Covariance Matrices\n\nThis paper investigates the asymptotic behavior of eigenvectors of sample covariance matrices defined as $M_{n}=(1/s)V_{n}V_{n}^{\\mathrm{T}}$ where $V_{n}=(v_{i j})$, with $v_{ij}$ being i.i.d. random variables satisfying $E(v_{11})=0$, $E(v_{11}^{2})=1$, and $E(v_{11}^{4})=3$. The paper explores the convergence properties of the largest eigenvalue $\\lambda_{\\sf max}^{(n)}$ and the behavior of eigenvectors, particularly focusing on the weak convergence of certain stochastic processes to a Brownian bridge."
  },
  {
    "qid": "statistic-posneg-1369-0-0",
    "gold_answer": "FALSE. The D_A-optimum design minimizes the generalized variance of the linear combinations A^Tβ by minimizing |A^T{𝒜(n)}^-1A|, not maximizing |A^T{𝒜(n)}^-1A|^-1. The latter would actually maximize the generalized variance, which is the opposite of the intended goal.",
    "question": "Is it true that the D_A-optimum design minimizes the generalized variance of the linear combinations A^Tβ by maximizing |A^T{𝒜(n)}^-1A|^-1?",
    "question_context": "Sequential D_A-Optimum Design in Clinical Trials\n\nFor the sake of generality we extend the single linear combination of the parameters $a^{T}\\beta$ in (8) to the set of s combinations $A^{T}\\beta$ , where A is a $(t+v+n_{h}-1)\\times s$ matrix of known constants. $\\mathrm{D}_{A}$ -optimum experimental designs for the linear regression model (1) maximize $|A^{T}\\{\\mathcal{A}(n)\\}^{-1}A|^{-1}$ and so minimize the generalized variance of these linear combinations, providing a normal theory confidence region of minimum volume. Such optimum designs can be constructed sequentially.<PARAGRAPH_SEPARATOR>We first consider univariate responses when $F_{i}$ in (1) becomes the vector $f_{i}^{T}$ , which includes the vectors of allocation and prognostic factors for the ith patient. When allocation is made to patient $n+1$ , all other allocations are known. A useful matrix result for D-optimum designs maximizing $\\left|\\mathcal{l}(n+1)\\right|$ is that<PARAGRAPH_SEPARATOR>$$|\\mathcal{L}(n+1)|=[1+f_{n+1}^{T}\\{\\mathcal{L}(n)\\}^{-1}f_{n+1}]|\\mathcal{L}(n)|=\\{1+d(j,n,x_{n+1})\\}|\\mathcal{L}(n)|.$$<PARAGRAPH_SEPARATOR>That treatment is allocated for which $d(j,n,x_{n+1})$ is a maximum.<PARAGRAPH_SEPARATOR>In the iterative construction of $\\mathrm{D}_{A}$ -optimum designs for a univariate response,<PARAGRAPH_SEPARATOR>$$d_{A}(j,n,x_{n+1})=f_{n+1}^{T}\\{\\boldsymbol{\\jmath}(n)\\}^{-1}A[\\boldsymbol{A}^{T}\\{\\boldsymbol{\\jmath}(n)\\}^{-1}\\boldsymbol{A}]^{-1}A^{T}\\{\\boldsymbol{\\jmath}(n)\\}^{-1}f_{n+1},\\quad(j=1,\\ldots,t).$$<PARAGRAPH_SEPARATOR>In the absence of randomization, patient $n+1$ would receive the treatment for which $d_{A}(j,n,x_{n+1})$ is a maximum.<PARAGRAPH_SEPARATOR>We now turn to multivariate data and find the equivalent of (A.2). Let the uth row of $F_{n+1}$ be denoted $f_{u,n+1}^{T}$ . We extend (A.2) to<PARAGRAPH_SEPARATOR>$$d_{A,u v}(j,n,x_{n+1})=f_{u,n+1}^{T}\\{\\boldsymbol{\\mathcal{I}}(n)\\}^{-1}A[\\boldsymbol{A}^{T}\\{\\boldsymbol{\\mathcal{I}}(n)\\}^{-1}\\boldsymbol{A}]^{-1}\\boldsymbol{A}^{T}\\{\\boldsymbol{\\mathcal{I}}(n)\\}^{-1}f_{v,n+1}.$$<PARAGRAPH_SEPARATOR>With element $u,v$ of $V^{-1}$ written $V^{u v}$ , the equivalent of (A.2) is<PARAGRAPH_SEPARATOR>$$d_{A}(j,n,x_{n+1})=\\sum_{u=1}^{n_{h}}\\sum_{v=1}^{n_{h}}V^{u v}d_{A,u v}(j,n,x_{n+1}).$$<PARAGRAPH_SEPARATOR>This is the function in our generic rule (18).<PARAGRAPH_SEPARATOR># A.2. An equivalence theorem<PARAGRAPH_SEPARATOR>As $n\\rightarrow\\infty$ , $d_{A}(j,n,x_{n+1})\\rightarrow0$ . If we replace the number of patients allocated to each treatment by the continuous distribution of asymptotic proportions of allocation we obtain a design measure $\\xi$ and an information matrix $\\boldsymbol{\\mathcal{I}}(\\boldsymbol{\\xi})$ . Also $d_{A}(j,n,x_{n+1})$ tends to the directional derivative $d_{A}(j,\\xi,x)$ . In the case of non-sequential design we can consider choosing treatments from a space $\\mathcal{J}$ and covariates from a space Z. The design region is then $\\mathcal{X}=\\mathcal{J}\\times\\mathcal{Z}$ . We now extend the General Equivalence Theorem of optimum design theory (Kiefer and Wolfowitz, 1960; Whittle, 1973) to multivariate $\\mathrm{D}_{A}$ -optimality.<PARAGRAPH_SEPARATOR>If we let<PARAGRAPH_SEPARATOR>$${\\overline{{d}}}_{A}(\\xi)=\\operatorname*{sup}_{j,x\\in\\mathcal{X}}d_{A}(x,\\xi),$$<PARAGRAPH_SEPARATOR>the equivalence theorem states that the $\\mathrm{D}_{A}$ -optimum design, denoted $\\xi^{*}$ , is such that<PARAGRAPH_SEPARATOR>$$\\begin{array}{r}{\\bar{d}_{A}(\\xi_{D A}^{*})=s.}\\end{array}$$<PARAGRAPH_SEPARATOR>Here $s$ is the number of independent linear combinations of the parameters specified by A.<PARAGRAPH_SEPARATOR>In the sequential construction of optimum designs for clinical trials there will asymptotically be balance for each treatment over the prognostic factors. With such balance we ignore $x_{n+1}$ and write<PARAGRAPH_SEPARATOR>$$d_{A}(j,\\xi^{*})=s,\\quad(j=1,\\dots,t).$$<PARAGRAPH_SEPARATOR>The balanced design used in (20) to derive the expression for loss is $n_{\\mathrm{effec}}n\\xi^{*}$ . The results in Table 1 for Rule D show how fast the sequential construction of the optimum design converges."
  },
  {
    "qid": "statistic-posneg-1462-8-2",
    "gold_answer": "FALSE. The proof explicitly extends the result from simple functions to all admissible functions (including non-simple ones) via approximation by simple functions and decomposition into positive and negative parts. The condition must hold for all admissible $g$ to conclude $(X,Y)\\in\\mathrm{IC}_{0}$.",
    "question": "Is it correct to conclude that $\\operatorname{Cov}(g(X),g(Y))=0$ for any simple function $g$ implies $(X,Y)\\in\\mathrm{IC}_{0}$ without considering non-simple functions?",
    "question_context": "Invariant Correlation and Quasi-Independence in Random Vectors\n\nLemma 2. For a random vector $(X,Y)\\sim H$ with the marginals $F_{\\cdot}$ , $(X,Y)$ is quasi- $_r$ -Fréchet if and only if (10) holds. <PARAGRAPH_SEPARATOR> Proof of Theorem 1. We first show the ‘‘only if’’ part. As $(X,Y)\\in\\mathrm{IC}_{0}$ , we have $\\operatorname{Cov}(g(X),g(Y))=0$ for any admissible $g$ . For any $A\\in{\\mathcal{B}}(\\mathbb{R})$ , taking $g(x)=\\mathbb{1}_{A}(x)$ , we have $\\operatorname{Cov}(g(X),g(Y))=\\operatorname{Cov}(\\mathbb{1}_{A}(X),\\mathbb{1}_{A}(Y))=0$ . For any $A,B\\in{\\mathcal{B}}(\\mathbb{R})$ , let $\\begin{array}{r}{g(\\boldsymbol{x})=\\mathbb{1}_{A}(\\boldsymbol{x})+\\mathbb{1}_{B}(\\boldsymbol{x}),}\\end{array}$ which leads to <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathrm{Cov}(g(X),g(Y))=\\mathrm{Cov}\\left(\\mathbb{1}_{A}(X)+\\mathbb{1}_{B}(X),\\mathbb{1}_{A}(Y)+\\mathbb{1}_{B}(Y)\\right)=\\mathrm{Cov}\\left(\\mathbb{1}_{A}(X),\\mathbb{1}_{B}(Y)\\right)+\\mathrm{Cov}\\left(\\mathbb{1}_{B}(X),\\mathbb{1}_{A}(Y)\\right)}\\ &{\\quad\\quad\\quad=\\mathbb{P}(X\\in A,Y\\in B)+\\mathbb{P}(X\\in B,Y\\in A)-\\mathbb{P}(X\\in A)\\mathbb{P}(Y\\in B)-\\mathbb{P}(X\\in B)\\mathbb{P}(Y\\in A).}\\end{array} $$ <PARAGRAPH_SEPARATOR> As $\\operatorname{Cov}(g(X),g(Y))=0$ , we have that $(X,Y)$ satisfies (8) for all $A,B\\in{\\mathcal{B}}(\\mathbb{R})$ . Hence, $(X,Y)$ is quasi-independent by Lemma 1. Next, we show that quasi-independence implies $\\operatorname{Cov}(g(X),g(Y))=0$ for any admissible $g$ . As quasi-independence implies (8), by taking $A=B$ , we have $\\mathbb{P}(X\\in A,Y\\in A)=\\mathbb{P}(X\\in A)\\mathbb{P}(Y\\in A)$ for any $A\\in{\\mathcal{B}}(\\mathbb{R})$ . Hence, $\\operatorname{Cov}(g(X),g(Y))=0$ for any indicator functions $g=\\mathbb{1}_{A}$ . <PARAGRAPH_SEPARATOR> First, let $\\begin{array}{r}{g(x)=\\sum_{i=1}^{n}c_{i}\\mathbb{1}_{A_{i}}(x)}\\end{array}$ be an admissible simple function where $A_{1},\\ldots,A_{n}\\subseteq\\mathbb{R}$ are disjoint measurable sets, and $c_{1},\\ldots c_{n}\\in\\mathbb{R}$ are real numbers. For the simple function $g$ , we have <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\displaystyle\\mathsf{C o v}(g(X),g(Y))=\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j}\\mathrm{Cov}\\left(\\mathbb{1}_{A_{i}}(X),\\mathbb{1}_{A_{j}}(Y)\\right)=\\sum_{1\\leqslant i<j\\leqslant n}\\left(c_{i}c_{j}\\mathrm{Cov}\\left(\\mathbb{1}_{A_{i}}(X),\\mathbb{1}_{A_{j}}(Y)\\right)+c_{j}c_{i}\\mathrm{Cov}\\left(\\mathbb{1}_{A_{j}}(X),\\mathbb{1}_{A_{j}}(Y)\\right)\\right.}\\ &{\\displaystyle\\left.=\\sum_{1\\leqslant i<j\\leqslant n}c_{i}c_{j}\\left(\\mathbb{P}\\left(X\\in A_{i},Y\\in A_{j}\\right)-\\mathbb{P}\\left(X\\in A_{i}\\right)\\mathbb{P}\\left(Y\\in A_{j}\\right)+\\mathbb{P}\\left(X\\in A_{j},Y\\in A_{i}\\right)-\\mathbb{P}\\left(X\\in A_{j}\\right)\\mathbb{P}\\left(Y\\in A_{i}\\right)\\right)=}\\end{array} $$ <PARAGRAPH_SEPARATOR> Therefore, we have $\\operatorname{Cov}(g(X),g(Y))=0$ for all admissible simple functions $g$ . <PARAGRAPH_SEPARATOR> Second, let $g$ be an admissible non-negative function. Admissibility of $g$ implies that $\\mathbb{E}[g(X)]<\\infty$ , $\\mathbb{E}[g(Y)]<\\infty$ and $\\mathbb{E}[g(X)g(Y)]<$ $\\infty$ . For a non-negative admissible function $g$ , there exists a sequence of non-negative simple functions $\\{g_{n}\\}_{n\\geqslant1}$ such that $g_{n}\\uparrow g$ pointwise. Thus, we can get <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathrm{Cov}\\left(g(X),g(Y)\\right)=\\mathrm{Cov}\\left(\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X),\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(Y)\\right)=\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X)\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(Y)\\right]-\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X)\\right]\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(Y)\\right]}\\ &{=\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X)g_{n}(Y)\\right]-\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X)\\right]\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(Y)\\right]=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}\\left[g_{n}(X)g_{n}(Y)\\right]-\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}\\left[g_{n}(X)\\right]\\mathbb{E}\\left[g_{n}(Y)\\right]}\\ &{=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\left\\{\\mathbb{E}\\left[g_{n}(X)g_{n}(Y)\\right]-\\mathbb{E}\\left[g_{n}(X)\\right]\\mathbb{E}\\left[g_{n}(Y)\\right]\\right\\}=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\mathrm{Cov}(g_{n}(X),g_{n}(Y))=0.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Therefore, we have $\\operatorname{Cov}(g(X),g(Y))=0$ for all admissible non-negative function $g$ . <PARAGRAPH_SEPARATOR> Finally, let $g$ be an admissible function. Define $g_{+}=\\operatorname*{max}(g,0)$ and $g_{-}=-\\operatorname*{min}(g,0)$ . It is clear that $g_{+}$ and $g_{-}$ are non-negative admissible functions and $g=g_{+}-g_{-}$ . Let $G=g_{+}+g_{-}$ . We have that $G$ is also a non-negative admissible function. Hence, we get $\\operatorname{Cov}(g_{+}(X),g_{+}(Y))=0_{\\cdot}$ , $\\operatorname{Cov}(g_{-}(X),g_{-}(Y))=0$ and $\\operatorname{Cov}(G(X),G(Y))=0,$ , which imply that $\\operatorname{Cov}(g_{+}(X),g_{-}(Y))+\\operatorname{Cov}(g_{-}(X),g_{+}(Y))=0~$ . As a result, we have <PARAGRAPH_SEPARATOR> $$ \\operatorname{Cov}\\left(g(X),g(Y)\\right)=\\operatorname{Cov}(g_{+}(X)-g_{-}(X),g_{+}(Y)-g_{-}(Y))=0. $$ <PARAGRAPH_SEPARATOR> Therefore, we can conclude $(X,Y)\\in\\mathrm{IC}_{0}$ . □ <PARAGRAPH_SEPARATOR> Proof of Proposition 2. For all $A,B\\in{\\mathcal{B}}(\\mathbb{R})$ , we have <PARAGRAPH_SEPARATOR> $$ \\mathbb{P}(X_{\\pi_{1}}\\in A,X_{\\pi_{2}}\\in B)=\\frac{1}{2}\\mathbb{P}(X_{1}\\in A,X_{2}\\in B)+\\frac{1}{2}\\mathbb{P}(X_{1}\\in B,X_{2}\\in A). $$ <PARAGRAPH_SEPARATOR> Since $X_{1}$ and $X_{2}$ have the same distribution, independence of $(X_{\\pi_{1}},X_{\\pi_{2}})$ is equivalent to <PARAGRAPH_SEPARATOR> $$ \\mathbb{P}(X_{\\pi_{1}}\\in A,X_{\\pi_{2}}\\in B)=\\mathbb{P}(X_{\\pi_{1}}\\in A)\\mathbb{P}(X_{\\pi_{2}}\\in B)=\\mathbb{P}(X_{1}\\in A)\\mathbb{P}(X_{2}\\in B)=\\mathbb{P}(X_{1}\\in B)\\mathbb{P}(X_{2}\\in A). $$ <PARAGRAPH_SEPARATOR> Hence, independence of $(X_{\\pi_{1}},X_{\\pi_{2}})$ is equivalent to <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\operatorname{\\mathbbP}(X_{1}\\in A,X_{2}\\in B)+\\operatorname{\\mathbbP}(X_{1}\\in B,X_{2}\\in A)=\\operatorname{\\mathbbP}(X_{1}\\in A)\\operatorname{\\mathbbP}(X_{2}\\in B)+\\operatorname{\\mathbbP}(X_{1}\\in B)\\operatorname{\\mathbbP}(X_{2}\\in A),}\\end{array} $$ <PARAGRAPH_SEPARATOR> which is also equivalent to $(X_{1},X_{2})\\in\\mathrm{IC}_{0}$ by Theorem 1. □"
  },
  {
    "qid": "statistic-posneg-1133-0-0",
    "gold_answer": "FALSE. The context explicitly states that ln(x)^T L_A ln(y) is not an inner product on (R_+^D, ⊕, ⊙) without additional constraints, because ln(x)^T L_A ln(x) = 0 only implies that ln(x) is in the null space of L_A, not that x = 0. An additional condition like ∑x_j = 1 is needed to make it a proper inner product.",
    "question": "Is the statement 'The bilinear form ln(x)^T L_A ln(y) is an inner product on (R_+^D, ⊕, ⊙) without any additional constraints' true based on the provided context?",
    "question_context": "Graph Simplex Space and Inner Products in Compositional Data Analysis\n\nThe paper discusses the extension of compositional data analysis to graph structures. It introduces the D-part Graph Simplex space, defined by constraints on sums of components within connected subgraphs. The space is equipped with perturbation and powering operations that preserve these constraints. The paper also defines an inner product based on the graph Laplacian matrix, extending the Aitchison inner product to graph-structured compositional data."
  },
  {
    "qid": "statistic-posneg-1429-0-1",
    "gold_answer": "FALSE. While the condition that both K and its Fourier transform ℱ(K) satisfy condition (2.1) is sufficient to ensure the absolute convergence of the series ∑_{μ∈ℤ\\{0}} ℱ(K)(2πμ/α), it is not strictly necessary. The lemma could potentially hold under weaker conditions that still ensure the convergence of the series and the applicability of the dominated convergence theorem. The condition (2.1) is used for convenience and simplicity in the proof, but other conditions guaranteeing the required decay properties of ℱ(K) could also suffice.",
    "question": "In Lemma A.2, the condition that both K and its Fourier transform ℱ(K) satisfy condition (2.1) is sufficient to ensure the absolute convergence of the series ∑_{μ∈ℤ\\{0}} ℱ(K)(2πμ/α). Is this condition also necessary for the result of the lemma to hold?",
    "question_context": "Multivariate Euler-Maclaurin Summation Formula and Kernel Density Estimation\n\nThe paper discusses the multivariate extension of the Euler-Maclaurin summation formula, which is less commonly covered in the literature compared to its one-dimensional counterpart. It presents a rigorous treatment with less restrictive regularity assumptions than previous works. The formula is used to approximate sums by integrals with error terms involving Bernoulli polynomials and their derivatives. The paper also connects this to kernel density estimation, showing how the approximation error behaves asymptotically under certain conditions on the kernel and its Fourier transform."
  },
  {
    "qid": "statistic-posneg-1340-0-0",
    "gold_answer": "FALSE. The context explicitly states that the MLE of $\\beta$ can be severely affected by outlying observations, and Croux et al. (2002) show that the MLE breaks down to zero when severe outliers are added to a data set. The logistic regression model, defined by the given formula, is sensitive to outliers because the MLE relies on the likelihood function, which can be heavily influenced by extreme values in the data.",
    "question": "Is the statement 'The MLE of $\\beta$ in logistic regression is robust to outliers' true given the context and the formula $P(Y=1|\\mathbf{X}=\\mathbf{x})=F(\\mathbf{x}^{\\prime}{\\pmb\\beta})$ where $F(t)=\\frac{\\exp(t)}{1+\\exp(t)}$?",
    "question_context": "Robust Testing in Logistic Regression Models\n\nIn the binomial regression model we assume that the response variable Y has a Bernoulli distribution such that $$P(Y=1|\\mathbf{X}=\\mathbf{x})=F(\\mathbf{x}^{\\prime}{\\pmb\\beta}),$$ where $F$ is a strictly increasing cumulative distribution function, $\\mathbf{X}\\in\\Re^{p}$ is the vector of explanatory variables and $\\beta\\in\\Re^{p}$ is the unknown regression parameter. When $$F(t)=\\frac{\\exp(t)}{1+\\exp(t)}$$ we have the logistic regression model, which is the model we will consider from now on. However, our results can be extended to other link functions. It is well known that the maximum likelihood estimator (MLE) of $\\beta$ can be severely affected by outlying observations. Croux et al. (2002) discuss the breakdown behavior of the MLE in the logistic regression model and show that the MLE breaks down to zero when severe outliers are added to a data set. In the last few decades, a lot of work has been done in order to obtain robust estimates of the parameter in this model and also in the more general framework of generalized linear models. Among others, we can mention the proposals given by Pregibon (1982), Stefanski et al. (1986), Künsch et al. (1989), Morgenthaler (1992), Carroll and Pederson (1993), Christmann (1994) and Bianco and Yohai (1996) and more recently Croux and Haesbroeck (2003) and Bondell (2005, 2008). We are interested in testing parametric hypotheses about the regression parameter of the logistic regression model. Robust testing in this setting has received much less attention than robust estimation. Testing procedures based on classical estimates inherit the sensitivity of these estimators to atypical data, in the sense that a small amount of outlying observations can affect the level or the power of the tests. Testing procedures that, under contamination, retain a stable level and also a good power under specified alternatives, are desirable. The works of Heritier and Ronchetti (1994) and Cantoni and Ronchetti (2001) go in this direction. Heritier and Ronchetti (1994) introduce robust tests for a general parametric model, which includes logistic regression. Cantoni and Ronchetti (2001) define robust deviances based on generalizations of quasi–likelihood functions and propose a family of test statistics for model selection in generalized linear models. They also investigate the stability of the asymptotic level under contamination."
  },
  {
    "qid": "statistic-posneg-358-0-1",
    "gold_answer": "FALSE. The condition $x > -k$ is not necessary for the analyticity of $\\psi_{k}(x;x,\\beta)$ in $H$. The context states that the proof of Theorem 3 is valid under the natural restriction $x > -k$, but the analyticity of $\\psi_{k}(x;x,\\beta)$ is established for all $\\alpha \\in H = \\{\\alpha: \\mathbb{Re} \\alpha > -k\\}$, which is a broader condition.",
    "question": "The condition $x > -k$ is necessary for the analyticity of $\\psi_{k}(x;x,\\beta)$ in $H$. Is this statement correct based on the given context?",
    "question_context": "Analyticity and Convergence in Characteristic Functions of Elliptical Distributions\n\nVolume 49, Number 1 (1994), in the article 'Characteristic Functions of a Class of Elliptical Distributions,' by Samuel Kotz and Iosif Ostrovskii, pages 164-178: On page 166, footnote 1 states that the authors have succeeded in proving Theorem 3 of the paper without the restriction $2N-2>$ $-\\mathrm{min}(2,k)$. Here the proof of Theorem 3 is briefly outlined without this restriction, and misprints are corrected. <PARAGRAPH_SEPARATOR> (1) Outline of the proof of the unrestricted Theorem 3. The goal is to show that the formula <PARAGRAPH_SEPARATOR> $$ \\psi_{k}\\left(x;x,\\beta\\right)=\\sum_{m=0}^{\\ast}\\left(-1\\right)^{m+1}C_{m}(k,\\alpha,\\beta)x^{m}, $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{C_{m}(k,\\alpha,\\beta)=\\displaystyle\\frac1{\\pi m!}\\:2^{\\beta m+(k/2)+x}\\displaystyle\\Gamma\\bigg(\\frac{x+\\beta m+k}2\\bigg)\\:\\Gamma\\bigg(\\frac{x+\\beta m+2}2\\bigg)}}\\ {{{}}}\\ {{{}~\\times\\sin\\bigg(\\displaystyle\\frac\\pi2\\left(x+\\beta m\\right)\\bigg),}}\\end{array} $$ <PARAGRAPH_SEPARATOR> remains valid under the natural restriction $x>-k$. It is sufficient to verify that both sides of (1) are analytic functions of $\\mathfrak{X}$ in $H=\\left\\{\\alpha:{\\mathbb{R e}}\\alpha>-k\\right\\}$. Recalling that <PARAGRAPH_SEPARATOR> $$ \\psi_{k}(x;z,\\beta)=\\int_{0}^{z}J_{(k/2)}\\quad_{1}(\\nu)\\:\\nu^{(k/2)+z}\\exp[-\\nu^{\\beta}x]\\:d\\nu,\\qquadx>0, $$ <PARAGRAPH_SEPARATOR> where $J_{(k/2)\\mathrm{~\\scriptsize~\\cdot~}1}(\\upnu)$ is the Bessel function of the first kind, it is straightforward to show that the function on the left-hand side of (1) as defined by (2) is indeed analytic in $H.$ <PARAGRAPH_SEPARATOR> To show that the right-hand side of (1) is analytic in $H.$, we note that in view of well-known properties of the $T$-function, the coefficients $C_{m}(k,x,\\beta)$ are analytic functions of $\\mathfrak{X}$ for $\\alpha\\in H.$ Let $E$ be a compact set in $H.$ Using the Stirling formula, it can be shown that <PARAGRAPH_SEPARATOR> $$ \\vert C_{m}(k,x,\\beta)\\vert<C m^{\\quad\\delta m}, $$ <PARAGRAPH_SEPARATOR> where $C$ and $\\delta$ are positive constants not depending on $m$ and on $\\pmb{\\alpha}\\in\\pmb{E}.$ This implies that the series on the right-hand side of (1) converges uniformly in $\\pmb{\\mathscr{x}}\\in\\pmb{E}$ and hence its sum is analytic in $H.$ Details are available from the authors."
  },
  {
    "qid": "statistic-posneg-602-0-2",
    "gold_answer": "TRUE. The inequality $h[v(v-1)-6\\{k(k-1)-t+1\\}]>\\sum_{h_{t m}<h}h_{l m}$ is used to determine the integer $t$ that partitions the weights $h_{l m}$ into those greater than $h$ and those less than $h$, allowing for an improved lower bound on $\\sum_{i+j}m_{i j}^{2}$.",
    "question": "Is the term $h[v(v-1)-6\\{k(k-1)-t+1\\}]$ used to find an integer $t$ such that it is greater than the sum of weights less than $h$?",
    "question_context": "Bounds for Sums of Weights in Neighbor Designs\n\nWe consider the sum in two parts, namely $\\sum_{i,j}m_{i j}^{2}=\\sum_{i}m_{i i}^{2}+\\sum_{i+j}m_{i j}^{2}$ Since $\\Sigma\\:m_{i i}=v r(k^{2}-1)/3$ is fixed, $\\Sigma~m_{i i}^{2}$ is minimized when the $m_{i i}$ are all equal. Thus $\\Sigma~m_{i i}^{\\bar{2}}\\geqslant\\dot{v}r^{2}(k-1)^{2}/9$ . Since each $m_{i i}$ is an integer if $r(k^{2}-1)$ is not a multiple of 3 this may be improved to $\\Sigma~m_{i i}^{2}\\geqslant v\\{r^{2}(k^{2}-1)^{2}+2\\}/9$ Now each $m_{\\mathrm{ij}}$ ,for $i\\neq j,$ is the sum of weights $h_{l m}$ where varieties $i$ and $j$ occur together in plots $\\boldsymbol{l}$ and $m$ of a block. Thus $\\sum_{i\\neq j}m_{i j}^{2}\\geqslant b\\sum_{I\\neq m}h_{l m}^{2}$ . When $v-1>r(k-1)$ some $\\boldsymbol{m}_{i j}$ are sums of two or more weights $h_{t_{m}}$ , and an improvement on this may be obtained by finding the integer $t$ such that, if $\\pmb{h}$ is the th largest element among the weights $h_{l m}$ , then $h[v(v-1)-6\\{k(k-1)-t+1\\}]>\\sum_{h_{t m}<h}h_{l m}.$ Thus $\\sum_{i+j}m_{i j}^{2}\\geq b\\sum_{h_{l m}>h}h_{l m}^{2}+(b\\sum_{h_{l m}<h}h_{l m}^{2})^{2}/[v(v-1)-b\\{k(k-1)-t+1\\}]$."
  },
  {
    "qid": "statistic-posneg-1020-0-0",
    "gold_answer": "TRUE. The formula $\\rho(x)={\\frac{\\phi_{M}\\slash_{M}(x)}{\\phi_{M}\\slash_{M}(x)+\\sum_{i=1}^{n_{I}}\\phi_{i}\\slash_{i}(x)}}$ correctly represents the probability of melt for a given IST $x$. This is derived from the mixture model's probability density function, where the numerator is the weighted density of the melt component, and the denominator is the sum of the weighted densities of all components (ice and melt). This ratio gives the probability that an observation $x$ belongs to the melt component, which is the definition of $\\rho(x)$ in this context.",
    "question": "Is the probability of melt $\\rho(x)$ correctly defined as the ratio of the melt component density to the total density of all components, given the context?",
    "question_context": "Truncated Gaussian Mixture Model for Ice Sheet Temperature Analysis\n\nSince melt occurs when the IST exceeds $0^{\\circ}C$ , positive temperatures are of most concern. For the majority of cells, particularly inland and/or at higher altitudes, there are too few positive observa­ tions to take $0^{\\circ}\\mathrm{C}$ as the PoT modelling threshold. However, when taking a modelling threshold below $0^{\\circ}\\mathrm{C}$ , there are problems with the PoT fit, as seen in Figure 4. Many cells have a secondary mode corresponding to a large mass of observations close to $0^{\\circ}\\mathrm{C}$ and a much lower-weighted tail covering small positive temperature values. The most likely explanation of this is that once the temperature exceeds $0^{\\circ}\\mathrm{C}$ the ice melts. This change in state from ice to melt water, which is no longer considered the surface of the ice sheet, creates a soft upper limit close to $0^{\\circ}C$ on ISTs. We cannot determine from the data which of the positive observations are water temperatures and which are due to measurement or recording uncertainty $(\\pm1^{\\circ}\\mathrm{C}$ Hall et al., 2018). Regardless of the reason, the soft upper limit results in a distribution mode close to $0^{\\circ}\\mathrm{C}$ , rendering the generalized Pareto distribution unsuitable. Lastly, we note that the shape of the tail distribu­ tion, and especially the behaviour at or near $0^{\\circ}C$ , varies primarily in response to altitude and prox­ imity to the coast. <PARAGRAPH_SEPARATOR> The objective now is to build a model to reflect these insights and establish how the melt thresh­ old impacts the distribution of ISTs. To facilitate ease of application at all locations, we propose a truncated Gaussian mixture model for the full distribution of ISTs. This circumvents the need to define a tail region—as discussed previously, for this particular data set such a definition is non­ trivial due to the varying cross-site behaviour of the upper tail. Utilizing information on behaviour in sub-tail regions should help better identify behaviour close to the melt threshold. Furthermore, a mixture model allows joint modelling of the ice and melt observations; this is critical since we have no information on which category each observation belongs to. <PARAGRAPH_SEPARATOR> The truncated Gaussian mixture model has separate model components for ice and melt temper­ atures. Observations are not preassigned to a component, rather the fit provides an estimate of the probability with which each observation belongs to each component. For $n_{I}$ ice components and a single melt component, let $\\phi_{i}$ be the weight associated with ice component $i$ and $\\phi_{M}$ be the weight associated with the melt component, such that $\\begin{array}{r}{\\sum_{i=1}^{n_{I}}\\phi_{i}+\\phi_{M}=1.}\\end{array}$ Let $f_{i}(x)$ and $f_{M}(x)$ be the probabil­ ity density functions of the ice $i\\in\\{1,...,n_{I}\\}$ and melt components, respectively, such that for each $i,X\\sim T\\dot{N}(\\mu_{i},\\sigma_{i}^{2},a_{i},b_{i})$ , where $\\mu_{i}$ is the mean, $\\sigma_{i}>0$ the standard deviation, and $a_{i}$ and $b_{i}$ the lower and upper truncation points. Then the mixture model probability density function of IST $x$ is: <PARAGRAPH_SEPARATOR> $$ \\boldsymbol{p}(\\boldsymbol{x})=\\sum_{i=1}^{n_{I}}\\phi_{i}f_{i}(\\boldsymbol{x})+\\phi_{M}f_{M}(\\boldsymbol{x}). $$ <PARAGRAPH_SEPARATOR> Ice temperature components are bounded above by $b=0$ , since these should not exceed $0^{\\circ}\\mathrm{C}$ , and have no lower bound since the only physical lower limit is absolute zero, which is far below the range of the data. For the melt component, there is no upper bound as ISTs are not feasibly phys­ ically limited on the ice sheet. The lower bound is set to $-1.65$ as this has previously been estimated as the melting point of saline ice and thereby acts as a conservative minimum estimate for the ice sheet. It follows that data in the range $[-1.65,0]$ can be modelled by either the ice or melt compo­ nents, capturing the uncertainty in the true nature of the ice sheet when these observations were measured. The probability of melt $\\rho(x)$ for given IST $x$ is defined as the ratio of the densities of the ice and melt components, such that: <PARAGRAPH_SEPARATOR> $$ \\rho(x)={\\frac{\\phi_{M}\\slash_{M}(x)}{\\phi_{M}\\slash_{M}(x)+\\sum_{i=1}^{n_{I}}\\phi_{i}\\slash_{i}(x)}}. $$ <PARAGRAPH_SEPARATOR> As a result of the model truncation points, $\\rho(x)=0$ below $-1.65^{\\circ}\\mathrm{C}$ and $\\rho(x)=1$ above $0^{\\circ}\\mathrm{C}$ . ISTs between these values vary smoothly within this range. Small discontinuities in the melt probability may arise at $-1.65^{\\circ}\\mathrm{C}$ and $0^{\\circ}\\mathrm{C}$ due to the truncation of the mixture components at these points; the magnitude of the discontinuity depends on the severity of the truncation. To find the most appro­ priate number of ice components for the model and allow for the diversity of distributions across the ice sheet, we fit the model with $n_{I}$ from 3 to 6 and compare the fits using Bayesian information criterion (BIC). In contrast, a single melt component is used since, in theory, the melt temperatures should show much less cross-cell variability than the ice temperatures. <PARAGRAPH_SEPARATOR> ![](images/b23c9243202e2204772598edc080551e0050a63326f8578bb0c9ab33bdce5701.jpg) Figure 3. Estimated scale parameter annual random effects for each weather station (thin black lines), the annual mean for all stations (central bold red line), and GMST (bold blue line, in degrees). Results cover the period 1950 to 2021 inclusive. <PARAGRAPH_SEPARATOR> We use the Expectation-maximization (EM) algorithm to estimate the parameters $\\phi,\\mu$ and $\\sigma^{2}$ for each model component. We found 800 iterations of the algorithm optimal in terms of both convergence and computational time. To improve the stability of the algorithm, particularly for cells with few or no observations above $-1.65^{\\circ}\\mathrm{C}$ , we set additional restrictions on two param­ eters: $\\phi\\ge0.0005$ and $\\sigma\\ge0.35$ . This allows a melt component—however small—to be fitted even if there are no observations in the melt temperature range, and avoids melt components with $\\sigma\\approx0$ since this results in a point-mass density for cells with only a single observation above $-1.65^{\\circ}\\mathrm{C}$ . We apply the model to 1,139 cells across the ice sheet; this represents a subsample of 1 in every 50 available cells in both horizontal and vertical directions. The cells are equally spaced in distance and are taken from those cells identified as ice by the MODIS ice mask. The number and spread of the sub-sampled cells accurately reflects the variety in glaciological and climatological conditions across the ice sheet. A range of average temperatures, surface conditions, latitudes, lon­ gitudes, elevations, and distances to the coast are all observed within the sample."
  },
  {
    "qid": "statistic-posneg-223-1-2",
    "gold_answer": "FALSE. The formulas show that N_t is a zero matrix only for 3 ≤ t ≤ k in H^(1) and for 1 ≤ t ≤ k-2 in U^(Ω). The definition is not universally true for all t from 1 to k-2 across all hypotheses.",
    "question": "For the case of k samples, the matrix N_t is defined as a zero matrix for all t from 1 to k-2. Is this definition consistent with the formulas provided?",
    "question_context": "Robustness of T02 Test in Multivariate Analysis with Unequal Variance-Covariance Matrices\n\nThe paper examines the robustness of the T02 test in multivariate analysis of variance when variance-covariance matrices are not equal. It presents specific matrix structures for hypotheses testing under different conditions, including cases with k samples. The matrices N_t are defined with specific non-zero elements depending on the sample sizes and conditions, and the paper compares approximate significance levels and powers of the T02 test with exact results from univariate analysis."
  },
  {
    "qid": "statistic-posneg-1773-0-0",
    "gold_answer": "TRUE. According to Proposition 5.3 in the context, if there exists a feasible solution $c$ to $(P^{\\delta})$ such that $c > 0$ µ-a.e., then the optimal solution to $(P^{\\delta})$ is indeed $c_{\\delta}$ given by the specified formula. This holds provided that $\\mathcal{I}(\\delta) < +\\infty$, ensuring the entropy is finite and the solution is valid.",
    "question": "Is the optimal solution to the problem $(P^{\\delta})$ always given by the copula density $c_{\\delta}$ when $\\mathcal{I}(\\delta) < +\\infty$?",
    "question_context": "Maximum Entropy Copula with Given Diagonal Section\n\nThe paper discusses the construction of maximum entropy copulas with given diagonal sections. It presents various formulas and conditions for determining the optimal copula density that maximizes entropy under given constraints. The context includes detailed mathematical derivations and examples of copula families with specific diagonal sections."
  },
  {
    "qid": "statistic-posneg-992-0-1",
    "gold_answer": "TRUE. The decision rule $\\delta_{o o b}(D_{N})=\\left\\{\\mathcal{E}_{1/2}^{o o b}\\leq F^{-1}(\\alpha)\\right\\}$ correctly states that the null hypothesis $H_{0}$ is rejected when $\\mathcal{E}_{1/2}^{o o b}$ is less than or equal to the $\\alpha$ quantile of the permutation distribution $F$. This is consistent with the permutation test framework described in the context.",
    "question": "Does the test statistic $\\delta_{o o b}(D_{N})$ reject the null hypothesis $H_{0}$ when $\\mathcal{E}_{1/2}^{o o b}$ is less than or equal to the $\\alpha$ quantile of the permutation distribution?",
    "question_context": "Random Forest Out-of-Bag Error for Two-Sample Testing\n\nNaturally, the split in training and test set is not ideal. For finite sample sizes, one would like to have as many (test) samples as possible to detect differences. At the same time, it would be preferable to have the classifier trained on many data points. This in fact resembles a bias-variance trade-off, similar to what was described in Lopez-Paz and Oquab (2018): Let $g_{1/2}^{*}$ and $L_{\\pi}^{(g_{1/2}^{*})}$ be the Bayes classifier and Bayes error respectively, both defined in Section 2.3. For $\\pi=1/2$ , there is a trade-off between the closeness of $L^{(\\hat{g})}$ to $L_{\\pi}^{(g_{1/2}^{*})}$ , which may be achieved through a large training set and the closeness of [(g) to Lg), which is generally only true in large test sets.\n\n<PARAGRAPH_SEPARATOR>\n\n# 2.2. Out-of-bag test\n\n<PARAGRAPH_SEPARATOR>\n\nFor the purpose of overcoming the arbitrary split in training and testing, Random Forest delivers an interesting tool: the OOB error introduced in Breiman (2001). Since each tree is built on a bootstrapped sample taken from $D_{N}$ , approximately $1/3$ of the trees will not use the ith observation $(\\ell_{i},\\mathbf{Z}_{i})$ . Thus we may use this ensemble of trees not containing observation i to obtain an estimate of the out-of-sample error for i. We slightly generalize this here, in assuming we have an ensemble learner $_g$ : That is, we assume to have iid copies of a random element $\\nu$ , $\\nu_{1},\\ldots,\\nu_{B}$ , such that each $\\hat{\\{\\mathbfcal g}}_{\\nu_{b}}({\\mathbf Z}):=g({\\mathbf Z},D_{N_{t r a i n}},\\nu_{b})$ is a different classifier. We then consider the average\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\hat{g}(\\mathbf{Z}):=\\frac{1}{B}\\sum_{b=1}^{B}\\hat{g}_{\\nu_{b}}(\\mathbf{Z}).$$\n\n<PARAGRAPH_SEPARATOR>\n\nFor $B\\to\\infty$ , it holds that (a.s.) $\\hat{g}(\\mathbf{Z})\\rightarrow\\mathbb{E}_{\\nu}[\\hat{g}_{\\nu}(\\mathbf{Z})]$ . For Random Forest, $\\nu$ usually represents the bootstrap sampling of observations and the sampling of variables to consider at each splitpoint for a given tree.\n\n<PARAGRAPH_SEPARATOR>\n\nLet as before, $\\begin{array}{r}{n_{0}:=\\sum_{i=1}^{\\bar{N}}\\bar{\\mathbb{I}}\\{\\ell_{i}=0\\}}\\end{array}$ and $\\begin{array}{r}{n_{1}:=\\sum_{i=1}^{N}\\mathbb{I}\\{\\ell_{i}=1\\}}\\end{array}$ , with $n_{0}\\geq1$ , $n_{1}\\geq1$ . We assume in the following that each $\\hat{g}_{\\nu_{b}}(\\mathbf{Z})$ uses a bootstra pped sample from the or iginal data, as Random Forest does. The class-wise OOB error of such an ensemble of learners trained on $N$ observations is defined as\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\mathcal{E}_{0}^{o o b}=\\frac{1}{n_{0}}\\sum_{i=1}^{N}\\mathbb{I}\\{\\ell_{i}=0\\}\\mathbb{I}\\{\\hat{\\pmb{g}}_{-i}(\\mathbf{Z}_{i})\\neq0\\},$$\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\begin{array}{l}{\\displaystyle\\mathcal{E}_{1}^{o o b}=\\frac{1}{n_{1}}\\sum_{i=1}^{N}\\mathbb{I}\\{\\ell_{i}=1\\}\\mathbb{I}\\{\\hat{\\pmb{g}}_{-i}(\\mathbf{Z}_{i})\\neq1\\},}\\ {\\displaystyle\\mathcal{E}_{p}^{o o b}=(1-p)\\mathcal{E}_{0}^{o o b}+p\\mathcal{E}_{1}^{o o b},}\\end{array}$$\n\n<PARAGRAPH_SEPARATOR>\n\nwhere $\\hat{g}_{-i}$ , represents the average over the ensemble of learners not containing the $i^{\\mathrm{th}}$ observation for training.\n\n<PARAGRAPH_SEPARATOR>\n\nUnfortunately, the test statistic $\\mathcal{E}_{1/2}^{o o b}$ is difficult to handle; due to the complex dependency structure between the elements of the sum, it is not clear what the (asymptotic) distribution under the null is. For theoretical purposes, we consider in Section 3 a solution based on the concept of U-statistics. Here, we recommend using the OOB error together with a permutation test. See e.g., Good (1994) or Kim et al. (2021), who use it in conjunction with the out-of-sample error evaluated on a test set: We first calculate the class-wise OOB errors $\\mathcal{E}_{0}^{o o b},\\mathcal{E}_{1}^{o o b}$ and then reshuffle the labels $K$ times to obtain $K$ permutations, $\\sigma_{1},\\dots,\\sigma_{K}$ say. For each of these new datasets $\\left(\\mathbf{Z}_{i},\\ell_{\\sigma_{k}\\left(i\\right)}\\right)_{i=1}^{N}$ , $k\\in\\{1,\\ldots,K\\}$ , we calculate the OOB errors\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\mathcal{E}_{j}^{o o b,k}:=\\frac{1}{n_{j}}\\sum_{i=1}^{N}\\mathbb{I}\\{\\ell_{\\sigma_{k}(i)}=j\\}\\mathbb{I}\\{\\hat{\\mathbf{g}}_{-i}(\\mathbf{Z}_{i})\\neq\\ell_{\\sigma_{k}(i)}\\},$$\n\n<PARAGRAPH_SEPARATOR>\n\nfor $j\\in\\{0,1\\}$ . Under $H_{0}$ , $(\\ell_{1},\\dots,\\ell_{N})$ and $(\\mathbf{Z}_{1},\\ldots,\\mathbf{Z}_{N})$ are independent and each $\\mathcal{E}_{1/2}^{o o b}$ is simply an iid draw from the distribution $F$ of the random variable $\\mathcal{E}_{1/2}^{o o b}|({\\mathbf{Z}}_{1},\\ldots,{\\mathbf{Z}}_{N})$ . As such we can accurately approximate the $\\alpha$ quantile $F^{-1}(\\alpha)$ of said distribution by performing a large number of permutations and use the decision rule\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\delta_{o o b}(D_{N})=\\left\\{\\mathcal{E}_{1/2}^{o o b}\\leq F^{-1}(\\alpha)\\right\\}.$$\n\n<PARAGRAPH_SEPARATOR>\n\nThus, as in the decision in Equation (2), the rejection region depends on the data at hand. Nonetheless, the level will be conserved, as proven e.g. in Hemerik and Goeman (2018, Theorem 1).\n\n<PARAGRAPH_SEPARATOR>\n\nHeuristically, this procedure will have power under the alternative, as in this case there is some dependence between $(\\ell_{1},\\dots,\\ell_{N})$ and $(\\mathbf{Z}_{1},\\ldots,\\mathbf{Z}_{N})$ , formed by the difference in the distribution of the $\\mathbf{Z}_{i}$ . The OOB error $\\mathcal{E}_{1/2}^{o o b}$ will thus be different than those observed under permutations.\n\n<PARAGRAPH_SEPARATOR>\n\nThe whole procedure is described in Algorithm 2. We name this test “hypoRF”."
  },
  {
    "qid": "statistic-posneg-1250-3-2",
    "gold_answer": "FALSE. While the graph total variance generalizes the classical total variance, they are not equivalent even when $\\mathbf{L}_{\\mathbf{W}} = \\mathbf{L}_{\\mathcal{A}}$. The classical total variance is scaled by $\\frac{1}{2D}$, whereas the graph version lacks this scaling factor. The two measures coincide only up to a constant factor, and their interpretations differ based on the underlying graph structure.",
    "question": "Is the graph total variance $\\operatorname{totvar}_{\\mathbf{L}_{\\mathbf{W}}}\\left(\\pmb{x}\\right)$ equivalent to the classical total variance $\\operatorname{totvar}({\\pmb x})$ when $\\mathbf{L}_{\\mathbf{W}} = \\mathbf{L}_{\\mathcal{A}}$?",
    "question_context": "Graph Compositional Data Analysis: Weighted Log-Ratios and Hilbert Spaces\n\nWe can generalize the theory developed for the space $(\\mathcal{S}_{\\pmb{W}}^{D},\\langle\\cdot,\\cdot\\rangle_{\\pmb{W}},\\oplus_{\\pmb{W}},\\odot_{\\pmb{W}})$ even further. One important extension that comes to mind is to allow the weights $w_{i j}$ to also take negative values as the following example shows: <PARAGRAPH_SEPARATOR> Example 1. Assume that through expert knowledge we are interested in analyzing only certain weighted combinations, say $\\begin{array}{r}{\\bar{\\sum_{j=1}^{D}}\\ln(\\frac{x_{i}}{x_{j}})w_{i j}=\\ln\\left(\\frac{x_{i}^{d(i)}}{\\prod_{j=1}^{D}x_{j}^{w_{i j}}}\\right)}\\end{array}$ , where $\\begin{array}{r}{d(i):=\\sum_{j=1}^{D}w_{i j},}\\end{array}$ for $i\\in\\{i_{1},\\ldots,i_{L}\\}\\subset\\{1,\\ldots,D\\}$ and $w_{i j}\\in\\mathbb{R}$ , not necessarily symmetric. We can collect these weighted combinations in a vector, $\\widetilde{\\mathbf{L}}_{\\mathsf{W}}\\ln(\\pmb{x})$ , where $\\widetilde{\\mathbf{L}}_{\\boldsymbol{\\mathsf{W}}}\\in\\mathbb{R}^{L\\times D}$ is a rectangular matrix, $L\\leq D$ , with $\\begin{array}{r}{(\\widetilde{\\mathbf{L}}_{\\mathbf{W}})_{\\ell i_{\\ell}}=\\sum_{j=1}^{D}w_{i j}}\\end{array}$ for $\\ell\\in\\{1,\\ldots,L\\}$ and $(\\widetilde{\\mathbf{L}}_{\\mathsf{W}})_{l j}=-w_{l j}$ ,  f˜or $j\\neq i_{l}$ . The m a˜trix $\\mathbf{L}_{\\mathbf{W}}:=\\widetilde{\\mathbf{L}}_{\\mathbf{W}}(\\widetilde{\\mathbf{L}}_{\\mathbf{W}})^{\\top}$ is symmetric, positive semi-˜definite,  has real valued entries and 1 in ˜its null space, $\\mathbf{L}_{\\mathsf{W}}\\mathbf{1}=\\mathbf{0}$ . LW can be used a s˜ a b˜uilding block for a scale-invariant Hilbert-space as explained below."
  },
  {
    "qid": "statistic-posneg-1136-0-0",
    "gold_answer": "FALSE. The condition $e(\\theta_{1},\\bar{\\theta}_{2})^{\\mathsf{T}}e(\\theta_{1},\\bar{\\theta}_{2})\\leqslant\\gamma$ is the original likelihood ratio-based confidence region, while $\\tau_{1}^{\\mathsf{T}}(I-T)\\tau_{1}\\leqslant\\rho$ is the ellipsoidal tangent plane approximation obtained after compensating for intrinsic nonlinearity. The two conditions are not equivalent; the latter is an approximation of the former after accounting for curvature in the expectation surface.",
    "question": "Is the condition for the likelihood ratio-based confidence region given by $e(\\theta_{1},\\bar{\\theta}_{2})^{\\mathsf{T}}e(\\theta_{1},\\bar{\\theta}_{2})\\leqslant\\gamma$ equivalent to the condition $\\tau_{1}^{\\mathsf{T}}(I-T)\\tau_{1}\\leqslant\\rho$ when compensating for intrinsic nonlinearity?",
    "question_context": "Confidence Regions for Parameter Subsets in Nonlinear Regression\n\nThe nonlinear regression model relates a vector of observations to the corresponding independent variables through the expectations, where the model function depends nonlinearly on the parameters. The additive error vector is assumed to have a spherical normal distribution with zero mean vector and covariance matrix. Improved approximations for confidence regions for the parameter subset are derived using a two-stage procedure. In the first stage, confidence regions are defined as a subset of the expectation surface, and approximate projections onto the tangent plane are obtained. In the second stage, the mapping from the relevant portion of the tangent plane to the parameter space is approximated."
  },
  {
    "qid": "statistic-posneg-1688-0-1",
    "gold_answer": "FALSE. While ρ_MS^2 > ρ_SS can lead to non-positive-definite covariance matrices for large k_i (sibship sizes), the paper explicitly notes that non-convergence was not a major issue in their simulations. The algorithm may still converge if the data are well-posed or if regularization techniques are implicitly applied.",
    "question": "Does the condition ρ_MS^2 > ρ_SS guarantee non-convergence of the Newton-Raphson algorithm due to non-positive-definite covariance matrices?",
    "question_context": "Maximum Likelihood Estimation of Interclass Correlations in Family Data\n\nThe paper presents a maximum likelihood estimation method for interclass correlations in family data, where measurements from N families are modeled with a multivariate normal distribution. The model includes parameters for means (μ_M, μ_S), variances (σ_M^2, σ_S^2), and correlations (ρ_MS, ρ_SS) between mother-offspring and sib-sib pairs. The likelihood function and iterative estimation procedures are derived, with a focus on handling varying sibship sizes and ensuring positive-definite covariance matrices."
  },
  {
    "qid": "statistic-posneg-706-1-1",
    "gold_answer": "TRUE. The CDF of Λ₃ is expressed using a series expansion involving terms θᵩᵏᵗ and generalized hypergeometric functions H. This form is derived from the density function and involves integrating the density over the range of Λ₃. The series expansion ensures that the CDF accounts for the noncentrality parameter Θ and the other distribution parameters, providing an exact expression for the cumulative probability.",
    "question": "Does the CDF of Λ₃ involve a series expansion with terms θᵩᵏᵗ and generalized hypergeometric functions H?",
    "question_context": "Noncentral Bimatrix Variate Beta Type V Distribution and Density Function of Λ₃\n\nThe paper introduces the noncentral bimatrix variate beta type V distribution, which is useful for test statistics requiring constant factors in the covariance matrices of Wishart matrix variates. The density function and CDF of Λ₃, defined as the product of determinants of matrices Q₁ and Q₂, are derived. The distribution is generated from ratios of Wishart matrices, and the paper provides detailed expressions for the density function, product moments, and CDF of Λ₃."
  },
  {
    "qid": "statistic-posneg-1143-1-1",
    "gold_answer": "FALSE. The context mentions that the EM-type estimator has a small positive bias, while the maximizing $\\pi$-type estimator has almost no bias. Therefore, the EM-type estimator is not unbiased, though the bias is small.",
    "question": "The EM-type estimator for the mixing proportion $\\pi$ in the model $m(x)=\\pi\\sigma^{2}\\chi_{p}^{2}(x)+(1-\\pi)f(x)$ is unbiased, as indicated by the simulation results in Table 6. Is this statement correct?",
    "question_context": "Mixing Proportion Estimation in Semi-Parametric Mixture Models\n\nWe are interested in the performance of our estimators for the case when the mixed density $f$ is multimodal. We generated 100 observations from $N(0,1)$ and 25 observations each from $N(2,0.1^{2})$ , $N(3,0.1^{2})$ , $N(4,0.1^{2})$ , and $N(5,0.1^{2})$ . As we can see from Table 6, the EM-type estimator has a small positive bias and the maximizing $\\pi$ -type estimator has almost no bias. <PARAGRAPH_SEPARATOR> # 5. Multi-dimensional cases <PARAGRAPH_SEPARATOR> In multi-dimensional cases, the density estimation requires many more observations to estimate a density as well as in one-dimensional cases. We propose to use the squared Euclidean distance to the mean and hence reduce the problem to the one-dimensional case. Since we know the mean, the squared Euclidean distance to the mean of the random variables from $N_{p}(\\underline{{0}},\\sigma^{2}I_{p})$ follows a $\\sigma^{2}\\chi_{p}^{2}$ distribution. Therefore, the mixture model is <PARAGRAPH_SEPARATOR> $$ m(x)=\\pi\\sigma^{2}\\chi_{p}^{2}(x)+(1-\\pi)f(x). $$ <PARAGRAPH_SEPARATOR> The estimators described in Sections 3.1 and 3.2 can be adopted to this case by changing the normal density to the $\\chi^{2}$ density. We will show the performance of our estimators with multi-dimensional data. <PARAGRAPH_SEPARATOR> Example 6. Multi-dimensional simulations. <PARAGRAPH_SEPARATOR> We generated observations from multivariate normal distributions. The mixed distributions are $N_{p}(\\underline{{0}},I_{p})$ and $N_{p}(\\underline{{2}},I_{p})$ for $p=2$ , 3, 4 and 5 and the true mixing proportion $\\pi$ is 0.5. The number of observations for each group is 200 (Table 7)."
  },
  {
    "qid": "statistic-posneg-820-1-2",
    "gold_answer": "TRUE. The context states that if $(\\hat{\\pmb{\\alpha}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\gamma}}_{t}^{\\mathrm{ls}})$ is an unbiased consistent estimator of $(\\pmb{\\alpha}_{t},\\pmb{\\beta}_{t},\\pmb{\\gamma}_{t})$, then the estimate of $E(Y_{t})$ obtained via the compensator method will also be unbiased and consistent. This is a direct consequence of the properties of unbiased and consistent estimators.",
    "question": "Is the estimating the compensator method guaranteed to produce an unbiased estimate of $E(Y_{t})$ if $\\hat{\\pmb{\\alpha}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\gamma}}_{t}^{\\mathrm{ls}}$ are unbiased and consistent estimators of $\\pmb{\\alpha}_{t},\\pmb{\\beta}_{t},\\pmb{\\gamma}_{t}$?",
    "question_context": "Linear Increment Model Estimation and Imputation Methods\n\nIn conclusion, the least-squares estimators are unbiased for the parameters of the LI model if weak DTIC holds and either (i) $\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}}$ is fixed at the true value of $\\beta_{t}$ , or (ii) there is no measurement error. Otherwise, they may not be. Henceforth, the only value at which we consider fixing $\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}}$ is $I$ . This is the case of most practical interest: one could fix $\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}}={\\pmb I}$ if one believed that expected increment $E(\\Delta Y_{t}\\mid\\mathcal{F}_{t-1})$ did not depend on $Y_{t-1}$ (i.e. $\\boldsymbol\\beta_{t}=\\boldsymbol I$ ). Otherwise, one could treat $\\beta_{t}$ as unknown and estimate it. It seems unlikely one would wish to fix $\\beta_{t}$ at a value other than $I$ .\n\n# 2.4. Estimating the compensator\n\nFrom Equation (2), $E(Y_{t}\\mid X,Y_{1})=\\alpha_{t}+\\beta_{t}^{\\top}E(Y_{t-1}\\mid X,Y_{1})+\\gamma_{t}^{\\top}X$ , and so\n\n$$\nE(Y_{t}\\mid X,Y_{1})=\\alpha_{t}+\\gamma_{t}^{\\top}X+\\left\\{\\sum_{j=2}^{t-1}\\left(\\prod_{k=j+1}^{t}\\beta_{k}^{\\top}\\right)(\\alpha_{j}+\\gamma_{j}^{\\top}X)\\right\\}+\\prod_{j=2}^{t}\\beta_{j}^{\\top}Y_{1}\n$$\n\nwhere $\\textstyle\\prod_{j=a}^{b}\\beta_{j}^{\\top}$ means $\\beta_{b}^{\\top}\\beta_{b-1}^{\\top}\\ldots\\beta_{a}^{\\top}$ . Thus, $E(Y_{i t}\\mid X_{i},Y_{i1})$ can be estimated for each individual $i$ in the dataset by replacing $\\pmb{\\alpha}_{t},\\pmb{\\beta}_{t}$ and $\\gamma_{t}$ in Equation (6) with $\\hat{\\pmb{\\alpha}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}}$ and $\\hat{\\gamma}_{t}^{\\mathrm{ls}}$ . Denote this estimate as $Y_{i t}^{\\mathrm{cpr}}$ . This is the estimating the compensator method. The overall mean $E(Y_{t})$ can then be estimated as $N^{-1}\\sum_{i=1}^{N}Y_{i t}^{\\mathrm{cpr}}$ . If $(\\hat{\\pmb{\\alpha}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\gamma}}_{t}^{\\mathrm{ls}})$ is an unbiased consistent estimator of $(\\pmb{\\alpha}_{t},\\pmb{\\beta}_{t},\\pmb{\\gamma}_{t})$ , then this estimate of $E(Y_{t})$ will also be unbiased and consistent.\n\nReturning to Example 1, if $\\hat{\\alpha}_{2}^{\\mathrm{ls}}=0.585$ and $\\hat{\\beta}_{2}^{\\mathrm{ls}}=0.267$ are used in Equation (6), then $E(Y_{2})$ is calculated to equal $\\hat{\\alpha}_{2}^{\\mathrm{ls}}=0.585$ , whereas its true value is zero.\n\n# 2.5. LI–LS imputation\n\nA&G propose an alternative to estimating the compensator when there is no measurement error. We call this method ‘LI-LS imputation’, because it is based on the LI model of Equation (2) and the least-squares (LS) estimators $\\hat{\\pmb{\\alpha}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\gamma}}_{t}^{\\mathrm{ls}}$ of $\\pmb{\\alpha}_{t},\\pmb{\\beta}_{t}$ and $\\gamma_{t}$ . (Later, we shall introduce other LI imputation methods that differ from LI-LS imputation only in that they use alternative estimators of $\\pmb{\\alpha}_{t},\\pmb{\\beta}_{t}$ and $\\gamma_{t}$ .) LI–LS imputation uses an actual outcome when it is observed and imputes a missing outcome as the most recently observed outcome updated by the expected increments. That is, letting $Y_{t}^{\\mathrm{est}}$ denotes the value of $Y_{t}$ in the imputed dataset, we have $Y_{t}^{\\mathrm{est}}=Y_{t}$ if $R_{0t}=1$ and $\\begin{array}{r}{Y_{t}^{\\mathrm{est}}=\\hat{\\alpha}_{t}^{\\mathrm{ls}}+(\\hat{\\beta}_{t}^{\\mathrm{ls}})^{\\top}Y_{t-1}^{\\mathrm{est}}+(\\hat{\\gamma}_{t}^{\\mathrm{ls}})^{\\top}X}\\end{array}$ if $R_{0t}=0$ .\n\nA&G show that if Equation (2) holds, there is no measurement error, strong DTIC holds and\n\n$$\nE\\{R_{0,t}(Y_{t-1}^{\\mathrm{est}}-Y_{t-1})\\mid X,Y_{1}\\}={\\bf0}\n$$\n\nthen $E(Y_{t}^{\\mathrm{est}}\\mid Y_{1},X)=E(Y_{t}\\mid Y_{1},X)$ . It follows that $N^{-1}\\sum_{i=1}^{N}Y_{i t}^{\\mathrm{est}}$ is an unbiased estimator of $E(Y_{t})$ and that if a linear regression model for $Y$ with any or all of $X$ and $t$ as covariates is fitted to the imputed dataset $\\{Y_{i t}^{\\mathrm{est}}:i=1,\\dots,N$ I $t=1,\\ldots,T\\}$ using least squares, then the parameter estimators of this model are consistent.\n\nThe following theorem shows that LI–LS imputation can also be used when there is measurement error, provided that $\\beta_{t}=I$ and $\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}}$ is fixed at $I$ .\n\nTheorem 2. Suppose that the increments model of Equation (2), the strong DTIC assumption of Equation (4) and Equation (7) hold, that ${\\boldsymbol{\\beta}}_{t}={\\boldsymbol{I}}$ , and that $\\hat{\\pmb{\\beta}}_{t}^{l s}$ is fixed at $I$ . Then, $E(Y_{t}^{e s t}\\mid$ $Y_{1},X)=E(Y_{t}\\mid Y_{1},X)$ ."
  },
  {
    "qid": "statistic-posneg-673-0-3",
    "gold_answer": "TRUE. The score function U(β̂0,0) is derived from the partial log-likelihood of the model λ(t|z)=λ0(t)exp{β′z0+ν1{zj≤c}}, which is used to test the null hypothesis H0:ν=0. The score function is evaluated at the maximum partial likelihood estimator β̂0 under the null hypothesis and ν=0.",
    "question": "Is the score function U(β̂0,0) derived from the partial log-likelihood of the model λ(t|z)=λ0(t)exp{β′z0+ν1{zj≤c}}?",
    "question_context": "Tree-Augmented Cox Proportional Hazards Models\n\nThe paper introduces a hybrid model combining Cox proportional hazards models with regression trees to improve model fit and interpretability. The model is defined as λ(t|z)=λ0(t)exp{β′z0+γ′z(T)}, where z(T) is a dummy vector defined by a tree structure T. The procedure involves growing a large tree, pruning it, and selecting the best-sized tree using BIC. The score test statistic S(ν|s)=U2(β̂0,0)/Σ̂ is used to find the best split, where U(β̂0,0) is the score function and Σ̂ is a consistent estimator of the variance of U(β0,0)."
  },
  {
    "qid": "statistic-posneg-675-4-0",
    "gold_answer": "TRUE. The term $(x-t_{j})_{+}$ is defined as the positive part of $(x-t_{j})$, which means it equals zero when $x \\leq t_{j}$ and equals $(x-t_{j})$ when $x > t_{j}$. This creates a linear spline basis function that is piecewise linear with a knot at $t_{j}$.",
    "question": "In the spline model $$ s(x)=\\beta_{0}+\\beta_{-1}x+\\sum_{j=1}^{K}\\beta_{j}(x-t_{j})_{+} $$, the term $(x-t_{j})_{+}$ represents a linear spline basis function. Is it correct to say that this function is zero for all $x \\leq t_{j}$ and linear for $x > t_{j}$?",
    "question_context": "Spline-Based Split Point Selection in Tree-Structured Mixed-Effects Regression\n\n2. Select $(\\bar{x}_{+}+\\bar{x}_{-})/2$ as the split point. <PARAGRAPH_SEPARATOR> Algorithm 3: Spline (S) split point selection for noncategorical $X$ <PARAGRAPH_SEPARATOR> 1. Calculate fitted values using the spline model, <PARAGRAPH_SEPARATOR> $$ s(x)=\\beta_{0}+\\beta_{-1}x+\\sum_{j=1}^{K}\\beta_{j}(x-t_{j})_{+}. $$ <PARAGRAPH_SEPARATOR> 2. Add knots achieving maximum reduction in residual sum of squares using a forward stepwise procedure. <PARAGRAPH_SEPARATOR> 3. Treat the knots on candidates for the split points. <PARAGRAPH_SEPARATOR> 4. Select the best split point of the candidate knots."
  },
  {
    "qid": "statistic-posneg-768-1-1",
    "gold_answer": "FALSE. The context clearly explains that adding the term $\\Delta_{n}q(X_{i})$ to the LOO estimator introduces a bias: $E[\\hat{\\boldsymbol{f}}_{n,i}^{\\mathrm{LR}}|X_{i}]-f_{{h}_{n}}(X_{i})=\\Delta_{n}{{q}}(X_{i})$. This bias is a trade-off for ensuring the LR estimator remains positive and avoids the issues of the LOO estimator.",
    "question": "The LR estimator $\\hat{\\boldsymbol f}_{n,i}^{\\mathrm{LR}}$ introduces no additional bias compared to the LOO estimator $\\hat{\\boldsymbol f}_{n,i}^{\\mathrm{LOO}}$ when estimating $f_{h_{n}}(X_{i})$. Is this statement correct?",
    "question_context": "Leave-and-Repair Estimator in Kernel Density Estimation\n\nThe aim of this section is to motivate the use of the LR estimator $\\hat{f}_{n}^{\\mathrm{LR}}$ in the definition of the fitness coefficient ${\\hat{\\alpha}_{n}}$ . Let $(X_{i})_{i\\geq1}$ be an independent and identically distributed sequence of $\\mathbb{R}^{d}$ -valued random variables having density $f_{0}$ with respect to the Lebesgue measure. The kernel density estimator of $f_{0}$ at $x\\in\\mathbb{R}^{d}$ is given by $$ {\\hat{f}}_{n}(x)={\\frac{1}{n h_{n}^{d}}}\\sum_{i=1}^{n}K\\left({\\frac{x-X_{i}}{h_{n}}}\\right). $$ For any $h>0$ , define the function $f_{h}$ as the convolution product between $K_{h}(\\cdot)=K(\\cdot/h)/h^{d}$ and $f_{0}$ , that is, $f_{h}(x)=(K_{h}\\star f_{0})(x)$ , $x\\in\\mathbb{R}^{d}$ . Note that $f_{h_{n}}(x)=\\mathbb{E}[\\hat{f}_{n}(x)]$ . But since $\\mathbb{E}[\\hat{f}_{n}(X_{i})|X_{i}]=f_{h_{n}}(X_{i})+$ $K(0)/(n h_{n}^{d})$ , we see that ${\\hat{f}}_{n}(X_{i})$ has a positive $h_{n}$ -dependent bias when estimating| $f_{h_{n}}(X_{i})$ , conditionally on $X_{i}$ . When studying the estimator decomposition, this bias term spreads to the diagonal terms of some $U$ -statistics and gives rise, in the end, to some nonnegligible terms. This phenomenon is common in semiparametric statistics, and has been noticed for instance in remark 4 in Portier and Segers (2018). To overcome the undesirable effects caused by this bias term, the leave-one-out (LOO) estimator of $f_{h_{n}}(X_{i})$ , given by $$ \\hat{f}_{n,i}^{\\mathrm{LOO}}=\\frac{1}{(n-1)h_{n}^{d}}\\sum_{j\\neq i}K\\left(\\frac{X_{i}-X_{j}}{h_{n}}\\right), $$ has been successfully used in several cross-validation procedures aiming at selecting the bandwidth, either based on the likelihood (Habbema, Hermans, & Van Den Broeck, 1974; Hall, 1987; Marron, 1985) or on the mean squared error (Rudemo, 1982; Stone, 1984) (see Marron (1987) for a comparison). Since then, LOO estimators have been frequently used in semiparametric studies (Delecroix et al., 2006). The LR estimator proposed in this article is inspired, but different, from the LOO estimator. In view of (2), the LR estimator satisfies $\\hat{\\boldsymbol f}_{n,i}^{\\mathrm{LR}}=\\bar{\\boldsymbol f}_{n,i}^{\\mathrm{LOO}}+\\Delta_{n}q(\\boldsymbol X_{i})$ . If $\\Delta_{n}=0$ the LR estimator is equal to the LOO estimator. If $q=K(0)$ and $\\Delta_{n}=1/((n-1)h_{n}^{d})$ the LR estimator is equal to $(n/(n-1))\\hat{f}_{n}(X_{i}).$ . In this article, $\\Delta_{n}$ shall be typically of order $1/n$ and $q$ shall be a density satisfying $q(X_{1})>0$ almost surely. The heuristic for using the LR estimator $\\hat{\\boldsymbol f}_{n,i}^{\\mathrm{LR}}$ instead of the LOO estimator $\\hat{\\boldsymbol f}_{n,i}^{\\mathrm{LOO}}$ is as follows. It is well known that the Kullback–Leibler divergence between kernel density estimates and the true density depends crucially on the tails of the true distribution $f_{0}$ (Hall, 1987; Schuster & Gregory, 1981). As shown in Hall (1987), if the tail is too heavy and the kernel $K(x)$ vanishes too quickly as $x$ becomes large then the associated Kullback–Leibler divergence diverges to minus infinity. This is because some of the $\\hat{\\boldsymbol f}_{n,i}^{\\mathrm{LOO}},i=1,\\dots,n.$ , might have very small values (possibly zero), leading to very large values (possibly infinite) for some of the $\\log(\\hat{f}_{n,i}^{\\mathrm{LOO}})$ . We built the LR estimator to overcome this issue by simply adding $\\Delta_{n}q(X_{i})$ to the LOO estimator. We coined the term leave-and-repair because the term $\\Delta_{n}q(X_{i})$ repairs the LOO estimator. Since ${\\hat{f}}_{n,i}^{\\mathrm{LR}}\\geq\\Delta_{n}q(X_{i}).$ , the LR estimator is not subjected to the difficulties of the LOO estimator. By adding the term $\\Delta_{n}q(X_{i})$ in (2), however, a bias is introduced: now one has $E[\\hat{\\boldsymbol{f}}_{n,i}^{\\mathrm{LR}}|X_{i}]-f_{{h}_{n}}(X_{i})=\\Delta_{n}{{q}}(X_{i})$ . Thus, there is a bias-variance trade-off controlled by the sequence $\\Delta_{n}$ that must go to zero slowly enough to keep $\\hat{\\boldsymbol f}_{n,i}^{\\mathrm{LR}}$ away from zero but also fast enough to keep the bias as small as possible. The right compromise is given in the conditions in Theorem 1 (for instance $\\Delta_{n}=1/n$ is one possibility). Let ${\\mathcal{P}}=\\{f_{\\theta}:\\theta\\in\\Theta\\}$ be the parametric model where $\\Theta\\subset\\mathbb{R}^{p}$ is such that for each $\\theta\\in\\Theta,f_{\\theta}:$ $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{\\geq0}$ and $\\textstyle\\int f_{\\theta}(x)\\mathrm{d}x=1$ . The maximum likelihood estimator of $f_{0}$ based on $\\mathcal{P}$ and $X_{1}$ , … , $X_{n}$ is $f_{\\hat{\\theta}_{n}}$ where $\\hat{\\theta}_{n}$ (when it exists; this is further assumed) is defined as $$ \\hat{\\theta}_{n}\\in\\mathop{\\mathrm{argmax}}_{\\theta\\in\\Theta}\\sum_{i=1}^{n}\\log(f_{\\theta}(X_{i})). $$ To conclude the section, we consider existence and uniqueness of the fitness coefficient ${\\hat{\\alpha}}_{n}$ . The existence follows from the use of the LR estimator $\\hat{\\boldsymbol f}_{n,i}^{\\mathrm{LR}}$ . Uniqueness of ${\\hat{\\alpha}_{n}}$ is obtained under the mild requirement that the parametric and nonparametric estimators are distinguishable on the observed data. Proposition 1. Suppose that $\\Delta_{n}>0$ and $q(X_{1})>0$ a.s. and $f_{\\widehat{\\theta}_{n}}(X_{i})\\neq\\widehat{f}_{n,i}^{\\mathrm{LR}}$ for at least one $i\\in\\{1,\\dots,n\\}$ . Then the fitness coefficient exists and is unique."
  },
  {
    "qid": "statistic-posneg-138-1-1",
    "gold_answer": "FALSE. The matrix $\\boldsymbol{W}_{x x}$ does not represent the covariance matrix of the concomitant variables after adjusting for the experimental design effects. Instead, it represents the sum of cross-products of the deviations of the concomitant variables from their cell means $\\widetilde{x}_{i j,\\cdot}^{(h)}$. This is a within-cell covariance matrix, capturing the variability of the concomitant variables within each cell of the experimental design, not the overall covariance matrix adjusted for design effects.",
    "question": "Does the matrix $\\boldsymbol{W}_{x x}$ represent the covariance matrix of the concomitant variables after adjusting for the experimental design effects, based on the formula $w_{h h^{\\prime}}=\\sum_{i,j,k}\\big(x_{i j k}^{(h)}-\\widetilde{x}_{i j,\\cdot}^{(h)}\\big)\\big(x_{i j k}^{(h^{\\prime})}-\\widetilde{x}_{i j,\\cdot}^{(h^{\\prime})}\\big)\\quad(h,h^{\\prime}=1,...,p)$?",
    "question_context": "Extension of Tukey's Method to Experimental Designs with Random Concomitant Variables\n\nThe paper extends Tukey's method of multiple comparisons to experimental designs with random concomitant variables. The linear model considered is: $$y_{i j k}=\\mu+\\theta_{i}+\\beta_{j}+(\\theta\\beta)_{i j}+\\displaystyle\\sum_{h=1}^{p}\\left(\\dot{x}_{i j k}^{(h)}-\\tilde{\\delta}^{(h)}\\right)u_{h}+\\epsilon_{i j k}$$ subject to the usual model restrictions $\\Sigma_{i}\\theta_{i}=\\Sigma_{j}\\beta_{j}=\\Sigma_{i}(\\theta\\beta)_{i j}=\\Sigma_{j}(\\theta\\beta)_{i j}=0,i=1,...,I,$ ${\\mathfrak{j}}=\\mathbf{1},...,{\\mathfrak{J}}$. Solution of the normal equations gives $\\hat{\\theta}_{i}=(\\overline{{y}}_{i..}-\\overline{{y}}_{i..})-\\sum_{h=1}^{p}(\\overline{{x}}_{i..}^{h)}-\\overline{{x}}_{i..}^{(h)})\\hat{\\Omega}_{h},$ where $\\widehat{\\pmb{\\mathscr{u}}}=\\boldsymbol{W}_{\\pi x}^{-1}\\boldsymbol{w}_{x y},\\boldsymbol{W}_{x x}$ has elements $w_{h h^{\\prime}}=\\sum_{i,j,k}\\big(x_{i j k}^{(h)}-\\widetilde{x}_{i j,\\cdot}^{(h)}\\big)\\big(x_{i j k}^{(h^{\\prime})}-\\widetilde{x}_{i j,\\cdot}^{(h^{\\prime})}\\big)\\quad(h,h^{\\prime}=1,...,p),$ and the vector $\\pmb{w_{x y}}$ has elements $w_{h,p+1}=\\sum_{i,j,k}(y_{i j k}-\\overline{{y}}_{i j,i})(x_{i j k}^{(h)}-\\overline{{x}}_{i j,\\cdot}^{(h)})(h=1,...,p),$ while $w_{p+1,p+1}=w_{w v}=\\sum_{i,j,k}(y_{i j k}-{\\overline{{y}}}_{i j.})^{\\perp}.$ The estimates $\\theta_{i}$ have the form $c_{i}^{\\prime}(y-Z\\hat{u})$ , where $c_{\\bf i}\\tt a r e\\cal I J K\\bf\\times1$ and $\\pmb{y}=\\left[\\begin{array}{c}{y_{111}}\\\\{\\vdots}\\\\{y_{I J K}}\\end{array}\\right],\\quad\\pmb{Z}=\\left[\\begin{array}{c c c}{(x_{111}^{(1)}-\\overrightarrow{x}_{...}^{(1)})}&{\\quad...}&{(x_{111}^{(p)}-\\overrightarrow{x}_{...}^{(p)})}\\\\{\\vdots}&{}&{\\}\\\\{(x_{I J K}^{(1)}-\\overrightarrow{x}_{...}^{(1)})}&{\\quad...}&{(x_{I j K}^{(p)}-\\overrightarrow{x}_{...}^{(p)})}\\end{array}\\right].$"
  },
  {
    "qid": "statistic-posneg-682-0-0",
    "gold_answer": "FALSE. The formula $\\mathbf{P}(r;\\mathcal{N}_{i})$ is not a blend of binomial and hypergeometric distributions. It is a compound probability distribution where the binomial term $\\pmb{\\mathscr{p}}_{s}^{\\:x}(1-\\pmb{\\mathscr{p}}_{s})^{N_{i}-x}$ represents the probability of 'x' successes (correct identifications due to sensory 'clicks') out of $\\mathcal{N}_{i}$ trials, and the hypergeometric-like term $\\frac{\\binom{\\mathcal{N}_{i}}{x}\\binom{\\mathcal{N}_{i}-x}{\\mathcal{N}_{i}-r}}{\\binom{\\mathcal{N}_{i}+\\mathcal{N}_{j}-x}{\\mathcal{N}_{j}}}$ accounts for the probability of the remaining $(\\mathcal{N}_{i}-r)$ correct placements being due to random guessing. The two components are multiplicatively combined, not blended, and the overall structure is more complex than a simple mixture of the two distributions.",
    "question": "Is the probability formula $\\mathbf{P}(r;\\mathcal{N}_{i})$ correctly described as a blend of a binomial and a hypergeometric distribution, given its structure and the context of sensory discrimination testing?",
    "question_context": "Sensory Discrimination Testing: Probabilistic Model for Sorting Tests\n\nWe now return to that common and simple and important question of taste testing for the null hypothesis that two items are sensorily indistinguishable. Pair comparison is merely the simplest of a whole family of experimental designs that can be used for this purpose. The literature is extensive. A large part of it is devoted to the problem of design effciency, and not allof this is crystal clear. There is a pervasive belief that because the aim is to test $\\mathbf{H_{0}}$ it follows that relative merits can be assessed on theoretical grounds alone. An assumption frequently made is that if, when the null hypothesis is true, an all-correct solution is statistically more improbable with design $\\mathbf{x}$ than with design Y, ergo, design X is the better. But this situation is rare in practice. Mostly, the null hypothesis is untrue, with, however, only a small difference between the compared items. So it is more realistic to discuss *betterness' in terms of the latent ability of the design to pick up a small sensory difference. <PARAGRAPH_SEPARATOR> Consider the general dichotomous sorting model in which we have $\\mathcal{N}_{i}$ samples of a product $\\mathbf{\\nabla}_{i}^{\\star}$ and ${\\mathcal{N}}_{j}$ of $\\mathfrak{f}^{\\mathfrak{v}}$ . They are presented to a subject, in coded random order (or in a coded randomly arranged group), and he is asked to unscramble them by taste differentiation alone. Attention will be restricted to the condition in which the subject is informed beforehand of the size of the $\\mathcal{N}\\mathfrak{s}.$ .We need a probabilistic model that will take into account what happens when the subject is neither completely insensitive to the difference between the items nor unambiguously recognises them. This situation often obtains: the subject sometimes places an item correctly because his sensory apparatus “clicks', and sometimes, when there is no ‘click', he makes a lucky guess. The process is known in psychology as subliminal perception or subception,7.17 because there is no necessary awarenessofwhich placements are fortuitous and which are subceived. On this basis empirical probabilities of sensory discrimination can be invoked. Let us define $\\pmb{\\phi}_{s i}$ and $\\pmb{\\phi}_{s j}$ as the probabilities of contextual subception of an $\\mathbf{\\nabla}^{6}\\vec{\\imath}^{,}$ or a $\\mathfrak{f}^{}$ ， respectively. Suppose, as the simplest hypothesis, that $\\pmb{\\phi}_{s i}=\\pmb{\\phi}_{s j}=$ , say, $\\pmb{\\mathscr{p}}_{\\pmb{\\mathscr{s}}}$ .Then it may be shownl1l that the probability of exactly $r$ 'successes' in the sorted-off subgroup of $\\mathcal{N}_{i}$ items $(\\mathcal{N}_{i}\\leq\\mathcal{N}_{j})$ will be a blend of a binomial and a hypergeometric distribution, with probability density: <PARAGRAPH_SEPARATOR> $$ \\mathbf{P}(r;\\mathcal{N}_{i})\\:=\\:\\binom{\\mathcal{N}_{j}}{\\mathcal{N}_{i}-r}\\:\\sum_{x=0}^{r}\\frac{\\binom{\\mathcal{N}_{i}}{x}\\binom{\\mathcal{N}_{i}-x}{\\mathcal{N}_{i}-r}}{\\binom{\\mathcal{N}_{i}+\\mathcal{N}_{j}-x}{\\mathcal{N}_{j}}}\\pmb{\\mathscr{p}}_{s}^{\\:x}(1-\\pmb{\\mathscr{p}}_{s})^{N_{i}-x} $$ <PARAGRAPH_SEPARATOR> An important limiting case is when $\\mathcal{N}_{i}$ is unity; then <PARAGRAPH_SEPARATOR> $$ \\mathbf{P}(r=1)=1-\\mathbf{P}(r=0)=\\frac{1+\\phi_{s}\\mathcal{N}_{j}}{1+\\mathcal{N}_{j}} $$ <PARAGRAPH_SEPARATOR> When $\\mathcal{N}_{j}$ is also unity we have pair comparison, and when it is 2 we have the 'triangle' test. Severai higher cases are to be found in the literature. <PARAGRAPH_SEPARATOR> Power functions can be obtained; i.e., we can seek the probability of the rejection of a false $\\mathrm{H}_{0}$ . But the difficulty encountered earlier arises again here: we cannot assume $\\pmb{\\phi}_{s}$ to be independent of design. However, as we said in discussing pair comparison with and without ties, intuition suggests, and experience substantiates, a negative correlation between the size of $\\pmb{\\mathscr{p}}_{s}$ and the complexity of the taster's task. With this in mind we may use power functions to arrive at tentative inferences about the merits of rival designs. Note the ‘tentative' ; here, supremely, the proof of the pudding is in the eating; only experimental observation can supply authentic answers. And we must particularise: findings on ice cream are unlikely to apply to canned soup, and still less to instant coffee. By and large, the safe rule is to aim for simplicity of design."
  },
  {
    "qid": "statistic-posneg-334-0-3",
    "gold_answer": "TRUE. The inequality is derived using the Rosenthal inequality and properties of the empirical distribution function. It holds under the assumption that $2^{j}\\leq{\\sqrt{n/\\ln(n)}}$ and is a key result used in the proof of the main theorems.",
    "question": "The paper states that the moment inequality $\\mathbb{E}\\left((\\hat{c}_{j,k}-c_{j,k})^{2p}\\right)\\leq C\\frac{1}{n^{p}}$ holds for all $j$ such that $2^{j}\\leq{\\sqrt{n/\\ln(n)}}$. Is this statement true?",
    "question_context": "Nonparametric Estimation of a Quantile Density Function Using Wavelets\n\nThe paper discusses nonparametric estimation of a quantile density function using wavelet methods. It presents several lemmas and theorems that establish bounds on the estimation error and convergence rates. Key formulas include bounds on the probability of large deviations of wavelet coefficient estimates and the expected Lp error of the wavelet-based estimator."
  },
  {
    "qid": "statistic-posneg-2108-0-1",
    "gold_answer": "FALSE. Not all such sequences correspond to valid preference matrices. The additional condition that the sum of any $r$ alphas must be $\\geqslant\\frac{1}{2}r(r-1)$ for $r=1,2,3,...,n-1$ is necessary to ensure the sequence can be derived from a preference matrix. This restricts the set of valid sequences beyond just the total sum condition.",
    "question": "Is it true that all sequences of $n$ integers summing to $\\frac{1}{2}n(n-1)$ correspond to valid preference matrices?",
    "question_context": "Preference Matrix Sequences and Their Validity Conditions\n\nThe sequences $\\{\\alpha_{1},...,\\alpha_{n}\\}$ which are derived from the preference matrices of order $\\pmb{n}$ are not all the possible sequences of $_{n}$ whose sum is $\\scriptstyle{\\frac{1}{2}}n(n-1)$ . We have the following lemma (referred to in $\\S3$ of the paper). The necessary and sufficient condition that the sequence, $\\{\\alpha_{1},\\alpha_{2},...,\\alpha_{n}\\}$ of $_{\\pmb{\\mathscr{n}}}$ integers, whose sum is $\\scriptstyle\\pmb{\\mathrm{j}}n(n-1)$ , corresponds to a preference matrix is that the sum of any $\\pmb{\\mathscr{r}}$ alphas $\\geqslant\\frac{1}{2}r(r-1)$ for $r=1,2,3,...,n-1$."
  },
  {
    "qid": "statistic-posneg-889-0-0",
    "gold_answer": "FALSE. The condition $r(\\theta-1)>\\theta s$ is necessary but not sufficient for the success lead stopping rule to be uniformly preferable. The paper specifies that the success lead rule is preferable only when θ* is close to 1 and the true ratio of probabilities of success is not appreciably closer to 1 than θ*. The condition must be evaluated in the context of the approximation for θ* close to 1 and the specific values of P*.",
    "question": "Is the condition $r(\\theta-1)>\\theta s$ sufficient to conclude that the success lead stopping rule is uniformly preferable over the complete parameter space?",
    "question_context": "Comparison of Play-the-Winner Sampling Procedures in Binomial Populations\n\nIn §§3 and 4, we have shown that play-the-winner sampling is uniformly preferable to vector-at-a-time sampling when the probability requirement for correct selection is expressed in terms of the ratio of probabilities of success. We now compare the stopping rules for play-the-winner sampling based on success lead and inverse sampling and in particular we investigate the conditions under which each rule gives a smaller expected number of trials on the poorer treatment. Our analytical results will be appropriate to the case when θ* is close to 1. Using (10) and (24), where for θ* close to 1 and hence γ large we replace the Φ functions by 1, we have\n\n$$\n\\begin{array}{r}{E(N_{P W,I}^{(1)})-E(N_{P W,L}^{(1)})\\sim\\frac{\\theta(1-p_{2})}{p_{2}(\\theta-1)(\\theta-p_{2})}\\{(\\theta-1)r-\\theta s+(s-\\frac{1}{2})p_{2}\\}.}\\end{array}\n$$\n\nIt follows that the success lead stopping rule will be preferable over the complete parameter space if $r(\\theta-1)>\\theta s$ From §2 , the selection constant δ for the success lead stopping rule is the smallest integer for which $(\\theta^{*})^{s}\\geqslant P^{*}/(1-P^{*})$ .If we replace the inequality by an equality and ignore terms that are $O(\\theta^{*}-1)^{2}$ ,wehave\n\n$$\ns\\sim\\frac1{(\\theta^{*}-1)}\\log\\left(\\frac{P^{*}}{1-P^{*}}\\right).\n$$\n\nFor θ* close to 1, the selection constant γ for the inverse sampling rule is given by (17), so the success lead rule is uniformly preferable if\n\n$$\n\\frac{\\theta-1}{\\theta}>\\left(\\frac{\\theta^{*}-1}{\\theta^{*}}\\right)\\frac{\\log\\left\\{P^{*}/(1-P^{*})\\right\\}}{2\\lambda^{2}(P^{*})}.\n$$\n\nThe values of $\\{2\\lambda^{2}(P^{*})\\}^{-1}\\log\\left\\{P^{*}/(1-P^{*})\\right\\}$ are 0.67, 0.54 and 0.42,respectively,for $P^{*}=0.90_{\\mathrm{:}}$ 0.95 and 0.99. Thus the success lead stopping rule will be preferable unless the true ratio of probabilities of success is appreciably closer to 1 than the specified value of θ*"
  },
  {
    "qid": "statistic-posneg-2020-0-3",
    "gold_answer": "TRUE. The paper explicitly states that assuming b = ϕ/2 is sufficient to obtain a closed-form expression for the marginal likelihood. This constraint ensures that the sum of the signal and noise terms (both generalized Laplace) remains a generalized Laplace random vector.",
    "question": "Does the paper suggest that the constraint b = ϕ/2 is necessary for the marginal likelihood to have a closed-form expression?",
    "question_context": "Generalized Laplace Distribution in PPCA Framework\n\nThe paper discusses the multivariate generalized asymmetric Laplace distribution and its properties within the PPCA framework. Key properties include the characteristic function, density function, and the Gauss-Laplace representation. The distribution is used to model the signal and noise terms in the PPCA model, leading to a closed-form expression for the marginal likelihood."
  },
  {
    "qid": "statistic-posneg-1822-0-1",
    "gold_answer": "FALSE. The context mentions that $H_{D}(k)$ has peaks at $\\frac{1}{8}, \\frac{8}{8}, \\frac{8}{8}, \\frac{7}{8}$, which is likely a typographical error. The correct peaks should be at $\\frac{1}{8}, \\frac{3}{8}, \\frac{5}{8}, \\frac{7}{8}$ for a distribution with four symmetric peaks. The formula for $H_{D}(k)$ is only provided for the interval $0\\leqslant x<\\frac{1}{4}$, and it is repeated with period $\\frac{1}{4}$ to fill the interval, which would naturally produce peaks at $\\frac{1}{8}, \\frac{3}{8}, \\frac{5}{8}, \\frac{7}{8}$.",
    "question": "Does the density function $H_{D}(k)$ describe a distribution with four peaks at $\\frac{1}{8}, \\frac{3}{8}, \\frac{5}{8}, \\frac{7}{8}$ as implied by the context?",
    "question_context": "Goodness-of-Fit Statistics for Circular Distributions\n\nSimilar studies were therefore undertaken with other alternative distributions, to examine whether situations exist in which any of the statistics might clearly be better or worse than the other two. <PARAGRAPH_SEPARATOR> # 5.2. Other alternatives to uniformity <PARAGRAPH_SEPARATOR> We should like to consider, as alternatives to uniformity, distributions which describe various degrees of clustering of observations which would be possible on 8 circle: one large cluster, two clusters, and so on. A very convenient set of alternatives is $H_{B},H_{C}$ and $H_{D},$ whichfollow: <PARAGRAPH_SEPARATOR> $$ H_{B}(k)\\colon f(x)={\\left\\{\\begin{array}{l l}{k(2x)^{k-1}}&{{\\mathrm{~(0\\leqslant~}}x<\\frac{1}{2}{\\mathrm{)}},}\\ {k\\{2(1-x)\\}^{k-1}}&{{\\mathrm{~(\\frac{1}{2}\\leqslant~}}x<1{\\mathrm{)}}.}\\end{array}\\right.} $$ <PARAGRAPH_SEPARATOR> This has a peak at ${\\pmb x}=\\pmb{\\updelta}_{:}$ increasing in relative importance as $\\pmb{k}$ becomes larger. A second setis <PARAGRAPH_SEPARATOR> $$ H_{O}(k)\\colon f(x)=\\left\\{{k\\mathord{\\left\\{\\begin{array}{l l}{k(4x)^{k-1}}&{\\mathrm{~(0\\leqslant~}x<\\frac{1}{\\pi})}\\ {k\\{4(\\frac{1}{2}-x)\\}^{k-1}}&{\\mathrm{~(4\\leqslant~}x<\\frac{1}{\\pi}).}\\end{array}}\\right.}\\right.} $$ <PARAGRAPH_SEPARATOR> This density is repeated with period $\\frac{1}{2}$ to fill the interval $\\Sigma\\leqslant x<1$ ; it has two peaks, at $x=\\ddagger$ and $x=\\frac{3}{4}$ . A third set, with 4 peaks, at $\\frac{1}{8},\\frac{8}{8},\\frac{8}{8},\\frac{7}{8}$ is <PARAGRAPH_SEPARATOR> $$ H_{D}(k)\\colon f(x)={\\left\\{\\begin{array}{l l}{k(8x)^{k-1}}&{(0\\leqslant x<{\\frac{1}{8}}),}\\ {k\\{8(\\mathtt{1}}-x)\\}^{k-1}}&{({\\frac{1}{8}}\\leqslant x<{\\frac{1}{4}}),}\\end{array}\\right.} $$ <PARAGRAPH_SEPARATOR> repeated with period $\\ddagger$ to fill the interval $\\pmb{\\mathrm{\\downarrow}}\\leqslant\\pmb{x}<1$ <PARAGRAPH_SEPARATOR> In all the above, $k\\geqslant0$ . Clearly this system may be extended. As $k{\\rightarrow}1$ they all tend to the uniform distribution: for a fixed $\\pmb{k}$ ,the succession $H_{B},H_{C},H_{D}$ represents steadily less extreme departures from uniformity and gives alternatives of 1, 2, and 4 clusters. <PARAGRAPH_SEPARATOR> We note that, of the set given above, $H_{B}$ is ‘nearest′ to the two-semicircle density $H_{A}$ ,if for the latter we imagine the semicircle of higher density centred at ${\\pmb x}=\\frac{1}{2}$ .We might imagine $A_{n}$ to perform well against alternative $H_{B}$ <PARAGRAPH_SEPARATOR> An advantage of this particular system as an alternative to uniformity on the circle, when making studies of power, is the existence of a standard of comparison. It is known that $Q=-2\\Sigma\\ln x_{i}$ is the most powerful statistic for testing uniformity $(k=1)$ againstthe alternative,for $k>0$ ? <PARAGRAPH_SEPARATOR> $$ H_{z}(k)\\colon f(x)=k x^{k-1}\\quad(0\\leqslant x\\leqslant1). $$ <PARAGRAPH_SEPARATOR> For $H_{B}$ we obtain & most powerful statistic by adapting $\\boldsymbol{Q}$ as follows: frst transform the $x_{t}$ to $\\pmb{u}_{\\pmb{\\ell}}$ given by $u_{i}=2x_{i}$ , $0\\leqslant x_{i}\\leqslant\\frac{1}{2}$ ,and $u_{\\downarrow}=2(1-x_{i})$ , $\\frac{1}{2}<x_{i}\\leqslant1$ ; then the test statistic becomes $Q=-2\\Sigma\\ln{u_{i}}$ . In both cases, on $H_{0},Q$ is exactly distributed as $\\chi_{\\mathfrak{L}_{\\mathfrak{R}}}^{\\mathfrak{L}}$ ; the lower tail is used against the alternative $k>1$ . Similarly $\\pmb{Q}$ may be adapted for the tests against $H_{C}$ or $H_{D}.$ . to give a most powerful statistic. <PARAGRAPH_SEPARATOR> # 5.3. Results of the power comparisons <PARAGRAPH_SEPARATOR> For the alternatives given above, $A_{n},U_{n}^{2}$ $V_{\\pmb{\\mathscr{n}}}$ and sometimes $Q_{\\mathfrak{n}}$ have been compared by Monte Carlo samples; some results are in Table 4. The samples were drawn from $H_{B}$ , $H_{c}$ or $H_{p}$ , and the test applied for uniformity; the table gives the proportion of each test statistic significant at the $10\\%$ level. For $H_{B},Q_{n}$ is included as a standard of comparison. The main conclusions are:"
  },
  {
    "qid": "statistic-posneg-1285-1-3",
    "gold_answer": "FALSE. The likelihood ratio order (lr) cannot be characterized by the same condition as the hazard rate order (hr). The paper shows that for k = 1, the condition for lr order is λ ≥ (∑_{i=1}^m p_i λ_i^2) / (∑_{i=1}^m p_i λ_i), which is different from the condition for hr order.",
    "question": "Can the likelihood ratio order (lr) be characterized by the same condition λ ≥ (∑_{i=1}^m p_i λ_i^k)^{1/k} as the hazard rate order (hr)?",
    "question_context": "Stochastic Ordering and Hazard Rate Bounds for Mixtures of Exponential Order Statistics\n\nThe paper discusses stochastic ordering and hazard rate bounds for mixtures of exponential order statistics. It presents various formulas and theorems to compare the survival and hazard rate functions of order statistics from heterogeneous independent exponential random variables with those from i.i.d. exponential random variables. Key formulas include the survival function of the kth order statistic, the hazard rate function, and conditions for stochastic and hazard rate ordering."
  },
  {
    "qid": "statistic-posneg-1537-0-1",
    "gold_answer": "TRUE. According to Definition 1, the structure is determined by the number of distinct means and their multiplicities. In this mixture: (1) The first two components share mean 1 (multiplicity 2), and (2) the third component has mean 5 (multiplicity 1). Therefore, the structure vector is (2,1), correctly representing two distinct means with multiplicities 2 and 1 respectively.",
    "question": "Is the mixture model 0.3𝒩(1,2)+0.2𝒩(1,4)+0.5𝒩(5,2.5) correctly described as having a (2,1) structure according to Definition 1 in the paper?",
    "question_context": "Bayesian Mixture Models with Shared Means\n\nThe paper discusses finite mixtures of normal distributions, where observed data is modeled as a random sample from a mixture with unknown number of components and parameters. The paper highlights issues with the standard Reversible Jump MCMC approach when some mixture components share the same mean, as such models are assigned zero posterior probability due to the continuous prior on means. Two example mixtures are given: 0.3𝒩(1,2)+0.2𝒩(2,4)+0.5𝒩(5,2.5) (three distinct means) and 0.3𝒩(1,2)+0.2𝒩(1,4)+0.5𝒩(5,2.5) (two distinct means). The paper proposes a way to distinguish models based on the number of distinct means."
  },
  {
    "qid": "statistic-posneg-1712-0-0",
    "gold_answer": "TRUE. The formula is derived under the assumption of mixed variates, where p represents the proportion of non-stochastic variance in the independent variables. The term (1-ρ²)² accounts for the sampling variability due to the stochastic components, while (2-ρ²p²) adjusts for the fixed component. This result generalizes the classical case (p=0) and the fixed variates case (p=1).",
    "question": "Is the asymptotic sampling variance of a canonical correlation coefficient under the assumption of mixed variates given by var r = (1/2T){(1-ρ²)²(2-ρ²p²)}?",
    "question_context": "Sampling Variance of Canonical Correlation Coefficients under Mixed Variates\n\nThis paper is concerned with the asymptotic variances of canonical correlation coefficients under alternative assumptions about the stochastic nature of the variables. The results obtained also apply to the cases of zero-order and multiple correlation coeffcients, since they are special cases of canonical correlations. The model and the assumptions are presented in §2, the results in §3, and some interpretations of the results in §4. In the Appendix detailed proofs of the results are given."
  },
  {
    "qid": "statistic-posneg-825-0-2",
    "gold_answer": "TRUE. The text explicitly states that although the prior on $(\\sigma^{2},\\beta_{0})$ is improper, the posterior distribution of $\\theta$ is proper. This is a well-known property in Bayesian linear regression when using non-informative priors under certain conditions, ensuring valid posterior inference.",
    "question": "Is it correct that the posterior distribution of $\\theta$ remains proper despite the use of an improper prior on $(\\sigma^{2},\\beta_{0})$?",
    "question_context": "Bayesian Variable Selection in Linear Regression with Zellner's g-Prior\n\nThe most commonly used setup for variable selection in Bayesian linear regression is described as follows. We have a response vector $Y=(Y_{1},\\dots,Y_{m})^{\\top}$ and a set of potential predictors $X_{1},\\dots,X_{q}$ , each a vector of length $m$ . Every subset of predictors is identified with a binary vector $\\gamma=(\\gamma_{1},\\dots,\\gamma_{q})^{\\top}\\in\\{0,1\\}^{q}$ , where $\\gamma_{j}=1$ if $X_{j}$ is included in the model and $\\gamma_{j}=0$ otherwise. For every $\\gamma$ , we have a model given by $Y=1_{m}\\beta_{0}+X_{\\gamma}\\beta_{\\gamma}+\\epsilon,$ where $1_{m}$ is the vector of m 1’s, $X_{\\gamma}$ is the design matrix whose columns consist of the predictor vectors corresponding to $\\gamma,\\beta_{\\gamma}$ is the vector of coefficients for that subset, and $\\epsilon\\sim\\mathcal{N}_{m}(0,\\sigma^{2}I)$ . For this setup, the unknown parameter is $\\theta=(\\gamma,\\sigma,\\beta_{0},\\beta_{\\gamma})$ , which includes the indicator of the subset of variables that go into the regression model. The prior on $\\theta$ is a hierarchy in which we first select the variables that go into the regression model, then a “noninformative prior” is given to $(\\sigma^{2},\\beta_{0})$ , and given $\\gamma$ and $\\sigma$ , we choose $\\beta_{\\gamma}$ from some proper distribution. The specific instance of this model that we will consider is indexed by two hyperparameters, $w\\in(0,1)$ and $g>0$ , and is given in detail as follows: $\\mathrm{given}\\gamma,\\sigma,\\beta_{0},\\beta_{\\gamma},\\quad Y\\sim\\mathcal{N}_{m}(1_{m}\\beta_{0}+X_{\\gamma}\\beta_{\\gamma},\\sigma^{2}I),$ $\\begin{array}{r}{\\operatorname{given}\\gamma,\\sigma,\\quad\\beta_{\\gamma}\\sim\\mathcal{N}_{q_{\\gamma}}\\big(0,g\\sigma^{2}(X_{\\gamma}^{\\top}X_{\\gamma})^{-1}\\big),}\\end{array}$ $(\\sigma^{2},\\beta_{0})\\sim p(\\beta_{0},\\sigma^{2})\\propto1/\\sigma^{2},$ $\\gamma\\sim p(\\gamma)=w^{q_{\\gamma}}(1-w)^{q-q_{\\gamma}}.$ The prior on $\\gamma$ given by (4.1d) is the so-called independence Bernoulli prior, in which every variable goes into the model with probability $w$ , independently of all the other variables. In (4.1b), $\\begin{array}{r}{q_{\\gamma}=\\sum_{j=1}^{q}\\gamma_{j}}\\end{array}$ is the number of predictors that go in the regression, and the prior on $\\beta_{\\gamma}$ is Zell ner’s $g$ -prior (Zellner 1986). Because $(\\sigma^{2},\\beta_{0})$ is given an improper prior (line (4.1c)), the prior on $\\theta$ is improper; however, it turns out that the posterior distribution of $\\theta$ is proper. Models of the type (4.1) were introduced by Mitchell and Beauchamp (1988) and have been studied in dozens of papers; see Liang et al. (2008) for a review and recent developments."
  },
  {
    "qid": "statistic-posneg-1462-8-1",
    "gold_answer": "TRUE. The proof shows that independence of $(X_{\\pi_{1}},X_{\\pi_{2}})$ is equivalent to the condition $(X_{1},X_{2})\\in\\mathrm{IC}_{0}$, as it ensures the joint probabilities factorize into the product of marginal probabilities for all measurable sets $A$ and $B$.",
    "question": "Does the condition $(X_{1},X_{2})\\in\\mathrm{IC}_{0}$ imply that $(X_{\\pi_{1}},X_{\\pi_{2}})$ are independent random variables, where $\\pi$ is a random permutation?",
    "question_context": "Invariant Correlation and Quasi-Independence in Random Vectors\n\nLemma 2. For a random vector $(X,Y)\\sim H$ with the marginals $F_{\\cdot}$ , $(X,Y)$ is quasi- $_r$ -Fréchet if and only if (10) holds. <PARAGRAPH_SEPARATOR> Proof of Theorem 1. We first show the ‘‘only if’’ part. As $(X,Y)\\in\\mathrm{IC}_{0}$ , we have $\\operatorname{Cov}(g(X),g(Y))=0$ for any admissible $g$ . For any $A\\in{\\mathcal{B}}(\\mathbb{R})$ , taking $g(x)=\\mathbb{1}_{A}(x)$ , we have $\\operatorname{Cov}(g(X),g(Y))=\\operatorname{Cov}(\\mathbb{1}_{A}(X),\\mathbb{1}_{A}(Y))=0$ . For any $A,B\\in{\\mathcal{B}}(\\mathbb{R})$ , let $\\begin{array}{r}{g(\\boldsymbol{x})=\\mathbb{1}_{A}(\\boldsymbol{x})+\\mathbb{1}_{B}(\\boldsymbol{x}),}\\end{array}$ which leads to <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathrm{Cov}(g(X),g(Y))=\\mathrm{Cov}\\left(\\mathbb{1}_{A}(X)+\\mathbb{1}_{B}(X),\\mathbb{1}_{A}(Y)+\\mathbb{1}_{B}(Y)\\right)=\\mathrm{Cov}\\left(\\mathbb{1}_{A}(X),\\mathbb{1}_{B}(Y)\\right)+\\mathrm{Cov}\\left(\\mathbb{1}_{B}(X),\\mathbb{1}_{A}(Y)\\right)}\\ &{\\quad\\quad\\quad=\\mathbb{P}(X\\in A,Y\\in B)+\\mathbb{P}(X\\in B,Y\\in A)-\\mathbb{P}(X\\in A)\\mathbb{P}(Y\\in B)-\\mathbb{P}(X\\in B)\\mathbb{P}(Y\\in A).}\\end{array} $$ <PARAGRAPH_SEPARATOR> As $\\operatorname{Cov}(g(X),g(Y))=0$ , we have that $(X,Y)$ satisfies (8) for all $A,B\\in{\\mathcal{B}}(\\mathbb{R})$ . Hence, $(X,Y)$ is quasi-independent by Lemma 1. Next, we show that quasi-independence implies $\\operatorname{Cov}(g(X),g(Y))=0$ for any admissible $g$ . As quasi-independence implies (8), by taking $A=B$ , we have $\\mathbb{P}(X\\in A,Y\\in A)=\\mathbb{P}(X\\in A)\\mathbb{P}(Y\\in A)$ for any $A\\in{\\mathcal{B}}(\\mathbb{R})$ . Hence, $\\operatorname{Cov}(g(X),g(Y))=0$ for any indicator functions $g=\\mathbb{1}_{A}$ . <PARAGRAPH_SEPARATOR> First, let $\\begin{array}{r}{g(x)=\\sum_{i=1}^{n}c_{i}\\mathbb{1}_{A_{i}}(x)}\\end{array}$ be an admissible simple function where $A_{1},\\ldots,A_{n}\\subseteq\\mathbb{R}$ are disjoint measurable sets, and $c_{1},\\ldots c_{n}\\in\\mathbb{R}$ are real numbers. For the simple function $g$ , we have <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\displaystyle\\mathsf{C o v}(g(X),g(Y))=\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j}\\mathrm{Cov}\\left(\\mathbb{1}_{A_{i}}(X),\\mathbb{1}_{A_{j}}(Y)\\right)=\\sum_{1\\leqslant i<j\\leqslant n}\\left(c_{i}c_{j}\\mathrm{Cov}\\left(\\mathbb{1}_{A_{i}}(X),\\mathbb{1}_{A_{j}}(Y)\\right)+c_{j}c_{i}\\mathrm{Cov}\\left(\\mathbb{1}_{A_{j}}(X),\\mathbb{1}_{A_{j}}(Y)\\right)\\right.}\\ &{\\displaystyle\\left.=\\sum_{1\\leqslant i<j\\leqslant n}c_{i}c_{j}\\left(\\mathbb{P}\\left(X\\in A_{i},Y\\in A_{j}\\right)-\\mathbb{P}\\left(X\\in A_{i}\\right)\\mathbb{P}\\left(Y\\in A_{j}\\right)+\\mathbb{P}\\left(X\\in A_{j},Y\\in A_{i}\\right)-\\mathbb{P}\\left(X\\in A_{j}\\right)\\mathbb{P}\\left(Y\\in A_{i}\\right)\\right)=}\\end{array} $$ <PARAGRAPH_SEPARATOR> Therefore, we have $\\operatorname{Cov}(g(X),g(Y))=0$ for all admissible simple functions $g$ . <PARAGRAPH_SEPARATOR> Second, let $g$ be an admissible non-negative function. Admissibility of $g$ implies that $\\mathbb{E}[g(X)]<\\infty$ , $\\mathbb{E}[g(Y)]<\\infty$ and $\\mathbb{E}[g(X)g(Y)]<$ $\\infty$ . For a non-negative admissible function $g$ , there exists a sequence of non-negative simple functions $\\{g_{n}\\}_{n\\geqslant1}$ such that $g_{n}\\uparrow g$ pointwise. Thus, we can get <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathrm{Cov}\\left(g(X),g(Y)\\right)=\\mathrm{Cov}\\left(\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X),\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(Y)\\right)=\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X)\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(Y)\\right]-\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X)\\right]\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(Y)\\right]}\\ &{=\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X)g_{n}(Y)\\right]-\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X)\\right]\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(Y)\\right]=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}\\left[g_{n}(X)g_{n}(Y)\\right]-\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}\\left[g_{n}(X)\\right]\\mathbb{E}\\left[g_{n}(Y)\\right]}\\ &{=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\left\\{\\mathbb{E}\\left[g_{n}(X)g_{n}(Y)\\right]-\\mathbb{E}\\left[g_{n}(X)\\right]\\mathbb{E}\\left[g_{n}(Y)\\right]\\right\\}=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\mathrm{Cov}(g_{n}(X),g_{n}(Y))=0.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Therefore, we have $\\operatorname{Cov}(g(X),g(Y))=0$ for all admissible non-negative function $g$ . <PARAGRAPH_SEPARATOR> Finally, let $g$ be an admissible function. Define $g_{+}=\\operatorname*{max}(g,0)$ and $g_{-}=-\\operatorname*{min}(g,0)$ . It is clear that $g_{+}$ and $g_{-}$ are non-negative admissible functions and $g=g_{+}-g_{-}$ . Let $G=g_{+}+g_{-}$ . We have that $G$ is also a non-negative admissible function. Hence, we get $\\operatorname{Cov}(g_{+}(X),g_{+}(Y))=0_{\\cdot}$ , $\\operatorname{Cov}(g_{-}(X),g_{-}(Y))=0$ and $\\operatorname{Cov}(G(X),G(Y))=0,$ , which imply that $\\operatorname{Cov}(g_{+}(X),g_{-}(Y))+\\operatorname{Cov}(g_{-}(X),g_{+}(Y))=0~$ . As a result, we have <PARAGRAPH_SEPARATOR> $$ \\operatorname{Cov}\\left(g(X),g(Y)\\right)=\\operatorname{Cov}(g_{+}(X)-g_{-}(X),g_{+}(Y)-g_{-}(Y))=0. $$ <PARAGRAPH_SEPARATOR> Therefore, we can conclude $(X,Y)\\in\\mathrm{IC}_{0}$ . □ <PARAGRAPH_SEPARATOR> Proof of Proposition 2. For all $A,B\\in{\\mathcal{B}}(\\mathbb{R})$ , we have <PARAGRAPH_SEPARATOR> $$ \\mathbb{P}(X_{\\pi_{1}}\\in A,X_{\\pi_{2}}\\in B)=\\frac{1}{2}\\mathbb{P}(X_{1}\\in A,X_{2}\\in B)+\\frac{1}{2}\\mathbb{P}(X_{1}\\in B,X_{2}\\in A). $$ <PARAGRAPH_SEPARATOR> Since $X_{1}$ and $X_{2}$ have the same distribution, independence of $(X_{\\pi_{1}},X_{\\pi_{2}})$ is equivalent to <PARAGRAPH_SEPARATOR> $$ \\mathbb{P}(X_{\\pi_{1}}\\in A,X_{\\pi_{2}}\\in B)=\\mathbb{P}(X_{\\pi_{1}}\\in A)\\mathbb{P}(X_{\\pi_{2}}\\in B)=\\mathbb{P}(X_{1}\\in A)\\mathbb{P}(X_{2}\\in B)=\\mathbb{P}(X_{1}\\in B)\\mathbb{P}(X_{2}\\in A). $$ <PARAGRAPH_SEPARATOR> Hence, independence of $(X_{\\pi_{1}},X_{\\pi_{2}})$ is equivalent to <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\operatorname{\\mathbbP}(X_{1}\\in A,X_{2}\\in B)+\\operatorname{\\mathbbP}(X_{1}\\in B,X_{2}\\in A)=\\operatorname{\\mathbbP}(X_{1}\\in A)\\operatorname{\\mathbbP}(X_{2}\\in B)+\\operatorname{\\mathbbP}(X_{1}\\in B)\\operatorname{\\mathbbP}(X_{2}\\in A),}\\end{array} $$ <PARAGRAPH_SEPARATOR> which is also equivalent to $(X_{1},X_{2})\\in\\mathrm{IC}_{0}$ by Theorem 1. □"
  },
  {
    "qid": "statistic-posneg-432-0-0",
    "gold_answer": "FALSE. The formula provided for the standard jackknife standard error $\\widetilde{S}$ is incorrect. The correct formula should be $\\widetilde{S}=[\\Sigma(\\widetilde{I}_{J}{-}\\widetilde{I}_{.})^{2}/\\big\\{n(n{-}1)\\big\\}]^{\\frac{1}{2}}$, not raised to the power of $\\frac{1}{4}$. The standard error is the square root of the variance, hence the exponent should be $\\frac{1}{2}$. The formula given in the context is a typographical error.",
    "question": "The standard jackknife standard error $\\widetilde{S}$ is calculated using the formula $\\widetilde{S}=[\\Sigma(\\widetilde{I}_{J}{-}\\widetilde{I}_{.})^{2}/\\big\\{n(n{-}1)\\big\\}]^{\\frac{1}{4}}$. Is this formula correct for estimating the standard error of the estimate $\\pmb{T}$?",
    "question_context": "Jackknife Confidence Limit Methods and Edgeworth Expansions\n\nJackknife methods are nonparametric methods for estimating the bias and standard error of an estimate. Approximate confidence limits for the estimand can be found by using a large-sample normal approximation. Somewhat curiously, little is known about possible improvements to the normal approximation in this context, although improvements based on Edgeworth expansions are familiar in other problems. In this paper we show that Edgeworth expansion methods can be applied in the jackknife context. Numerical results for a particular application raise the possibility that the resulting improvements of jackknife methods can be matched by a suitable use of bootstrap methods (Efron, 1982). <PARAGRAPH_SEPARATOR> We suppose that $X_{1},...,X_{n}$ are independent random variables with common distribution function, of which $\\theta=t(F)$ is the single characteristic of interest; for example, the mean would be $m(F)=\\int x d F(x)$. Our estimate of $\\theta$ is the nonparametric maximum likelihood estimate, where $\\hat{F}$ is the sample distribution function defined by ${\\hat{F}}(x)=n^{-1}\\sum_{j=1}^{n}\\Delta(x-X_{j})$, with $\\boldsymbol{\\Delta}(z)=0$ or 1 according as $z<0$ or $z\\geqslant0$. Particular confidence limit methods for $\\pmb{\\theta}$ will be based on a studentized form $Z=(T-\\theta)/S$, where the standard error $\\boldsymbol{s}$ is computed by a jackknife technique. <PARAGRAPH_SEPARATOR> Two particular techniques will be considered, namely the standard jackknife and the infinitesimal jackknife (Miller, 1974; Efron, 1982). In the former, we first define $\\tilde{I}_{j}=(n-1)(T-T_{/j})\\quad(j=1,...,n)$, where $T_{/j}$ refers to the estimate of $\\pmb\\theta$ calculated from the sample with $X_{j}$ omitted. The jackknife standard error $\\tilde{s}$ for $\\pmb{T}$ is then given by $\\widetilde{S}=[\\Sigma(\\widetilde{I}_{J}{-}\\widetilde{I}_{.})^{2}/\\big\\{n(n{-}1)\\big\\}]^{\\frac{1}{4}}$, with $\\tilde{I}_{.}=n^{-1}\\Sigma I_{j}$. The infinitesimal jackknife standard error is obtained from the influence function $I_{t}(x,F)$ of $t(F)$; see Efron (1982). We define $v(F)=\\operatorname{var}\\left\\{I_{t}(X,F)\\right\\}$, which is estimated by $v(\\hat{F})=\\Sigma\\left\\{I_{t}(X_{j},\\hat{F})\\right\\}^{2}/n$, then define the infinitesimal jackknife standard error of $\\pmb{T}$ to be $\\hat{S}=\\{v(\\hat{F})/n\\}^{\\frac{1}{2}}$. <PARAGRAPH_SEPARATOR> Whichever method is used to calculate $\\boldsymbol{s}$, the approximate standard normal distribution of $Z=(T-\\theta)/S$ provides approximate $(1-p)$ confidence limits $\\mathbf{\\Delta}T\\pm k_{\\pm p}\\mathbf{\\Delta}S$ for $\\pmb\\theta$ with $k_{q}$ denoting the $\\pmb{q}.$ quantile of the standard normal distribution. In some applications the normal approximation is not adequate, and it can in principle be improved using corrections for bias, skewness, and so forth, using Edgeworth expansions. The basic theory underlying such corrections is described for the infinitesimal jackknife. Corresponding theory for the standard jackknife follows readily. <PARAGRAPH_SEPARATOR> Bootstrap methods (Efron, 1982) provide an alternative way of approximating the distribution of $\\boldsymbol{z}$ and are outlined. The jackknife and bootstrap methods are compared for ratio estimation. The bootstrap gives results as good as the more complicated jackknife methods."
  },
  {
    "qid": "statistic-posneg-1437-2-1",
    "gold_answer": "FALSE. While the linear score function performs excellently across a wide range of distributions and is easier to compute, the context indicates that its performance is not uniformly superior in all scenarios. Specifically, for heavy-tailed distributions, $W_{l n p-1}$ (using the linear score) has slightly lower efficiencies than $S_{n}$, and for light-tailed distributions, its efficiencies are slightly less than those of PR and $W_{w n p-1}$ (which uses the van der Waerden score). The linear score's advantages are more about balanced performance and computational simplicity rather than universal dominance.",
    "question": "Does the linear score function $\\phi(u)=\\lambda u+(1-\\lambda)1$, where $\\lambda=\\{\\ln(p+e-2)\\}^{-1}$, always perform better than the van der Waerden score function in all distributional scenarios?",
    "question_context": "Score Function Efficiency in Multivariate Testing\n\nIf $\\phi(u)\\equiv u$ , then, for elliptically symmetric distributions, $W_{n p-1}$ will have the same AREs as the test statistic obtained by applying Peters and Randles [30] signed-rank interdirection test to the complete block design setting (see, [20]). These test statistics generally perform well over a wide range of distributions, yet their performance deteriorates as the dimension increases, especially in the light-tailed cases and the normal case. The test will be denoted by PR. <PARAGRAPH_SEPARATOR> “Optimal” score functions have been discussed in the literature (see [12,14]). For example, if the underlying distribution is the multivariate normal then the van der Waerden score function, $\\phi(u)=\\sqrt{(\\chi^{2})^{-1}(u)}$ , is optimal where $\\left(\\chi^{2}\\right)^{-1}\\left(u\\right)$ is the inverse of the distribution function of a $\\chi_{p-1}^{2}$ . With such a score function and for elliptically symmetric distributions, $W_{n p-1}$ AREs with respect to Hotelling–Hsu $T^{2}$ will always be greater than 1 with equality when the distribution is the multivariate normal. Note that with this score function the test will be denoted by $W_{w n p-1}$ . <PARAGRAPH_SEPARATOR> While tests based on optimal scores have nice properties, they are seldom used in practice. The score function that we now recommend combines excellence in performance with simplicity in implementation. It is defined as <PARAGRAPH_SEPARATOR> $$ \\phi(u)=\\lambda u+(1-\\lambda)1, $$ <PARAGRAPH_SEPARATOR> where $\\lambda=\\{\\ln(p+e-2)\\}^{-1}$ depends on $p$ , the dimension of the data. This score function was introduced in [28]. With this linear score function, $W_{n p-1}$ has an excellent performance as good as and in some cases better than $W_{w n p-1}$ but yet it is still easy to compute as $S_{n}$ and PR. $W_{l n p-1}$ will be used to denote $W_{n p-1}$ with the score function defined in 13. <PARAGRAPH_SEPARATOR> We now show the effect of the score function on the efficiencies of the corresponding test by computing and comparing the AREs, with respect to Hotelling–Hsu’s $T^{2}$ , of the four test statistics, $W_{l n p-1}$ , $W_{w n p-1}$ , PR, and $S_{n}$ . Tables 1 and 2 display the AREs of these four statistics, for the case when the $\\mathbf{Y}$ ’s come from the power family of distributions and the family of $t\\cdot$ -distributions, respectively. These two families of distributions belong to the class of elliptically symmetric distributions. The power family is such that <PARAGRAPH_SEPARATOR> $$ h(t)=\\exp\\bigg\\{-\\bigg(\\frac{t}{c_{0}}\\bigg)^{\\nu}\\bigg\\},~\\nu>0, $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ c_{0}={\\frac{p r(p/2\\nu)}{r((p+2)/2\\nu)}},~\\mathrm{and}~K_{p}={\\frac{\\nu{r(p/2)}}{{r(p/2\\nu)}{[}\\pi c_{0}{]^{p/2}}}}. $$ <PARAGRAPH_SEPARATOR> It includes the multivariate normal density $(\\nu=1)$ ), as well as heavier-tailed distributions $0<\\nu<1$ ) and lighter-tailed distributions $y>1$ ). For the multivariate $t$ -distribution, $h(\\cdot)$ and $K_{p}$ in (9) are defined as <PARAGRAPH_SEPARATOR> $$ h(t)=\\left[1+\\frac{1}{\\nu}(t)\\right]^{-(p+\\nu)/2} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ K_{p}=\\frac{{\\cal T}((p+\\nu)/2)}{{\\cal T}(\\nu/2)(\\pi\\nu)^{p/2}}. $$ <PARAGRAPH_SEPARATOR> It is clear from Tables 1 and 2 that $W_{l n p-1}$ has very competitive ARE values in a wide variety of distributions, ranging from very heavy-tailed distribution to very light-tailed ones. These efficiencies do not deteriorate as the dimension increases (as is the case for PR) or as the underlying distribution shifts from being heavy-tailed to being light-tailed (as is the case with $S_{n}$ ). For normal alternatives, $W_{l n p-1}$ not only performs virtually as well as Hotelling–Hsu’s $T^{2}$ and $W_{w n p-1}$ (recall this test is optimal for the normal distribution) but also has higher efficiencies than $S_{n}$ and ${\\mathrm{PR}}_{n}$ . For the heavy-tailed distributions of the power family, $W_{l n p-1}$ has higher efficiencies than $W_{w n p-1}$ and PR but slightly lower efficiencies than those of $S_{n}$ . For the light-tailed distributions of this family, the efficiencies of $W_{l n p-1}$ exceed those of $S_{n}$ but are slightly less than those of PR and $W_{w n p-1}$ . However, the slight advantages of some tests over $W_{l n p-1}$ whether in the light-tailed cases or the heavy-tailed cases disappear slowly as the dimension increases."
  },
  {
    "qid": "statistic-posneg-533-0-0",
    "gold_answer": "FALSE. The inequality is not directly derivable from the given assumptions about V-paths. The paper's context suggests this bound is an imposed condition rather than a derived result, as it combines terms from different V-paths without clear justification from the model's probabilistic structure. The bound $(2r)^{\\alpha\\sum n_j}$ appears to be an ad hoc choice for controlling the growth rate rather than a consequence of the eigenvector moment properties.",
    "question": "Is the inequality $E(|v_{a k_{1}},...,v_{b k_{r}}|)E(|v_{c\\underline{{{k}}}_{1}},...,v_{d\\underline{{{k}}}_{r}}|)\\leqslant(2r)^{\\alpha\\sum_{n_{j}\\geqslant3}n_{j}}$ correctly derived from the assumptions about V-paths and their distinct elements?",
    "question_context": "Bounds on Eigenvector Moments in Large Random Matrices\n\nThe paper analyzes the moments of eigenvectors in large random matrices, focusing on deriving bounds for sums involving V-paths. The key formulas include bounds on expected values of products of eigenvector components and combinatorial counts of canonical V-paths. The analysis involves intricate inequalities and combinatorial arguments to control the growth of these terms as the matrix size n increases."
  },
  {
    "qid": "statistic-posneg-751-3-0",
    "gold_answer": "FALSE. The context explicitly states that while the random walk hypothesis is roughly correct for a majority of stock data, there are examples that deviate significantly from it. These deviations include non-Gaussian distributions, stepwise time dependence in the variance, and correlations in parts of the series. This suggests that the random walk hypothesis is not universally accurate for all stock price data.",
    "question": "Is the random walk hypothesis for stock prices, which assumes that logarithmic price changes are independent identically distributed Gaussian random variables, always accurate according to the context?",
    "question_context": "Random Walk Hypothesis and Stock Price Modeling\n\nFrom an economic point of view (see Fama (1965)) it is most sensible to analyse stock prices in terms of logarithmic price changes; i.e. if $\\left\\{S_{t}\\right\\}$ is the stock price series then $\\left\\{\\bar{U}_{t}\\right\\}=\\left\\{\\ln S_{t}-\\ln S_{t-1}\\right\\}$ is the series that should be investigated. The assumption that $\\left\\{U_{t}\\right\\}$ consists of independent identically distributed Gaussian random variables is the so-called random walk hypothesis for stock prices, and many empirical investigations (see, for example, Ali and Giacotto (1982) and references therein) have been carried out to test this hypothesis. Similar studies exist for commodity prices (Bird, 1985). <PARAGRAPH_SEPARATOR> Although roughly correct for a majority of stock data, it has been noted by several researchers (e.g. Fielitz (1971), Hsu et al. (1974), Wichern et al. (1976) and Ali and Giacotto (1982)) that examples can be found which deviate significantly from the random walk hypothesis. There are examples of a non-Gaussian distribution, a stepwise time dependence in the variance and of correlations in parts of $\\left\\{U_{t}\\right\\}$ .Since correlations mean that the eficient market hypothesis does not hold (Fama, 1970), it is important to detect and pinpoint sections of $\\left\\{U_{t}\\right\\}$ where correlations exist. Also this should be examined in the context of time dependence of the variance since (Bird, 1985) certain relationships between the correlation and variance can be given an economic interpretation. Correlations and variance can be measured in terms of AR parameters and residual variance, and our methods are especially designed for finding data segments with constant parameters and for pinpointing change points. In addition we would like to give an economic interpretation of the transition points between various states. <PARAGRAPH_SEPARATOR> It should also be noted that the random walk hypothesis is directly related to more recent work based on continuous time models for the stock prices. In a very influential paper on option pricing Black and Scholes (1973) assumed that stock prices are described by an Ito stochastic differential equation <PARAGRAPH_SEPARATOR> $$ \\frac{\\mathrm{d}S(t)}{S(t)}=\\mu~\\mathrm{d}t+\\sigma~\\mathrm{d}W(t) $$ <PARAGRAPH_SEPARATOR> where $\\left\\{W(t)\\right\\}$ is a Wiener process, $\\pmb{\\mu}$ the drift parameter and $\\sigma^{2}$ the variance. The process $\\langle S(t)\\rangle$ itself is termed geometric Brownian motion. Equation (5.1) may be solved analytically for $s(t)$ and for $t\\geqslant0$ we obtain <PARAGRAPH_SEPARATOR> $$ S(t)=S(0)\\exp[(\\mu-{\\textstyle{\\frac{1}{2}}}\\sigma^{2})t+\\sigma W(t)]. $$ <PARAGRAPH_SEPARATOR> Sampling $S(t)$ at equal time intervals $\\Delta$ it follows that <PARAGRAPH_SEPARATOR> $$ \\ln S(k\\Delta)-\\ln S[(k-1)\\Delta]=\\Delta(\\mu-{\\textstyle{\\frac{1}{2}}}\\sigma^{2})+e_{k} $$ <PARAGRAPH_SEPARATOR> where $\\left\\{\\boldsymbol{e}_{k}\\right\\}=\\sigma[W(k\\Delta)-W((k-1)\\Delta)]$ consists of independent identically distributed Gaussian random variables. This means that the random walk model emerges irrespective of the length of the sampling interval $\\Delta.$ . Thus, if we examine the properties of the time series $\\left\\{U_{t}\\right\\}=\\left\\{\\ln S_{t}-\\ln\\bar{S}_{t-1}\\right\\}$ , this implies that we are also examining the validity of the continuous time model (5.1), which has been extensively used recently in the theory of finance (Merton, 1982). The original paper by Black and Scholes already contained some evidence that equation (5.1) may not always be an accurate model, and more general models have been introduced (see, for example, Aase (1984) and references therein), where the stock price can change suddenly by introducing a <PARAGRAPH_SEPARATOR> Poisson process component. The models proposed in this paper are meant to describe sudden structural changes via the AR parameters, and this offers an interesting alternative to the generalized continuous time models. We now illustrate our methods on the IBM example. <PARAGRAPH_SEPARATOR> # 5.1. IBM series"
  },
  {
    "qid": "statistic-posneg-1764-3-1",
    "gold_answer": "TRUE. The proof involves the decomposition $\\left(\\frac{S}{T} - \\frac{a}{b}\\right) = \\frac{a}{b}\\left(\\frac{S}{a} - 1\\right) + \\frac{a}{b}\\left(1 - \\frac{T}{b}\\right) + \\left(\\frac{S}{T} - \\frac{a}{b}\\right)\\left(1 - \\frac{T}{b}\\right)$. Using the triangular inequality and the condition $0 < \\eta < \\frac{1}{2}$, it is shown that $\\frac{2\\eta}{1-\\eta} < 4\\eta$, validating the inclusion.",
    "question": "Does Lemma 6 correctly assert that $\\left\\{\\left|\\frac{S}{T}-\\frac{a}{b}\\right|>4\\eta\\left|\\frac{a}{b}\\right|\\right\\}\\subseteq\\left\\{\\left|\\frac{S}{a}-1\\right|>\\eta\\right\\}\\cup\\left\\{\\left|\\frac{T}{b}-1\\right|>\\eta\\right\\}$ for $0 < \\eta < \\frac{1}{2}$?",
    "question_context": "Frontier Estimation via Kernel Regression: Asymptotic Normality and Convergence\n\nThe context discusses the asymptotic properties of kernel regression estimators for frontier estimation. It includes lemmas and proofs related to the convergence and asymptotic normality of estimators such as $\\widehat{g}_{n}(x)$, $\\widehat{r}_{n}(x)$, and $\\widehat{\\varphi}_{n}(x)$. Key formulas involve bounds on deviations, convergence in probability, and distributional limits."
  },
  {
    "qid": "statistic-posneg-2126-0-0",
    "gold_answer": "FALSE. The text states that $G_{n}\\in C_{I}$, but this is only true if both $G_{n}^{1}$ and $G_{n}^{2}$ are in ${\\mathcal{C}}_{I}$ and the max operation preserves the $d$-convexity property. However, the preservation of $d$-convexity under the max operation is not automatically guaranteed for all cases and would depend on specific properties of $G_{n}^{1}$ and $G_{n}^{2}$ that are not explicitly verified in the given context.",
    "question": "Is the function $G_{n}(x_{1},\\ldots,x_{d})=\\operatorname*{max}\\{G_{n}^{1}(x_{1},\\ldots,x_{d}),G_{n}^{2}(x_{1},\\ldots,x_{d})\\}$ guaranteed to be in $C_{I}$ for all $n$?",
    "question_context": "Convex Minorant Construction and Properties in Multidimensional Space\n\nProof. Let $l_{1}<l_{2}$ be two real numbers. Without loss of generality, assume that $T_{I}(S)(x_{1},\\dots,x_{d})$ is differentiable for $x_{1}\\in(l_{1},l_{2})$ . We can now find sequences $\\{G_{n}^{1}\\},\\{G_{n}^{2}\\}\\subset{\\mathcal{C}}_{I}$ such that $G_{n}^{1}(l_{1},x_{2},\\dots,x_{d})\\uparrow T_{I}(S)(l_{1},x_{2},\\dots,x_{d})$ and $G_{n}^{2}(l_{2},x_{2},\\dots,x_{d})\\uparrow T_{I}(S)(l_{2},x_{2},\\dots,x_{d})$ as $n\\to\\infty$ . Define $G_{n}(x_{1},\\ldots,x_{d})=\\operatorname*{max}\\{G_{n}^{1}(x_{1},\\ldots,x_{d}),G_{n}^{2}(x_{1},\\ldots,x_{d})\\}$ . Note that $G_{n}(l_{i},x_{2},\\dots,x_{d})\\to$ $T_{I}(S)(l_{i},x_{2},\\ldots,x_{d})$ for $i=1,2$ as $n\\to\\infty$ and $G_{n}\\in C_{I}$ . Then the result follows from Lemma 3. ▪"
  },
  {
    "qid": "statistic-posneg-768-7-0",
    "gold_answer": "TRUE. The condition ensures that the normalized perturbation and bandwidth terms decay appropriately, allowing the proof of Theorem 3 to hold. Specifically, it guarantees that the terms involving $\\Delta_{n}$ and $h_{n}$ do not dominate the likelihood ratio, leading to $\\hat{\\alpha}_{n} \\to 0$ in probability under misspecification.",
    "question": "Is the condition $\\vert\\log(\\Delta_{n})\\vert^{1/\\beta}(\\sqrt{\\vert\\log(h_{n})\\vert/n h_{n}^{d}}+h_{n}^{2})\\rightarrow0$ sufficient to ensure the convergence in probability $\\alpha_{n}\\to0$ when the model is misspecified?",
    "question_context": "Parametric Maximum Likelihood Estimator Convergence\n\nThe paper discusses the conditions under which the maximum likelihood estimator (MLE) converges when the model is misspecified (i.e., $f_{0}\\notin\\mathcal{P}$). Key conditions include the Glivenko-Cantelli property of the model class, integrability of the envelope function, and specific convergence rates for bandwidth and perturbation terms. The paper also provides auxiliary lemmas supporting the main theorems."
  },
  {
    "qid": "statistic-posneg-423-2-1",
    "gold_answer": "FALSE. The paper demonstrates that the prior mass for M2 increases with the variance of the prior distribution for its parameters. For example, when comparing a standard normal (M1) to a normal with variable mean (M2), the mass for M2 is proportional to exp{Var(μ)}, which increases as Var(μ) increases. This reflects greater uncertainty about the simpler model being true, leading to more mass on the complex model.",
    "question": "Is the statement 'The prior mass for the more complex model M2 decreases as the variance of the prior distribution for its parameters increases' correct based on the paper's analysis?",
    "question_context": "Nested Model Selection Using Kullback-Leibler Divergence\n\nThe paper discusses model selection between nested models using Kullback-Leibler (KL) divergence. For two nested models M1 (simpler) and M2 (more complex), the prior mass assigned to M1 is proportional to 1 (associated with zero loss), while the mass for M2 is determined by the expected minimum KL divergence from M2 to M1. The paper provides examples with normal distributions and Student's t-distributions, illustrating how the prior mass on the more complex model increases with the variance of the prior distribution for its parameters."
  },
  {
    "qid": "statistic-posneg-1921-0-1",
    "gold_answer": "FALSE. The weights ω_{ik}(t) are time-varying and depend on whether the subject is in the subcohort (ξ_i = 1) or is a sampled case outside the subcohort (η_{ik} = 1). The weights are constructed to reflect the two-phase sampling design and are not constant over time. Specifically, the weights involve estimates of the sampling probabilities α_k(t) and q_k(t), which can vary over time.",
    "question": "The weighted estimating function for the case-cohort design uses time-varying weights ω_{ik}(t) that are constant over time for all subjects. True or False?",
    "question_context": "Marginal Additive Hazards Model for Case-Cohort Studies\n\nThe paper presents a marginal additive hazards model for case-cohort studies with multiple disease outcomes. The model assumes the hazard function for the kth disease outcome of the ith subject is given by λ_{ik}(t|Z_{ik}(t)) = λ_{0k}(t) + β_0^T Z_{ik}(t), where λ_{0k}(t) is the baseline hazard and β_0 is a vector of regression parameters. The estimation involves solving weighted estimating equations to account for the sampling design, where weights are assigned based on whether subjects are in the subcohort or are sampled cases outside the subcohort."
  },
  {
    "qid": "statistic-posneg-1543-0-1",
    "gold_answer": "TRUE. The asymptotic results for $E_{0}(T_{m,n}/N)$ and $\\operatorname{var}_{0}(T_{m,n}/\\sqrt{N})$ are derived under the condition that $m/N \to \\lambda \\in (0,1)$ as $N \to \\infty$. These results show that $T_{m,n}^{*} = \\sqrt{N}\\{T_{m,n}/N - 2\\lambda(1-\\lambda)\\}$ converges in distribution to $N(0, 4\\lambda^{2}(1-\\lambda)^{2})$, which justifies the use of a normal approximation for large samples. This asymptotic normality facilitates hypothesis testing for large $m$ and $n$.",
    "question": "The asymptotic expectation and variance of $T_{m,n}/N$ under $H_0$ are given by $E_{0}(T_{m,n}/N) \to 2\\lambda(1-\\lambda)$ and $\\operatorname{var}_{0}(T_{m,n}/\\sqrt{N}) \to 4\\lambda^{2}(1-\\lambda)^{2}$ as $N \to \\infty$. Is this claim correct, and does it justify the use of a normal approximation for large samples?",
    "question_context": "Distribution-Free Two-Sample Run Test for High-Dimensional Data\n\nThe paper discusses a distribution-free two-sample run test applicable to high-dimensional data, where the test statistic $T_{m,n}$ generalizes the univariate run test to higher dimensions while retaining the distribution-free property. Under the null hypothesis $H_0$, the exact null distribution of $T_{m,n}$ matches that of the univariate run statistic, given by specific combinatorial probabilities. For large samples, the asymptotic distribution of $T_{m,n}$ is derived, showing convergence to a normal distribution under certain conditions. The computation of $T_{m,n}$ involves solving a shortest Hamiltonian path problem, which is addressed using heuristic algorithms like Kruskal's method."
  },
  {
    "qid": "statistic-posneg-205-0-3",
    "gold_answer": "FALSE. The matrix $B^{-1}$ itself does not provide the variances of the inverse regression coefficients. The variances and covariances of the $b^{ji}$ are derived from the product of matrices involving $B^{-1}$, $T^{-1}$ (inverse of the sum of products matrix of $x$ variables), and $V$ (residual sum of products matrix of $y$ variables), as shown in the paper's exact analysis.",
    "question": "Does the matrix $B^{-1}$ directly provide the variances of the inverse regression coefficients $b^{ji}$?",
    "question_context": "Simultaneous Regression Equations in Chemical Analysis\n\nFisher, Hansen & Norton (1955) discuss the quantitative determination of glucose and galactose simultaneously in solutions of unknown chemical composition, by means of optical density measurements. Without going into the technical details, which are given in the paper referred to, we can simply state that solutions of glucose and galactose are treated to develop a colour; the optical density of the solution to light of two different wavelengths is then determined, and the two data thus obtained are used to estimate the amount of each sugar in solution. It is assumed that, within the range of concentrations studied, optical density for each sugar is proportional to amount of sugar; then use is made of the fact that each sugar differs in its density to light of different wavelengths. <PARAGRAPH_SEPARATOR> Solutions containing known amounts of glucose and galactose were prepared, and the density' at two different wavelengths (470 and $560\\mathtt{m}\\mu)$ determined. The data enable a regression of density on amount of each sugar to be determined for each wavelength. These regressions then constitute a calibration of the apparatus, such that if optical densities for some unknown solution are substituted in the equations, the amount of each sugar can beestimated. <PARAGRAPH_SEPARATOR> Thus, if ${\\pmb y}_{1}$ and ${\\pmb y}_{2}$ are the optical densities at 470 and $560\\mathrm{m}\\mu$ ,respectively,and ${\\pmb x_{1}}$ and ${\\pmb x_{2}}$ the amounts of each sugar (in milligrammes), the regression equations may be written <PARAGRAPH_SEPARATOR> $$ Y_{1}=b_{11}x_{1}+b_{21}x_{2},~Y_{2}=b_{12}x_{1}+b_{22}x_{2}. $$ <PARAGRAPH_SEPARATOR> These equations have no constant term, since the optical densities are zero at zero concentration of the sugars. In the practical use of these equations, the y's will be observed values and the x's predicted. If the equations are solved for this purpose, we get <PARAGRAPH_SEPARATOR> where the matrix <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{X_{1}=b^{11}y_{1}+b^{21}y_{2},}&{{}X_{2}=b^{12}y_{1}+b^{22}y_{2},}\\\\ {\\left[b^{11}\\quad b^{21}\\right]}\\\\ {\\qquad\\quad b^{12}}&{{}b^{22}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> is the inverse of the original matrix of regression coefficients, <PARAGRAPH_SEPARATOR> $$ \\left[{\\begin{array}{l l}{b_{11}}&{b_{21}}\\\\ {b_{12}}&{b_{22}}\\end{array}}\\right]. $$ <PARAGRAPH_SEPARATOR> The equations (2) will be called inverse regression equations, and the $\\boldsymbol{X}$ -valuesinverse estimates. It will be seen that in practically every calibration problem, inverse estimates are required, since the quantities arbitrarily assigned in the calibration are unknown in the application to estimation."
  },
  {
    "qid": "statistic-posneg-256-0-0",
    "gold_answer": "FALSE. While the condition $E[1-|W^{2}-1|]^{-1}<\\infty$ is sufficient for the convergence of the series expansion, it is not strictly necessary. The series may converge under weaker conditions, but this condition ensures absolute uniform convergence on $(-\\infty, \\infty)$ and is close to being the best possible for the given expansion form.",
    "question": "Is the condition $E[1-|W^{2}-1|]^{-1}<\\infty$ necessary for the convergence of the series expansion of the distribution function of a normal scale mixture $ZW$ as stated in Theorem 1?",
    "question_context": "Polynomial Expansions of Density and Distribution Functions for Scale Mixtures\n\nLet $Y$ be an absolutely continuous random variable and $W$ a nonnegative variable independent of $Y.$ It is to be expected that when $\\boldsymbol{\\mathscr{W}}$ is close to 1 in some sense, the distribution of the scale mixture YW will be close to Y. This notion has been investigated by a number of workers, who have provided bounds on the difference between the distribution functions of $Y$ and YW. In this paper we examine the deeper problem of finding asymptotic expansions of the form $\\begin{array}{r}{P(Y W\\leqslant x)=P(Y\\leqslant x)+\\sum_{n=1}^{\\infty}E(W^{r}-1)^{n}G_{n}(x),}\\end{array}$ where $r>0$ and the functions $G_{n}$ do not depend on $W$ We approach the problem very generally, and then consider the normal and gamma distributions in greater detail. Our results are applied to obtain better uniform and nonuniform estimates of the difference between the distribution functions of $Y$ and YW."
  },
  {
    "qid": "statistic-posneg-914-0-2",
    "gold_answer": "TRUE. By selecting the node $\\nu^{*}$ with the highest total normalized weight (sum of outflow and inflow weights), the path membership is assigned based on the strongest alignment connections. This ensures that topics with high mutual alignment weights are grouped together in the same path, maintaining the coherence of similar topics across different models.",
    "question": "In the path construction for topic reordering, the path membership of a node $\\nu\\in V_{m}$ is determined by the node $\\nu^{*}$ from levels $m+1,\\ldots,M$ that shares the highest total normalized weight with $\\nu$. Is it correct to say that this ensures topics with strong alignment weights are grouped together in the same path?",
    "question_context": "Optimal Transport and Topic Alignment in Multiscale Analysis\n\n3.2.1. Weight estimation We propose two methods for estimating weights $w\\left(e\\right)$ , one using sample composition $(\\gamma_{i})$ and another using topic composition $(\\beta_{k})$ . We call the approaches product alignment and transport alignment, respectively. <PARAGRAPH_SEPARATOR> In product alignment, we set $w\\left(e\\right)=\\gamma\\left(\\nu\\right)^{T}\\gamma\\left(\\nu^{\\prime}\\right)$ . Intuitively, if two topics have a similar pattern of $\\gamma_{i k}$ across samples $i$ , then they are given a high weight (Figure 2a). Further, topics that have small $\\gamma_{i k}$ across all samples are given lower weight, regardless of their similarity. <PARAGRAPH_SEPARATOR> In transport alignment, we compute $w\\left(e\\right)$ by solving a collection of optimal transport problems (Figure 2b). Consider two subsets $V_{p},V_{q}\\subset V $ with $V_{p}\\cap V_{q}=\\emptyset$ ; we take these two sets to be all topics $\\nu$ from models $m$ and $m^{\\prime}$ . Let $p=\\left(\\gamma\\left(\\nu\\right)^{T}\\mathbf{1}_{N}\\right)_{\\nu\\in V_{p}}$ and $\\boldsymbol{q}=\\left(\\gamma\\left(\\nu\\right)^{T}\\mathbf{1}_{N}\\right)_{\\nu\\in V_{\\boldsymbol{q}}}.$ . These summarize the “mass” of each topic across all samples, within each of the two sets. For example, these will both sum to $N$ if the $V_{p}$ and $V_{q}$ equal to the sets of topics from two models, since each $\\gamma_{i}$ lies in the simplex. Define the cost of transporting mass from node $\\nu$ to $\\nu^{\\prime}$ by $C\\left(\\nu,\\nu^{\\prime}\\right):=J S D\\left(\\beta\\left(\\nu\\right),\\beta\\left(\\nu^{\\prime}\\right)\\right)$ . This ensures that weights are lower between topics with very different distributions, regardless of sample weights $\\gamma_{i k}$ . Arrange these costs into a matrix $C$ of size $\\left|V_{p}\\right|\\times\\left|V_{q}\\right|$ . The weight matrix $W$ between pairs of topics in $V_{p}$ and $V_{q}$ is the R|Vp|×|Vq| matrix formed by solving the transport problem <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\underset{{\\mathscr W}\\in{\\mathscr U}(p,q)}{\\operatorname*{min}}\\langle C,W\\rangle}\\ &{\\quad\\mathscr U(p,q):=\\{W\\in{\\mathbb R}_{+}^{\\left|V_{p}\\right|\\times\\left|V_{q}\\right|}:W\\mathbf{1}_{\\left|V_{q}\\right|}=p\\mathrm{and}W^{T}\\mathbf{1}_{\\left|V_{p}\\right|}=q\\}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> We note that in the case that $V_{p}$ and $V_{q}$ contain topics from models $m$ and $m+1$ , it is natural to construct a directed graph, with edges from topics in model $m$ to those in $m+1$ . In this case, we refer to the topic subsets as $V_{m}$ , $V_{m+1}$ , respectively. For a directed graph, it is possible to normalize weights according to either the total inflow or outflow for each node. We will use these normalized weights in the computations of the topic orderings given in Section S1 of the Supplementary material available at Biostatistics online and in the computations of some of the diagnostic scores given in Section 3.3. Specifically, we normalize weights for edges flowing out of $\\nu$ according to $\\begin{array}{r}{w_{\\mathrm{out}}\\left(\\nu,\\nu^{\\prime}\\right)=\\frac{w\\left(\\nu,\\nu^{\\prime}\\right)}{\\sum_{\\tilde{\\nu}:\\nu\\to\\tilde{\\nu}}w\\left(\\nu,\\tilde{\\nu}\\right)}}\\end{array}$ Smilarly, nmalization for edges fHowing into vis dened by wn (V,v)= <PARAGRAPH_SEPARATOR> Figure 3 provides visualizations of product and transport alignments on simulated data. Note that topics are not returned by the LDA fit in a specific order. Consequently, topics connected by high weights across models may have different index $k$ within their respective models. For visualization purposes, we order topics within each model such that similar topics are close to each other. The ordering procedure is described in Section S2 of the Supplementary material available at Biostatistics online. <PARAGRAPH_SEPARATOR> 3.2.2. Paths Topic reordering places topics with high alignment weights next to one another, giving the appearance of chains of mutually similar topics. To highlight this phenomenon, we partition the alignment graph into a collection of paths. The partition is grown iteratively, adding topics to existing subsets based on alignment weights. <PARAGRAPH_SEPARATOR> Let Path $(\\nu)$ be the path ID associated with topic $\\nu$ , and let $M$ be the model with the largest number of topics. For each topic $\\boldsymbol{\\nu}\\in V_{M}$ , we initialize Path $\\mathbf{\\rho}(\\nu)=k\\mathbf{\\rho}(\\nu)$ . Suppose Path $(\\nu)$ is known for all $\\nu\\in V_{m+1}$ . Then, the path membership Path $(\\nu)$ of a node $\\nu\\in V_{m}$ is set to Path $(\\nu^{*})$ , where <PARAGRAPH_SEPARATOR> $$ \\nu^{*}:=\\arg\\operatorname*{max}_{\\nu^{\\prime}\\in V_{(m+1):M}}\\big(w_{\\mathrm{out}}\\left(\\nu,\\nu^{\\prime}\\right)+w_{\\mathrm{in}}\\left(\\nu,\\nu^{\\prime}\\right)\\big), $$ <PARAGRAPH_SEPARATOR> is the topic from one of the levels $m+1,\\ldots,M$ that shares the highest total normalized weight with $\\nu$. <PARAGRAPH_SEPARATOR> # 3.3. Diagnostics <PARAGRAPH_SEPARATOR> We next propose three diagnostic measures that compactly describe the results of a topic alignment. These statistics reflect the added value of introducing each additional topic, the specificity of ancestor–descendant ties, and the coherence of topics across $K$ . In addition to summarizing the alignment, these statistics can also serve to diagnose model mis-specification in the original fits."
  },
  {
    "qid": "statistic-posneg-1958-0-1",
    "gold_answer": "FALSE. The model specifies B_ij'|S_ij',B_ij ~ Bernoulli(B_ij × [expit(α_0' + α_1'S_ij')]), which means the probability of detecting sgRNA is explicitly conditional on both the true sgRNA shedding status (S_ij') and the genomic RNA detection (B_ij). In fact, if B_ij = 0 (no genomic RNA detected), then B_ij' must be 0 as well, showing a direct dependence.",
    "question": "Does the model assume that the probability of detecting sgRNA (B_ij') is independent of the genomic RNA detection (B_ij)?",
    "question_context": "Bayesian Hierarchical Model for Viral RNA Trajectory and Detection\n\nThe paper presents a joint Bayesian hierarchical model to estimate viral RNA trajectories and detection probabilities. It models the genomic RNA viral load trajectory using a piecewise linear function with upward and downward slopes, and incorporates person-level variability through multivariate normal random effects. The model also accounts for true positive and negative rates in viral detection, and includes a separate sub-model for sgRNA viral load in relation to genomic RNA."
  },
  {
    "qid": "statistic-posneg-1910-0-0",
    "gold_answer": "FALSE. The paper establishes that the maximum diameter of the confidence region R_N is asymptotically bounded by 2d as d approaches 0, but it does not guarantee exact equality for any finite sample size N. The result relies on asymptotic properties and the condition that N(d) → ∞ as d → 0.",
    "question": "Is it true that the sequential procedure described in the paper guarantees that the maximum diameter of the confidence region R_N is exactly 2d for any finite sample size N?",
    "question_context": "Sequential Confidence Regions for Regression Parameters\n\nIn this paper Chow and Robbins's (1965) sequential theory has been extended to construct a confidence region with prescribed maximum width and prescribed coverage probability for (i) the linear regression parameters, and (i) the mean vector of a multivariate population. Except for the finiteness of the unknown variance and covariances in the respective two cases, no assumption regarding the form of their distribution functions is made. This result was announced in Srivastava (1966a). For some related results, refer to Gleser (1965), Srivastava (1966b) and Starr (1966)."
  },
  {
    "qid": "statistic-posneg-44-1-1",
    "gold_answer": "TRUE. The model $M_1$ explicitly includes an interaction term $\\gamma_3\\xi_{i2}\\xi_{i3}$ in addition to the linear terms $\\gamma_1\\xi_{i2}$ and $\\gamma_2\\xi_{i3}$, making it a non-linear structural equation model.",
    "question": "Does the non-linear structural equation model $M_1$ include an interaction term between the latent variables $\\xi_{i2}$ and $\\xi_{i3}$?",
    "question_context": "Bayesian Analysis of Non-Linear Structural Equation Models with Non-Ignorable Missing Data\n\nThe paper presents a Bayesian analysis of non-linear structural equation models with non-ignorable missing data. It discusses various missingness mechanism models and compares them using path sampling to compute Bayes factors. The models include logistic regression models for missing data mechanisms and a non-linear structural equation model for latent variables. The analysis is applied to a real-world dataset from the World Values Survey, focusing on female respondents in Russia."
  },
  {
    "qid": "statistic-posneg-685-0-1",
    "gold_answer": "TRUE. The formula for $s_{p}$ is derived to ensure that the lattice $L$ can accommodate the required number of points $n$ by adjusting the size of the lattice in the $p$-th dimension, while also considering the separation distance constraints through the product term $\\prod_{k=1}^{p-1}\\left(2\\lceil s_{k}/2\\rceil\\right)$.",
    "question": "The formula for $s_{p}$ in Step 2 is designed to ensure that the lattice $L$ can accommodate the required number of points $n$ while maintaining the separation distance constraints. Is this statement true?",
    "question_context": "Interleaved Lattice-Based Maximin Distance Designs\n\nOur second algorithm is faster than Algorithm 1 for $p>5$ . To reduce computation, we do not try all standard interleaved lattices; instead we first find some promising lattices and then focus on designs based on them. As discussed in $\\S\\2,m(L,s)$ depends on $s$ , $q=\\log_{2}|L\\cap\\{0,1\\}^{p}|$ and $r=|\\{k:e_{k}\\in L\\}|$ and is almost irrelevant to the specific $L$ . Taking this into consideration, we propose to search through all practical $(s,q,r)$ combinations, each time focusing on the best lattice. <PARAGRAPH_SEPARATOR> Clearly, the best lattice should yield designs with optimal separation distance and near-optimal size. Such lattices are detected as follows. Firstly, let $\\bar{x}^{(k)}$ denote the vector $x$ in $\\{e_{1},\\ldots,e_{p}\\}$ with the $k$ th-highest $d\\{(s-1_{p})^{-1}\\otimes x\\}$ . To maximize $\\mathrm{min}_{e_{k}\\in L}\\{w_{k}/(s_{k}-1)\\}$ in (7), the best lattice $L$ must contain $x^{(1)},\\ldots,x^{(r)}$ but not $x^{(r+1)},\\ldots,x^{(p)}$ . Secondly, sort vectors $x$ in $\\Gamma=\\{x:x_{k}=$ 0 or $(x_{k}=1$ and $e_{k}\\notin L),k=1,\\ldots,p,\\textstyle\\sum_{k=1}^{p}x_{k}\\geqslant2\\}$ by $d\\{(s-1_{p})^{-1}\\otimes x\\}$ and decide which of them should be contained in $L_{0}$ in (5 ). To maximize $\\begin{array}{r}{\\operatorname*{min}_{x\\in L_{0},x\\neq0_{p}}d\\{(s-1_{p})^{-1}\\otimes x\\}}\\end{array}$ in (7), starting from the $x\\in\\Gamma$ with lowest $d\\{(s-1_{p})^{-1}\\otimes x\\}$ , we put a vector in $L_{0}$ unless no proper $L$ with the given $q$ exists. We obtain $L_{0}$ and therefore $L$ after going over all vectors one by one. From Theorem 2, <PARAGRAPH_SEPARATOR> $$ \\rho(D)=\\operatorname*{min}\\Bigg[d\\{(s-1_{p})^{-1}\\otimes x^{(r)}\\},d\\{(s-1_{p})^{-1}\\otimes y\\},\\operatorname*{min}_{s_{k}>2}\\{2w_{k}/(s_{k}-1)\\}\\Bigg], $$ <PARAGRAPH_SEPARATOR> where $y$ is the first vector in $\\Gamma$ that must be put in $L_{0}$ . Therefore, we only consider $s$ and $r$ small enough so that $2w_{k}/(s_{k}-1)>\\rho_{B}$ $(k=1,\\dots,p)$ and $d\\{(s-1_{p})^{-1}\\otimes x^{(r)}\\}>\\rho_{B}$ . While being optimal in separation distance, the best lattices tend to attain high second-lowest pairwise distance as well. <PARAGRAPH_SEPARATOR> Algorithm 2 has the following six steps. <PARAGRAPH_SEPARATOR> Step 1. Initialize $\\rho_{B}=0$ . Try every $q=p,\\ldots,1$ from largest to smallest. <PARAGRAPH_SEPARATOR> Step 2. For each $q$ , initialize $s_{1}=\\cdot\\cdot\\cdot=s_{p-1}=2$ and <PARAGRAPH_SEPARATOR> $$ s_{p}=\\operatorname*{max}\\left[2\\left[2^{p-q-1}n\\left\\{\\prod_{k=1}^{p-1}\\left(2\\lceil s_{k}/2\\rceil\\right)\\right\\}^{-1}\\right]-1,2\\right]. $$ <PARAGRAPH_SEPARATOR> Step 3. Check if $L$ exists such that $q(L)=q$ and $\\{x\\in\\{0,1\\}^{p}:x\\neq0_{p},d\\{(s-1_{p})^{-1}\\otimes x\\}\\leqslant$ $\\rho_{B}\\}\\cap L=\\emptyset$ . If not, go to Step 5. <PARAGRAPH_SEPARATOR> Step 4. For all possible $r$ from largest to smallest, obtain the best lattice $L$ and compute $m(L,s)$ . If $m(L,s)<n$ , set $s_{p}=s_{p}+1$ , break the loop on $r$ and go to Step 3. Otherwise, update $L_{B}=L$ , $s_{B}=s$ and $\\rho_{B}=\\rho(L_{B},s_{B})$ . <PARAGRAPH_SEPARATOR> Step 5. Find the largest integer $j\\leqslant p$ such that $s_{j}>2$ and the largest integer $k\\leqslant j-1$ such that $2w_{k}/s_{k}>\\rho_{B}$ . If no such $j>1$ exists or no such $k$ exists, end the search for the $q$ . Otherwise, set $s_{k}=s_{k}+1,s_{l}=2$ for $l=k+1,\\ldots,p-1$ , and $s_{p}$ as in (10), and go to Step 3."
  },
  {
    "qid": "statistic-posneg-669-0-3",
    "gold_answer": "FALSE. The bound is achieved when the extremal distribution is symmetric, as inspection of the distribution reveals. The paper explicitly states that this bound coincides with the one obtained without assuming symmetry, but the achieving distribution is indeed symmetric.",
    "question": "Can the bound $E(X_{2:2})\\leqslant\\left({\\frac{p-1}{2p-1}}\\right)^{(p-1)/p}$ be achieved without assuming symmetry?",
    "question_context": "Bounds on Expected Range of Order Statistics\n\nThe paper discusses bounds on the expected range of order statistics for identically distributed, possibly dependent random variables. It provides sharp bounds under various conditions, including symmetry and independence. The expected range is expressed as $E(R_{n})=E(X_{n:n}-X_{1:n})$, and the paper derives bounds using Holder's inequality and specific distribution assumptions."
  },
  {
    "qid": "statistic-posneg-585-0-0",
    "gold_answer": "FALSE. The log-likelihood function $L_{n}(\\eta)$ is not correctly specified for the given model. The correct log-likelihood should account for the determinant of the covariance matrix $\\Gamma$, which is missing in the provided formula. The correct log-likelihood should be: $$L_{n}(\\eta) = -\\frac{1}{2}\\left[\\log\\det(\\Gamma) + \\tilde{\\mathbf{Y}}^{\\top}\\Gamma^{-1}\\tilde{\\mathbf{Y}}\\right],$$ where $\\Gamma = \\mathrm{diag}\\{\\eta\\sigma^{*2}\\lambda_{1} + (1-\\eta)\\sigma^{*2}, \\ldots, \\eta\\sigma^{*2}\\lambda_{n} + (1-\\eta)\\sigma^{*2}\\}$. The provided formula simplifies the log-likelihood by omitting the determinant term and using a weighted average of squared residuals, which is not equivalent to the correct log-likelihood for this model.",
    "question": "Is the log-likelihood function $L_{n}(\\eta)$ correctly specified for estimating heritability in the given model, where $\\mathbf{Y}$ follows a normal distribution with covariance matrix $\\eta^{*}\\sigma^{*2}{\\bf R}+(1-\\eta^{*})\\sigma^{*2}{\\bf I d}_{\\mathbb{R}^{n}}$?",
    "question_context": "Heritability Estimation via Maximum Likelihood in Linear Mixed Models\n\nFor estimating heritability, we used the approach that we proposed in Bonnet et al. (2015). It is based on a maximum likelihood strategy and was implemented in the $\\mathbf{R}$ package HiLMM. Let us recall how this method works. In the case where $q=1$ , which corresponds to the non-sparse case, $$\\begin{array}{r}{{\\bf Y}\\sim\\mathcal{N}\\big\\{0,\\eta^{*}\\sigma^{*2}{\\bf R}+(1-\\eta^{*})\\sigma^{*2}{\\bf I d}_{\\mathbb{R}^{n}}\\big\\},}\\end{array}$$ with $\\boldsymbol{\\sigma}^{*2}=N\\sigma_{u}^{*2}+\\sigma_{e}^{*2}$ and ${\\bf R}={\\bf Z}_{\\mathrm{final}}{\\bf Z}_{\\mathrm{final}}^{\\prime}/N_{\\mathrm{final}}$ , where $\\mathbf{Z}_{\\mathrm{final}}$ denotes the matrix $\\mathbf{Z}$ in which the columns that are selected in the variable selection step that was described in Section 3.1 are kept. Let $\\mathbf{U}$ be defined as follows: $\\mathbf{U}^{\\prime}\\mathbf{U}=\\mathbf{U}\\mathbf{U}^{\\prime}=\\operatorname{Id}_{\\mathbb{R}^{n}}$ and $\\mathbf{URU}^{\\prime}{=}\\operatorname{diag}(\\lambda_{1},\\ldots,\\lambda_{n})$ , where the last quantity denotes the diagonal matrix having its diagonal entries equal to $\\lambda_{1},\\ldots,\\lambda_{n}$ . Hence, in the case where $q=1$ , $$\\Gamma=\\mathrm{diag}\\{\\eta^{*}\\sigma^{*^{2}}\\lambda_{1}+(1-\\eta^{*})\\sigma^{*^{2}},\\ldots,\\eta^{*}\\sigma^{*^{2}}\\lambda_{n}+(1-\\eta^{*})\\sigma^{*^{2}}\\},$$ where the $\\lambda_{i}\\mathbf{s}$ are the eigenvalues of $\\mathbf{R}$. We propose to define $\\hat{\\eta}$ as a maximizer of the log-likelihood $$L_{n}(\\eta)=-\\log\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\tilde{Y}_{i}^{2}}{\\eta(\\lambda_{i}-1)+1}\\right\\}-\\frac{1}{n}\\sum_{i=1}^{n}\\log\\big\\{\\eta(\\lambda_{i}-1)+1\\big\\},$$ where the $\\tilde{Y}_{i}\\mathrm{s}$ are the components of the vector $\\tilde{\\mathbf{Y}}=\\mathbf{U}^{\\prime}\\mathbf{Y}$."
  },
  {
    "qid": "statistic-posneg-440-0-0",
    "gold_answer": "FALSE. The term $\\rho\\sum_{j=1}^{N}w_{i j}Y_{j t}$ is described as the endogenous spatial lag capturing global productivity spillovers, not just local effects. While the spatial weight matrix $w_{i j}$ is truncated at 50km to focus on local clusters, the term itself represents a global spillover effect as it influences the dependent variable $Y_{i t}$ through the weighted sum of neighboring outputs. The distinction between local and global spillovers is crucial in spatial econometrics, with local spillovers being captured by terms like $\\sum_{j=1}^{N}w_{i j}L_{j t}\\theta_{L}$ and $\\sum_{j=1}^{N}w_{i j}K_{j t}\\theta_{K}$, which are explicitly related to input variables.",
    "question": "In the spatial stochastic frontier model, the term $\\rho\\sum_{j=1}^{N}w_{i j}Y_{j t}$ captures only local spillover effects among hotels within a 50km radius. True or False?",
    "question_context": "Spatial Stochastic Frontier Model for Hotel Efficiency with Spillover Effects\n\nThe SDF-STE model is estimated over a balanced sample of 4,257 individual observations representing hotels located in Italy. Indeed, the Italian tourism and travel sector is one of the fastest growing and most profitable industrial sectors of Italy, contributing to $13\\%$ of the Italian GDP (including indirect effects) and giving employment to $14.7\\%$ of the Italian workforce in 2017 (OECD, 2020). Moreover, the accommodation sector can be considered as highly characterized by spillover effects among nearby facilities. As highlighted by Novelli et al. (2006), tourist destinations can be seen as forms of industrial clusters, made up of groups of small and medium enterprises (SMEs) that cooperate to build up a successful tourism product. SMEs clusters, through networking, alliances, active collaboration, and innovation can succeed in successfully competing in the global tourism market through local cooperation (Smeral, 1998). Therefore, accommodation facilities located in tourism clusters can accumulate new knowledge and innovate more easily than isolated hotels (Marco-Lajara et al., 2016). Accordingly, estimating our model using a sample of Italian hotels should be an interesting case study providing noticeable insights into the role that spatial spillovers have in affecting the performance of Italian accommodation facilities. <PARAGRAPH_SEPARATOR> The data are collected from the AIDA Bureau Van Dijk database, which provides information on the consolidated accounts of Italian companies as well as on the geographical detail (i.e., longitudes and latitudes). In particular, we concentrated on the ATECO 55 sector over the period 2011–2019. Comparing the sample with the related population using census data from the Italian National Institute of Statistics (ISTAT) in 2018, our sample results to be fairly good in representing the ATECO 55 sector, covering the $7.85\\%$ of Italian hotels, the $26.88\\%$ of total employees, and the $33.44\\%$ of the total value added generated in the sector. <PARAGRAPH_SEPARATOR> The empirical model defined based on a Cobb–Douglas production function for $i,j=1,...,N$ $(i\\neq j)$ and $t=1,...,T$ is specified as <PARAGRAPH_SEPARATOR> $$ Y_{i t}=\\beta_{0}+t\\beta_{t}+t^{2}\\beta_{2t}+L_{i t}\\beta_{L}+K_{i t}\\beta_{K}+\\rho\\sum_{j=1}^{N}w_{i j}Y_{j t}+\\sum_{j=1}^{N}w_{i j}L_{j t}\\theta_{L} $$ <PARAGRAPH_SEPARATOR> $$ +\\sum_{j=1}^{N}w_{i j}K_{j t}\\theta_{K}-u_{i t}+\\nu_{i t}, $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{\\displaystyle\\mu_{i i}={\\phi_{0}}+\\mathrm{D}_{i i}{\\phi_{\\mathrm{LP}}}+\\mathrm{OCF}_{i i}{\\phi_{\\mathrm{OCF}}}+\\mathrm{LEV}_{i i}{\\phi_{\\mathrm{LEV}}}+\\mathrm{NOD}_{i i}{\\phi_{\\mathrm{NOD}}}}\\ {\\displaystyle\\qquad+\\mathrm{corp}_{i i}{\\phi_{\\mathrm{corp}}}+\\mathrm{ex}_{i i}{\\phi_{\\mathrm{ex}}}+\\mathrm{crisis}_{i i}{\\phi_{\\mathrm{crisis}}}+\\sum_{j=1}^{N}w_{i j}\\mathrm{LP}_{j i}\\delta_{\\mathrm{LP}}}\\ {\\displaystyle\\qquad+\\sum_{j=1}^{N}w_{i j}\\mathrm{OCF}_{j i}\\delta_{\\mathrm{OCF}}+\\sum_{j=1}^{N}w_{i j}\\mathrm{LEV}_{j i}\\delta_{\\mathrm{LEV}}+\\sum_{j=1}^{N}w_{i j}\\mathrm{NOD}_{j i}\\delta_{\\mathrm{NOD}}}\\ {\\displaystyle\\qquad+\\sum_{j=1}^{N}w_{i j}\\mathrm{corp}_{j i}\\delta_{\\mathrm{corp}}+\\sum_{j=1}^{N}w_{i j}\\mathrm{ex}_{j i}\\delta_{\\mathrm{ex}}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Specifically, $Y_{i t}$ represents the logarithm of the value added of hotel $i$ at time $t$ and $L_{i t}$ and $K_{i t}$ are the logarithms of the number of employees and of fixed assets, respectively, representing the two production inputs. Following Glass et al. (2016), we assume Hicks-neutral technical change and therefore the time trend variable $t$ and its square $t^{2}$ are added to the model specification. Moreover, $\\rho\\sum_{j=1}^{N}w_{i j}Y_{j t}$ is the endogenous spatial lag capturing global productivity spillovers while $\\textstyle\\sum_{j=1}^{N}w_{i j}L_{j t}\\theta_{L}$ and $\\textstyle\\sum_{j=1}^{N}w_{i j}K_{j t}\\theta_{K}$ are the spatial lags related to the two input variables representing labour and capital local spillover effects. $w_{i j}$ indicates the generic spatial weight belonging to the sparse row standardized inverse distance spatial weight matrix W truncated at $50\\mathrm{km}$ with all zeros on the main diagonal. Therefore, to capture spatial spillover effects, only those hotels located within a radius of $50\\mathrm{km}$ are considered neighbours. Indeed, dense (i.e., not truncated) inverse distance spatial weight matrices take relations of firms with all territorial units into account, considering global spatial interactions without concentrating on local clusters (Kopczewska et al., 2017). In the accommodation sector case, it is very unlikely that a hotel located in Sicily could affect a hotel located in Rome (unless they are in a corporate group). Therefore, it appears to be more reasonable to choose a cut-off point for W to evaluate the spatial effects only for hotels belonging to neighbouring destinations. Moreover, defining W using an inverse distance specification allows obtaining non-endogenous spatial weights. However, in Subsection 5.4 we test the robustness of our results considering a dense W and a set of different truncation points for the distance function. <PARAGRAPH_SEPARATOR> Considering the determinants of technical inefficiency, in the efficiency model a series of indicators measuring labour productivity (LP), operational cash flow (OCF) and financial leverage (LEV) are included, as well as information about the number of companies in corporate group (corp), the level of experience determined by the years of hotel’s activity (ex) and two dummy variables identifying hotels with no long-term debts (NOD) and the years of the crisis of the sovereign debt from 2011 to 2013 (crisis). Financial indicators such as OCF and financial leverage are key elements to examine the variability in firms’ performance because they are strongly associated with firms’ failure and bankruptcy (Alberca & Parte, 2020). In particular, higher values of OCF and LP are expected to positively influence firms’ efficiency, while LEV is expected to have an opposite effect. Accordingly, hotels that do not have long-term debts, hotels belonging to corporate groups and facilities with a major working experience in the sector are expected to reach an increased efficiency level. Further details on the definition and measurement of all these variables and some brief descriptive statistics are given in Table 2. The efficiency model shown in Eq. (14) also includes the spatial lag of all the determinants of hotels’ inefficiency with the exception of crisis to capture how hotels are influenced by the factors that determine the efficiency level of neighbouring accommodation facilities. Indeed, being surrounded by hotels belonging to corporate groups or by hotels with a high level of experience can positively influence neighbours according to knowledge spillover economic theory. Similarly, higher levels of LP and OCF, as well as lower levels of LEV, can be relevant indicators of more competitive destinations generating internal efficiency spillover effects. Specifically, combining $\\rho$ with the $\\phi$ and the $\\delta$ estimates, the indirect effects originating from the $Z$ variables and affecting neighbouring firms’ efficiency can be computed using the method proposed in Section 3.3."
  },
  {
    "qid": "statistic-posneg-1169-0-3",
    "gold_answer": "TRUE. Proposition 2 states that $G_{1}$ has this probability density function, which is derived from the asymptotic behavior of $N_{n}^{u}$ under the nonstationary functional process.",
    "question": "Is the probability density function of $G_{1}$ given by $g_{1}(u) = \\frac{1}{\\sqrt{\\pi}} \\exp(-u^{2}/4)$ for $u \\geq 0$?",
    "question_context": "Functional Record Analysis in Time Series\n\nThe paper discusses the properties of functional records in time series analysis. It defines upper and lower functional records and studies their behavior over time. The counting process for upper functional records, $N_{i}^{u}$, is analyzed under different conditions: independent and identically distributed (iid) sequences, stationary functional time series, and nonstationary functional processes. The growth rates of $N_{i}^{u}$ are derived for these cases, showing $O(\\log i)$ for iid and stationary series, and $O(\\sqrt{n})$ for nonstationary processes. The paper also provides visualizations and theoretical results supporting these growth rates."
  },
  {
    "qid": "statistic-posneg-1320-0-1",
    "gold_answer": "FALSE. While $\\mu_{t,1}$ and $\\mu_{t,2}$ control the first two moments (zero mean and unit variance), additional moment conditions are required to ensure serial independence. The paper includes conditions like $\\mu_{t,i+2}=\\tilde{z}_{t}\\tilde{z}_{t-i}$ (for serial correlation) and $\\mu_{t,j+g_{1}+2}=\\tilde{z}_{t}^{2}\\tilde{z}_{t-j}^{2}-1$ (for squared residual correlation) to enforce independence. Thus, the first two conditions alone are insufficient.",
    "question": "Is it correct to assume that the moment conditions $\\mu_{t,1}=\\tilde{z}_{t}$ and $\\mu_{t,2}=\\tilde{z}_{t}^{2}-1$ are sufficient to ensure the standardized residuals $\\tilde{z}_{t}$ are i.i.d. with zero mean and unit variance?",
    "question_context": "GMM Estimation for Volatility Forecast Combination\n\nThe paper introduces a novel GMM approach to combine volatility forecasts from different models. The data generating process (DGP) is given by $y_{t}=x_{t}+u_{t}, u_{t}=h_{t}z_{t}$, where $z_{t}\\sim(0,1)$ are i.i.d. random variables. The combined predictor for the level and volatility are defined as $\\tilde{x}_{t}=\\sum_{i=1}^{k}\\tilde{w}_{i}^{(x)}\\hat{x}_{t i}$ and $\\tilde{h}_{t}^{2}=\\sum_{i=1}^{k}\\tilde{w}_{i}^{(h)}\\hat{h}_{t i}^{2}$, respectively. The weights are estimated by minimizing the GMM criterion $\\tilde{\\mathbf{w}}=\\underset{\\mathbf{w}}{\\mathrm{argmin}}m_{T}(\\mathbf{w})^{\\prime}M_{T}m_{T}(\\mathbf{w})$, where $m_{T}(\\mathbf{w})$ is a vector of moment conditions ensuring the standardized residuals $\\tilde{z}_{t}=\\frac{y_{t}-\\tilde{x}_{t}}{\\tilde{h}_{t}}$ behave as i.i.d. zero mean, unit variance random variables."
  },
  {
    "qid": "statistic-posneg-205-0-0",
    "gold_answer": "TRUE. The inverse regression equations are derived by solving the system of linear equations for $x_1$ and $x_2$ in terms of $y_1$ and $y_2$. The coefficients $b^{11}, b^{21}, b^{12}, b^{22}$ are elements of the inverse matrix of the original coefficient matrix $B = \\begin{bmatrix} b_{11} & b_{21} \\\\ b_{12} & b_{22} \\end{bmatrix}$. This is a standard method for solving simultaneous linear equations.",
    "question": "Is the inverse regression equation $X_{1}=b^{11}y_{1}+b^{21}y_{2}$ correctly derived from the original regression equations $Y_{1}=b_{11}x_{1}+b_{21}x_{2}$ and $Y_{2}=b_{12}x_{1}+b_{22}x_{2}$ by solving for $x_1$ and $x_2$?",
    "question_context": "Simultaneous Regression Equations in Chemical Analysis\n\nFisher, Hansen & Norton (1955) discuss the quantitative determination of glucose and galactose simultaneously in solutions of unknown chemical composition, by means of optical density measurements. Without going into the technical details, which are given in the paper referred to, we can simply state that solutions of glucose and galactose are treated to develop a colour; the optical density of the solution to light of two different wavelengths is then determined, and the two data thus obtained are used to estimate the amount of each sugar in solution. It is assumed that, within the range of concentrations studied, optical density for each sugar is proportional to amount of sugar; then use is made of the fact that each sugar differs in its density to light of different wavelengths. <PARAGRAPH_SEPARATOR> Solutions containing known amounts of glucose and galactose were prepared, and the density' at two different wavelengths (470 and $560\\mathtt{m}\\mu)$ determined. The data enable a regression of density on amount of each sugar to be determined for each wavelength. These regressions then constitute a calibration of the apparatus, such that if optical densities for some unknown solution are substituted in the equations, the amount of each sugar can beestimated. <PARAGRAPH_SEPARATOR> Thus, if ${\\pmb y}_{1}$ and ${\\pmb y}_{2}$ are the optical densities at 470 and $560\\mathrm{m}\\mu$ ,respectively,and ${\\pmb x_{1}}$ and ${\\pmb x_{2}}$ the amounts of each sugar (in milligrammes), the regression equations may be written <PARAGRAPH_SEPARATOR> $$ Y_{1}=b_{11}x_{1}+b_{21}x_{2},~Y_{2}=b_{12}x_{1}+b_{22}x_{2}. $$ <PARAGRAPH_SEPARATOR> These equations have no constant term, since the optical densities are zero at zero concentration of the sugars. In the practical use of these equations, the y's will be observed values and the x's predicted. If the equations are solved for this purpose, we get <PARAGRAPH_SEPARATOR> where the matrix <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{X_{1}=b^{11}y_{1}+b^{21}y_{2},}&{{}X_{2}=b^{12}y_{1}+b^{22}y_{2},}\\\\ {\\left[b^{11}\\quad b^{21}\\right]}\\\\ {\\qquad\\quad b^{12}}&{{}b^{22}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> is the inverse of the original matrix of regression coefficients, <PARAGRAPH_SEPARATOR> $$ \\left[{\\begin{array}{l l}{b_{11}}&{b_{21}}\\\\ {b_{12}}&{b_{22}}\\end{array}}\\right]. $$ <PARAGRAPH_SEPARATOR> The equations (2) will be called inverse regression equations, and the $\\boldsymbol{X}$ -valuesinverse estimates. It will be seen that in practically every calibration problem, inverse estimates are required, since the quantities arbitrarily assigned in the calibration are unknown in the application to estimation."
  },
  {
    "qid": "statistic-posneg-1996-0-1",
    "gold_answer": "FALSE. While the context suggests that the 2-moment score test generally has favorable operating characteristics, it does not claim universal superiority over the 3-moment test. The performance depends on the specific nature of the departure from independence (e.g., pure location shift, pure scale shift, or both). The 3-moment test may capture higher-order dependencies not detected by the 2-moment test, though the latter is often sufficient and more parsimonious.",
    "question": "Does the 2-moment score test always outperform the 3-moment score test in detecting departures from independence in the latent logistic setting?",
    "question_context": "Cumulative-Logit Location-Scale Model for Independence Testing\n\nThe second set of tables we simulated used row probabilities based on discretizing logistic distributions. Specifically, the table probabilities were generated as follows: Let $(C\\leq j)=(C^{*}\\leq\\alpha_{j})$ , where $C^{*}|(R=i)\\sim\\sigma_{i}L-\\beta_{i}$ , $i={1,2,3}$ , where $L\\sim L o g i s t i c(0,1)$ . It follows that the cumulative row probabilities have the form $$ P(C\\leq j|R=i)=P(C^{*}\\leq\\alpha_{j}|R=i)=\\frac{\\exp\\{(\\alpha_{j}+\\beta_{i})/\\sigma_{i}\\}}{1+\\exp\\{(\\alpha_{j}+\\beta_{i})/\\sigma_{i}\\}}. $$ That is, the table probabilities satisfy the cumulative-logit location–scale model of McCullagh (1980), as described in the Appendix A. When the latent logistic location parameters $\\beta_{i}$ ’s are identical and the dispersions $\\sigma_{i}$ ’s are identical, the independence model holds. When the $\\beta_{i}$ ’s vary across the rows, we say there is a latent location shift. Similarly, when the $\\sigma_{i}$ ’s vary across the rows, we say there is a latent scale shift."
  },
  {
    "qid": "statistic-posneg-1989-0-0",
    "gold_answer": "FALSE. While the journal is printed in Japanese characters, making it unintelligible to most Western readers, it includes English abstracts of some papers and presents mathematics in Western symbols. Additionally, Western proper names are printed in roman type, which means some parts can be understood by Western readers.",
    "question": "Is it true that the Japanese journal Statistical Quality Control is entirely unintelligible to Western readers due to its use of Japanese characters?",
    "question_context": "Statistical Quality Control Training and Application\n\nTHE Japanese journal Statistical Quality Control gives evidence of considerable activity in the field of quality control in Japan, since the 1954 volume contains over goo pages. It is printed in Japanese characters and so is unintelligible to most Western readers -although mathematics is presented in Western symbols and Western proper names are printed in roman type-- but English abstracts of some of the papers indicate that they cover the usual topics of the subject. The cover is elaborately designed with numerous small panels, each containing a mathematical or statistical device (a curve or formula used as a design), or a small symbolical representation of an industrial process. <PARAGRAPH_SEPARATOR> A tutorial course of weekly lectures which has been held at Management House, London, this winter is the result of an expression of interest by several leading firms in having staff trained in the use of statistical methods. The course was attended by sixteen technically qualified men in executive or research positions in the eight firms represented. It is notable that these men were nominated by their firms and that the decision to attend was made after some initial experience of the application of the methods to the work of the firm. There must be many other firms to whom the methods would be equally valuable. <PARAGRAPH_SEPARATOR> The purpose was to provide an opportunity for the technologist, already a specialist in one subject, to acquire these techniques of dealing with the variability commonly encountered in production processes or in the results of experiments. The lectures aimed at giving him an acquaintance with the working tools of the statistician up to and including the basic designs for efficient experiment. <PARAGRAPH_SEPARATOR> The lectures came to an end in early April and are to be supplemented by a short residential course at Cambridge in the early summer on practical statistical calculations. This should serve to consolidate the knowledge already gained and to provide opportunities for discussion of difficulties. Being residential, this supplementary course will appeal to some unable to attend the London lectures; provision is therefore being made for a modified course to run parallel to the main course with a shortened syllabus suitable for those beginning their study of the subject."
  },
  {
    "qid": "statistic-posneg-885-0-1",
    "gold_answer": "FALSE. While the nonlinear additive autoregressive model simplifies the high-dimensional autoregressive function $g(Z_{t-1},\\ldots,Z_{t-p})$ by breaking it into additive components, it does not completely avoid the curse of dimensionality. The curse of dimensionality still affects the estimation of each individual function $f_{i}$, especially when $p$ is large, as the number of required data points grows exponentially with the number of dimensions. However, the additive structure does mitigate some of the challenges compared to a fully nonparametric approach.",
    "question": "Does the nonlinear additive autoregressive model $Z_{t}=f_{1}(Z_{t-1})+\\cdots+f_{p}(Z_{t-p})+\\varepsilon_{t}$ avoid the curse of dimensionality when $p$ is large?",
    "question_context": "Nonlinear Additive Autoregressive Models and Forecasting Methods\n\nForecasting future observations is probably the most important task of time series analysis and this fascinates statisticians and economists. Among recent papers, see for instance Heij et al. (2007), Koopman and Ooms (2006). The purpose of this paper is to introduce two new forecasting methods, both based on a factorial analysis method called spline principal component analysis with respect to instrumental variables (spline PCAIV). Spline PCAIV is a multivariate method introduced by Durand (1993). <PARAGRAPH_SEPARATOR> A classical approach for forecasting time series is postulating parametric models, estimating a few coefficients and computing the forecasts. Let $\\{Z_{t}\\}_{1\\leqslant t\\leqslant T}$ denote a univariate time series process, the most popular model is the autoregressive model of order $p$ <PARAGRAPH_SEPARATOR> $$ Z_{t}=\\sum_{i=1}^{p}\\alpha_{i}Z_{t-i}+\\varepsilon_{t}, $$ <PARAGRAPH_SEPARATOR> which is a particular model of the well-known autoregressive moving average (ARMA) models. Model (1) represents the current state of $Z_{t}$ through its immediate $p$ past values. This is done in a linear form where the intercept is excluded. We suppose $\\mathbb{E}(Z_{t})=0$ and, in practice the sample mean from the data is subtracted before fitting. <PARAGRAPH_SEPARATOR> The limitation of that type of linear models is well known and infinitely many forms of nonlinear models have been explored since the eighties. The development in nonparametric regression provides techniques for modelling time series through the following model: <PARAGRAPH_SEPARATOR> $$ Z_{t}=g(Z_{t-1},\\ldots,Z_{t-p})+\\varepsilon_{t}. $$ <PARAGRAPH_SEPARATOR> However, when $p$ is large, the nonparametric approach suffers from the “curse of dimensionality” and a natural simplification is the nonlinear additive autoregressive models <PARAGRAPH_SEPARATOR> $$ Z_{t}=f_{1}(Z_{t-1})+\\cdots+f_{p}(Z_{t-p})+\\varepsilon_{t}. $$ <PARAGRAPH_SEPARATOR> Additive models are very useful for approximating the high-dimensional autoregressive function $g\\left(.\\right)$ given above. They and their extensions have become one of the widely used nonparametric techniques. The functions $f_{i}$ could be estimated by several nonparametric methods: spline, kernel. . . . An excellent survey is given in Fan and Yao (2003). Note, however, that one can add a constant to a component and subtract that constant from another, thus the functions $f_{1},\\ldots,f_{p}$ are not identifiable. To prevent ambiguity, usual conditions are imposed <PARAGRAPH_SEPARATOR> $$ \\mathbb{E}(f_{i}(Z))=0,\\quad i=1,\\dots,p. $$ <PARAGRAPH_SEPARATOR> with this constraints, $\\mathbb{E}(Z_{t})=0$ . Therefore, we assume that the mean has already been removed from the series or we add an intercept to the model. <PARAGRAPH_SEPARATOR> Recall that following the seminal paper of Rao (1964), the goal of linear principal component analysis with respect to instrumental variables (PCAIV) is to explain a few response $Y_{1},\\dots,Y_{q}$ by a linear model based on explanatory variables $X_{1},\\ldots,X_{p}$ . The key idea is to use the same linear combinations of explanatory variables for each response variables. These linear combinations are chosen to be $2\\times2$ orthogonal and to be ordered from the most explanatory to the least one. This classical presentation can be viewed as PCA of the projection of $Y$ on the vector space spanned by $X_{1},\\ldots,X_{p}$ . Another classical presentation of PCAIV, which leads to the same linear combination of explanatory variables, is given in Section 2.1. This definition of PCAIV is used by Durand (1993) to propose a nonlinear PCAIV using B-splines. A presentation of this nonlinear multivariate tool is given in Section 2.2. <PARAGRAPH_SEPARATOR> In 1999, Imam et al. used spline PCAIV to estimate the order $p$ of a nonlinear autoregressive model and the functions $f_{i}$ in model (2). Following their work, two methods derived from spline PCAIV are proposed to forecast time series. The first one is classical, the functions $f_{i}$ are estimated then the forecasts are computed. The second one is completely new, as we optimize a criteria according to the unknown value we want to forecast. Section 3 is devoted to the presentation of these forecasting methods. In Section 4, the forecasting methods are applied to two well-known real time series and the results of the forecast are compared with other classical forecasting methods."
  },
  {
    "qid": "statistic-posneg-728-0-0",
    "gold_answer": "FALSE. The term θ^(lin)u_iv_j specifically captures the linear-by-linear association between the ordered categories, where u_i and v_j are fixed scores assigned to the categories. The main effects are separately captured by θ_i^(X) and θ_j^(Y).",
    "question": "In the linear-by-linear association model formula π_ij = exp(θ_i^(X) + θ_j^(Y) + θ^(lin)u_iv_j) / sum(exp(θ_i*^(X) + θ_j*^(Y) + θ^(lin)u_i*v_j*)), the term θ^(lin)u_iv_j captures only the main effects of the categories and not their ordered association. True or False?",
    "question_context": "Linear-by-Linear Association Models and Power Divergence Statistics\n\nThe context discusses linear-by-linear association models and various power divergence statistics used for model selection and parameter estimation. The key formula presented is the probability model for cell probabilities in a contingency table, incorporating main effects and a linear-by-linear association term. The context also provides simulation results comparing different statistics (e.g., G², X², C-R) across different sample sizes and models."
  },
  {
    "qid": "statistic-posneg-1207-2-1",
    "gold_answer": "TRUE. The decision rule $\\delta_{o o b}(D_{N})=\\left\\{\\mathcal{E}_{1/2}^{o o b}\\leq F^{-1}(\\alpha)\\right\\}$ correctly states that the null hypothesis $H_{0}$ is rejected when $\\mathcal{E}_{1/2}^{o o b}$ is less than or equal to the $\\alpha$ quantile of the permutation distribution $F$. This is consistent with the permutation test framework described in the context.",
    "question": "Does the test statistic $\\delta_{o o b}(D_{N})$ reject the null hypothesis $H_{0}$ when $\\mathcal{E}_{1/2}^{o o b}$ is less than or equal to the $\\alpha$ quantile of the permutation distribution?",
    "question_context": "Random Forest Out-of-Bag Error for Two-Sample Testing\n\nNaturally, the split in training and test set is not ideal. For finite sample sizes, one would like to have as many (test) samples as possible to detect differences. At the same time, it would be preferable to have the classifier trained on many data points. This in fact resembles a bias-variance trade-off, similar to what was described in Lopez-Paz and Oquab (2018): Let $g_{1/2}^{*}$ and $L_{\\pi}^{(g_{1/2}^{*})}$ be the Bayes classifier and Bayes error respectively, both defined in Section 2.3. For $\\pi=1/2$ , there is a trade-off between the closeness of $L^{(\\hat{g})}$ to $L_{\\pi}^{(g_{1/2}^{*})}$ , which may be achieved through a large training set and the closeness of [(g) to Lg), which is generally only true in large test sets.\n\n<PARAGRAPH_SEPARATOR>\n\n# 2.2. Out-of-bag test\n\n<PARAGRAPH_SEPARATOR>\n\nFor the purpose of overcoming the arbitrary split in training and testing, Random Forest delivers an interesting tool: the OOB error introduced in Breiman (2001). Since each tree is built on a bootstrapped sample taken from $D_{N}$ , approximately $1/3$ of the trees will not use the ith observation $(\\ell_{i},\\mathbf{Z}_{i})$ . Thus we may use this ensemble of trees not containing observation i to obtain an estimate of the out-of-sample error for i. We slightly generalize this here, in assuming we have an ensemble learner $_g$ : That is, we assume to have iid copies of a random element $\\nu$ , $\\nu_{1},\\ldots,\\nu_{B}$ , such that each $\\hat{\\{\\mathbfcal g}}_{\\nu_{b}}({\\mathbf Z}):=g({\\mathbf Z},D_{N_{t r a i n}},\\nu_{b})$ is a different classifier. We then consider the average\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\hat{g}(\\mathbf{Z}):=\\frac{1}{B}\\sum_{b=1}^{B}\\hat{g}_{\\nu_{b}}(\\mathbf{Z}).$$\n\n<PARAGRAPH_SEPARATOR>\n\nFor $B\\to\\infty$ , it holds that (a.s.) $\\hat{g}(\\mathbf{Z})\\rightarrow\\mathbb{E}_{\\nu}[\\hat{g}_{\\nu}(\\mathbf{Z})]$ . For Random Forest, $\\nu$ usually represents the bootstrap sampling of observations and the sampling of variables to consider at each splitpoint for a given tree.\n\n<PARAGRAPH_SEPARATOR>\n\nLet as before, $\\begin{array}{r}{n_{0}:=\\sum_{i=1}^{\\bar{N}}\\bar{\\mathbb{I}}\\{\\ell_{i}=0\\}}\\end{array}$ and $\\begin{array}{r}{n_{1}:=\\sum_{i=1}^{N}\\mathbb{I}\\{\\ell_{i}=1\\}}\\end{array}$ , with $n_{0}\\geq1$ , $n_{1}\\geq1$ . We assume in the following that each $\\hat{g}_{\\nu_{b}}(\\mathbf{Z})$ uses a bootstra pped sample from the or iginal data, as Random Forest does. The class-wise OOB error of such an ensemble of learners trained on $N$ observations is defined as\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\mathcal{E}_{0}^{o o b}=\\frac{1}{n_{0}}\\sum_{i=1}^{N}\\mathbb{I}\\{\\ell_{i}=0\\}\\mathbb{I}\\{\\hat{\\pmb{g}}_{-i}(\\mathbf{Z}_{i})\\neq0\\},$$\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\begin{array}{l}{\\displaystyle\\mathcal{E}_{1}^{o o b}=\\frac{1}{n_{1}}\\sum_{i=1}^{N}\\mathbb{I}\\{\\ell_{i}=1\\}\\mathbb{I}\\{\\hat{\\pmb{g}}_{-i}(\\mathbf{Z}_{i})\\neq1\\},}\\ {\\displaystyle\\mathcal{E}_{p}^{o o b}=(1-p)\\mathcal{E}_{0}^{o o b}+p\\mathcal{E}_{1}^{o o b},}\\end{array}$$\n\n<PARAGRAPH_SEPARATOR>\n\nwhere $\\hat{g}_{-i}$ , represents the average over the ensemble of learners not containing the $i^{\\mathrm{th}}$ observation for training.\n\n<PARAGRAPH_SEPARATOR>\n\nUnfortunately, the test statistic $\\mathcal{E}_{1/2}^{o o b}$ is difficult to handle; due to the complex dependency structure between the elements of the sum, it is not clear what the (asymptotic) distribution under the null is. For theoretical purposes, we consider in Section 3 a solution based on the concept of U-statistics. Here, we recommend using the OOB error together with a permutation test. See e.g., Good (1994) or Kim et al. (2021), who use it in conjunction with the out-of-sample error evaluated on a test set: We first calculate the class-wise OOB errors $\\mathcal{E}_{0}^{o o b},\\mathcal{E}_{1}^{o o b}$ and then reshuffle the labels $K$ times to obtain $K$ permutations, $\\sigma_{1},\\dots,\\sigma_{K}$ say. For each of these new datasets $\\left(\\mathbf{Z}_{i},\\ell_{\\sigma_{k}\\left(i\\right)}\\right)_{i=1}^{N}$ , $k\\in\\{1,\\ldots,K\\}$ , we calculate the OOB errors\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\mathcal{E}_{j}^{o o b,k}:=\\frac{1}{n_{j}}\\sum_{i=1}^{N}\\mathbb{I}\\{\\ell_{\\sigma_{k}(i)}=j\\}\\mathbb{I}\\{\\hat{\\mathbf{g}}_{-i}(\\mathbf{Z}_{i})\\neq\\ell_{\\sigma_{k}(i)}\\},$$\n\n<PARAGRAPH_SEPARATOR>\n\nfor $j\\in\\{0,1\\}$ . Under $H_{0}$ , $(\\ell_{1},\\dots,\\ell_{N})$ and $(\\mathbf{Z}_{1},\\ldots,\\mathbf{Z}_{N})$ are independent and each $\\mathcal{E}_{1/2}^{o o b}$ is simply an iid draw from the distribution $F$ of the random variable $\\mathcal{E}_{1/2}^{o o b}|({\\mathbf{Z}}_{1},\\ldots,{\\mathbf{Z}}_{N})$ . As such we can accurately approximate the $\\alpha$ quantile $F^{-1}(\\alpha)$ of said distribution by performing a large number of permutations and use the decision rule\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\delta_{o o b}(D_{N})=\\left\\{\\mathcal{E}_{1/2}^{o o b}\\leq F^{-1}(\\alpha)\\right\\}.$$\n\n<PARAGRAPH_SEPARATOR>\n\nThus, as in the decision in Equation (2), the rejection region depends on the data at hand. Nonetheless, the level will be conserved, as proven e.g. in Hemerik and Goeman (2018, Theorem 1).\n\n<PARAGRAPH_SEPARATOR>\n\nHeuristically, this procedure will have power under the alternative, as in this case there is some dependence between $(\\ell_{1},\\dots,\\ell_{N})$ and $(\\mathbf{Z}_{1},\\ldots,\\mathbf{Z}_{N})$ , formed by the difference in the distribution of the $\\mathbf{Z}_{i}$ . The OOB error $\\mathcal{E}_{1/2}^{o o b}$ will thus be different than those observed under permutations.\n\n<PARAGRAPH_SEPARATOR>\n\nThe whole procedure is described in Algorithm 2. We name this test “hypoRF”."
  },
  {
    "qid": "statistic-posneg-1646-7-1",
    "gold_answer": "FALSE. The formula $l_{j,j+r}=l_{j,j+r-1}+C_{j+r-1}$ is incorrect as it does not properly account for the observations in the downward steps. The correct formula should include the mean observations for both upward and downward transitions, as well as the final three observations in the downward step from peak $k$ to $k-1$. The provided formula only accumulates the upward transitions ($C_{j+r-1}$) and misses the downward observations.",
    "question": "Does the general formula $l_{j,j+r}=l_{j,j+r-1}+C_{j+r-1}$ for $r\\geqslant2$ correctly account for the cumulative mean observations from valley $j$ to peak $j+r$?",
    "question_context": "Expected Observations in Sequential Quantal Response Estimation\n\nIt is desirable to have some idea of the number of observations required in an experiment. Given below is a procedure which can be used for calculating the mean number of observations required when we start at a valley and continue for $\\pmb{\\mathscr{n}}$ subsequent valleys, using UDTR rule number 2 of Table ${\\mathfrak{a}}a$ . This procedure follows from the finite Markov chain approximation outlined in $\\S5$ . The theory was checked by reference to empirical sampling trials results, such as those given in the penultimate column of Table 4.<PARAGRAPH_SEPARATOR>Let<PARAGRAPH_SEPARATOR>$$G_{j}(\\nu)=\\frac{(1-p_{j})\\nu+p_{j}(1-p_{j})\\nu^{2}+p_{j}^{8}(1-p_{j})\\nu^{8}}{(1-p_{j}^{8})}.$$<PARAGRAPH_SEPARATOR>This is the probability generating function of the number of observations in a run, given transition $j\\ t o j+1$ ;and note that $\\nu^{8}$ is the probability generating function of the number of observations in a run, given transition $j$ to ${\\dot{\\jmath}}-1$<PARAGRAPH_SEPARATOR>$C_{j}=G_{j}^{\\prime}(1)$ is the mean number of observations given transition $j$ to $(j+1)$ ,then<PARAGRAPH_SEPARATOR>$$C_{j}=\\frac{(1-p_{j})(1+2p_{j}+3p_{j}^{8})}{(1-p_{j}^{8})},$$<PARAGRAPH_SEPARATOR>whereas the mean number of observations given transition $j$ to ${\\dot{\\jmath}}-1$ is3·0.<PARAGRAPH_SEPARATOR>Let $\\iota_{j k}$ be the mean number of observations given that $j$ is a valley and $\\pmb{k}$ is the next peak. Then $\\ell_{j k}$ includes the three observations in the final downward step $\\pmb{k}$ to $\\pmb{k}-\\mathbf{1}$ but excludes the number of observations in the frst upward step $j$ to ${\\mathfrak{j}}+1$ . Therefore, we have<PARAGRAPH_SEPARATOR>$$\\begin{array}{l}{{l_{j,j+1}=3\\cdot0,}}\\ {{l_{j,j+2}=C_{j+1}+3\\cdot0,}}\\ {{l_{j,j+3}=C_{j+1}+C_{j+3}+3\\cdot0=l_{j,j+2}+C_{j+2};}}\\end{array}$$<PARAGRAPH_SEPARATOR>in general<PARAGRAPH_SEPARATOR>$$l_{j,j+r}=l_{j,j+r-1}+C_{j+r-1}\\quad(r\\geqslant2).$$<PARAGRAPH_SEPARATOR>Let $q_{j k}$ denote the probability that the next peak is at $\\pmb{k}$ given that the current state is 8 valley at $j$ . Then write $\\lambda_{j k}=q_{j k}l_{j k}$ thus defining an $m\\times m$ matrix, $\\pmb{\\lambda}$<PARAGRAPH_SEPARATOR>Similarly,if $u_{j k}$ isthe mean number of observations given that $\\pmb{j}$ is 8 peak and $\\pmb{k}$ is the next valley, then we can define another $m\\times m$ matrix, $\\upmu$ 88<PARAGRAPH_SEPARATOR>$$\\mu_{j k}=\\gamma_{j k}u_{j k},$$<PARAGRAPH_SEPARATOR>where<PARAGRAPH_SEPARATOR>$$u_{j k}=\\alpha_{k}+(j-k-1)^{3}~(k<j),$$<PARAGRAPH_SEPARATOR>and where $r_{j k}$ denotes the probability that the next valley is at $\\pmb{k}$ given that the current state is & peak at $\\pmb{j}$ . Now if we start at a valley and stop at the next valley, i.e. after two changes, then $\\mathbf{v}=\\mathbf{A}\\mathbf{B}$ , where A and $\\mathbf{B}$ are as defined in $\\S5$ .The elements ${\\pmb v}_{\\pmb{j}\\pmb{k}}$ of $\\mathbf{v}$ &re valley to valley transition probabilities for $j,k=1,...,m-1$<PARAGRAPH_SEPARATOR>Let $\\mathbf{C}=\\lambda\\mathbf{B}+\\mathbf{A}\\mu$ . Then elements $c_{j k}$ $\\mathbf{c}$ are $v_{j k}\\times$ (mean number of observations, given that $j$ is a valley and $\\pmb{k}$ is the next valley).<PARAGRAPH_SEPARATOR>If $\\pmb{\\pi}$ be a row vector giving the distribution of valleys, then<PARAGRAPH_SEPARATOR>$${\\pmb{\\pi}}=({\\pmb{\\pi}}_{1},{\\pmb{\\pi}}_{2},...,{\\pmb{\\pi}}_{m-1}).$$<PARAGRAPH_SEPARATOR>Suppose we start at a valley and stop after $\\pmb{\\mathscr{n}}$ subsequent valleys, then the mean number of observations is approximately nrC1, where 1 is 8 column vector of 1's.<PARAGRAPH_SEPARATOR>Table 12 shows that we need almost twice a8 many observations for the estimation of $\\pmb{L_{0\\cdot8\\otimes1}}$ a8 comp&red withthose for $\\pmb{L_{0:784}}$<PARAGRAPH_SEPARATOR>Table 12. Mean number of observations required when we start at a valley and stop at the next valley (i.e. for two changes). Starting value of the first valley a8 shown. Spacing 1·0"
  },
  {
    "qid": "statistic-posneg-341-0-1",
    "gold_answer": "TRUE. According to Brown's sufficient condition, the generalized Bayes estimator with respect to $G(\\|\\mu\\|)$ is admissible when $b\\leq1$ and inadmissible when $b>1$. This aligns with the quasi-admissibility and quasi-inadmissibility conditions derived in the paper, where $b<1$ implies quasi-admissibility and $b>1$ implies quasi-inadmissibility.",
    "question": "Does the generalized Bayes estimator with respect to $G(\\|\\mu\\|)$ become inadmissible when $b>1$, as implied by Brown's sufficient condition?",
    "question_context": "Quasi-Admissibility Boundary in Generalized Bayes Estimation\n\nA1 $\\phi(0)=0$ and $\\phi(w)\\geq0$ for any $w\\geq0$ ; A2 $\\phi(w)$ has at most finitely many local extrema; A3 lim $\\mathrm{inf}_{w\\to\\infty}w\\phi^{\\prime}(w)/\\phi(w)\\geq0$ and lim $\\begin{array}{r}{\\operatorname*{sup}_{w\\to\\infty}w\\phi^{\\prime}(w)/\\phi(w)\\leq1.}\\end{array}$ <PARAGRAPH_SEPARATOR> Assumption A2 assumes that $\\phi$ does not oscillate excessively and that $\\scriptstyle\\operatorname*{lim}_{w\\to\\infty}\\phi(w)$ exists. As far as we know, Assumptions A1–A3 cover all minimax and smooth estimators in the literature including [10–12]. Assumptions A1–A3 are also satisfied by linear estimators of the form $\\delta(X)=\\alpha X$ for $\\alpha\\in[0,1]$ and for which $\\phi(w)=(1-\\alpha)w$ . These estimators are unique proper Bayes and admissible in the normal case for $\\alpha\\in[0,1)$ . We emphasize that while we address quasiadmissibility and inadmissibility only for $\\delta_{\\phi}$ for $\\phi\\in\\varPhi_{A}\\subset\\phi$ , we allow competitive estimators of the form $\\delta_{\\phi+g}$ for $g\\in\\varPhi$ . <PARAGRAPH_SEPARATOR> In Section 2 we will show the following result, which establishes <PARAGRAPH_SEPARATOR> $$ \\phi(w)=\\frac{p-2}{n+2}-\\frac{\\beta_{\\star}}{\\ln w} $$ <PARAGRAPH_SEPARATOR> as the asymptotic boundary between quasi-admissibility and quasi-inadmissibility where <PARAGRAPH_SEPARATOR> $$ \\beta_{\\star}=\\frac{d_{n}(1+c_{p,n})}{2}=\\frac{2(p+n)}{(n+2)^{2}}. $$ <PARAGRAPH_SEPARATOR> Quasi-admissibility: If $\\phi\\in\\varPhi_{A}$ and there exists $w_{*}$ and $b<1$ such that <PARAGRAPH_SEPARATOR> $$ \\forall_{w\\geq w_{*}}\\quad\\phi(w)\\geq\\frac{p-2}{n+2}-b\\frac{\\beta_{\\star}}{\\ln w}, $$ <PARAGRAPH_SEPARATOR> then $\\delta_{\\phi}$ is quasi-admissible. <PARAGRAPH_SEPARATOR> Quasi-inadmissibility: If $\\phi\\in\\varPhi_{A}$ and there exists $w_{*}$ and $b>1$ such that <PARAGRAPH_SEPARATOR> $$ \\forall_{w\\geq w_{*}}\\quad\\phi(w)\\leq\\frac{p-2}{n+2}-b\\frac{\\beta_{\\star}}{\\ln w}, $$ <PARAGRAPH_SEPARATOR> then $\\delta_{\\phi}$ is quasi-inadmissible (and hence inadmissible). <PARAGRAPH_SEPARATOR> In Section 3, we find a generalized Bayes estimator with asymptotic behavior <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{w\\to\\infty}\\ln w\\left\\{\\frac{p-2}{n+2}-\\phi(w)\\right\\}=b\\beta_{\\star}, $$ <PARAGRAPH_SEPARATOR> for all $b>0$ . The corresponding generalized prior is given by <PARAGRAPH_SEPARATOR> $$ {\\frac{1}{\\sigma^{2}}}\\times{\\frac{1}{\\sigma^{p}}}G(\\|\\theta\\|/\\sigma) $$ <PARAGRAPH_SEPARATOR> with <PARAGRAPH_SEPARATOR> $$ G(\\|\\mu\\|)=\\int_{0}^{1}\\left(\\frac{\\lambda}{1-\\lambda}\\right)^{p/2}\\exp\\left(-\\frac{\\lambda}{1-\\lambda}\\frac{\\|\\mu\\|^{2}}{2}\\right)\\lambda^{-2}\\left(\\ln\\frac{1}{\\lambda}\\right)^{b}\\mathrm{d}\\lambda. $$ <PARAGRAPH_SEPARATOR> Hence, $b<1$ and $b>1$ imply quasi-admissibility and quasi-inadmissibility, respectively, of the associated generalized Bayes estimators. Interestingly, the boundary $b=1$ also appears in the known $\\sigma^{2}$ case when estimating $\\mu$ with $Z\\sim\\mathcal{N}_{p}(\\mu,I_{p})$ . By using Brown’s sufficient condition [2], the generalized Bayes estimator with respect to $G(\\|\\mu\\|)$ above is admissible (resp. inadmissible) when $b\\leq1$ (resp. $b>1$ ). This nice correspondence leads naturally to the conjecture: a quasi-admissible generalized Bayes estimator satisfying (9) is admissible. <PARAGRAPH_SEPARATOR> An extension to the general class of spherically symmetric distributions is briefly considered in Section 2.1. We give some concluding remarks in Section 4. Some technical proofs are given in Appendix. <PARAGRAPH_SEPARATOR> # 2. Quasi-admissibility <PARAGRAPH_SEPARATOR> The main result of this paper, Theorem 1, gives sufficient conditions for quasi-admissibility and quasi-inadmissibility for estimators $\\delta_{\\phi}$ of the form (2), for $\\phi\\in\\varPhi_{A}$ . In preparation, we first give several lemmas. Recall that the unbiased estimator of the difference in risk between $\\delta_{\\phi}$ and $\\delta_{\\phi+g}$ is given by <PARAGRAPH_SEPARATOR> $$ (n+2)\\varDelta(w;\\phi,g)=(n+2)\\lbrace D_{\\phi}(w)-D_{\\phi+g}(w)\\rbrace=(n+2)g(w)\\lbrace\\varDelta_{1}(w;\\phi)+\\varDelta_{2}(w;\\phi,g)\\rbrace $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ \\varDelta_{1}(w;\\phi)=2\\frac{c_{p,n}-\\phi(w)}{w}+d_{n}\\phi^{\\prime}(w) $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\varDelta_{2}(w;\\phi,g)=-\\frac{g(w)}{w}+d_{n}g^{\\prime}(w)+d_{n}\\frac{g^{\\prime}(w)}{g(w)}\\{1+\\phi(w)\\}, $$ <PARAGRAPH_SEPARATOR> and where $c_{p,n}=(p-2)/(n+2)$ and $d_{n}=4/(n+2)$ . Note that $\\varDelta_{2}(w;\\phi,g)$ is well-defined for $w$ such that $g(w)\\neq0$ , but $\\varDelta(w;\\phi,g)$ is well-defined even when $g(w)=0$ . The first lemma gives necessary conditions on $g(w)$ for $\\varDelta(w;\\phi,g)$ to be nonnegative for all $w\\geq0$ . <PARAGRAPH_SEPARATOR> Lemma 1. Suppose $\\varDelta(w;\\phi,g)\\geq0$ for all $w\\geq0$ with $\\phi\\in\\varPhi_{A}$ and $g\\in\\varPhi$ . Then <PARAGRAPH_SEPARATOR> B1 $g(w)\\geq0$ for all $w\\ge0$ ;   B2 if $g(w_{0})>0$ , then, for any $w\\ge w_{0},g(w)>0.$ ."
  },
  {
    "qid": "statistic-posneg-2132-4-2",
    "gold_answer": "TRUE. Table 9 shows that the AD statistic consistently has the lowest RMSE values for θ1, θ2, and k compared to the other EDF statistics (ADL, CM, KS, etc.). For example, for θ1, the AD statistic has an RMSE of 0.258, which is lower than the next best (ADL at 0.265). Similarly, for θ2 and k, the AD statistic also has the lowest RMSE values (0.178 and 0.367, respectively), supporting the claim that it is the most efficient estimator among the tested methods.",
    "question": "The paper suggests that the AD statistic provides the most efficient estimator for the unknown parameters in the first GPD model. Is this supported by the data in Table 9?",
    "question_context": "Generalized Pareto Distribution (GPD) Model Estimation with Covariates\n\nThe paper discusses the use of the Moment Generating Function (MGF) estimation method to fit generalized linear models based on the Generalized Pareto Distribution (GPD). Two models are presented, each incorporating covariates into the GPD parameters. The first model uses a scale parameter θ that varies linearly with a covariate Xi, while the second model allows both the scale and shape parameters to vary with Xi. The MGF method is highlighted as uniquely capable of handling these heterogeneous cases, unlike other methods such as MOM, PWM, EPM, and QML. The performance of MGF estimators is evaluated using bias and RMSE metrics across different EDF statistics."
  },
  {
    "qid": "statistic-posneg-1962-0-0",
    "gold_answer": "TRUE. The context explicitly states that right-censoring is obtained by setting $\\gamma_{q}$ to the censoring level and $\\delta_{q}=\\infty$. This simplifies the formula by making the second term inside the inverse normal CDF ($\\Phi^{-1}$) approach zero, effectively handling the right-censoring scenario.",
    "question": "The formula $$v_{q j}=\\mu_{q j}+\\sigma_{q}\\Phi^{-1}\\left(\\Phi\\left(\\frac{\\gamma_{q}-\\mu_{q j}}{\\sigma_{q}}\\right)(1-R_{q j})+\\Phi\\left(\\frac{\\delta_{q}-\\mu_{q j}}{\\sigma_{q}}\\right)R_{q j}\\right),$$ can be simplified to handle right-censoring by setting $\\gamma_{q}$ to the censoring level and $\\delta_{q}=\\infty$. Is this statement true based on the provided context?",
    "question_context": "Truncated Gaussian Models for Censored Spatial Data\n\nThe basic idea behind all the procedures used here is that, given $\\mathbf{U}=\\mathbf{u}$ and $\\mathbf{v}=$ v, all of the necessary calculations are straightforward. In this sense, these procedures share a common feature with data augmentation schemes such as the EM algorithm (Dempster, Laird, and Rubin 1977) and the IP algorithm of Tanner and Wong (1987). Indeed, the IP algorithm could be used to calculate the posterior distribution for $\\pmb\\theta$ in some circumstances. In particular, when the mean m and the scale c are the only unknown parameters, the posterior step can be carried out easily if the prior distribution on $\\textbf{\\em m}$ and $c$ is the standard conjugate normal-gamma prior. The imputation step could be done by generating $\\mathbf{v}$ under the density $h_{\\theta}$ , as in Section 4, and using importance sampling as discussed by Tanner and Wong (1987, p. 548). As Dempster (1987) noted, however, when Monte Carlo simulations are needed to implement the $\\mathbf{IP}$ algorithm, it is not clear that there is any advantage to using this algorithm as opposed to a noniterative scheme like the one used here.<PARAGRAPH_SEPARATOR>![](images/5050b26c6e4bafa157a30b6de13d103235d3d04b8855c170173615642b0ef0c6.jpg)  \nFigure 6. Predictive (Empty Bars) and Plug-In (Shaded Bars) Distributions of $W(\\cdot)$ at theFourSitesMarked inFigure1.<PARAGRAPH_SEPARATOR>The basic model used here can be easily extended to allow the censoring point to vary with location and to include both left- and right-censoring. Another type of censoring is interval censoring, in which a quantity is only known to fall in some finite interval. Dubrule and Kostov (1986) discussed how interval censoring can occur when driling exploratory oil wells. If the process controlling the censoring can be assumed independent of the process of interest, and the process of interest can be transformed to Gaussian, then the procedures used here can be applied with minor modification. For example, if it is known that $\\gamma_{q}\\le V_{q}\\le\\delta_{q}$ , then (2.4) should be modified to read<PARAGRAPH_SEPARATOR>$$v_{q j}=\\mu_{q j}+\\sigma_{q}\\Phi^{-1}\\left(\\Phi\\left(\\frac{\\gamma_{q}-\\mu_{q j}}{\\sigma_{q}}\\right)(1-R_{q j})+\\Phi\\left(\\frac{\\delta_{q}-\\mu_{q j}}{\\sigma_{q}}\\right)R_{q j}\\right),$$<PARAGRAPH_SEPARATOR>which gives back (2.4) when $\\gamma_{q}=-\\infty$ and $\\delta_{q}=0$ . Right-censoring is obtained by setting $\\gamma_{q}$ to the censoring level and $\\delta_{q}=\\infty$<PARAGRAPH_SEPARATOR>One limitation of the simulation methodology developed here is that it cannot be applied directly to very large data sets. Indeed, even when untruncated Gaussian models are used, it is generally very difficult to do the necessary matrix calculations for data sets with more than a few hundred observations. In the geostatistical literature, the standard way to reduce the computations is to allow the prediction to depend only on observations near the site at which one is predicting. This same idea can be used with the truncated Gaussian model. If predictions are to be made at many different locations, then a separate simulation would have to be done for each set of observations being used to make a prediction at an unobserved location. Thus, predictions at nearby locations should be done using the same set of observations to minimize the number of simulations needed. For calculating likelihood functions, Vecchia (1988) described a method of approximating the likelihood function for large data sets and Gaussian models. It may be possible to combine his approach with the simulation methods used here to approximate likelihoods for large data sets using truncated Gaussian models.<PARAGRAPH_SEPARATOR># ACKNOWLEDGMENTS"
  },
  {
    "qid": "statistic-posneg-1220-1-3",
    "gold_answer": "TRUE. Proposition 2 states that $G_{1}$ has this probability density function, which is derived from the asymptotic behavior of $N_{n}^{u}$ under the nonstationary functional process.",
    "question": "Is the probability density function of $G_{1}$ given by $g_{1}(u) = \\frac{1}{\\sqrt{\\pi}} \\exp(-u^{2}/4)$ for $u \\geq 0$?",
    "question_context": "Functional Record Analysis in Time Series\n\nThe paper discusses the properties of functional records in time series analysis. It defines upper and lower functional records and studies their behavior over time. The counting process for upper functional records, $N_{i}^{u}$, is analyzed under different conditions: independent and identically distributed (iid) sequences, stationary functional time series, and nonstationary functional processes. The growth rates of $N_{i}^{u}$ are derived for these cases, showing $O(\\log i)$ for iid and stationary series, and $O(\\sqrt{n})$ for nonstationary processes. The paper also provides visualizations and theoretical results supporting these growth rates."
  },
  {
    "qid": "statistic-posneg-389-0-1",
    "gold_answer": "FALSE. The paper explicitly discourages interpreting the chi-squared test for two samples as a contingency table analysis. It argues that doing so confuses the differences in assumptions and constraints between the two methods. The paper prefers to view the test as a comparison of the difference in relative frequencies between two samples and a parent population, emphasizing the need to clearly define the parent population's characteristics.",
    "question": "Does the paper suggest that the chi-squared test for two samples should be treated as a contingency table analysis?",
    "question_context": "Goodness of Fit Test for Two Samples\n\nThe paper discusses the application of the chi-squared test for comparing two samples to a parent population. It emphasizes the importance of considering the parent population's characteristics when applying the test. The formulas provided illustrate the calculation of the chi-squared statistic and its distribution under specific conditions. The paper also cautions against misinterpreting the test as a contingency table analysis, highlighting the differences in assumptions and constraints between the two approaches."
  },
  {
    "qid": "statistic-posneg-2137-4-1",
    "gold_answer": "FALSE. A p-value of .557 is much higher than conventional significance levels (e.g., 0.05), indicating that the data does not provide strong evidence against the null hypothesis. The context states that the p-value 'favors the said hypothesis' (i.e., y > 1) because y=1.0468 > 1, but this does not imply statistical significance—it merely means the point estimate aligns with the alternative hypothesis directionally.",
    "question": "The p-value of .557 obtained from the permutation test for the hypothesis that bodyfat (y) is larger than one when $x_{1}=71$ and $x_{2}=-209.25$ strongly supports rejecting the null hypothesis. True or False?",
    "question_context": "Multivariate Isotonic Regression Estimation and Hypothesis Testing\n\nFor this data also, we compute the $p$ -value of the test based on $T_{n}$ to check whether bodyfat (i.e., y) is larger than one or not when $x_{1}=71$ and $x_{2}=-209.25$ . To compute the $p$ -value, as we did for Auto MPG dataset, we carry out well-known permutation test and obtain the $p$ -value $=.557$ (i.e., favors the said hypothesis) as expected since for $x_{1}=71$ and $x_{2}=-209.25$ , $y=1.0468>1$ . <PARAGRAPH_SEPARATOR> # 6 CONCLUDING REMARKS <PARAGRAPH_SEPARATOR> In this article, we propose a least squares estimator of the multivariate isotonic regression function and study its asymptotic properties along with applications in the testing of hypothesis and the formulation of the pointwise confidence interval. In this context, we would like to point out that even for the smooth (i.e., differentiable) multivariate isotonic regression function, our estimator is not smooth enough; strictly speaking, it is not a differentiable function. To overcome this issue, one may consider the kernalized version of the IRE described here, which can be defined as follows. <PARAGRAPH_SEPARATOR> $$ \\hat{f}_{n,s m}(u_{1}^{*},\\ldots,u_{d}^{*})=\\frac{1}{n h_{n}}\\sum_{i=1}^{n}k\\left(\\frac{\\mathbf{u}^{*}-\\mathbf{x}_{i}}{h_{n}}\\right)\\hat{f}_{n}(u_{1}^{*},\\ldots,u_{d}^{*}), $$ <PARAGRAPH_SEPARATOR> where $k$ is a sufficiently smooth kernel function with bandwidth ${\\bf\\alpha}=h_{n}$ , $\\mathbf{u}^{*}=(u_{1}^{*},\\ldots,u_{d}^{*})$ , and $\\mathbf{x}_{i}$ is the ith covariate. One can hope that under some conditions, the smoothness of $\\cdot\\hat{f}_{n,s m}(u_{1}^{*},\\ldots,u_{d}^{*})$ will be the same as that of the kernel $k$ , and the isotonicity property of the estimator depends on the choice of the kernel $k$ . <PARAGRAPH_SEPARATOR> The issue of robustness is another topic of research since $\\hat{f}_{n}(.)$ is an average based estimator, and hence, it is expected that $\\hat{f}_{n}(.)$ will be influenced by the presence of the outliers/influential observations. Technically speaking, the breakdown point or the influence function may give us an idea about how much the estimator will be robust against the outliers. Moreover, one may also consider the median or the trimmed mean type of estimator so that the estimator possess good robustness property. For instance, in the case of univariate isotonic regression function, Dhar (2016) showed that trimmed mean IRE may achieve $25\\%$ asymptotic breakdown point. <PARAGRAPH_SEPARATOR> In order to implement the test described in Section 3, one needs to compute a certain quantile of $T(\\mathbb{W})^{\\prime}(0,\\ldots,0)$ , which is not easily tractable. As we indicated earlier, note here that $T(\\mathbb{W})^{\\prime}(0,\\ldots,0)$ can be thought as a multivariate extension of well-known Chernoff's distribution; however, unlike the univariate Chernoff's distribution, the accurate computation of the quantiles of multivariate version of Chernoff's distribution is not available in the literature. For univariate case, the readers may see Groeneboom and Wellner (2001). Instead of the simulation-based procedure of computing quantiles of $T(\\mathbb{W})^{\\prime}(0,\\ldots,0)$ described in Section 4.2, it may be of future interest of research about how to compute the quantiles exactly of the distribution of $T(\\mathbb{W})^{\\prime}(0,\\ldots,0)$ like the univariate Chernoff's distribution."
  },
  {
    "qid": "statistic-posneg-2107-2-0",
    "gold_answer": "FALSE. The condition $(C^{\\prime}\\psi^{-1}C)_{ij} = 0$ actually implies that there are no pure collision paths between exogenous variables $i$ and $j$ of the first kind in the associated graph $G$. This is because the term $(C^{\\prime}\\psi^{-1}C)_{ij}$ sums over all possible paths involving endogenous variables (first kind paths), and setting it to zero ensures no such paths exist. The confusion might arise from misinterpreting the condition as implying the presence of such paths, when in fact it ensures their absence.",
    "question": "In the linear structural equation model (LSEM) described by $Y = BY + CX + E$, is it true that the condition $(C^{\\prime}\\psi^{-1}C)_{ij} = 0$ implies there are no pure collision paths between exogenous variables $i$ and $j$ of the first kind in the associated graph $G$?",
    "question_context": "Graphical Markov Property in Linear Structural Equation Models\n\nWe will now formulate the main result of this paper. The theorem was first formulated and proven by Koster (1996) and Spirtes (1995). The approach of the current paper, applying Pearl's $d$ -separation concept directly to the present type of graph, allows a straightforward proof of the theorem. <PARAGRAPH_SEPARATOR> # Theorem 1 <PARAGRAPH_SEPARATOR> Consider the linear equations system $Y=B Y+C X+E$ where $X$ is a vector of q exogenous variables, $Y$ is a vector of $p$ endogenous variables, $E$ is $a$ vector of $p$ latent residual variables, $B$ is a $p\\times p$ -matrix such that $I-B$ is invertible, $C$ is a $p\\times q$ -matrix, and <PARAGRAPH_SEPARATOR> $$ (E,X)\\sim N_{p+q}\\biggl(0,\\left[\\begin{array}{c c}{{\\psi}}&{{0}}\\\\ {{0}}&{{\\phi}}\\end{array}\\right]\\biggr). $$ <PARAGRAPH_SEPARATOR> The set of normal probability distributions of $(Y,X)$ which satisfy the linear equations system is  denotedby $\\pi$ Let $G=(V,F)$ be the graph associated with the equations system. Then every $P\\in\\varPi$ is $G$ Markov. <PARAGRAPH_SEPARATOR> Proof. Suppose $a$ $^b$ and $c$ are pairwise disjoint subsets of $V,$ and assume $^{a}$ and $b$ are separated by $c$ in $G$ $a$ and $b$ non-empty). We must show that it holds: $a\\perp b|c[P]$ ,for each $P\\in\\varPi$ .By proposition 2 we may assume that $d:=a\\cup b\\cup c$ is an ancestral set of $G$ .Now notice that in that case the variables in $d$ satisfy a subsystem of the full linear equations system. To be precise, define $e:=\\{j\\in d|j$ endogenous}, and $f:=\\{j\\in d|j$ exogenous}. Then $Y_{e}=B_{e e}Y_{e}+$ $C_{e f}X_{f}+E_{e}$ , and $\\{P_{d}|P\\in\\pi\\}=\\{Q|Q$ is a probability distribution of $(Y_{e},X_{f})$ satisfying the subsystem $Y_{e}=B_{e e}Y_{e}+C_{e f}X_{f}+E_{e}\\}$ . It follows that we may assume, without loss of generality, that $d=V$ , i.e. $a\\cup b\\cup c=V$ . Let $i\\in a$ $j\\in b$ be arbitrary. By lemma 3, there are no pure collision paths from $i$ to $j$ in $G$ We now distinguish three cases: <PARAGRAPH_SEPARATOR> 1. Variables $i$ and $j$ are both exogenous. Using lemma 5 (ii) in combination with lemma 1, we must show that, for all values of the free parameters in $C,\\quad\\varPsi.$ and $\\phi$ ，it holds $({\\Sigma}^{-1})_{i j}=(C^{\\prime}{\\bar{\\psi}}^{-1}C+{\\varPhi}^{-1})_{i j}=0$ . This means that $(C^{\\prime}\\psi^{-1}C)_{i j}=0$ , and $(\\phi^{-1})_{i j}=0$ (for all values of the free parameters in $C,\\psi,$ and $\\phi$ ). Now notice that in any graph associated with a linear equations system, there are only two possibilities for a pure collision path $\\pi=(e_{1},....,e_{n})$ between  two exogenous  variables $i=i_{0}$ ， and $j=i_{n}$ .First, $n\\geqslant2$ $e_{1}=i\\rightarrow i_{1},~e_{k}=i_{k-1}\\leftrightarrow i_{k}$ $(1<k<n)$ ， $e_{n}=i_{n-1}\\leftarrow j_{}$ . and all intermediate points $i_{1},...,i_{n-1}$ of $\\pi$ are endogenous variables such that successive variables have correlated errors in the LSEM (since the arrow between successive intermediate points is two-headed). Second, $n\\geqslant1$ ， $e_{k}=i_{k-1}\\leftrightarrow i_{k}$ $1\\leqslant k\\leqslant n,$ , and all points $i=i_{0},\\ldots$ ， $i_{n}=j$ of $\\pi$ are exogenous  variables.  Using   corollary  7  w   obtain: $\\begin{array}{r}{(C^{\\prime}\\psi^{-1}C)_{i j}=\\sum_{k=1}^{p}\\sum_{l=1}^{p}}\\end{array}$ $c_{k i}(\\psi^{-1})_{k l}c_{l j}=0$ for all values of the free parameters in $C$ and $\\psi\\Leftrightarrow c_{k i}({\\psi}^{-1})_{k l}c_{l j}=0$ for all endogenous variables $k$ and $l_{:}$ . and for all values of the free parameters in $C$ and $\\varPsi\\Leftrightarrow$ in $G$ there are no pure collision paths between $i$ and $j$ of the frst kind. Furthermore, $(\\varPhi^{-1})_{i j}=0$ for all values of the free parameters in $\\boldsymbol{\\varPhi}\\Leftrightarrow$ in $G$ there are no pure collision paths between $i$ and $j$ of the second kind."
  },
  {
    "qid": "statistic-posneg-786-0-1",
    "gold_answer": "FALSE. The paper claims $\\mathcal{E}(\\tau)\\Delta^8$ equals $R(\\tau)$ times $\\theta$, but this is incorrect. The expectation density $\\mathcal{E}(\\tau)$ represents the conditional probability of an event at $\\tau$ given an event at 0, while $R(\\tau)$ is a second moment (average product of event counts at times separated by $\\tau$). They are related but not proportional via $\\theta$; $\\mathcal{E}(\\tau)$ typically includes a Dirac delta at $\\tau=0$ for the initial event, which $R(\\tau)$ lacks. The correct relationship involves the event rate and higher-order statistics, not simple multiplication by $\\theta$.",
    "question": "Does the expectation density function $\\mathcal{E}(\\tau)$ equal the autocorrelation function $R(\\tau)$ multiplied by the mean interval duration $\\theta$ of the combined process, as stated in the paper?",
    "question_context": "Superposition of Random Event Sequences: Interval Probability and Autocorrelation Analysis\n\nThe paper analyzes the superposition of multiple random event sequences, each characterized by interval probability density functions $p_i(\\tau)$. The combined sequence's interval probability density $\\mathcal{P}(\\tau)$ is derived, considering the probabilities of events from different sequences. The autocorrelation function $R(\\tau)$ and expectation density $\\mathcal{E}(\\tau)$ are also examined, with $\\mathcal{E}(\\tau)$ defined as the probability of an event occurring in $[\\tau, \\tau+d\\tau]$ given an event at $\\tau=0$. Special cases for $N=2$ sequences are explored, including applications to neural discharge data."
  },
  {
    "qid": "statistic-posneg-2067-2-1",
    "gold_answer": "FALSE. The EP metric does not measure the absolute difference in jump magnitudes directly. Instead, it measures the relative difference in the jump magnitudes between the true image $f$ and the denoised image $\\widehat{f}$, normalized by the jump magnitude of the true image $\\mathrm{JS}(f)$. This relative measure is used to assess how well the denoising method preserves edges and fine details compared to the original image.",
    "question": "Does the Edge Preservation (EP) metric $\\mathrm{EP}(\\widehat{f})=|\\mathrm{JS}(f)-\\mathrm{JS}(\\widehat{f})|/\\mathrm{JS}(f)$ measure the absolute difference in jump magnitudes between the true and denoised images?",
    "question_context": "Performance Evaluation of Image Denoising Methods via MISE and Edge Preservation Metrics\n\nIn this section, we present some numerical results concerning the performance of the proposed image denoising method, denoted as NEW, in comparison with three state-ofthe-art image denoising methods that are widely used in the literature. The three competing methods include the image denoising method based on total variation minimization (see Rudin, Osher, and Fatemi 1992), denoted as TV, the adaptive image smoothing method using the steering kernel (see Takeda, Farsiu, and Milanfar 2007), denoted as ASSK, and the optimized nonlocal means image denoising method (see Coupe et al. 2008), denoted as ONLM. The TV method has a regularization parameter, which controls the amount of smoothing and edge preservation. The ASSK method is accomplished by adaptive smoothing using various neighborhoods with different shapes and sizes determined by the observed image intensities. This is an iterative procedure, and it has two parameters to choose: one is a global smoothing parameter and the other is the number of iterations. The ONLM method has two bandwidth parameters and another smoothing parameter to choose. Our proposed denoising method NEW has three parameters, $h_{n},u_{n}$ , and $B_{n}$ , to choose. <PARAGRAPH_SEPARATOR> The numerical study presented here includes one artificial image, one real fingerprint image, and one real MRI of human brain. First, the true artificial image with $64\\times64$ pixels is presented in the first image of Figure 2. Its image intensities range from 0 to 1.3125. This image has a comb-like fine structure near the right boundary, several thin lines, and a rectangular object. Moreover, there is a small “L”-like structure in the lower-middle part of the image. Then, we generate noisy versions of the true artificial image by adding iid noise from the $N(0,\\sigma^{2})$ distribution with $\\sigma=0.05,0.10,0.15$ , and 0.20. These noisy versions are presented in the second, third, fourth, and fifth panels of Figure 2. <PARAGRAPH_SEPARATOR> Because the methods TV, ASSK, and ONLM do not provide data-driven procedures to chose their procedure parameters, to make a fair comparison, we search their procedure parameters by minimizing the estimated MISE value, defined to be the sample mean of <PARAGRAPH_SEPARATOR> $$ \\mathrm{ISE}(\\widehat{f},f)=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}(\\widehat{f}(x_{i},y_{j})-f(x_{i},y_{j}))^{2}, $$ <PARAGRAPH_SEPARATOR> computed from 100 replicated simulations, where $\\widehat{f}$ denotes the denoised image by a related denoising method. While the MISE criterion measures the overall performance of an image denoising procedure, it cannot tell us how well the edges and other fine details of the image are preserved. To measure the preservation of such fine details of the image, Hall and Qiu (2007) defined a measure of JS of an image. Its discretized version for the true image intensity function $f$ can be written as <PARAGRAPH_SEPARATOR> $$ \\mathbf{J}\\mathbf{S}(f)=\\frac{1}{(n-2)^{2}}\\sum_{i=2}^{n-1}\\sum_{j=2}^{n-1}|f(x_{i}^{\\prime},y_{j}^{\\prime})-f(x_{i},{'}^{\\prime}y_{j}^{\\prime\\prime})|, $$ <PARAGRAPH_SEPARATOR> where $(x_{i}^{\\prime},y_{j}^{\\prime})$ and $(x_{i}^{\\prime\\prime},y_{j}^{\\prime\\prime})$ are two immediately neighboring pixels of $(x_{i},y_{j})$ on its two different sides along the estimated gradient direction of $f$ at $(x_{i},y_{j})$ . Obviously, if $(x_{i},y_{j})$ is an edge pixel, then $|f(x_{i}^{\\prime},y_{j}^{\\prime})-f(x_{i}^{\\prime\\prime},y_{j}^{\\prime\\prime})|$ is close to the JS of $f$ at $(x_{i},y_{j})$ . If $(x_{i},y_{j})$ is a continuity pixel of $f$ , then $|\\bar{f}(x_{i}^{\\prime},y_{j}^{\\prime})-\\bar{f}(x_{i}^{\\prime\\prime},y_{j}^{\\prime\\prime})|$ is close to 0. Thus, $\\operatorname{JS}(f)$ is a reasonable measure of the accumulative jump magnitude of $f$ along the JLCs. After $\\widehat{f}$ is obtained by an image denoising method, we can compute $\\operatorname{JS}(\\widehat{f})$ using the estimated gr a dient directions of $f$ . Then, <PARAGRAPH_SEPARATOR> $$ \\mathrm{EP}(\\widehat{f})=|\\mathrm{JS}(f)-\\mathrm{JS}(\\widehat{f})|/\\mathrm{JS}(f) $$ <PARAGRAPH_SEPARATOR> is a reasonable measure of the edge preservation for the image denoising method in question. In the literature, there are a number of different methods to estimate the gradient of $f$ . Since we are interested in the gradient directions rather than their magnitudes, a computationally simple filter should serve our purpose well. In all numerical examples presented in this article, the $3\\times3$ Sobel filter (see Qiu 2005, Section 4.4.3) is used when estimating $f_{x}^{\\prime}$ and $f_{y}^{\\prime}$ . <PARAGRAPH_SEPARATOR> The numerical results for the artificial image are presented in Table 1, where the first row in each entry presents the estimated MISE value and its standard error (in parentheses), the second row presents the estimated EP value and its standard error (in parentheses), and the third row presents the searched procedure parameter values. When comparing two methods in terms of MISE, if their estimated MISE values are $\\mathrm{MSE}_{1}$ and ${\\mathrm{MSE}}_{2}$ with standard errors $\\mathrm{SE}_{1}$ and $\\mathrm{SE}_{2}$ , respectively, and if $\\mathrm{MISE}_{1}<\\mathrm{MISE}_{2}$ , then a commonly used practical guideline is that we conclude that method 1 is significantly better than method 2 when $\\mathrm{{MISE}}_{2}-\\mathrm{{MISE}}_{1}>\\nu(\\mathrm{SE}_{1}+\\mathrm{SE}_{2})$ , where $\\nu>0$ is a given number. In practice, people often choose $\\nu$ from the interval [1, 3]. In our numerical examples, we use $\\nu=2$ . Similar comparisons can be made among different methods in terms of EP. We consider the performance of the denoising method 1 to be better than method 2 if (i) $\\mathrm{MISE}_{1}<\\mathrm{MISE}_{2}$ significantly, or (ii) $\\mathrm{MISE}_{1}\\approx\\mathrm{MISE}_{2}$ and $\\mathrm{EP}_{1}<\\mathrm{EP}_{2}$ significantly. <PARAGRAPH_SEPARATOR> Table 1. In each entry, the first line presents the estimated MISE value based on 100 simulations and the corresponding standard error (in parentheses), the second line presents the value of EP and its standard error (in parentheses), and the third line presents the searched procedure parameter values. This table is about the artificial image shown in Figure 2"
  },
  {
    "qid": "statistic-posneg-199-1-1",
    "gold_answer": "FALSE. The correlation matrix $\\mathbf{R}\\left(\\Psi_{1}\\right)$ shows very high temporal correlations (0.97 to 0.98) for Group 1, which are actually slightly higher than those in Group 2 (0.89 to 0.91). The context confirms that both groups exhibit high temporal correlations, with Group 1's correlations being marginally stronger.",
    "question": "Does the correlation matrix $\\mathbf{R}\\left(\\Psi_{1}\\right)$ indicate lower temporal correlations for Group 1 compared to Group 2?",
    "question_context": "Matrix-Variate Contaminated Normal Mixtures: Correlation and Contamination Analysis\n\nWe now report the estimated parameters of the MVCNM model to analyze the differences among the two estimated groups. In the following, Group 1 identifies the families of bachelor’s degrees whereas Group 2 refers to the families of master’s degrees. Figure 4 displays the line plots of the estimated means for both groups over the three years but separately for the three variables. <PARAGRAPH_SEPARATOR> As we can see, the two groups are on average quite different when looking at the “Credits” and “Continue” variables, whereas they are bit more similar in the “Re-enroll” variable. Additionally, Group 1 is always worse than Group 2, suggesting that students of bachelor’s degrees face more difficulties than the (more experienced) master’s degrees students. We also observe that the mean of the “Credits” variable is increasing over time for Group 2 and slowly decreasing for Group 1. Conversely, with respect to the “Continue” variable, both groups show a similar average pattern, that is, a small increase in 2016 followed by a reduction in 2017. Lastly, when the “Re-enroll” variable is considered, we note that after an increase in 2016, the mean for Group 2 decreases, whereas it is gradually growing for Group 1. <PARAGRAPH_SEPARATOR> To evaluate the correlations of the variables with each other and over time, for the two groups, we now report the correlation matrices related to $\\pmb{\\Sigma}_{g}$ and $\\Psi_{g}$ , that is, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\mathbf{R}\\left(\\pmb{\\Sigma}_{1}\\right)=\\left(\\begin{array}{c c c}{1.00}&{0.44}&{-0.13}\\\\ {0.44}&{1.00}&{0.03}\\\\ {-0.13}&{0.03}&{1.00}\\end{array}\\right),}\\end{array} $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{{\\bf R}\\left({\\pmb{\\Sigma}}_{2}\\right)=\\left(\\begin{array}{c c c}{1.00}&{0.14}&{-0.17}\\\\ {0.14}&{1.00}&{0.10}\\\\ {-0.17}&{0.10}&{1.00}\\end{array}\\right),}\\end{array} $$ <PARAGRAPH_SEPARATOR> $$ \\mathbf{R}\\left(\\Psi_{1}\\right)=\\left(\\begin{array}{c c c}{1.00}&{0.98}&{0.97}\\\\ {0.98}&{1.00}&{0.98}\\\\ {0.97}&{0.98}&{1.00}\\end{array}\\right) $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\mathbf{R}\\left(\\Psi_{2}\\right)=\\left(\\begin{array}{c c c}{1.00}&{0.91}&{0.89}\\\\ {0.91}&{1.00}&{0.90}\\\\ {0.89}&{0.90}&{1.00}\\end{array}\\right). $$ <PARAGRAPH_SEPARATOR> When ${\\bf R}\\left({\\pmb{\\Sigma}}_{1}\\right)$ and $\\mathbf{R}\\left(\\pmb{\\Sigma}_{2}\\right)$ are considered, notice that the correlation between the “Credits” and “Continue” variables are quite different between the two groups. In both cases, and as it might be reasonable to expect, these correlations are positive, but Group 1 has a higher value. On the contrary, both groups have similar negative correlations between the “Credits” and “Re-enroll” variables. Finally, “Continue” and “Re-enroll” are practically uncorrelated for Group 1, and weakly positively correlated for Group 2. With respect to $\\mathbf{R}\\left(\\Psi_{1}\\right)$ and R $(\\Psi_{2})$ , there are high correlations between time points in both groups. <PARAGRAPH_SEPARATOR> The estimated proportion of typical points and degree of contamination for the first group are $\\hat{\\alpha}_{1}=0.89$ and $\\hat{\\eta}_{1}=7.49$ , respectively, whereas for the second group they are $\\hat{\\alpha}_{2}=0.98$ and ${\\hat{\\eta}}_{2}~=~49.33$ . Therefore, while there are more outlying observations in Group 1, the single atypical observation found in Group 2 is more severe, as reflected by the much greater value of the inflation parameter. These aspects can be better understood by looking at Table 3, where the degree families marked as atypical are reported, along with their estimated probabilities to be typical points. <PARAGRAPH_SEPARATOR> As we can see, the only study program flagged as being atypical for the second group, that is, “Geophysical sciences”, has a very small $\\hat{\\nu}_{i g}$ value. This means that the corresponding"
  },
  {
    "qid": "statistic-posneg-779-0-0",
    "gold_answer": "FALSE. The formula presented involves complex transformations and G-functions, but the specific form and the conditions under which it is derived are not universally applicable. The transformations and the resulting joint distribution depend heavily on the assumptions about the ordered variates and the specific cases considered (e.g., k+l=n or k+l=n-1). The paper's derivations are valid only under these specific conditions and may not generalize to other scenarios without further adjustments.",
    "question": "Is the condition for the joint distribution of transformed variates given by the formula involving G-functions and exponential terms correct, and does it accurately represent the transformation of ordered variates?",
    "question_context": "Distribution of the Extreme Deviate from the Sample Mean and Its Studentized Form\n\nThe paper discusses the distribution of the extreme deviate from the sample mean and its studentized form, focusing on the use of G-functions in deriving these distributions. It presents various formulas and transformations to handle ordered variates and derives the joint distribution of transformed variates. The paper also considers special cases where simplifications occur, allowing for more tractable forms of the distributions."
  },
  {
    "qid": "statistic-posneg-873-0-2",
    "gold_answer": "TRUE. The paper shows that the min function in model H1 can be replaced by the max function due to the algebraic relationship min(wi, wj) = wi + wj - max(wi, wj). This property allows for alternative formulations of the model that may reveal more compact representations of the response surface.",
    "question": "In the context of mixture models, is the function min(wi, wj) used in model H1 equivalent to the function max(wi, wj) when fitting the response surface?",
    "question_context": "Mixture Response Models and Additive Effects\n\nThe response to a mixture is taken to be dependent on the proportions of the components present but not on the total amount of mixture. The paper discusses various models for analyzing mixture responses, including polynomial models and homogeneous models of degree one. It highlights the challenges in interpreting coefficients when components have additive effects and proposes alternative models to address these issues."
  },
  {
    "qid": "statistic-posneg-657-2-2",
    "gold_answer": "TRUE. Theorem 1(3) clearly states that when $\\lambda\\eta\\to\\infty$, the estimators $\\hat{r}_{\\lambda,\\eta}^{(0)}(x)$ and $\\tilde{r}_{\\lambda,\\eta}^{N W}(x)$ converge to $\\tilde{r}_{l a d,\\eta}(x)$ with a convergence rate of $O_{p}((\\eta\\lambda)^{-1})$. This is further explained in part (b) of the theorem, which indicates that the new estimators adapt to the local additivity of the underlying regression function when $\\lambda$ and $\\eta$ are large enough.",
    "question": "Does Theorem 1(3) imply that when $\\lambda\\eta\\to\\infty$, the estimators $\\hat{r}_{\\lambda,\\eta}^{(0)}(x)$ and $\\tilde{r}_{\\lambda,\\eta}^{N W}(x)$ perform like the local additive estimator with a convergence rate of $O_{p}((\\eta\\lambda)^{-1})$?",
    "question_context": "Local-linear-additive-estimation convergence properties\n\nThen we have the following main theorem. Theorem 1. For model (1.1) with conditions (C1)–(C4), the local linear–additive estimator and local N-W-additive estimator have the following properties: (1) if $\\lambda\\eta\\to0$ , then for $x\\in[-1,1]^{p}$ , $$\\hat{r}_{\\lambda,\\eta}^{(0)}(x)=\\tilde{r}_{l l}(x)+O_{p}(\\lambda\\eta)\\quad a n d\\quad r_{\\lambda,\\eta}^{N W}(x)=\\tilde{r}^{N W}(x)+O_{p}(\\lambda\\eta).$$ (2) if $p\\leq8$ and $\\backslash\\eta\\leq c$ for a positive constant $c$ , then for $x\\in(-1,1)^{p}$ , $$\\hat{r}_{\\lambda,\\eta}^{(0)}(x)=r(x)+O_{p}\\left(h^{2}+\\frac{1}{\\sqrt{n h^{p}}}\\right)\\textit{\\textbf{a n d}}\\widetilde{r}_{\\lambda,\\eta}^{N W}(x)=r(x)+O_{p}\\left(h^{2}+\\frac{1}{\\sqrt{n h^{p}}}\\right).$$ (3) if $\\lambda\\eta\\to\\infty$ , then for $x\\in[-1,1]^{p}$ , $$\\hat{r}_{\\lambda,\\eta}^{(0)}(x)=\\tilde{r}_{l a d,\\eta}(x)+O_{p}((\\eta\\lambda)^{-1})\\quad a n d\\quad\\tilde{r}_{\\lambda,\\eta}^{N W}(x)=\\tilde{r}_{l a d,\\eta}(x)+O_{p}((\\eta\\lambda)^{-1}).$$ (4) if model (1.1) is globally additive and $\\lambda\\eta\\to\\infty$ , then for $x\\in(-1,1)^{p}$ , $$\\begin{array}{l}{{\\hat{r}_{\\lambda,\\eta}^{(0)}(x)=r(x)+{\\cal O}_{p}((\\eta\\lambda)^{-1})+{\\cal O}_{p}\\left(h^{2}+\\displaystyle\\frac{1}{\\sqrt{n\\eta^{p-1}h}}\\right)}}\\ {{}}\\ {{a n d\\quad\\tilde{r}_{\\lambda,\\eta}^{N W}(x)=r(x)+{\\cal O}_{p}((\\eta\\lambda)^{-1})+{\\cal O}_{p}\\left(h^{2}+\\displaystyle\\frac{1}{\\sqrt{n\\eta^{p-1}h}}\\right).}}\\end{array}$$ By the theorem, we have the following findings: (a) the conclusions in Theorem 1(1) and (2) together imply that for the case without global additive structure, the new estimators perform like the local linear estimator (or N-W estimator) and therefore they can achieve the optimal (or the standard) convergence rate provided that $\\lambda$ and $\\eta$ are not very large. For achieving this goal, the theoretical constraints on $\\lambda$ and $\\eta$ are quite mild, for example $\\lambda$ , $\\eta\\in[0,c]$ with $c$ being an arbitrary fixed positive constant. By comparison, the new estimators outperform the additive estimators (e.g., the backfitting estimator and marginal integral estimator) when the model under study is nonadditive; (b) the results of Theorem 1(3) indicate that when $\\lambda$ and $\\eta$ is large enough, the new estimators perform like the local additive estimator and therefore they can adapt to the local additivity of the underlying regression function;"
  },
  {
    "qid": "statistic-posneg-367-0-2",
    "gold_answer": "TRUE. The negative binomial mixture model exhibits overdispersion because its variance $r\\left(1-p_{i}\\right)/p_{i}^{2}$ is greater than its mean $r\\left(1-p_{i}\\right)/p_{i}$ (since $p_{i} < 1$). This contrasts with the Poisson mixture model, where the variance equals the mean. The degree of overdispersion depends on the values of $r$ and $p_{i}$.",
    "question": "Does the negative binomial mixture model always exhibit overdispersion compared to the Poisson mixture model?",
    "question_context": "Robustness of Mixture Complexity Estimation under Model Misspecification\n\nIn conclusion, when the postulated model is correct, our MHD-based method is competitive with the LRT method of KX (1999) in that it is very successful in correctly determining the mixture complexity if the models are well separated and sample sizes are large enough. <PARAGRAPH_SEPARATOR> # 5.2. Robustness of $\\hat{m}_{n}$ under model misspecification <PARAGRAPH_SEPARATOR> Here, we describe an approach to assess the robustness of $\\hat{m}_{n}$ in terms of its ability to correctly identify the true mixture complexity, when the postulated Poisson mixture model is incorrect. Generally, one examines the robustness of MHD estimators against $100\\%$ gross-error contaminated mixture models and using $\\alpha$ -influence functions defined in terms of Hellinger functionals (Beran, 1977). KX (1998, see section 6.3) postulated a 2-component Poisson mixture model, $f_{\\pmb{\\theta}_{2}}(x)=\\pi f\\left(x|\\lambda_{1}\\right)+\\left(1-\\pi\\right)f\\left(x|\\lambda_{2}\\right)$ , where $f$ denotes a Poisson p.m.f. and $\\pmb{\\theta}_{2}=(\\pi,\\lambda_{1},\\lambda_{2})$ , and showed via simulations that the performance of their MHD estimator of $\\pmb{\\theta}_{2}$ remains unaffected even when the data are generated from a $100\\%$ gross-error contaminated Poisson mixture model defined by <PARAGRAPH_SEPARATOR> $$ f_{\\theta_{2},\\varepsilon,\\lambda_{3}}(x)=\\left(1-\\varepsilon\\right)f_{\\theta_{2}}(x)+\\varepsilon f\\left(x\\left|\\lambda_{3}\\right.\\right), $$ <PARAGRAPH_SEPARATOR> where \u000b is the proportion of contamination and the (contaminating) value $\\lambda_{3}$ is large compared to those of $\\lambda_{1}$ and $\\lambda_{2}$ . <PARAGRAPH_SEPARATOR> While it is a common practice to study robustness of MHD estimators against gross-error contamination models, such an approach would be inappropriate in our context because, by virtue of its consistency, our estimator $\\hat{m}_{n}$ would (correctly) identify (for sufficiently large $n$ ) the number of components in the mixture from which data are generated, which in the above setup would be 3 instead of 2. Notice also that there is no Hellinger function representation of our estimator of mixture complexity which would facilitate the study of $\\alpha$ -influence functions. <PARAGRAPH_SEPARATOR> In view of these, we assess the robustness of $\\hat{m}_{n}$ when the postulated model is a Poisson mixture but the data are generated from a mixture with negative binomial components. More precisely, we perform simulation studies when the postulated model is a 2-component Poisson mixture $f_{\\pmb{\\theta}_{2}}(x)$ defined above with $\\lambda_{1}$ and $\\lambda_{2}$ as its component means, but the data are generated from a 2-component negative binomial mixture given by <PARAGRAPH_SEPARATOR> $$ f(x)=\\pi f_{1}(x)+(1-\\pi)f_{2}(x), $$ <PARAGRAPH_SEPARATOR> where, for $i=1$ , 2, $f_{i}(x){=}\\varGamma(r+x)/(\\varGamma(r)x!)p_{i}^{r}(1-p_{i})^{x}$ , $x=0$ , $1,\\ldots;r>0$ . Let $f_{1}$ and $f_{2}$ be the p.m.f.s associated with random variables, say, $X_{1}$ and $X_{2}$ , respectively. Then, it is easily seen that the component means and the variances are $\\begin{array}{r}{E\\left(X_{i}\\right)=r\\left(1-p_{i}\\right)/p_{i}}\\end{array}$ and V ar $\\left(X_{i}\\right)=r\\left(1-p_{i}\\right)/p_{i}^{2}$ , for $i=1$ , 2. It is also well known that if, for each $i=1$ , 2, $r\\rightarrow\\infty$ and $p_{i}\\to1$ such that $r\\left(1-p_{i}\\right)\\rightarrow\\lambda_{i}$ , then $E\\left(X_{i}\\right)\\rightarrow\\lambda_{i}$ and $V a r\\left(X_{i}\\right)\\rightarrow\\lambda_{i}$ , which agrees with the component Poisson means and variances. In fact, under these conditions, it can be shown that the negative binomial family of distributions includes the Poisson distribution as a limiting case. <PARAGRAPH_SEPARATOR> The hallmark of the postulated Poisson components is that the mean is equal to the variance. However, the component variance of a negative binomial mixture from which the count data are generated may be more than can be expected on the basis of the postulated model. This phenomenon, known as overdispersion, in the negative binomial components in (5.1) may also be moderate to more extreme depending on the values of $r$ and $p_{i}$ , for $i=1$ , 2. <PARAGRAPH_SEPARATOR> In our simulation studies, we consider two scenarios. In both the scenarios, we set the component means of the sampling model to be the same as that of the postulated model, that is, $r\\left(1-p_{i}\\right)/p_{i}=\\lambda_{i}$ , for $i=1,2$ . In the first scenario, we set $r=10$ and $\\lambda_{1}=1$ (this sets $E\\left(X_{1}\\right)=1$ and V ar $(X_{1})=1.1\\AA$ ), but vary the values of $\\lambda_{2}$ . More specifically, for illustration, we set $\\lambda_{2}=2,5,7$ , with corresponding values of V $x r\\left(X_{2}\\right)=2.4$ , 7.5, 11.9. Note that the $V a r(X_{2})$ values are progressively much larger compared to the corresponding values of $E\\left(X_{2}\\right)\\left(=\\lambda_{2}\\right)$ , thus creating a moderate to more extreme overdispersion in the second negative binomial component."
  },
  {
    "qid": "statistic-posneg-925-0-9",
    "gold_answer": "FALSE. While the ADE and AIE are constant in some models (e.g., the linear-in-means model), they can depend on $\\pi$ in others, such as models with nonlinear interference effects. For example, in settings with herd immunity or complex nonlinear interactions, the effects may vary with the treatment probability.",
    "question": "Is it true that the ADE and AIE are always independent of the treatment probability $\\pi$ in all structural models?",
    "question_context": "Average Direct and Indirect Causal Effects Under Interference\n\nWe study different experimental designs using the potential outcomes model. The main difference between a setting with interference and the standard Neyman–Rubin model is that potential outcomes for the ith unit may also depend on the intervention given to the jth unit with $j\\neq i$ (e.g., Hudgens & Halloran, 2008; Aronow & Samii, 2017). For convenience, we use the shorthand $Y_{i}(w_{j}=x;W_{-j})$ to denote the potential outcome we would observe for the ith unit if we were to assign the jth unit to treatment status $x\\in\\{0,1\\}$ and maintain all units, but the $j$ th at their realized treatments $W_{-j}\\in\\{0,1\\}^{n-1}$ . Expectations $E$ are over the treatment assignment only; potential outcomes are held fixed."
  },
  {
    "qid": "statistic-posneg-1487-0-0",
    "gold_answer": "FALSE. The decomposition does not aim to minimize the number of 1's in $E$ directly. Instead, it seeks to minimize the description length, which is a weighted sum of the number of types ($t$) and the number of unit entries in $R$, $C$, and $E$ ($r$, $c$, $e$). The weights are derived from the log probabilities of the parameters $\\tau,\\rho,\\gamma,\\varepsilon$. The best approximation is the one that maximizes the posterior probability, which corresponds to minimizing this weighted sum, not merely the number of 1's in $E$.",
    "question": "In the binary matrix decomposition $A=R C^{\\prime}+E$, the error matrix $E$ is used to account for discrepancies in the approximation of the original matrix $A$ by the product of $R$ and $C^{\\prime}$. Given that all arithmetic is modulo 2, is it correct to say that the decomposition aims to minimize the number of 1's in $E$ to achieve the best approximation?",
    "question_context": "Binary Matrix Decomposition for Classification Approximation\n\nThe data are represented by an $m\\times n0{-}1$ matrix $A$, where $A_{i j}=1$ means the ith individual has the jth character, and $A_{i j}=0$ means the ith individual does not have the jth character. Some entries might be missing. We seek a decomposition $A=R C^{\\prime}+E$, similar to principal component analysis. The $R$ matrix is an $m\\times t~0{-}1$ matrix specifying underlying types on individuals, $C$ is an $n\\times t0{-}1$ matrix specifying which type approximates each given class, and $E$ is a 0-1 error matrix. All arithmetic is modulo 2. A probability model guides the choice among decompositions, with parameters $\\tau,\\rho,\\gamma,\\varepsilon$ for the number of types, and Bernoulli expectations for $R$, $C$, and $E$. The log probability of $A=R C^{\\prime}+E$ is given by a weighted sum of the number of types and unit entries in $R,C,E$, which is taken as the description length. The maximum posterior probability solution minimizes this weighted sum. Markov chain Monte Carlo can explore decompositions with comparable probability, and the model can predict unknown entries in $A$."
  },
  {
    "qid": "statistic-posneg-1489-0-1",
    "gold_answer": "TRUE. The Inverse Gaussian distribution is used as part of the data augmentation strategy to represent the Laplace distribution as a scale mixture of normals. This allows the use of conjugate priors for $\beta$ and $\\sigma^2$ in the Gibbs sampling steps, simplifying the sampling process. Specifically, the Inverse Gaussian latent variables $Z_i$ enable the conditional distributions of $\beta$ and $\\sigma^2$ to be standard distributions (normal and inverse gamma, respectively).",
    "question": "In the DA algorithm described, is it correct that the Inverse Gaussian distribution is used to augment the data to facilitate Gibbs sampling for $\beta$ and $\\sigma^2$?",
    "question_context": "Bayesian Linear Regression with Laplace Errors: MCMC Algorithms and Posterior Propriety\n\nLet $\\{Y_{i}\\}_{i=1}^{n}$ be independent random variables such that $Y_{i}=x_{i}^{T}\beta+\\sigma\\epsilon_{i},$ where $x_{i}\\in\\mathbb{R}^{p}$ is a vector of known covariates associated with $Y_{i}$ , $\beta\\in\\mathbb{R}^{p}$ is a vector of unknown regression coefficients, and $\\sigma\\in(0,\\infty)$ is an unknown scale parameter. The errors, $\\{\\epsilon_{i}\\}_{i=1}^{n}$ , are assumed to be i.i.d. from the Laplace distribution with scale equal to two, so the common density is $d(\\epsilon)=e^{-\\frac{|\\epsilon|}{2}}/4$. <PARAGRAPH_SEPARATOR> We consider a Bayesian model with an improper prior on $({\beta,\\sigma^{2}})$ that takes the form $\\pi(\beta,\\sigma^{2})=(\\sigma^{2})^{-(a+1)/2}I_{\\mathbb{R}_{+}}(\\sigma^{2}).$ , where $\\mathbb{R}_{+}:=(0,\\infty)$ and $a$ is a hyper-parameter. The standard default prior can be recovered by taking $a=1$. <PARAGRAPH_SEPARATOR> The joint density of the data is given by $f(y|\beta,\\sigma^{2})=\\frac{1}{4^{n}\\sigma^{n}}\\exp\\left\\{-\\frac{1}{2\\sigma}\\sum_{i=1}^{n}|y_{i}-x_{i}^{T}\beta|\right\\},$ where $y=(y_{1},\\ldots,y_{n})$. By definition, the posterior density is proper if $m(y):=\\int_{\\mathbb{R}_{+}}\\int_{\\mathbb{R}^{p}}f(y|\beta,\\sigma^{2})\\pi(\beta,\\sigma^{2})d\beta d\\sigma^{2}<\\infty.$ <PARAGRAPH_SEPARATOR> Proposition 1. The posterior is proper (that is $m(y)<\\infty,$ if and only if $X$ has full column rank, $a>-n+p+1$ , and $y\not\\in C(X)$. <PARAGRAPH_SEPARATOR> Assume now that $m(y)<\\infty$ so that the posterior density, which we denote by $\\pi(\beta,\\sigma^{2}|y)$, is well-defined. The posterior is intractable in the sense that posterior expectations cannot be computed in closed form. In this paper, we analyze a pair of Markov chain Monte Carlo (MCMC) algorithms for this problem. One is a data augmentation (DA) algorithm that is based on a representation of the Laplace density as a scale mixture of normals with respect to the inverse gamma distribution. The other is a variant of the DA algorithm that requires one additional simulation step at each iteration. <PARAGRAPH_SEPARATOR> Let $\\phi~=~\\{(\beta_{m},\\sigma_{m}^{2})\\}_{m=0}^{\\infty}$ be a Markov chain with state space $\times=\\mathbb{R}^{p}\times\\mathbb{R}_{+}$ whose dynamics are defined (implicitly) through the following three-step procedure for moving from the current state, $(\beta_{m},\\sigma_{m}^{2})=(\beta,\\sigma^{2})$ , to $(\beta_{m+1},\\sigma_{m+1}^{2})$. <PARAGRAPH_SEPARATOR> $Z_{i}\\sim\\mathrm{InverseGaussian}\\left(\\frac{\\sigma}{2|y_{i}-x_{i}^{T}\beta|},\\frac{1}{4}\right),$ <PARAGRAPH_SEPARATOR> 2. Draw $\\sigma_{m+1}^{2}\\sim\\operatorname{IG}\\left(\\frac{n-p+a-1}{2},\\frac{y^{T}Q^{-1}y-\theta^{T}\\varOmega^{-1}\theta}{2}\right)$ & ${\\mathrm{Draw}\beta_{m+1}\\sim\\mathrm{N}_{p}\\left(\theta,\\sigma_{m+1}^{2}\\varOmega\right)}$"
  },
  {
    "qid": "statistic-posneg-1903-0-1",
    "gold_answer": "FALSE. The inequality shows a decrease in the objective function, but it does not guarantee monotonicity because the terms $\\rho_{1}^{-1}\\|\\nu^{(m+1)} - \\nu^{(m)}\\|_{F}^{2}$ and $\\rho_{2}^{-1}\\|\\nu^{(m+1)} - \\nu^{(m)}\\|_{F}^{2}$ could potentially offset the decrease. The actual inequality provided in the context is more complex and includes these additional terms.",
    "question": "The inequality $L_{\\rho_{1},\\rho_{2}}(\\beta^{(m+1)}, \\eta^{(m+1)}, \\alpha^{(m+1)}, \\nu^{(m+1)}, \\nu^{(m+1)}) - L_{\\rho_{1},\\rho_{2}}(\\beta^{(m)}, \\eta^{(m)}, \\alpha^{(m)}, \\nu^{(m)}, \\nu^{(m)}) \\leq -2^{-1}c\\|\\beta^{(m+1)} - \\beta^{(m)}\\|_{F}^{2}$ implies that the objective function decreases monotonically. Is this statement true?",
    "question_context": "Convergence Analysis of High-Dimensional Integrative Models with Homogeneity\n\nThe paper discusses the convergence properties of high-dimensional integrative models with homogeneity. It presents several lemmas and theorems to establish the convergence of the proposed algorithm. Key formulas include the updates for the variables in the optimization process and the conditions under which convergence is guaranteed."
  },
  {
    "qid": "statistic-posneg-1635-2-0",
    "gold_answer": "FALSE. The condition βD_high - D_low > (1/2)(1-β)Y is not discussed in the provided context. The context focuses on Wasserstein distances, empirical processes, and their convergence properties, not on natural rates or deleveraging shocks.",
    "question": "The condition for a negative natural rate derived as βD_high - D_low > (1/2)(1-β)Y is consistent with the formula for 1+r_S < 1 in the context of the paper's model.",
    "question_context": "Wasserstein Distance and Empirical Processes in Statistical Inference\n\nThe context discusses the use of Wasserstein distances in statistical inference, focusing on empirical processes and their convergence properties. It includes proofs of theorems related to the Wasserstein distance, empirical quantile processes, and their applications in statistical tests and bootstrap procedures. The mathematical derivations involve quantile functions, Brownian bridges, and Taylor expansions to establish convergence results."
  },
  {
    "qid": "statistic-posneg-1401-1-1",
    "gold_answer": "TRUE. The second component's equation $$h_{t,2}^{2}=a_{1,2}u_{t-1}^{2}+(1-a_{1,2})h_{t-1,2}^{2}$$ implies that the sum of the coefficients on $u_{t-1}^{2}$ and $h_{t-1,2}^{2}$ is 1 ($a_{1,2} + (1-a_{1,2}) = 1$), which is the defining characteristic of an IGARCH model. This restriction ensures that the model parameters are identifiable and helps capture long-memory effects in volatility.",
    "question": "The CGARCH model combines two GARCH(1,1) components, where the second component $$h_{t,2}^{2}=a_{1,2}u_{t-1}^{2}+(1-a_{1,2})h_{t-1,2}^{2}$$ is restricted to be an integrated GARCH (IGARCH) model to ensure parameter identifiability. Is this restriction correctly described?",
    "question_context": "Volatility Modeling: TGARCH and CGARCH Specifications\n\nThe volatility of the mean corrected data $u_{t}$ has been modelled using three different parameterizations, a GARCH, a threshold GARCH (TGARCH) model (Rabemananjara and Zakoïan, 1993) and a two-component GARCH (CGARCH) model (Ding and Granger, 1996). All the models used are assumed to be of order (1,1). For the TGARCH(1,1) model, the updating equation for the conditional variance is given by $$h_{t}^{2}=(\\omega+a_{1}|u_{t-1}|+b_{1}h_{t-1}+c_{1}I(u_{t-1}<0)|u_{t-1}|)^{2},$$ with $I(\\cdot)$ being the indicator function. Note that, differently from standard GARCH, this model focuses on the dynamic structure of the conditional standard deviation rather than the conditional variance. Also, it allows to account for the presence of leverage effects in the conditional variance dynamics. The CGARCH(1,1) model is defined as a mixture of two GARCH(1,1) models. Namely, the resulting conditional variance equation is given by $$h_{t}^{2}=\\eta_{1}h_{t,1}^{2}+(1-\\eta_{1})h_{t,2}^{2},$$ where $$\\begin{array}{r}{h_{t,1}^{2}=\\omega_{1}+a_{1,1}u_{t-1}^{2}+b_{1,2}h_{t-1,1}^{2},}\\ {h_{t,2}^{2}=a_{1,2}u_{t-1}^{2}+(1-a_{1,2})h_{t-1,2}^{2}.}\\end{array}$$ The second volatility component is restricted to be integrated GARCH in order to guarantee the identifiability of the model parameters. The reason for which we have considered the CGARCH model as candidate model is that it allows to reproduce long-memory effects in the volatility dynamics."
  },
  {
    "qid": "statistic-posneg-60-0-0",
    "gold_answer": "FALSE. The condition $(\\omega-1)(\\omega+2)^{2}=\\beta_{1}$ is used to solve for $\\omega$ given $\\beta_{1}$, and then $\\beta_{2}$ is evaluated independently using $\\omega^{4}+2\\omega^{3}+3\\omega^{2}-3$. The two equations are not directly derived from each other but are part of a sequential process to fit the Johnson curves.",
    "question": "Is the condition $(\\omega-1)(\\omega+2)^{2}=\\beta_{1}$ used to determine $\\omega$ for $s_{L}$ curves consistent with the subsequent evaluation of $\\beta_{2}=\\omega^{4}+2\\omega^{3}+3\\omega^{2}-3$?",
    "question_context": "Fitting Johnson Curves by Moments\n\nThis algorithm supplements Tables 34, 35 and 36 of Pearson and Hartley (1972), for $s_{v}$ and $S_{B}$ curves. These tables are perfectly adequate for many purposes, but interpolation or extrapolation may be hazardous when the required curve is near one of the boundaries. <PARAGRAPH_SEPARATOR> # NUMERICAL METHOD <PARAGRAPH_SEPARATOR> Defining, as is customary, $\\sqrt{\\beta_{1}}$ as $\\mu_{3}/\\sigma^{3}$ and $\\beta_{\\mathbf{2}}$ as $\\mu_{4}/\\sigma^{4}$ the $s_{L}$ curves lie on a line in the $\\beta_{\\mathbf{1}}\\beta_{\\mathbf{2}}$ plane—thus for these curves $\\beta_{\\mathbf{1}}$ determines $\\beta_{\\mathbf{2}}$ Using $\\omega$ to denote exp $(\\delta^{-2})$ the $S_{L}\\beta_{\\mathbf{2}}$ value is found by solving <PARAGRAPH_SEPARATOR> $$ (\\omega-1)(\\omega+2)^{2}=\\beta_{1} $$ <PARAGRAPH_SEPARATOR> for $\\pmb{\\omega}$ , and then evaluating <PARAGRAPH_SEPARATOR> $$ \\beta_{2}=\\omega^{4}+2\\omega^{3}+3\\omega^{2}-3. $$ <PARAGRAPH_SEPARATOR> If the required $\\beta_{\\mathbf{2}}$ is less than this value, $\\pmb{S_{B}}$ (or $s_{x}$ is appropriate; if greater, $s_{v}$ is appropriate. <PARAGRAPH_SEPARATOR> (1) $s_{L}$ curves $\\pmb{\\omega}$ having been evaluated as above, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l l}{{\\delta=(\\ln\\omega)^{-\\frac{1}{4}},}}&{{\\gamma=\\frac{1}{2}\\delta\\ln\\left\\{\\omega(\\omega-1)/\\mu_{2}\\right\\},}}\\ {{\\xi=\\pm\\mu_{1}^{\\prime}-\\exp\\left\\{(1/2\\delta-\\gamma)/\\delta\\right\\},}}&{{\\lambda=\\pm1,}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where the $\\pm$ is determined in each case to be the sign of $\\pmb{\\mu_{3}}$ .As Johnson (1949) points out, only three parameters are necessary for an $s_{L}$ curve, but we have found it convenient to include $\\lambda$ as above. <PARAGRAPH_SEPARATOR> # (2) $s_{v}$ curves <PARAGRAPH_SEPARATOR> When $\\beta_{\\mathbf{1}}=0$ , the required curve is symmetrical, and <PARAGRAPH_SEPARATOR> $$ \\omega=\\{(2\\beta_{2}-2)^{\\frac{1}{2}}-1\\}^{\\frac{1}{2}};~\\hat{\\delta}=(\\ln\\omega)^{-\\frac{1}{2}};~\\gamma=0. $$ <PARAGRAPH_SEPARATOR> For an asymmetrical curve <PARAGRAPH_SEPARATOR> $$ \\omega_{1}=\\{(2\\beta_{2}-2{\\cdot}8\\beta_{1}-2)^{\\sharp}-1\\}^{\\sharp} $$ <PARAGRAPH_SEPARATOR> is taken as a first estimate, and $\\omega,\\delta$ and $\\gamma$ found by Johnson's iterative method (Elderton and Johnson, 1969, p. 127). The sign of $\\gamma$ is set to be the opposite of that of $\\pmb{\\mu_{3}}$ <PARAGRAPH_SEPARATOR> In either case $\\pmb{\\xi}$ and $\\lambda$ are then found from <PARAGRAPH_SEPARATOR> $$ \\mu_{2}={\\textstyle\\frac{1}{2}}\\lambda^{2}(\\omega-1)\\{\\omega\\cosh{(2\\gamma/\\delta)}+1\\};\\mu_{1}^{\\prime}=\\xi-\\lambda\\omega^{\\frac{1}{2}}\\sinh{(\\gamma/\\delta)}. $$ <PARAGRAPH_SEPARATOR> (3) $\\pmb{S_{B}}$ curves <PARAGRAPH_SEPARATOR> Approaching the $s_{\\tau}$ boundary, $\\mathfrak{d}{\\to}0$ ; approaching the $s_{L}$ boundary 8 tends to the same value as for an $s_{L}$ curve. A first approximation to 8 can be found by interpolating between these two values. The interpolation is made by assuming the shape of the function to be the same at the required $\\beta_{\\mathbf{1}}$ value as it is between the same two 8 values when $\\beta_{1}=0$ This is well approximated by <PARAGRAPH_SEPARATOR> $$ \\hat{\\delta}=(0\\cdot626\\beta_{2}-0\\cdot408)/(3\\cdot0-\\beta_{2})^{0\\cdot479}\\quad\\mathrm{if~}\\beta_{2}\\geqslant1\\cdot8, $$ <PARAGRAPH_SEPARATOR> and by <PARAGRAPH_SEPARATOR> $$ \\updelta=0{\\cdot}8(\\beta_{2}{-}1)\\quad\\mathrm{otherwise}. $$ <PARAGRAPH_SEPARATOR> For a given $\\beta_{\\pm}$ and frst approximation to 8, a first approximation to $\\gamma$ is found using formulae due to Draper (1951). <PARAGRAPH_SEPARATOR> # APPLIEDSTATISTICS <PARAGRAPH_SEPARATOR> Evaluation of the first six moments at the given 8 and $\\gamma$ values, using Draper's (1952) form of Goodwin's (1949) integral, then enables a two-dimensional Newton-Raphson process to converge on the required values."
  },
  {
    "qid": "statistic-posneg-459-0-2",
    "gold_answer": "FALSE. The paper does not claim that the phase can be uniquely determined from the amplitude alone in general. It mentions that under additional assumptions (e.g., the filter is of minimum phase type), the phase can be determined from the amplitude. However, in the general case, the phase is determined using the recursive formula (3.13), which depends on both the amplitude and the phase of the cross-spectral densities.",
    "question": "Does the paper claim that the phase of the filter A(λ) can be uniquely determined from the amplitude |A(λ)| alone?",
    "question_context": "Nonlinear Time Series System Identification with Gaussian Inputs\n\nThe paper discusses the identification of nonlinear time series systems with Gaussian inputs. It presents a model where the output series Y(t) is generated by a nonlinear transformation G of a linear combination of input series X(t), plus noise. The key results involve relationships between covariances and cumulants of the input and output series, which are used to identify the system parameters. The paper also considers the case where the input series are jointly normal, showing that linear regression coefficients are proportional to the parameters of a broader class of nonlinear models."
  },
  {
    "qid": "statistic-posneg-454-0-2",
    "gold_answer": "TRUE. The Gaussian kernel function is indeed used with γ = 1/p, where p is the number of input variables, as empirical evidence suggests this value generally works well.",
    "question": "Is the statement 'The Gaussian kernel function K(xᵢ, xⱼ) = exp(−γ‖xᵢ − xⱼ‖²) is used with γ = 1/p, where p is the number of input variables' true or false?",
    "question_context": "Kernel Fisher Discriminant Analysis and Variable Selection\n\nThe paper discusses Kernel Fisher Discriminant Analysis (KFDA) and its application to variable selection. The KFDA classifier is given by the sign of a discriminant function involving a kernel function. The paper also introduces recursive feature elimination (RFE) for variable selection in support vector machines (SVMs), comparing different criteria for variable elimination."
  },
  {
    "qid": "statistic-posneg-412-6-0",
    "gold_answer": "TRUE. The term $\\frac{8A^{2}}{1-E e^{-A^{2}/(\\bar{\\sigma}^{2}+\\bar{s}^{2})}}$ in the risk bound becomes very large when $E e^{-A^{2}/(\\bar{\\sigma}^{2}+\\bar{s}^{2})}$ approaches 1, making the bound less informative. Similarly, a large $E(\\underline{{s}}^{-2})$ amplifies the second term in the bound, which involves the infimum over procedures. This occurs when $\\underline{s}$ (the minimum of $\\hat{\\sigma}(x)$) is very small, indicating potential instability in the variance estimation.",
    "question": "Is it true that the risk bound $R((u,\\sigma);n;\\delta_{*})$ becomes less useful when $E e^{-A^{2}/(\\bar{\\sigma}^{2}+\\bar{s}^{2})}$ is close to 1 or $E(\\underline{{s}}^{-2})$ is large, as stated in the remarks?",
    "question_context": "Adaptive Estimation Risk Bounds in Regression\n\nSometimes, it is possible to have a good estimator of $\\sigma^{2}$ , e.g., by nearest neighbor method (e.g., Stone, 1977) or based on additional information. Then a different adaptation recipe can be used. Let $\\hat{\\sigma}$ be an estimator of $\\sigma$ independent of the sample $Z_{1},...,Z_{n}$ (or one could set aside a portion of data for the estimation of $\\sigma$ ). Then we use this estimator in the definition of $q_{l}$ 's (instead of mixing over $\\boldsymbol{\\Xi}$ ) in Section 3.1, i.e., $q_{n-N+1}(x,y)$ is redefined as $\\begin{array}{r}{\\sum_{j\\geqslant1}\\pi_{j}p_{\\hat{u}_{j,n-N+1},\\hat{\\sigma}(x)}(x,y)}\\end{array}$ and we make similar modifications for others. Proceed as before and let $\\tilde{u}_{n}(x)$ be the minimizer of the Hellinger distance $d_{H}(\\hat{g}_{n}(\\cdot\\mid x),\\phi_{t,\\hat{\\sigma}(x)})$ between $\\hat{g}_{n}(y\\mid x)$ and the normal density $\\phi_{t,\\hat{\\sigma}(x)}(y)$ with mean $t$ and variance ${\\hat{\\sigma}}^{2}(x)$ over choices of $t$ with $|t|\\leqslant A$ . Take ${\\tilde{u}}_{n}$ as our final adaptive estimator and call this estimation procedure $\\delta_{*}$ . Let $\\underline{s}$ and $\\bar{s}$ denote the minimum and maximum value of ${\\hat{\\sigma}}(x)$ over $x.$ , respectively. Similarly, let $\\underline{{\\sigma}}$ and $\\bar{\\sigma}$ denote the minimum and maximum of $\\sigma(x)$ , respectively.<PARAGRAPH_SEPARATOR>Theorem 4. For the combined procedure, for any regression function u with $\\|u\\|_{\\infty}\\leqslant A$ , we have<PARAGRAPH_SEPARATOR>$$\\begin{array}{r}{R((u,\\sigma);n;\\delta_{*})\\leqslant{\\displaystyle{\\frac{8A^{2}}{1-E e^{-A^{2}/(\\bar{\\sigma}^{2}+\\bar{s}^{2})}}}\\cdot\\left(E\\left(\\left(2+\\log\\left(1+{\\displaystyle{\\frac{\\bar{s}^{2}}{\\underline{{\\sigma}}^{2}}}\\right)\\right)\\left\\|{\\frac{\\sigma^{2}-\\hat{\\sigma}^{2}}{\\hat{\\sigma}^{2}}}\\right\\|_{\\infty}^{2}\\right)\\right.}}\\ {+\\operatorname*{inf}_{j}\\left\\{{\\displaystyle{\\frac{2}{N}}\\log{\\displaystyle{\\frac{1}{\\pi_{j}}}}+E\\left({\\displaystyle{\\frac{1}{\\underline{{s}}^{2}}}}\\right){\\displaystyle{\\frac{1}{N}}\\sum_{l=n-N+1}^{n}R((u,\\sigma);l;\\delta_{j})}\\right\\}}}\\end{array}$$<PARAGRAPH_SEPARATOR>Remarks 1. The above bound is not very useful when $E e^{-A^{2}/(\\Bar{\\sigma}^{2}+\\Bar{s}^{2})}$ is close to 1 or $E(\\underline{{s}}^{-2})$ is large. If $\\sigma(x)$ is uniformly upper bounded, a good estimator should have $E\\bar{e}^{-A^{2}/(\\bar{\\sigma}^{2}+\\bar{s}^{2})}$ bounded away from 1. The term $E(\\underline{{s}}^{-2})$ is likely to be large when $\\sigma(x)$ may be close to zero. To get it controlled mathematically, one can always add independently generated noise to the responses and restrict attention accordingly to $\\underline{s}$ bounded away from zero. The increase of noise level usually do not change the risk beyond a constant factor.<PARAGRAPH_SEPARATOR>2. If there are several plausible estimators of $\\sigma(x)$ available independent of $Z^{n}.$ , one can mix over them as well to obtain a similar result.<PARAGRAPH_SEPARATOR>From the above theorem, with $\\sigma^{2}$ estimated, the adaptive procedure basically pays the price of the discrepancy of the variance estimator and $(1/N)\\log(1/\\pi_{j})$ (of order $1/n$ ). Note that we do not require a known upper bound on $\\sigma$ above. If an estimator $\\hat{\\sigma}^{2}$ converges at rate $1/n$ in mean square error, then unlike the adaptation procedure by discretizing $\\sigma^{2}$ , the above adaptation risk bound does not lose a logarithmic factor for parametric cases."
  },
  {
    "qid": "statistic-posneg-2093-2-0",
    "gold_answer": "TRUE. The derivation is correct as it follows from the law of total probability and the Markov property. The formula accounts for two scenarios: the current state being 1 (with probability p) leading to an increment in the count (k-1 to k), and the current state being 0 (with probability 1-p) leading to no change in the count (k remains k). This aligns with the properties of a Markov chain where the probability of the next state depends only on the current state.",
    "question": "Is the recursive formula $P(S_{n}=k)=p P(S_{n-1}=k-1)+(1-p)P(S_{n-1}=k)$ correctly derived from the given Markov chain properties?",
    "question_context": "Recursive Algorithm for Computing Markov Chain Distributions\n\nTable 1 Transition probabilities and initial distributions used in Fig. 1 <PARAGRAPH_SEPARATOR> <html><body><table><tr><td>p=1/2</td><td>p(1|1,1,1) =3/4</td><td>π(1)=1/2</td><td>π(1,1,1)=21/100</td></tr><tr><td>p(1|1)=3/5</td><td>p(1|1,1,0)=7/12</td><td>π(0)=1/2</td><td>π(0,1,1)=9/100</td></tr><tr><td>p(1|0)=2/5</td><td>p(1|1,0,1) =17/52</td><td></td><td>π(1,0,1)=13/200</td></tr><tr><td></td><td>p(1|1,0,0)=55/108</td><td></td><td>π(0,0,1)=27/200</td></tr><tr><td>p(1|1,1)=7/10</td><td>p(1]0,1,1) =19/60</td><td>π(1,1)=3/10</td><td>π(1,1,0)=9/100</td></tr><tr><td>p(1|1,0)=9/20</td><td>p(1]0,1,0)=73/220</td><td>π(0,1)=1/5</td><td>π(0,1,0)=11/100</td></tr><tr><td>p(1|0,1)=13/40</td><td>p(1]0,0,1)=331/540</td><td>π(1,0)=1/5</td><td>π(1,0,0)=27/200</td></tr><tr><td>p(1|0,0)=9/20</td><td>p(110,0,0)=209/660</td><td>π(0,0)=3/10</td><td>π(0,0,0)=33/200</td></tr></table></body></html> <PARAGRAPH_SEPARATOR> using <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{P(S_{n}=k)=P(S_{n}=k,X_{n}=1)+P(S_{n}=k,X_{n}=0)}\\ &{\\qquad=P(S_{n-1}=k-1,X_{n}=1)+P(S_{n-1}=k,X_{n}=0)}\\ &{\\qquad=p P(S_{n-1}=k-1)+(1-p)P(S_{n-1}=k).}\\end{array} $$ <PARAGRAPH_SEPARATOR> The initial distributions and transition probabilities used for each of the models of the example are listed in Table 1. With transition probabilities and initial distributions so defined, the Markovian sequences of the various orders are stationary sequences, where the initial distributions correspond to limiting distributions. Also, the initial distributions for each order may be computed by using transition probabilities of lower orders. For example, for $m=3$ , $\\pi(x_{0},x_{-1},1)=p\\times p(x_{-1}|1)\\times p(x_{0}|x_{-1},1)$ . (See Martin 2000, where an alternative set of “orthogonal” parameters was used to model stationary Markovian sequences.) We set up the example in this manner because it was easy to see the effect of adding in additional parameters when moving to higher-order models. <PARAGRAPH_SEPARATOR> We note how the computed probabilities change with assumed model order. This indicates the need to use an appropriate order when analyzing Markovian data sets. In Martin (2001), a strategy was discussed for using the information from computed probabilities to help determine an appropriate order of Markovian dependence for a data set. <PARAGRAPH_SEPARATOR> We also mention that the run time for each of the sequences of the example given in this section was minimal, about 1 s or less. Runs with sample size $n=10$ , 000 take about a minute and a half."
  },
  {
    "qid": "statistic-posneg-409-0-1",
    "gold_answer": "FALSE. The formula is incorrectly presented. The correct residual standard deviation should be $$\\sigma_{X^{\\prime}}\\sqrt{1-\\gamma^{2}}$$, where γ is the correlation coefficient. The inclusion of the derivative terms (\\frac{\\d}{\\d x^{3},\\d x^{3}}) is not standard and does not align with conventional statistical derivations for residual standard deviation.",
    "question": "Does the standard deviation formula $$\\sigma_{X^{\\prime}}\\sqrt{(1-\\gamma^{2}\\frac{\\d}{\\d x^{3},\\d x^{3}})}$$ correctly represent the residual standard deviation after accounting for the correlation between the samples?",
    "question_context": "Experimental Analysis of Goodness-of-Fit Using Chi-Square Test\n\nThe study involves testing the goodness-of-fit between raw basic samples, graduated basic samples, and the parent population using the chi-square test. The main experimental work tests 100 samples of 30 drawn from the parent population against the graduated basic sample, treated as a step-parent population. The analysis includes the mean difference, standard deviation, and correlation between the samples. The formulas provided relate to the variance and standard deviation calculations under specific conditions."
  },
  {
    "qid": "statistic-posneg-1953-4-0",
    "gold_answer": "FALSE. The stoichiometry matrix S should represent the net change in species counts per reaction. For the aphid growth model, the given matrix does not align with typical reaction schemes where each column corresponds to a reaction and each row to a species. The correct stoichiometry matrix should have columns representing reactions and rows representing species, which is not evident from the provided context.",
    "question": "In the aphid growth model, the stoichiometry matrix S is given as a 2x2 matrix with elements (1, -1) in the first row and (1, 0) in the second row. Does this matrix correctly represent the net change in species counts for the reactions described in the model?",
    "question_context": "Stochastic Kinetic Models: Aphid Growth and Lotka-Volterra Dynamics\n\nThe paper presents stochastic kinetic models for aphid growth and Lotka-Volterra dynamics, detailing the stoichiometry matrices, hazard functions, and associated differential equations for the Chemical Langevin Equation (CLE) and Linear Noise Approximation (LNA). The models describe the evolution of system states over time, incorporating reaction rates and stochastic noise."
  },
  {
    "qid": "statistic-posneg-1585-3-0",
    "gold_answer": "FALSE. The formula provided is a polynomial approximation of the recombination probability under specific conditions, but the exact derivation would require considering all possible recombination paths and their probabilities, which may not be fully captured by the given polynomial. The conditions (i), (ii), and (iii) describe specific recombination scenarios, but the polynomial is a simplified representation that may not account for all possible genetic linkage scenarios accurately.",
    "question": "Is the probability formula for $r$ correctly derived as $\\frac{1}{32}(2+4\\eta+5\\eta^{2}+8\\eta^{3}+8\\eta^{4}+4\\eta^{5}+\\eta^{6})$ based on the given conditions for non-recombinations and recombinations?",
    "question_context": "Linkage Detection in Human Genetics: Probability and Efficient Scoring\n\nNow let T, t, etc., be alleles linked with r. It is of interest to find the probabilities of $Z$ having the various genotypes TT, Tt, etc., when $Z$ is already known to be rr, the r genes originally coming from $Q$. The two $T\\cdot$ genes carried by $Z$ will be derived from a single original gene carried by $Q$ or $s$ if and only if one of the following conditions holds good (see Fig. 2): <PARAGRAPH_SEPARATOR> (i) All steps $Q U,U X,X Z,Q V,V Y,V Z$ are non-recombinations (ii) Steps $Q U,Q V$ are both recombinations, all others are non-recombinations (ii) $U$ and $V$ get the same gene from $s$ $U X$ and $V Y$ are recombinations and $X Z$ and $Y Z$ are non-recombinations. <PARAGRAPH_SEPARATOR> The combined probability of these three (mutually exclusive) possibilities is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\displaystyle r=\\left(\\frac{1+\\eta}{2}\\right)^{6}+\\left(\\frac{1-\\eta}{2}\\right)^{2}\\left(\\frac{1+\\eta}{2}\\right)^{4}+\\frac{1}{2}\\left(\\frac{1-\\eta}{2}\\right)^{2}\\left(\\frac{1+\\eta}{2}\\right)^{2}}}\\\\ {{\\mathrm{}~=\\frac{1}{32}(2+4\\eta+5\\eta^{2}+8\\eta^{3}+8\\eta^{4}+4\\eta^{5}+\\eta^{6}).~}}\\end{array} $$ <PARAGRAPH_SEPARATOR> The original gene which is thus transmitted to $Z$ both via $X$ and via $Y$ has a chance $p(\\mathbf{T})$ of being T, $p(\\mathbf{t})$ of being t, and so on; so this mechanism provides a proportionf . $p(\\mathbf{T})$ of individuals <PARAGRAPH_SEPARATOR> $Z$ of genotype TT, a proportion $\\boldsymbol{f}\\cdot\\boldsymbol{p}(\\mathbf{t})$ of type tt, etc. The alternative possibility is that $Z$ gets the two $\\pmb{T}$ genes from different sources: this has a chance $(1-f)$ of happening. But if, apart from the cousin marriage, mating is at random (a plausible assumption), these genes will then be statistically independent. So under this alternative $Z$ has a chance $(1-f)[p(\\mathbf{T})]^{2}$ of being homozygous TT, and $(1-f)\\mathrm{~.~}2p(\\mathbf{T})p(\\mathbf{t})$ of being heterozygous Tt. The total probability that $Z$ is TT is therefore <PARAGRAPH_SEPARATOR> $$ {\\cal P}(\\mathrm{TT})=f p(\\mathrm{T})+(1-f)[p(\\mathrm{T})]^{2}, $$ <PARAGRAPH_SEPARATOR> and that $Z$ is $\\mathbf{Tt}$ is <PARAGRAPH_SEPARATOR> $$ P(\\mathbf{T}\\mathbf{t})=2(1-f)p(\\mathbf{T})p(\\mathbf{t}). $$ <PARAGRAPH_SEPARATOR> [f linkage is complete, $\\eta=1,f=1$ , and all recessives $Z$ will be homozygous in the $T$ genes.If :here is no linkage, $\\begin{array}{r}{\\gamma=0,f=\\frac{1}{\\bar{1}\\bar{6}}}\\end{array}$ and <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{P(\\mathbf{T}\\mathbf{T})=p(\\mathbf{T})\\left[1+15p(\\mathbf{T})\\right]/16,}\\\\ {P(\\mathbf{T}\\mathbf{t})=30p(\\mathbf{T})p(\\mathbf{t})/16.}\\end{array} $$ <PARAGRAPH_SEPARATOR> For example, in the $A{-}B{-}O$ blood-groups there will be 45 per cent. of group $o$ when there is no linkage, and 66 per cent. when linkage is complete. <PARAGRAPH_SEPARATOR> It follows from these formulae that cousin marriage produces an association between the genotypes of children (Haldane, 1949). This is so even if there is no linkage, but close linkage greatly intensifies the effect."
  },
  {
    "qid": "statistic-posneg-1667-0-0",
    "gold_answer": "TRUE. The equivalence is derived by rearranging the terms in the original hypothesis. Starting from H0: σt11/σ11 = σt22/σ22, multiplying both sides by k and adding 1 to each side yields (1 + kσt11/σ11) = (1 + kσt22/σ22), which simplifies to (σ11 + kσt11)/σ11 = (σ22 + kσt22)/σ22. This transformation maintains the equality under the null hypothesis.",
    "question": "Is the null hypothesis of equal sensitivity formulated as H0: σt11/σ11 = σt22/σ22 equivalent to (σ11 + kσt11)/σ11 = (σ22 + kσt22)/σ22?",
    "question_context": "Comparison of Sensitivities in Measurement Methods: Variance Ratio Test\n\nIn this paper the comparison of the sensitivities of two methods of measurement in the case of the type I situation, under the assumption of model II of the analysis of variance, is considered in detail. The exact distribution, as well as very useful approximations to it, are obtained for a test statistic to be used in comparing the sensitivities of experiments. Tests of hypotheses and the calculation of critical values are discussed. It is also shown how the results can be applied to the type II and type IlI situations."
  },
  {
    "qid": "statistic-posneg-806-0-0",
    "gold_answer": "FALSE. The method does reduce the bias, but the reduction is achieved by using the sharpened data points $\\gamma_r(X_i)$ in the kernel density estimator, not by directly replacing $X_i$ with $\\gamma_r(X_i)$. The key is the transformation via $H_r$ and its inverse $\\gamma_r = H_r^{-1}(\\bar{F})$, which ensures the bias reduction to $O(h^4)$ or $O(h^6)$.",
    "question": "Is it true that the data sharpening method described in the context reduces the bias of the kernel density estimator from $O(h^{2})$ to $O(h^{4})$ or $O(h^{6})$ by replacing the original data points $X_i$ with their sharpened forms $\\gamma_r(X_i)$?",
    "question_context": "High Order Data Sharpening for Density Estimation\n\nJones and Signorini (1997) have reviewed a wider variety of bias reducing methods, including those of Abramson (1982, 1984) and Jones et al. (1995). In their present forms these are generally restricted to reducing bias from $O(h^{2})$ to $O(h^{4})$ . Hossjer (1996) has developed a general theory which simultaneously addresses the methods of Abramson (1982), Ruppert and Cline (1994) and Jones et al. (1995) among others, although excluding data sharpening techniques. See also Hossjer and Ruppert (1996). <PARAGRAPH_SEPARATOR> # 2. Methodology <PARAGRAPH_SEPARATOR> Let <PARAGRAPH_SEPARATOR> $$ {\\hat{f}}(x)=(n h)^{-1}\\sum_{i=1}^{n}K\\left({\\frac{x-X_{i}}{h}}\\right) $$ <PARAGRAPH_SEPARATOR> denote a kernel estimator of a density $f,$ , computed using a kernel $K$ that is a smooth, bounded, symmetric probability density, a bandwidth $h$ , and a random sample $X_{1},\\dots,X_{n}$ drawn from the distribution with density $f.$ It is well known that such a traditional estimator has bias equal to $O(h^{2})$ . See, for example, Wand and Jones (1995), pages 20±21. <PARAGRAPH_SEPARATOR> The key to our method is the following result, derived in Appendix A: if $H_{r}$ is a smooth distribution function with the property <PARAGRAPH_SEPARATOR> $$ \\int K(u)H_{r}^{\\prime}(x-h u)\\mathrm{d}u=f(x)+O(h^{r}), $$ <PARAGRAPH_SEPARATOR> and if we de®ne $\\gamma_{r}=H_{r}^{-1}(\\bar{F})$ , where $\\bar{F}$ is the distribution function corresponding to the density $\\bar{f}=E(\\hat{f})$ , then under smoothness conditions on $f$ <PARAGRAPH_SEPARATOR> $$ {\\frac{1}{h}}E\\biggl[K\\biggl\\{{\\frac{x-\\gamma_{r}(X_{i})}{h}}\\biggr\\}\\biggr]=f(x)+O(h^{r}). $$ <PARAGRAPH_SEPARATOR> Therefore, on replacing the data $X_{i}$ by their sharpened form $\\gamma_{r}(X_{i})$ in the de®nition of ${\\hat{f}}(x)$ , we obtain an estimator of $f(x)$ of which the bias equals $O(h^{4})$ or $O(h^{6})$ , rather than the $O(h^{2})\\operatorname{for}{\\hat{f}}.$ Examples of functions $H_{r}$ with property (2.1) include <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{H_{4}=\\bar{F}-h^{2}\\frac{\\kappa_{2}}{2}\\bar{f}^{\\prime},}}\\ {{H_{6}=H_{4}+h^{4}\\Bigg\\{\\left(\\frac{\\kappa_{2}^{2}}{2}-\\frac{\\kappa_{4}}{24}\\right)\\bar{f}^{(3)}-\\frac{\\kappa_{2}^{2}}{4}\\frac{\\bar{f}^{\\prime\\prime}\\bar{f}^{\\prime}}{\\bar{f}}\\Bigg\\},}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\begin{array}{r}{\\kappa_{r}=\\int t^{r}K(t)\\mathrm{d}t}\\end{array}$ . The functions $\\bar{F}$ and $\\bar{f}^{(j)}$ are of course not known, but they have elementary plug-in estimators. Indeed, if $\\begin{array}{r}{\\hat{F}(x)=\\int_{u\\leqslant x}\\hat{f}(u)\\mathrm{d}u}\\end{array}$ then empirical forms of $H_{4}$ and $H_{6}$ are given by <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{\\hat{H}_{4}=\\hat{F}-h^{2}\\displaystyle{\\frac{\\kappa_{2}}{2}}\\hat{f}^{\\prime},}}\\ {{\\hat{H}_{6}=\\hat{H}_{4}+h^{4}\\displaystyle{\\left\\{\\left(\\frac{\\kappa_{2}^{2}}{2}-\\frac{\\kappa_{4}}{24}\\right)\\hat{f}^{(3)}-\\frac{\\kappa_{2}^{2}}{4}\\displaystyle{\\frac{\\hat{f}^{\\prime\\prime}\\hat{f}^{\\prime}}{\\hat{f}}}\\right\\}}}}\\end{array} $$ <PARAGRAPH_SEPARATOR> respectively. (We assume that $K$ has three derivatives, and so $\\hat{f}^{(j)}$ is well de®ned for $j\\leqslant3.$ ) If $\\hat{H}_{r}$ is strictly monotone increasing in a neighbourhood of ${\\hat{F}}(x)$ then we de®ne $\\hat{\\gamma}_{r}=\\hat{H}_{r}^{-1}(\\hat{F})$ there. Otherwise, put $\\hat{\\gamma}_{r}=I_{\\mathrm{}}$ , the identity. Our sharpened estimator is <PARAGRAPH_SEPARATOR> $$ {\\hat{f}}_{r}(x)=\\left(n h\\right)^{-1}\\sum_{i=1}^{n}K\\left\\{{\\frac{x-{\\hat{\\gamma}}_{r}(X_{i})}{h}}\\right\\}. $$ <PARAGRAPH_SEPARATOR> Having motivated our technique through the function $H_{r}$ , a more explicit approach may be developed by Taylor expansion of the function (or its estimator $\\hat{H}_{r}$ ), producing an approximation, $\\tilde{\\gamma}_{r}$ say, to $\\hat{\\gamma}_{r}$ . For example: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{\\hat{\\gamma}_{4}\\approx\\tilde{\\gamma}_{4}\\equiv I+h^{2}\\displaystyle\\frac{\\kappa_{2}}{2}\\displaystyle\\frac{\\hat{f}^{\\prime}}{\\hat{f}},}}\\ {{\\hat{\\gamma}_{6}\\approx\\tilde{\\gamma}_{6}\\equiv\\tilde{\\gamma}_{4}+h^{4}\\left\\{\\left(\\displaystyle\\frac{\\kappa_{4}}{24}-\\displaystyle\\frac{\\kappa_{2}^{2}}{2}\\right)\\displaystyle\\frac{\\hat{f}^{(3)}}{\\hat{f}}+\\displaystyle\\frac{\\kappa_{2}^{2}}{2}\\displaystyle\\frac{\\hat{f}^{\\prime\\prime}\\hat{f}^{\\prime}}{\\hat{f}^{2}}-\\displaystyle\\frac{\\kappa_{2}^{2}}{8}\\displaystyle\\frac{(\\hat{f}^{\\prime})^{3}}{\\hat{f}^{3}}\\right\\},}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $I$ denotes the identity function. Formulae (2.6) are simple and direct, and may be applied without any function inversion. However, the disadvantage of $\\tilde{\\gamma}_{r}$ is its relative complexity; a rule of thumb asserts that increased complexity of an asymptotic approximation can lead to a reduced performance in small samples. <PARAGRAPH_SEPARATOR> If we replace $\\hat{\\gamma}_{r}$ by $\\tilde{\\gamma}_{r}$ then we obtain instead of $\\hat{f}_{r}$ the estimator <PARAGRAPH_SEPARATOR> $$ {\\tilde{f}}_{r}(x)=(n h)^{-1}\\sum_{i=1}^{n}K\\left\\{{\\frac{x-{\\tilde{\\gamma}}_{r}(X_{i})}{h}}\\right\\}. $$ <PARAGRAPH_SEPARATOR> The advantages of data sharpening (faster convergence rates, less \\`wiggliness' of the density estimator and a guarantee of positivity and integration to 1) remain valid. To guard against numerical problems we might simply replace $\\hat{\\gamma}_{r}$ or $\\tilde{\\gamma}_{r}$ by the identity function if fewer than a certain threshold number, $\\nu$ say, of data are used to compute $\\hat{\\boldsymbol f}^{(j)}$ for $0\\leqslant j\\leqslant3$ . <PARAGRAPH_SEPARATOR> Fig. 1 shows the empirical transformations ${\\tilde{\\gamma}}_{4}$ and ${\\widetilde{\\gamma}}_{6}$ for the 22 points from the Ahrens (1965) chondrite meteorite data when $K=\\phi$ , the standard normal density, and $h=1$ . The original and transformed values of the data are marked by $\\ '_{\\bigcirc},\\ '_{\\bigcirc}$ and $^{\\circ}\\times^{\\prime}$ . Note that ${\\widetilde{\\gamma}}_{4}$ is monotone, but $\\tilde{\\gamma}_{6}$ is not quite, although its most erratic behaviour occurs only in regions with no data and so has no eect on the ®nal estimate. <PARAGRAPH_SEPARATOR> The estimates associated with these transformations, $\\tilde{f_{4}}$ and $\\tilde{f}_{6}$ , along with the original ${\\hat{f}},$ may be seen in Fig. 2. The estimates indeed sharpen the peaks and valleys, as we would expect given that these are the places where a traditional second-order estimate will have greatest bias. The locations of the original points $(^{\\leftarrow}+)$ , fourth-order transformed points $(\\mathrm{{^{\\leftarrow}o})}$ and sixth-order transformed points $(^{\\circ}\\times^{\\prime})$ may also be seen, showing the mechanism of the sharpening process. These estimates are all examples of variable location estimators, in the terminology of Jones and Signorini (1997), and $\\tilde{f}_{4}$ is in fact the empirical version of an estimator that was brie¯y mentioned by Samiuddin and el-Sayyad (1990)."
  },
  {
    "qid": "statistic-posneg-1885-0-1",
    "gold_answer": "TRUE. The normalization step applies a warping function that does not alter the cluster assignments made during the alignment step. The condition $\\lambda(\\pmb{\\varphi}_{[q]},\\tilde{\\mathbf{c}}_{i[q]})=\\lambda(\\pmb{\\varphi}_{[q]},\\mathbf{c}_{i[q]})$ confirms that the clustering structure is preserved despite the normalization.",
    "question": "Does the assignment and alignment step in the k-mean alignment algorithm preserve the clustering structure, meaning that the cluster assignment of a curve remains unchanged after normalization, as indicated by the condition $\\lambda(\\pmb{\\varphi}_{[q]},\\tilde{\\mathbf{c}}_{i[q]})=\\lambda(\\pmb{\\varphi}_{[q]},\\mathbf{c}_{i[q]})$ for all $i$?",
    "question_context": "k-Mean Alignment Algorithm for Curve Clustering\n\nLet $\\underline{{\\varphi}}_{[q-1]}=\\{\\varphi_{1[q-1]},...,\\varphi_{k[q-1]}\\}$ be the set of templates after iteration $q-1$ , and $\\{\\mathbf{c}_{1^{[q-1]}},\\dotsc,\\mathbf{c}_{N^{[q-1]}}\\}$ be the N curves aligned and clustered to $\\underline{{\\varphi}}_{\\mathrm{[}q-1\\mathrm{]}}$ . At the qth iteration the algorithm performs the following steps. <PARAGRAPH_SEPARATOR> Template identification step. <PARAGRAPH_SEPARATOR> F $\\mathbf{\\omega}_{0\\mathrm{{r}}j}=1,\\dots,k$ the template of the jth cluster, $\\varphi_{j^{[q]}}$ , is estimated using all curves assigned to cluster $j$ at iteration $q-1$ , i.e. all curves ${\\bf c}_{i[q-1]}$ such that $\\begin{array}{r}{\\lambda\\big(\\underline{{\\varphi}}_{[q-1]},\\mathbf{c}_{i[q-1]}\\big)=j.}\\end{array}$ Ideally, the template $\\varphi_{j^{[q]}}$ should be estimated as the curve $\\varphi\\in{\\mathcal{C}}$ that maximizes the total similarity: <PARAGRAPH_SEPARATOR> $$ \\sum_{i:\\lambda(\\underline{{\\varphi}}[q-1],{\\bf c}_{i}[q-1])=j}\\rho(\\varphi,{\\bf c}_{i^{[q-1]}}). $$ <PARAGRAPH_SEPARATOR> Assignment and alignment step. <PARAGRAPH_SEPARATOR> The set of curves $\\{\\mathbf{c}_{1^{[q-1]}},\\dotsc\\dotsc,\\mathbf{c}_{N^{[q-1]}}\\}$ is clustered and aligned to the set of templates $\\underline{{{\\varphi_{[q]}}}}~=~\\{\\varphi_{1^{[q]}},\\ldots,\\varphi_{k^{[q]}}\\}$ : for $i=1,\\ldots,N$ , the ith curve ${\\bf c}_{i[q-1]}$ is aligned to $\\varphi_{\\lambda(\\underline{{\\varphi}}[q],{\\bf c}_{i}[q-1])}$ and the aligned curve $\\tilde{\\mathbf{c}}_{i[q]}=\\mathbf{c}_{i[q-1]}\\circ h_{i[q]}$ is assigned to cluster $\\lambda(\\underline{{\\varphi}}_{[q]},\\mathbf{c}_{i[q-1]})\\equiv\\lambda(\\underline{{\\varphi}}_{[q]},\\widetilde{\\mathbf{c}}_{i[q]}).$ . <PARAGRAPH_SEPARATOR> Normalization step. <PARAGRAPH_SEPARATOR> After each assignment and alignment step, we also perform a normalization step. In detail, for $j=1,\\ldots,k,$ all the $N_{j[q]}$ curves $\\tilde{\\mathbf{c}}_{i[q]}$ assigned to cluster $j$ are warped along the warping function $(\\bar{h}_{j^{[q]}})^{-1}$ , where <PARAGRAPH_SEPARATOR> $$ \\bar{h}_{j^{[q]}}=\\frac{1}{N_{j^{[q]}}}\\sum_{i:\\lambda(\\underline{{\\varphi}}[q],\\tilde{\\mathbf{c}}_{i^{[q]}})=j}h_{i^{[q]}}, $$ <PARAGRAPH_SEPARATOR> thus obtaining $\\mathbf{c}_{i[q]}=\\tilde{\\mathbf{c}}_{i[q]}\\circ(\\bar{h}_{j[q]})^{-1}=\\mathbf{c}_{i[q-1]}\\circ h_{i[q]}\\circ(\\bar{h}_{j[q]})^{-1}$ . In this way, at each iteration, the average warping undergone by curves assigned to cluster $j$ is the identity transformation $h(s)=s.$ Indeed: <PARAGRAPH_SEPARATOR> $$ \\frac{1}{N_{j^{[q]}}}\\sum_{i:\\lambda(\\underline{{\\varphi}}[q],{\\bf c}_{i}[q])=j}\\big(h_{i^{[q]}}\\circ(\\bar{h}_{j^{[q]}})^{-1}\\big)(s)=s,\\quad j=1,\\dots,k. $$ <PARAGRAPH_SEPARATOR> The normalization step is thus used to select, among all candidate solutions to the optimization problem, the one that leaves the average locations of the clusters unchanged, thus avoiding the drifting apart of clusters or the global drifting of the overall set of curves. Note that the normalization step preserves the clustering structure chosen in the assignment and alignment step, i.e., $\\lambda(\\pmb{\\varphi}_{[q]},\\tilde{\\mathbf{c}}_{i[q]})=\\lambda(\\pmb{\\varphi}_{[q]},\\mathbf{c}_{i[q]})$ for all $i$ . <PARAGRAPH_SEPARATOR> The algorithm is initialized with a set of initial templates $\\underline{{\\varphi}}_{[0]}=\\{\\varphi_{1^{[0]}},\\cdot\\cdot\\cdot,\\varphi_{k^{[0]}}\\}\\subset\\mathcal{C}$ , and with $\\{{\\bf c}_{1^{[0]}},\\ldots,{\\bf c}_{N^{[0]}}\\}=$ $\\{\\mathbf{c}_{1},\\hdots,\\mathbf{c}_{N}\\}$ , and stopped when, in the assignment and alignment step, the increments of the similarity indexes are all lower than a fixed threshold. <PARAGRAPH_SEPARATOR> # 4. Technical details for the implementation of the $\\pmb{k}$ -mean alignment algorithm"
  },
  {
    "qid": "statistic-posneg-1865-1-0",
    "gold_answer": "FALSE. The MAVE method, as described, uses least squares estimation which is sensitive to outliers. The paper discusses the need for robust enhancements to MAVE, such as replacing the least squares estimation with local L- or M-estimation, to handle heavy-tailed and contaminated data effectively.",
    "question": "Is the MAVE method robust to outliers in the data?",
    "question_context": "Robust MAVE through Nonconvex Penalized Regression\n\nIn this section, we provide a short review of the MAVE and the robust MAVE estimation. A traditional regression type model in dimension reduction can be written as y = g(B^T X) + ε, where g(·) is an unknown smooth link function and B is a p×d orthonormal matrix consisting of (β_1,…,β_d) with d < p. The random error ε is independent of x with E(ε)=0. Given a random sample {(X_i,y_i),i=1,...,n} and a known d, Xia et al. (2002) proposed the following objective function to estimate a basis B: min_{B:B^T B=I_d} (∑_{j=1}^n ∑_{i=1}^n [y_i - {a_j + b_j^T B^T (X_i - X_j)}]^2 w_ij), where I_d denotes a d×d identity matrix, and a_j and b_j are the coefficients of the local linear expansion g(B^T X_i) ≈ a_j + b_j^T B^T (X_i - X_j). The kernel weight w_ij is chosen as w_ij = K_h{B^T (X_i - X_j)} / ∑_{i=1}^n K_h{B^T (X_i - X_j)}, where K_h(ν) = h^{-1}K(ν/h) with K(ν) being a symmetric kernel function and h being the bandwidth, and B̂ denotes an initial estimator of B."
  },
  {
    "qid": "statistic-posneg-1107-0-0",
    "gold_answer": "FALSE. The correct convergence rate under the spectral norm is $O_{\\mathsf{p}}\\big\\{r_{n}(n^{-1}\\log p)^{1/2}+\\delta^{2(r_{n}+j)+1}\\big\\}$, which simplifies to $O_{\\mathsf{p}}\\big\\{\\log(n/\\log p)(n^{-1}\\log p)^{1/2}\\big\\}$ only when the term $\\delta^{2(r_{n}+j)+1}$ is dominated by the first term. The $L_1$ norm convergence rate is correctly stated as $O_{\\mathbb{p}}\\big\\{\\log(n/\\log p)(n^{-1}\\log p)^{1/2}\\big\\}$.",
    "question": "Is the convergence rate of the banding estimator $\\hat{\\Sigma}_{j}^{(r_{n})}$ under the spectral norm $O_{\\mathsf{p}}\\big\\{\\log(n/\\log p)(n^{-1}\\log p)^{1/2}\\big\\}$ as stated in Theorem 4?",
    "question_context": "Banded Vector Autoregression: Estimation and Convergence Rates\n\nRemark 3. In Theorem $1,k_{0}$ is assumed to be fixed, as in applications small $k_{0}$ is of particular interest. We can, however, allow the bandwidth parameter $k_{0}$ to diverge as $n,p\\to\\infty$ ; but to show its consistency in that case, the regularity conditions would need to be strengthened. To be specific, if $k_{0}\\ll C_{n}^{-1}n/\\log(p\\vee n)$ , then $\\mathrm{pr}(\\hat{k}=k_{0})\\rightarrow1$ as $n\\rightarrow\\infty$ under Conditions $1^{\\prime}$ and 2–4; see the Supplementary Material. <PARAGRAPH_SEPARATOR> Since $k_{0}$ is unknown, we replace it by $\\hat{k}$ in the estimation procedure for $A_{1},\\ldots,A_{d}$ described in $\\S2\\cdot2$ , and still denote the resulting estimators by $\\hat{A}_{1},\\dotsc,\\hat{A}_{d}$ . Theorem 2 addresses their convergence rates. <PARAGRAPH_SEPARATOR> THEOREM 2. Suppose that Conditions 1–4 hold. As $n\\to\\infty$ , for j = 1, . . . , d, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\left\\|\\hat{A_{j}}-A_{j}\\right\\|_{\\mathrm{F}}=O_{\\mathsf{p}}\\big\\{(p/n)^{1/2}\\big\\},\\quad\\left\\|\\hat{A_{j}}-A_{j}\\right\\|_{2}=O_{\\mathsf{p}}\\big\\{(\\log p/n)^{1/2}\\big\\}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Condition 4(i) and (ii) impose, respectively, a high moment condition and an exponential tail condition on the innovation distribution. Although the convergence rates in Theorem 2 contain the same expressions in terms of $n$ and $p$ , because of the different conditions imposed on them in Condition 4(i) and (ii), the actual convergence rates are different in the two settings. For example, Condition $4(\\mathrm{i})$ allows $p$ to grow at a rate of order $n^{\\beta}$ , which implies the convergence rate $(\\dot{1}0\\mathrm{g}n/n)^{1/2}$ for $\\hat{A}_{j}$ under the spectral norm. On the other hand, Condition 4(ii) allows $p$ to possibly diverge at the rate of $\\exp\\{n^{\\alpha/(2-\\alpha)-2\\epsilon}\\}$ for a small constant $\\epsilon>0$ , and the implied convergence rate for A; under the spectral norm would then be n1/2+e-α/(4-2α). <PARAGRAPH_SEPARATOR> # 4. ESTIMATION FOR AUTO-COVARIANCE FUNCTIONS <PARAGRAPH_SEPARATOR> For the banded vector autoregressive process $y_{t}$ defined by (1), the auto-covariance function $\\Sigma_{j}=\\mathrm{cov}(y_{\\mathrm{t}},y_{\\mathrm{t+j}})$ is unlikely to be banded. For example, for a stationary banded autoregressive process of order 1, it can be shown that <PARAGRAPH_SEPARATOR> $$ \\Sigma_{0}\\equiv\\mathrm{var}(\\gamma_{t})=\\Sigma_{\\varepsilon}+\\sum_{i=1}^{\\infty}A_{1}^{i}\\Sigma_{\\varepsilon}(A_{1}^{\\mathrm{T}})^{i}. $$ <PARAGRAPH_SEPARATOR> For any banded matrices $B_{1}$ and $B_{2}$ with bandwidths $2k_{1}+1$ and $2k_{2}+1$ , in general the product $B_{1}B_{2}$ is a banded matrix with the enlarged bandwidth $2(k_{1}+k_{2})+1$ . Hence $\\Sigma_{0}$ in (12) is not <PARAGRAPH_SEPARATOR> a banded matrix. Nevertheless, if $\\mathrm{var}(\\varepsilon_{t})=\\Sigma_{\\varepsilon}$ is also banded, Theorem 3 shows that $\\Sigma_{j}$ can be approximated by some banded matrices. <PARAGRAPH_SEPARATOR> Condition 5. The matrix $\\Sigma_{\\varepsilon}$ is banded with bandwidth $2s_{0}+1$ and $\\begin{array}{r}{\\|\\sum_{\\varepsilon}\\|1\\leqslant C<\\infty}\\end{array}$ , where $C,s_{0}>0$ are constants independent of $p$ and $s_{0}$ is an integer. <PARAGRAPH_SEPARATOR> THEOREM 3. Suppose that Conditions 1 and 5 hold. Then, for any integers $r,j\\geqslant0_{.}$ , there exists a banded matrix $\\Sigma_{j}^{(r)}$ with bandwidth $2\\{(2r+j)k_{0}+s_{0}\\}+1$ such that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\big\\|\\Sigma_{j}^{(r)}-\\Sigma_{j}\\big\\|_{2}\\leqslant C_{1}\\delta^{2(r+j)+1},\\quad\\big\\|\\Sigma_{j}^{(r)}-\\Sigma_{j}\\big\\|_{1}\\leqslant C_{2}r\\delta^{2(r+j)+1},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $C_{1}$ and $C_{2}$ are positive constants independent of $r$ and $p$ , and $\\delta\\in(0,1)$ is specified in Condition 1. <PARAGRAPH_SEPARATOR> Under Condition 5, $\\begin{array}{r}{\\Sigma_{0}^{(r)}=\\Sigma_{\\varepsilon}+\\sum_{1\\leqslant i\\leqslant r}A_{1}^{i}\\Sigma_{\\varepsilon}(A_{1}^{\\mathrm{T}})^{i}}\\end{array}$ is a banded matrix with bandwidth $2(2r k_{0}+s_{0})+1.$ Theorem 3 ensures that  the norms of the difference $\\begin{array}{r}{\\Sigma_{0}-\\Sigma_{0}^{(r)}=\\sum_{i>r}A_{1}^{i}\\Sigma_{\\varepsilon}(A_{1}^{\\mathrm{T}})^{i}}\\end{array}$ admit the required upper bounds. Theorem 3 also paves the way for estimat ing $\\Sigma_{j}$ using the banding method of Bickel & Levina (2008), as $\\Sigma_{j}$ can be approximated by a banded matrix with a bounded error and may thus be effectively treated as a banded matrix. To this end, we define the banding operator as follows: for any matrix $H=(h_{i j}),B_{r}(H)=\\{h_{i j}I(|i-j|\\leqslant r)\\}$ . Then the banding estimator for $\\Sigma_{j}$ is defined as <PARAGRAPH_SEPARATOR> $$ \\hat{\\Sigma}_{j}^{(r_{n})}=B_{r_{n}}(\\hat{\\Sigma}_{j}),\\quad\\hat{\\Sigma}_{j}=\\frac{1}{n}\\sum_{t=1}^{n-j}(y_{t}-\\bar{y})(y_{t+j}-\\bar{y})^{\\mathrm{T}},\\quad\\bar{y}=\\frac{1}{n}\\sum_{t=1}^{n}y_{t}, $$ <PARAGRAPH_SEPARATOR> where $r_{n}=C\\log(n/\\log p)$ and $C>0$ is a constant greater than $(-4\\log\\delta)^{-1}$ . Theorem 4 presents the convergence rates for $\\hat{\\Sigma}_{j}^{(r_{n})}$ , which are faster than those in Bickel & Levina (2008), due to the approximate banded structure in Theorem 3. <PARAGRAPH_SEPARATOR> THEOREM 4. Suppose that Conditions 1–5 hold. Then, for any integer $j\\geqslant0$ , as $n,p\\to\\infty$ , <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\big\\|\\hat{\\Sigma}_{j}^{(r_{n})}-\\Sigma_{j}\\big\\|_{2}=O_{\\mathsf{p}}\\big\\{r_{n}(n^{-1}\\log p)^{1/2}+\\delta^{2(r_{n}+j)+1}\\big\\}=O_{\\mathsf{p}}\\big\\{\\log(n/\\log p)\\big(n^{-1}\\log p\\big)^{1/2}\\big\\}}\\end{array} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\Big\\|\\hat{\\Sigma}_{j}^{(r_{n})}-\\Sigma_{j}\\Big\\|_{1}=O_{\\mathbb{p}}\\big\\{\\log(n/\\log p)(n^{-1}\\log p)^{1/2}\\big\\}. $$ <PARAGRAPH_SEPARATOR> In practice we need to specify $r_{n}$ . An ideal choice would be $r_{n}=\\mathrm{arg}\\mathrm{min}_{r}R_{j}(r)$ , where <PARAGRAPH_SEPARATOR> $$ R_{j}(r)=E\\big(\\|\\hat{\\Sigma}_{j}^{(r)}-\\Sigma_{j}\\|_{1}\\big), $$ <PARAGRAPH_SEPARATOR> but in practice this is unavailable because $\\Sigma_{j}$ is unknown. Instead, we replace it with an estimator obtained by a wild bootstrap. To this end, let $u_{1},\\ldots,u_{n}$ be independent and identically distributed with $E(u_{t})=\\operatorname{var}(u_{t})=1.$ . A bootstrap estimator for $\\Sigma_{j}$ is defined as <PARAGRAPH_SEPARATOR> $$ \\Sigma_{j}^{*}=\\frac{1}{n}\\sum_{t=1}^{n-j}u_{t}(\\boldsymbol{y}_{t}-\\bar{\\boldsymbol{y}})(\\boldsymbol{y}_{t+j}-\\bar{\\boldsymbol{y}})^{\\mathrm{T}}. $$ <PARAGRAPH_SEPARATOR> For example, we may draw $u_{t}$ from the standard exponential distribution. Consequently, the bootstrap estimator for $R_{j}(r)$ is defined as <PARAGRAPH_SEPARATOR> $$ R_{j}^{*}(r)=E\\big\\{\\|B_{r}(\\Sigma_{j}^{*})-\\hat{\\Sigma}_{j}\\|_{1}\\big|y_{1},\\dots,y_{n}\\big\\}. $$ <PARAGRAPH_SEPARATOR> We choose $r_{n}$ to minimize $R_{j}^{*}(r)$ . In practice we use the approximation <PARAGRAPH_SEPARATOR> $$ R_{j}^{*}(r)\\approx\\frac{1}{q}\\sum_{k=1}^{q}\\|B_{r}(\\Sigma_{j,k}^{*})-\\hat{\\Sigma}_{j}\\|_{1}, $$ <PARAGRAPH_SEPARATOR> where $\\Sigma_{j,1}^{*},\\Sigma\\Sigma_{j,q}^{*}$ are $q$ bootstrap estimates for $\\Sigma_{j}$ , obtained by repeating the above wild bootstrap scheme $q$ times, with $q$ being a large integer."
  },
  {
    "qid": "statistic-posneg-1130-0-1",
    "gold_answer": "TRUE. The context in Remark 5 states that the discrepancy between $\\mathbf{H}_{\\mathrm{AMISE}}$ and $\\mathbf{H}_{\\mathrm{MISE}}$, given by $O(\\mathbf{J}_{d^{*}}n^{-2/(d+4)})$, dominates the AMSE rate from Theorem 1 for dimensions d > 4. This is because the rate of discrepancy becomes slower (larger exponent) compared to the AMSE rate as the dimension increases beyond 4.",
    "question": "Is the claim that the discrepancy between $\\mathbf{H}_{\\mathrm{AMISE}}$ and $\\mathbf{H}_{\\mathrm{MISE}}$ dominates the AMSE rate from Theorem 1 for dimensions d > 4 correct?",
    "question_context": "Convergence Rates for Bandwidth Matrix Selectors in Kernel Density Estimation\n\nRemark 3. The asymptotic properties of $\\check{\\mathbf{H}}_{\\mathrm{AMSE}}$ are superior to those of $\\check{\\mathbf{H}}_{\\mathrm{SAMSE}}$ : Nonetheless, the difference in rates of convergence is not great. In particular, for the important bivariate case the relative rate of convergence to $\\mathbf{H}_{\\mathrm{AMISE}}$ for $\\check{\\mathbf{H}}_{\\mathrm{AMSE}}$ is $n^{-2/7}$ and for $\\check{\\mathbf{H}}_{\\mathrm{SAMSE}}$ is $n^{-1/4}$ : Even for a sample of size $n=100,000$ the ratio of $n^{-2/7}$ to $n^{-1/4}$ is only about 1.5, so comparison of the convergence rates alone will provide little guidance as to whether AMSE or SAMSE approaches should be preferred in practice. <PARAGRAPH_SEPARATOR> Remark 4. The relative rate of convergence for a plug-in selector of a diagonal bandwidth matrix $n^{-\\operatorname*{min}(8,d+4)/(2d+12)}$ ; as demonstrated by Wand and Jones [12]. This rate is faster than those for the full bandwidth selectors. Intuitively speaking, this indicates that choosing the orientation of the kernel functions is the most difficult aspect of the bandwidth selection problem for both AMSE and SAMSE plug-in methods. <PARAGRAPH_SEPARATOR> Remark 5. It is straightforward to show that <PARAGRAPH_SEPARATOR> $$ \\operatorname{vech}(\\mathbf{H}_{\\mathrm{AMISE}}-\\mathbf{H}_{\\mathrm{MISE}})=O(\\mathbf{J}_{d^{*}}n^{-2/(d+4)})\\operatorname{vech}\\mathbf{H}_{\\mathrm{MISE}} $$ <PARAGRAPH_SEPARATOR> so that the discrepancy between $\\mathbf{H}_{\\mathrm{AMISE}}$ and $\\mathbf{H}_{\\mathrm{MISE}}$ is asymptotically negligible in comparison to the relative rate of convergence of $\\check{\\mathbf{H}}_{\\mathrm{SAMSE}}$ to $\\mathbf{H}_{\\mathrm{AMISE}}$ : However, the discrepancy between $\\mathbf{H}_{\\mathrm{AMISE}}$ and $\\mathbf{H}_{\\mathrm{MISE}}$ dominates the AMSE rate from Theorem 1 for $d>4$ : <PARAGRAPH_SEPARATOR> Proof of Theorem 1. From Wand and Jones [12] we know that the estimator $\\check{\\psi}_{r}$ has slowest rate if at least one element of $r$ is odd because it is impossible to annihilate the leading term in the bias in this instance. When $|r|=4$ the optimal pilot bandwidth for these functional estimators is $g_{r,\\mathrm{AMSE}}=O(n^{-2/(d+12)})$ ; giving <PARAGRAPH_SEPARATOR> $$ s\\check{\\psi}_{r}(g_{r,\\mathrm{AMSE}})=O(g_{r,\\mathrm{AMSE}}^{2})=O(n^{-4/(d+12)}), $$ <PARAGRAPH_SEPARATOR> $$ \\operatorname{Var}\\check{\\psi}_{r}(g_{r,\\mathrm{AMSE}})=O(n^{-2}g_{r,\\mathrm{AMSE}}^{-d-8})=O(n^{-8/(d+12)}). $$ <PARAGRAPH_SEPARATOR> It follows that <PARAGRAPH_SEPARATOR> $$ \\mathbb{E}[D_{\\mathbf{H}}(\\mathrm{PI}-\\mathrm{AMISE})(\\mathbf{H}_{\\mathrm{AMISE}})]=O(\\mathbf{J}_{d^{*}}n^{-4/(d+12)}){\\mathrm{vech~H}}_{\\mathrm{AMISE}} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathrm{Var}[D_{\\mathbf{H}}(\\mathrm{PI}-\\mathrm{AMISE})(\\mathbf{H}_{\\mathrm{AMISE}})]}\\ &{\\quad=O(\\mathbf{J}_{d^{*}}n^{-8/(d+12)})(\\mathrm{vech}\\mathbf{H}_{\\mathrm{AMISE}})(\\mathrm{vech}^{T}\\mathbf{H}_{\\mathrm{AMISE}}).}\\end{array} $$ <PARAGRAPH_SEPARATOR> The Hessian matrix <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{D_{\\mathbf{H}}^{2}\\mathrm{AMISE}\\hat{f(\\cdot;\\mathbf{H})}=\\frac{1}{4}n^{-1}(4\\pi)^{-d/2}|\\mathbf{H}|^{-1/2}\\mathbf{D}_{d}^{T}(\\mathbf{H}^{-1}\\otimes\\mathbf{I}_{d})~}\\ {\\times[(\\mathrm{vec}\\mathbf{I}_{d})(\\mathrm{vec}^{T}\\mathbf{I}_{d})+2\\mathbf{I}_{d^{2}}](\\mathbf{I}_{d}\\otimes\\mathbf{H}^{-1})\\mathbf{D}_{d}+\\frac{1}{2}\\Psi_{4}~}\\end{array} $$ <PARAGRAPH_SEPARATOR> converges to a constant, positive-definite matrix as $n\\to\\infty$ : Here vec is the vector operator, so that vec $\\mathbf{H}$ is concatenation of the columns of $\\mathbf{H}$ : The duplication matrix of order $d$ is $\\mathbf{D}_{d}$ and it relates the vec and vech operators in the following ways: <PARAGRAPH_SEPARATOR> vec $\\mathbf{H}=\\mathbf{D}_{d}$ vech H; <PARAGRAPH_SEPARATOR> $$ \\mathbf{D}_{d}^{T}\\mathrm{vec}\\mathbf{H}=\\mathrm{vech}(\\mathbf{H}+\\mathbf{H}^{T}-\\mathrm{dg}\\mathbf{H}). $$ <PARAGRAPH_SEPARATOR> Also $\\otimes$ is the Kronecker (or tensor) product operator between two matrices. The proof of part (i) follows immediately by substituting (7) and (8) into the expansion of $\\mathbf{AMSE}(\\mathrm{vech}\\check{\\mathbf{H}})$ obtained from Lemma 1. <PARAGRAPH_SEPARATOR> From Duong and Hazelton [1], the SAMSE pilot bandwidth is $g_{4,\\mathrm{SAMSE}}=$ $O(n^{-1/(d+6)})$ : Straightforward calculations then give <PARAGRAPH_SEPARATOR> $\\mathrm{Bias}\\check{\\psi}_{r}(g_{j,\\mathrm{SAMSE}})=O(g_{j,\\mathrm{SAMSE}}^{2})=O(n^{-2/(d+6)})$ when it follows that <PARAGRAPH_SEPARATOR> $$ \\mathbb{E}[D_{\\mathbf{H}}(\\mathrm{PI}-\\mathrm{AMISE})(\\mathbf{H}_{\\mathrm{AMISE}})]=O(\\mathbf{J}_{d^{*}}n^{-2/(d+6)})\\mathrm{vech~\\mathbf{H}_{A M I S E}}. $$ <PARAGRAPH_SEPARATOR> It can be shown that in the SAMSE plug-in method the variance of $\\check{\\psi}_{r}$ is dominated by the leading term of the squared bias. Part (ii) then follows by substituting (9) into the result of Lemma 1, and noting the asymptotic constancy of the Hessian matrix as for part (i). & <PARAGRAPH_SEPARATOR> # 4. Relative rates of convergence for BCV selectors <PARAGRAPH_SEPARATOR> In this section we compute the relative rate of the BCV selector when $K$ is Gaussian. The results can be extended to more general kernel functions at the expense of more complex proofs."
  },
  {
    "qid": "statistic-posneg-768-6-0",
    "gold_answer": "TRUE. The paper states that if $f_{\\hat{\\theta}_{n}}(X_{i})$ is not identically equal to $\\hat{f}_{n,i}^{\\mathrm{LR}}$ for all $i$, the function $L_{n}(\\alpha)$ is strictly concave, ensuring the uniqueness of the maximizer $\\hat{\\alpha}_{n}$. Additionally, the extreme value theorem guarantees the existence of $\\hat{\\alpha}_{n}$ under the given conditions.",
    "question": "Is the fitness coefficient $\\hat{\\alpha}_{n}$ guaranteed to exist and be unique when $f_{\\hat{\\theta}_{n}}(X_{i})$ is not identically equal to $\\hat{f}_{n,i}^{\\mathrm{LR}}$ for all $i=1,\\ldots,n$?",
    "question_context": "Fitness Coefficient and Density Estimation in Parametric vs Nonparametric Models\n\nThe paper discusses the fitness coefficient $\\hat{\\alpha}_{n}$, which is a maximizer of the mixture likelihood function $L_{n}(\\alpha)$. This function is defined as a weighted sum of the log-likelihoods of a parametric model $f_{\\hat{\\theta}_{n}}(X_{i})$ and a nonparametric estimator $\\hat{f}_{n,i}^{\\mathrm{LR}}$. The fitness coefficient helps in determining the adequacy of the parametric model compared to the nonparametric one. The paper also provides bounds and convergence results for the estimators under different conditions."
  },
  {
    "qid": "statistic-posneg-322-2-0",
    "gold_answer": "FALSE. The definition of $\\gamma_{Z}$ is not derived from any standard tail index estimation method. Typically, the tail index is estimated using extreme value theory methods like Hill's estimator or Pickands' estimator. The given formula appears arbitrary and lacks justification from the context provided. The correct approach would involve deriving the tail index from the conditional distribution's tail behavior, not from the $L^{2}$ norm of the covariate process.",
    "question": "Is the conditional tail index $\\gamma_{Z}$ correctly defined as $\\gamma_{Z}:=\\big(8\\left\\|Y_{Z}\\right\\|_{L^{2}}^{2}-3\\big)/2.5$ given the context of the model?",
    "question_context": "Estimation of Extreme Multivariate Expectiles with Conditional Tail Index\n\nWe propose here to illustrate the finite-sample performances of our estimation method through a parametric model. The considered covariate model mimics Gardes and Girard [19], Girard et al. [23] by considering $Z$ uniformly distributed on $[1/4,1]$ and the following random process as covariate lying in $E=L^{2}([0,1])$: $Y_{Z}(t):=\\cos(2\\pi Z t),\\quad t\\in[0,1]$. The conditional tail index is defined as $\\gamma_{Z}:=\\big(8\\left\\|Y_{Z}\\right\\|_{L^{2}}^{2}-3\\big)/2.5$. The considered conditional distributional model is a Lomax/centered Pareto type II marginal distributions linked through a survival Clayton dependence structure. The survival Clayton copula is given by $C_{y}^{\\mathrm{Cl},S}(u)=\\operatorname*{max}\\left(\\left(\\sum_{j=1}^{d}u_{j}^{-\\gamma_{y}}-d+1\\right)^{-1/\\gamma_{y}},0\\right),\\quad u=(u_{1},\\ldots,u_{d})\\in[0,1]^{d}$. The tail dependence function is $\\lambda_{j,k,y}^{\\mathrm{Cl}}(x_{1},x_{2})=\\left(x_{1}^{-\\gamma_{y}}+x_{2}^{-\\gamma_{y}}\\right)^{-1/\\gamma_{y}}$. The sampling method is by mixture Maume-Deschamps et al. [32], generating $G$ a Gamma distribution with shape parameter $1/\\gamma_{y}$ and scale parameter 1, then drawing $d$ independent exponential distributions with rate parameter $G/s_{j}$."
  },
  {
    "qid": "statistic-posneg-666-0-0",
    "gold_answer": "TRUE. The likelihood function integrates over the random effect $\\xi_i$ (family-level variability) via $J_i$, which captures the shared dispersion/correlation for all siblings in family $i$, including single-sibling cases. Fixed-effects models (e.g., Liang et al. 1992) fail here because they rely on pairwise odds ratios, which are undefined for single-sibling families. The mixed model's random effect $\\sigma\\xi_i$ naturally induces overdispersion unconditionally, even for $n_i=1$, resolving the arbitrariness of pairwise association assumptions.",
    "question": "The binary mixed model's likelihood function $\\log L = \\sum_{i=1}^{K}\\sum_{j=1}^{n_{i}}y_{i j}x_{i j}^{\\mathrm{T}}\\beta + \\sum_{i=1}^{K}\\log J_{i}$ explicitly accounts for family-level random effects through the term $J_{i} = \\int_{-\\infty}^{\\infty}\\exp(\\sigma s_{i}\\xi_{i})\\varLambda_{i}\\phi(\\xi_{i})\\mathrm{d}\\xi_{i}$. Is this statement true, and does it address the limitation of fixed-effects models in handling single-sibling families?",
    "question_context": "Binary Mixed Model Likelihood Inference for Correlated Sibling Data\n\nThe paper discusses the analysis of correlated binary responses (IPF status) among siblings of COPD patients using a binary mixed model. It critiques previous approaches using fixed logistic models and pairwise odds ratios, proposing instead a random effects model to account for family-level overdispersion and correlation. The likelihood approach involves integrals approximated via simulation, with detailed formulas for score functions and information matrices."
  },
  {
    "qid": "statistic-posneg-427-0-3",
    "gold_answer": "TRUE. The expression for P_X includes three terms: (1) joint probability of A failing before B and T_0, (2) a dependence adjustment via α, and (3) censoring terms at T_0. The integral terms handle continuous failure times, while the products account for censoring, making it valid for α ≠ 0 and finite T_0.",
    "question": "Does the formula for P_X correctly account for both dependent failures (α ≠ 0) and censoring at T_0?",
    "question_context": "Bivariate Time on Trial Estimates: Robustness to Departures from Independence and Exponentiality\n\nThe paper investigates the robustness of bivariate 'time on trial' estimates when there are slight departures from the assumptions of independence and exponential distributions for failure times. The estimates are derived under a Morgenstern-Farlie bivariate distribution model, allowing for dependence between the failure times of two causes (A and B). Key formulas include the conditional expectation of the estimator λ given k and j failures, and the overall expectation considering both k=0 and k>0 scenarios. Numerical results assess the bias in estimating failure rates under various dependence structures and marginal distributions."
  },
  {
    "qid": "statistic-posneg-815-0-0",
    "gold_answer": "FALSE. The correct specification should account for the degrees of freedom properly. The term $\\left(N-1\\right)\\tau^{2}$ should be $\\left(N-n\\right)\\tau^{2}$ to reflect the within-progeny variance, and the between-progeny variance should involve $\\left(n-1\\right)\\tau^{2}$. The model mixes up the degrees of freedom for within and between progenies.",
    "question": "Is the variance decomposition model correctly specified as $\\mathcal{\\bar{L}}(\\mathcal{w}-\\overline{{\\omega}})^{2}=\\left(\\frac{\\mathcal{N}^{2}-\\mathcal{L}k_{r}^{2}}{N}\\right)\\sigma^{2}+\\left(N-1\\right)\\tau^{2}$ for the total sum of squares of deviations in egg weights?",
    "question_context": "Variance Decomposition in Egg Weight Analysis\n\nFirst let us consider those eggs which hatched. Let the weight of such an egg be $W+w$ where $\\pmb{W}$ is the true mean, or the mean of a very large sample. Let ${\\pmb w}={\\pmb x}{\\pmb\\sigma}+{\\pmb y}{\\pmb\\tau}$ where $\\pmb{x}$ and $\\pmb{y}$ are uncorrelated reduced normal variables, i.e. $\\overline{{x}}=\\overline{{y}}=0$ , $\\overline{{x^{2}}}=\\overline{{y^{2}}}=1$ ; and let $\\pmb{x}$ be constant for any given duck. That is, $_{\\sigma x}$ represents the deviation from mean egg weight due to a given duck's make-up, and $\\tau y$ the deviation of the egg weight from $\\pmb{W}+\\sigma\\pmb{x}$ during the duck's life. We wish to estimate $\\sigma$ and $\\tau$ , and to know whether they differ significantly from $\\sigma^{\\prime}$ and $\\tau^{\\prime}$ , the corresponding quantities, for unhatched eggs. <PARAGRAPH_SEPARATOR> Let there be $\\pmb{\\mathscr{n}}$ ducks laying $\\dot{\\pmb{N}}$ good eggs in all. Let the rth duck lay $k_{r}$ good eggs, so that <PARAGRAPH_SEPARATOR> Then the total sum of squares of deviations <PARAGRAPH_SEPARATOR> $$ \\mathcal{\\bar{L}}(\\mathcal{w}-\\overline{{\\omega}})^{2}=\\left(\\frac{\\mathcal{N}^{2}-\\mathcal{L}k_{r}^{2}}{N}\\right)\\sigma^{2}+\\left(N-1\\right)\\tau^{2}, $$ <PARAGRAPH_SEPARATOR> and the coresponding variance (mean quare) is N2-2Ng2+12, or[1+ $\\biggl[1+\\frac{N-\\Sigma k_{r}^{2}}{N(N-1)}\\biggr]\\sigma^{2}+\\tau^{2},$ which is nearly ${\\boldsymbol{\\sigma}}^{\\mathfrak{z}}+{\\boldsymbol{\\tau}}^{\\mathfrak{z}}$ . The sum of squares of deviations within progenies of individual ducks $\\varSigma(w-\\overleftarrow{w_{r}})^{2}=\\left(N-n\\right)\\tau^{2}$ . and the corresponding inean square is $\\tau^{2}$ .Subtracting the two sums of squares, the sum of squares of (leviations between progenies is <PARAGRAPH_SEPARATOR> $$ (\\ L;\\overline{{{w}}}_{r}-\\overline{{{w}}})^{2}=\\frac{N^{2}-\\Sigma k^{2}}{N}\\sigma^{2}+\\left(n-1\\right)\\tau^{2}, $$ <PARAGRAPH_SEPARATOR> and the mean square is <PARAGRAPH_SEPARATOR> $$ {\\frac{N^{2}-{\\bar{Z}}k^{2}}{N(n-1)}}\\sigma^{2}+\\tau^{2}. $$ <PARAGRAPH_SEPARATOR> # We have therefore: <PARAGRAPH_SEPARATOR> <html><body><table><tr><td></td><td>Degrees of freedom</td><td>Sum of squares</td><td>Mean square</td></tr><tr><td>Within progenies Between progenies</td><td>N-n n-1</td><td>(N-n)t N-EH +(n-1) t N</td><td>N' -- EK +b N(n-1)</td></tr><tr><td>Total</td><td>N-1</td><td>N-EH ²+(N-1) N</td><td>N-2H ？+b N(N- 1)</td></tr></table></body></html> <PARAGRAPH_SEPARATOR> The sum of squares of deviations between means of progenies, and the total, are readily found, and the variance $\\tau^{2}$ within progenies is found by subtraction. ${\\pmb\\sigma}^{\\pmb{\\mathscr{s}}}$ is then readily found."
  },
  {
    "qid": "statistic-posneg-1349-0-0",
    "gold_answer": "FALSE. The formulas show that for GT/gt and Gt/gT matings, the right-hand side of the genotype probabilities for repulsion with linkage value +η is identical to that for coupling with linkage value -η. This means that, based on the genotype probabilities alone, the two phases (coupling and repulsion) are indistinguishable when only considering the next generation. The paper explicitly states that linkage values +η and -η cannot be distinguished statistically in such cases, although biological reasons suggest η must be positive.",
    "question": "Is it true that the eta measure (η) can distinguish between coupling and repulsion phases in all cases, given the formulas provided for GT/gt and Gt/gT matings?",
    "question_context": "Linkage Detection in Human Genetics Using the Eta Measure\n\nThe paper discusses the detection of linkage in human genetics, focusing on the challenges and methodologies involved. It introduces the eta measure (η = 1 - 2χ) as an alternative to the recombination fraction (χ) for quantifying linkage. The fundamental formulas provided describe the probabilities of different genotypes in offspring given parental genotypes and the linkage measure η. The paper also addresses issues such as biased sampling, the significance level of tests, and the indistinguishability of coupling and repulsion phases in certain scenarios."
  },
  {
    "qid": "statistic-posneg-839-0-1",
    "gold_answer": "FALSE. While the proof does provide bounds on the probability of deviations at the vertices, it does not directly establish the uniform convergence claim. The proof's focus on vertices and their neighborhoods is a step towards the result, but the final assertion of uniform convergence requires additional justification that is not fully detailed in the provided context.",
    "question": "The proposition claims that $\\operatorname*{sup}\\{\\parallel S_{n}(t)-S(t)\\parallel_{2}\\colon\\parallel t\\parallel_{2}\\leqslant C\\}\\stackrel{P}{\\to}0$ as $n \\to \\infty$. Is this claim supported by the proof that bounds the probability of large deviations in terms of the behavior at the vertices of the cubes $K_j$?",
    "question_context": "Convergence of Monotone Random Fields in High Dimensions\n\nThe paper discusses the convergence properties of monotone random fields in high-dimensional spaces. It presents a lemma and a proposition that establish bounds on the difference between a random field $S_n(t)$ and its limit $S(t)$ in terms of the $L_1$ and $L_2$ norms. The key results include inequalities that relate the difference at any point $t$ within a cube to the maximum difference at the vertices of the cube, scaled by a factor depending on the cube's dimensions. The proposition then uses these bounds to show uniform convergence in probability of $S_n(t)$ to $S(t)$ over compact sets."
  },
  {
    "qid": "statistic-posneg-87-0-1",
    "gold_answer": "TRUE. The representation is correct for absolutely continuous random variables, as derived in Bedford and Cooke (2001). The joint density is decomposed into the product of marginal densities and the product of pair-copula densities, which capture the dependencies between variables conditioned on other variables in the vine structure.",
    "question": "In the joint density decomposition formula for vine copulas, the product of marginal densities $\\prod_{i=1}^{d}f_{i}(y_{i})$ is multiplied by the product of all pair-copula densities $\\prod_{[j k|S]\\in E(\\mathcal{V})}\\tilde{c}_{j k;S}(y_{j},y_{k};\\mathbf{y}_{S})$. Is this representation correct for absolutely continuous random variables?",
    "question_context": "Vine Copula Regression: Joint Density Decomposition and Simplifying Assumption\n\nDefinition 2.1 (Regular Vine). $\\nu$ is a regular vine on $d$ elements, with $\\textstyle E(\\mathcal{V})=\\bigcup_{i=1}^{d-1}E(T_{i})$ denoting the set of edges of $\\nu$ , if\n\n1. $\\nu=(T_{1},\\dots,T_{d-1})$ [consists of $d-1$ trees];   \n2. $T_{1}$ is a connected tree with nodes $N(T_{1})=\\{1,2,\\dots,d\\}$ , and edges $E(T_{1}),T_{\\ell}$ is a tree with nodes $N(T_{\\ell})=E(T_{\\ell-1})$ [edges in a tree becomes nodes in the next tree];   \n3. (proximity) for $\\ell=2,\\ldots,d-1$ , for $\\{n_{1},n_{2}\\}\\in E(T_{\\ell}),\\#(n_{1}\\triangle n_{2})=2,$ where $\\triangle$ denotes symmetric difference and # denotes cardinality [nodes joined in an edge differ by two elements].\n\nTo get a vine copula or pair-copula construction, for each edge $[j k|S]\\in E(\\mathcal{V})$ in the vine, there is a bivariate copula $C_{j k;S}$ associated with it. Let $\\tilde{c}_{j k;S}(\\cdot;\\mathbf{y}_{S})$ be as defined in Section 2.1 for $C_{j k;S}(\\cdot;\\mathbf{y}_{S})$ when the conditioning value is $\\mathbf{y}_{S}$ , and let $C_{j|k;S}(a|b;\\mathbf{y}_{S})=\\partial C_{j k;S}(a,b;\\mathbf{y}_{S})/\\partial b$ and $C_{k|j;S}(b|a;\\mathbf{y}_{S})=\\partial C_{j k;S}(a,b;\\mathbf{y}_{S})/\\partial a.C_{j|k;S}$ and $C_{k|j;S}$ are the conditional CDFs of the copula $C_{j k;S}$ . The joint density of $(Y_{1},\\dots,Y_{d})$ can be decomposed according to the vine structure $\\nu$ .\n\n$$\n{\\cal f}_{1:d}(y_{1},\\dots,y_{d})=\\prod_{i=1}^{d}f_{i}(y_{i})\\cdot\\prod_{[j k|S]\\in E(\\mathcal{V})}\\tilde{c}_{j k;S}(y_{j},y_{k};\\mathbf{y}_{S}).\n$$\n\nThe above representation for the case of absolutely continuous random variables is derived in Bedford and Cooke (2001); its extension to include some discrete variables is in Section 3.9.5 of Joe (2014). For simplicity of notation, we denote $F_{j|S}^{+}=F_{j|S}(y_{j}|\\mathbf{y}_{S})$ and $\\begin{array}{r}{F_{j|S}^{-}=\\operatorname*{lim}_{t\\uparrow y_{j}}F_{j|S}(t|\\mathbf{y}_{S}).}\\end{array}$ If it is assumed that the copulas on edges of trees 2 to $d-1$ do not depend on the values of the conditioning values, then ${\\cal C}_{j k;S}$ and $\\tilde{c}_{j k;S}$ in (2.2) do not depend on $\\mathbf{y}_{S}$ ; i.e., $c_{j k;S}(\\cdot)=c_{j k;S}(\\cdot;\\mathbf{y}_{S})$ and $\\tilde{c}_{j k;S}(\\cdot)=\\tilde{c}_{j k;S}(\\cdot;{\\bf y}_{S})$ . This is called the simplifying assumption. With the simplifying assumption, we have the following definition of jk;s.\n\n• If $Y_{j}$ and $Y_{k}$ are both continuous, then $\\tilde{c}_{j k;S}(y_{j},y_{k}):=c_{j k;S}(F_{j|S}^{+},F_{k|S}^{+})$ .\n\n• If $Y_{j}$ is continuous and $Y_{k}$ is discrete, then\n\n$$\n\\widetilde{c}_{j k;S}(y_{1},y_{k}):=\\big[C_{k|j;S}(F_{k|S}^{+}|F_{j|S}^{+})-C_{k|j;S}(F_{k|S}^{-}|F_{j|S}^{+})\\big]/f_{k|S}(y_{k}|\\mathbf{y}_{S}).\n$$\n\n• If $Y_{j}$ is discrete and $Y_{k}$ is continuous, then\n\n$$\n\\begin{array}{r}{\\widetilde{c}_{j k;S}(y_{j},y_{k}):=\\left[C_{j|k;S}(F_{j|S}^{+}|F_{k|S}^{+})-C_{j|k;S}(F_{j|S}^{-}|F_{k|S}^{+})\\right]/f_{j|S}(y_{j}|\\mathbf{y}_{S}).}\\end{array}\n$$\n\n• If $Y_{j}$ and $Y_{k}$ are both discrete, then\n\n$$\n\\tilde{c}_{j k;5}(y_{j},y_{k}):=\\big[C_{j k;5}(F_{j|5}^{+},F_{k|5}^{+})-C_{j k;5}(F_{j|5}^{-},F_{k|5}^{+})-C_{j k;5}(F_{j|5}^{+},F_{k|5}^{-})+C_{j k;5}(F_{j|5}^{-},F_{k|5}^{-})\\big]/\\big[f_{j|5}(y_{j}||\\mathbf{y}_{5})f_{k|5}(y_{k}||\\mathbf{y}_{5})\\big]\n$$\n\nA $t$ -truncated vine copula results if the copulas for trees $T_{t+1},\\dots,T_{d-1}$ are all independence copulas, representing conditional independencies.\n\n# 3. Vine copula regression\n\nConsider the data structure for multiple regression with $p$ explanatory variables $x_{1},\\ldots,x_{p}$ and response variable $y$ as a sample of size $n$ ; the data are $(x_{i1},\\dots,x_{i p},y_{i})$ for $i=1,\\ldots,n$ , considered as independent realizations of a random vector $(X_{1},\\ldots,X_{p},Y)$ . If these data are considered as a sample in an observational study, then a natural approach is to fit a joint multivariate density to the variables $x_{1},\\ldots,x_{p},y.$ This can be done using a flexible, parametric vine copula."
  },
  {
    "qid": "statistic-posneg-768-2-0",
    "gold_answer": "TRUE. Explanation: For Gaussian tails, the condition $h_n^2 \\log(n) \\to 0$ ensures that the supremum term $\\exp(2\\sqrt{-\\log(b_n)}h_n)$ in the verification of (H3) converges appropriately. This condition is automatically satisfied under (H2), which requires $n h_n^{2d+4} \\to 0$ and $n h_n^d / |\\log(h_n)| \\to \\infty$. Thus, Gaussian tails naturally satisfy (H3) under (H2 without additional restrictions.",
    "question": "Is it true that for densities with Gaussian tails, the condition $h_n^2 \\log(n) \\to 0$ is sufficient to satisfy assumption (H3) under (H2)?",
    "question_context": "Kernel Density Estimation and Tail Behavior Analysis\n\nThe paper discusses kernel density estimation under various tail behaviors of the underlying density function. It introduces assumptions (H1)-(H3) to ensure uniform consistency of the estimator. The key focus is on how the tail behavior of the density (Gaussian, exponential, polynomial) affects the bandwidth selection and the convergence rates."
  },
  {
    "qid": "statistic-posneg-421-0-3",
    "gold_answer": "FALSE. The paper clearly states in equation (9) that when $\\upmu$ is known, the posterior distribution of $\\pmb{\\Sigma}^{-1}$ is $W(n + \\nu + q - 1, (\\mathbf{U} + \\boldsymbol{\\uprho})^{-1})$. The degrees of freedom parameter is $n + \\nu + q - 1$, not $n + \\nu + q - 2$ as suggested in the question. The latter value applies when $\\upmu$ is unknown.",
    "question": "Does the paper claim that the posterior distribution of $\\pmb{\\Sigma}^{-1}$ when $\\upmu$ is known follows a Wishart distribution with degrees of freedom parameter $n + \\nu + q - 2$?",
    "question_context": "Bayesian Estimation of Multivariate Normal Parameters\n\nA significant omission from the work of Raiffa and Schlaifer (1961) relating to a multivariate normal distribution is consideration of the case when the dispersion matrix is completely unknown. This paper partially fills the gap by applying the methods of Raiffa and Schlaifer to the particular problems of estimating the mean $\\upmu$ , the dispersion matrix $\\pmb{\\Sigma}$ , and the generalized variance $|\\pmb{\\Sigma}|,$ wheneitherorbothof $\\upmu$ and $\\pmb{\\Sigma}$ are unknown."
  },
  {
    "qid": "statistic-posneg-888-0-0",
    "gold_answer": "FALSE. The condition $\\|\\widehat{Q}_{S^{c}S}(\\widehat{Q}_{S S})^{-1}\\|_{\\infty}\\leq1-\\gamma/2$ is sufficient but not necessary for strict dual feasibility. The paper shows that this condition, along with $\\|\\dot{\\ell}(\\beta^{0})\\|_{\\infty}\\leq\\frac{\\gamma}{8+2\\gamma}\\lambda$, ensures strict dual feasibility, but other conditions might also suffice under different assumptions or contexts.",
    "question": "Is the condition $\\|\\widehat{Q}_{S^{c}S}(\\widehat{Q}_{S S})^{-1}\\|_{\\infty}\\leq1-\\gamma/2$ necessary for strict dual feasibility in the Lasso estimator for high-dimensional Cox models?",
    "question_context": "Sign Consistency of Lasso in High-Dimensional Cox Models\n\nThe paper discusses the sign consistency of the Lasso estimator in high-dimensional Cox models with counting process data and time-dependent covariates. It provides sufficient and necessary conditions for sign consistency, including mutual incoherence and minimum coefficient value conditions. The analysis involves martingale theory and concentration inequalities to verify strict dual feasibility and derive convergence rates."
  },
  {
    "qid": "statistic-posneg-1108-0-0",
    "gold_answer": "FALSE. The paper states that Plackett's approximation is very good when correlations are -0.5 but also mentions that it cannot lead to a large error for larger correlations, implying it is not strictly limited to the -0.5 case.",
    "question": "Is Plackett's approximate formula for P(5,5) valid only when all correlations are exactly -0.5?",
    "question_context": "Approximation of Multivariate Normal Probabilities for Ordered Alternatives\n\nThe paper discusses the approximation of probabilities for the positive quadrant of a four-variate normal distribution with a specific correlation matrix. The focus is on the case where the number of categories (k) is 5, and the correlations are equal to minus one-half. Plackett's approximate formula for P(5,5) is provided, and its validity is discussed for small correlations. The paper also compares exact and approximate probability integrals for the case where correlations are -0.5."
  },
  {
    "qid": "statistic-posneg-601-0-0",
    "gold_answer": "TRUE. The joint model assumes that the responses (continuous and ordinal) are independent conditional on the random effects. This means the joint density of the responses given the random effects is indeed the product of the individual conditional densities. This assumption simplifies the likelihood function, allowing for the integration over the random effects to obtain the marginal likelihood.",
    "question": "In the joint model, the conditional independence assumption implies that the responses are independent given the random effects, and thus the joint density is the product of the individual conditional densities. Is this statement true based on the model formulation?",
    "question_context": "Joint Normal-Ordinal (Probit) Model for Longitudinal Data\n\nThe paper presents a joint model for analyzing longitudinal data with both continuous and ordinal responses. The model employs random effects to account for correlations within and between responses. The continuous response is modeled using a linear mixed model, while the ordinal response is modeled using a random-effects ordinal regression model with a probit link. The joint likelihood integrates over the random effects to estimate parameters via maximum likelihood, with numerical approximations such as adaptive Gaussian quadrature used for the integrals."
  },
  {
    "qid": "statistic-posneg-71-0-0",
    "gold_answer": "TRUE. The full-model empirical likelihood ratio $\\mathcal{R}^{F}(\beta,\\alpha)$ is defined such that it equals 1 when evaluated at the full-model GEE estimate $(\\hat{\beta}^{\\mathrm{ST}},\\hat{\\alpha}_{1}^{\\mathrm{ST}},\\hat{\\alpha}_{2}^{\\mathrm{ST}})$, since these estimates solve the equation $\\frac{1}{n}\\sum_{i}^{n}g^{F}(Y_{i},X_{i},\beta,\\alpha_{1},\\alpha_{2};R_{F})=0$. This ensures that the weights $w_i$ in the empirical likelihood ratio are all $1/n$, making $\\mathcal{R}^{F}(\\hat{\beta}^{\\mathrm{ST}},\\hat{\\alpha}_{1}^{\\mathrm{ST}},\\hat{\\alpha}_{2}^{\\mathrm{ST}})=1$.",
    "question": "Is it true that the full-model empirical likelihood ratio $\\mathcal{R}^{F}(\beta,\\alpha)$ is always equal to 1 for the full model GEE estimate $(\\hat{\beta}^{\\mathrm{ST}},\\hat{\\alpha}_{1}^{\\mathrm{ST}},\\hat{\\alpha}_{2}^{\\mathrm{ST}})$?",
    "question_context": "Empirical Likelihood for Working Correlation Structure Selection in GEE Models\n\nThe paper discusses the use of empirical likelihood to select the optimal working correlation structure in Generalized Estimating Equations (GEE) models. It introduces a full-model empirical likelihood ratio, which serves as a unified measure to compare different working correlation structures. The paper also proposes modified versions of AIC and BIC, called EAIC and EBIC, which use empirical likelihood instead of parametric likelihood for model selection."
  },
  {
    "qid": "statistic-posneg-1839-1-0",
    "gold_answer": "FALSE. While the paper claims that $\\hat{\\pmb{\\mu}}^{\\mathrm{P^{+}}}$ is superior to $\\hat{\\pmb{\\mu}}^{\\mathrm{P}}$ regardless of the null hypothesis, the actual performance of $\\hat{\\pmb{\\mu}}^{\\mathrm{P^{+}}}$ depends on the correctness of the UPI (Uncertainty Parameter Interval). Specifically, when $F_{(\\alpha)}$ takes a value outside the interval $[0,c]$, $\\hat{\\pmb{\\mu}}^{+}$ behaves similarly to $\\hat{\\pmb{\\mu}}^{\\mathrm{~r~h~}}$ compared to $\\hat{\\pmb{\\mu}}$. This indicates that the superiority is conditional on the UPI's correctness.",
    "question": "Is the estimator $\\hat{\\pmb{\\mu}}^{\\mathrm{P^{+}}}$ always superior to $\\hat{\\pmb{\\mu}}^{\\mathrm{P}}$ regardless of the correctness of the null hypothesis, as claimed in the paper?",
    "question_context": "Risk Improvement Analysis of Estimators in Statistical Models\n\nThe paper discusses the superiority of the estimator $\\hat{\\pmb{\\mu}}^{\\mathrm{P^{+}}}$ over $\\hat{\\pmb{\\mu}}^{\\mathrm{P}}$ regardless of the correctness of the null hypothesis. It also introduces the formula for percentage improvement in risks of various estimators over the UE (Unbiased Estimator) using $P I_{b}=\\frac{100(R_{b}-R_{1})}{R_{1}}\\quad b=2,3,4,5,$ where $R_{1}{-}R_{5}$ are the risks of $\\hat{\bar{\\mu}},\\hat{\bar{\\mu}}^{\\mathrm{S}^{+}},\\hat{\bar{\\mu}}^{\\mathrm{S}},\\hat{\bar{\\mu}}^{\\mathrm{P}^{+}},\\mathrm{and}\\hat{\bar{\\mu}}^{\\mathrm{P}}$, respectively. The paper provides tables showing percentage improvements for different combinations of $(n,p,\\alpha)$ and $\\varDelta=0$, 0.5, 1.0, 2.0."
  },
  {
    "qid": "statistic-posneg-1729-0-0",
    "gold_answer": "TRUE. The independence assumptions (12)–(13) make $\\gamma_{2}(t)$ behave as a nugget effect, adding a term $\\mathrm{Var}(\\gamma_{2}(t_{0}))$ to the prediction error variance. Since $\\operatorname{Var}(\\gamma_{2}(\\cdot))$ is derived from the variance of the underlying field $Y$, which is larger than the prediction errors $\\hat{Y}-Y$, the prediction error variance is overestimated at non-lattice locations. This overestimation does not occur at lattice locations where $\\mathrm{Var}(\\gamma_{2}(t_{0}))=0$.",
    "question": "Is the prediction error variance at non-lattice locations overestimated due to the independence assumptions (12)–(13) of $\\gamma_{2}(t)$?",
    "question_context": "Fast Kriging Prediction with Gaussian Markov Random Fields\n\nThis makes $\\gamma_{1}(\\cdot)$ a continuous Gaussian process that equals the GMRF $\\tilde{\\gamma}$ at grid locations. Elsewhere $\\gamma_{1}(\\cdot)$ is the weighted mean value of the four closest lattice points. The variance of $\\gamma_{1}(\\cdot)$ is tuned to $\\gamma(\\cdot)$ at lattice locations (Eq. (8)), and thus the variance of $\\gamma_{1}(\\cdot)$ will be lower than that of $\\gamma(\\cdot)$ within cells. The addition of $\\gamma_{2}(\\cdot)$ in (10) gives the stipulated variances (though not covariances) even for locations within cells.<PARAGRAPH_SEPARATOR># 3.2. Prediction<PARAGRAPH_SEPARATOR>This section contains an algorithm for efficient use of the suggested model when kriging large data sets. Let $\\pmb{K}=(k_{i j})$ be the $n\\times N$ matrix defined as $k_{i j}=k({t_{i}},\\tilde{{t}}_{j})$ . Then from (7)–(13) we have<PARAGRAPH_SEPARATOR>$$\\pmb{\\Sigma}=\\pmb{K}\\pmb{Q}^{-1}\\pmb{K}^{\\mathrm{T}}+\\pmb{D}.$$<PARAGRAPH_SEPARATOR>Here ${\\pmb Q}=({\\pmb I}_{N}-{\\pmb W})\\tau^{-2}$ is the precision matrix of $\\tilde{\\gamma}$ and $\\pmb{D}$ is a (diagonal) matrix with the variance of $\\gamma_{2}(t)+\\varepsilon(t)$ at the locations where the observations are collected. Thus, from (14) it follows that<PARAGRAPH_SEPARATOR>$$D=\\sigma_{\\gamma}^{2}I_{n}-\\mathrm{diag}(K\\tilde{\\Sigma}K^{\\mathrm{T}})+\\sigma_{\\varepsilon}^{2}I_{n},$$<PARAGRAPH_SEPARATOR>where $\\tilde{\\Sigma}=(\\mathbf{Cov}(\\gamma(\\tilde{t}_{i}),\\gamma(\\tilde{t}_{j})))_{i,j=1:N}.$ . In order to calculate $\\tilde{K\\Sigma K^{\\mathrm{T}}}$ only the diagonals corresponding to adjacent lattice locations must be known in $\\tilde{\\Sigma}$ .<PARAGRAPH_SEPARATOR>For an arbitrary point $\\pmb{t}_{0}$ , where we want to predict $Y$ , define the vector of $k$ -weights for the point, ${\\pmb k}=(k_{1},\\ldots,k_{N})^{\\mathrm{T}}$ , where $k_{i}=k(\\pmb{t}_{0},\\tilde{\\pmb{t}}_{i})$ , to obtain<PARAGRAPH_SEPARATOR>$${\\pmb{\\omega}}={\\pmb{K}}{\\pmb{Q}}^{-1}{\\pmb{k}}.$$<PARAGRAPH_SEPARATOR>In all equations it is assumed that the locations where predictions are wanted do not coincide with any location with an observation used in the calculations unless such a location is also a lattice location. As the index $t$ is continuous, this restriction does not affect the usefulness of the method.<PARAGRAPH_SEPARATOR>Should approximations other than (12)–(14) of $\\gamma_{2}(t)$ be used, Eqs. (17)–(19) would change.<PARAGRAPH_SEPARATOR>Insertion of expressions (17)–(19) into (3) gives the interpolation weights for our model. While direct calculation does not give any substantial computational gain, the following method of computation will considerably ease the computational burden of calculating the kriging predictions for large data sets.<PARAGRAPH_SEPARATOR>(1) Initially predict $Y(\\cdot)$ on all grid points. Let therefore<PARAGRAPH_SEPARATOR>and $\\hat{\\mathbf{\\calY}}=(\\hat{Y}(\\tilde{t}_{1}),\\dots,\\hat{Y}(\\tilde{t}_{N}))^{\\mathrm{T}}$ .<PARAGRAPH_SEPARATOR>Using Eqs. (2) and (3) for $\\pmb{t}_{0}=\\tilde{\\pmb{t}}_{1},\\dots,\\tilde{\\pmb{t}}_{N}$ we conclude<PARAGRAPH_SEPARATOR>$$\\hat{\\pmb{Y}}=\\left(\\pmb{\\Omega}+\\mathbf{1}_{n}\\left(\\frac{\\mathbf{1}_{N}^{\\mathrm{T}}-\\mathbf{1}_{n}^{\\mathrm{T}}\\pmb{\\Sigma}^{-1}\\pmb{\\Omega}}{\\mathbf{1}_{n}^{\\mathrm{T}}\\pmb{\\Sigma}^{-1}\\mathbf{1}_{n}}\\right)\\right)^{\\mathrm{T}}\\pmb{\\Sigma}^{-1}\\pmb{Z}.$$<PARAGRAPH_SEPARATOR>(2) At an arbitrary point $\\pmb{t}_{0}$<PARAGRAPH_SEPARATOR>$$\\hat{Y}(t_{0})=k^{\\mathrm{T}}\\hat{Y},$$<PARAGRAPH_SEPARATOR>see Appendix A for details.<PARAGRAPH_SEPARATOR>From (20) and (21) we get $\\hat{Y}(\\pmb{t}_{0})$ at any point $\\scriptstyle t_{0}$ . It is therefore enough to seek computationally effective expressions for $\\pmb{\\Sigma}^{-1}\\pmb{\\Omega}$ and $\\pmb{\\Sigma}^{-1}\\mathbf{1}_{n}$ .<PARAGRAPH_SEPARATOR>The definition of the intermediate-scale variation gives<PARAGRAPH_SEPARATOR>$$\\Omega=K Q^{-1}$$<PARAGRAPH_SEPARATOR>that together with (17) yields<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}&{\\Sigma^{-1}\\Omega=\\Sigma^{-1}\\Omega(Q+K^{\\operatorname{T}}D^{-1}K)(Q+K^{\\operatorname{T}}D^{-1}K)^{-1}}\\ &{\\qquad=\\Sigma^{-1}\\Sigma D^{-1}K\\cdot(Q+K^{\\operatorname{T}}D^{-1}K)^{-1}}\\ &{\\qquad=D^{-1}K(Q+K^{\\operatorname{T}}D^{-1}K)^{-1}.}\\end{array}$$<PARAGRAPH_SEPARATOR>Further, since the row-sums of $K$ equal 1,<PARAGRAPH_SEPARATOR>$$\\Sigma^{-1}\\mathbf{1}_{n}=\\Sigma^{-1}K\\mathbf{1}_{N}=\\Sigma^{-1}\\pmb{\\Omega}\\pmb{Q}\\mathbf{1}_{N}.$$<PARAGRAPH_SEPARATOR>From (23) and (24) we can thus conclude that to calculate (20), it suffices to invert a band limited $N\\times N$ matrix $Q+K^{\\mathrm{T}}D^{-1}K$ . When $K$ interpolates only grid points within the Markov neighbourhood (as for nearest neighbour or bilinear interpolation), the addition of $\\hat{K^{\\mathrm{T}}D^{-1}K}$ adds no extra bandwidth to the GMRF precision matrix $\\boldsymbol{\\mathscr{Q}}$ , and thus no extra computational cost.<PARAGRAPH_SEPARATOR># 3.3. Prediction error<PARAGRAPH_SEPARATOR>To calculate the prediction error variance for the Markov approach define<PARAGRAPH_SEPARATOR>$$C_{r}(t_{1},t_{2})=\\operatorname{Cov}(r(t_{1}),r(t_{2}))$$<PARAGRAPH_SEPARATOR>to be the covariance function of the prediction errors. Let $\\tilde{\\omega}_{i}=\\mathrm{Cov}(\\pmb{Z},Y(\\tilde{t}_{i}))$ be the $i$ th column of $\\pmb{\\Omega}$ , and generalize (4) to get<PARAGRAPH_SEPARATOR>$$C_{r}(\\tilde{t}_{i},\\tilde{t}_{j})=C_{\\gamma}(\\tilde{t}_{i},\\tilde{t}_{j})-\\tilde{\\omega}_{i}^{\\mathrm{T}}\\Sigma^{-1}\\tilde{\\omega}_{j}+\\frac{(\\mathbf{1}_{n}^{\\mathrm{T}}\\Sigma^{-1}\\tilde{\\omega}_{i}-\\mathbf{1})(\\mathbf{1}_{n}^{\\mathrm{T}}\\Sigma^{-1}\\tilde{\\omega}_{j}-\\mathbf{1})}{\\mathbf{1}_{n}^{\\mathrm{T}}\\Sigma^{-1}\\mathbf{1}_{n}}.$$<PARAGRAPH_SEPARATOR>As $\\mathrm{Var}(\\gamma_{2}(\\tilde{\\pmb{t}}_{j}))=0$ , it follows that $C_{\\gamma}(\\tilde{t}_{i},\\tilde{t}_{j})=C_{\\gamma_{1}}(\\tilde{t}_{i},\\tilde{t}_{j})=\\mathrm{Cov}(\\tilde{\\gamma}_{i},\\tilde{\\gamma}_{j})$ . Thus after some manipulations given in Appendix B, the covariance matrix $C_{\\tilde{r}}=(C_{r}(\\tilde{t}_{i},\\tilde{t}_{j}))_{i j}$ is given by<PARAGRAPH_SEPARATOR>$$C_{\\tilde{r}}=(Q+K^{\\mathrm{T}}{\\cal D}^{-1}K)^{-1}+\\frac{(\\mathbf{1}_{n}^{\\mathrm{T}}{\\boldsymbol\\Sigma}^{-1}\\boldsymbol\\Omega-\\mathbf{1}_{N}^{\\mathrm{T}})^{\\mathrm{T}}(\\mathbf{1}_{n}^{\\mathrm{T}}{\\boldsymbol\\Sigma}^{-1}\\boldsymbol\\Omega-\\mathbf{1}_{N}^{\\mathrm{T}})}{\\mathbf{1}_{n}^{\\mathrm{T}}{\\boldsymbol\\Sigma}^{-1}\\mathbf{1}_{n}}.$$<PARAGRAPH_SEPARATOR>For the conditional prediction error the last term of (25) vanishes.<PARAGRAPH_SEPARATOR>At nonlattice locations use $\\tilde{r}=(r(\\tilde{t}_{1}),\\dots,r(\\tilde{t}_{N}))$ and the independence assumptions (12) and (13) to obtain<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}&{r(t_{0})=Y(t_{0})-\\hat{Y}(t_{0})=\\mu+\\gamma(t_{0})-k^{\\mathrm{T}}\\hat{Y}=\\mu+\\gamma_{1}(t_{0})+\\gamma_{2}(t_{0})-k^{\\mathrm{T}}\\hat{Y}}\\ &{\\qquad=k^{\\mathrm{T}}\\mu\\mathbf{1}_{N}+k^{\\mathrm{T}}\\tilde{\\gamma}+\\gamma_{2}(t_{0})-k^{\\mathrm{T}}\\hat{Y}=\\gamma_{2}(t_{0})+k^{\\mathrm{T}}(\\mu\\mathbf{1}_{N}+\\tilde{\\gamma}-\\hat{Y})}\\ &{\\qquad=\\gamma_{2}(t_{0})+k^{\\mathrm{T}}\\tilde{r}.}\\end{array}$$<PARAGRAPH_SEPARATOR>Thus<PARAGRAPH_SEPARATOR>$$\\sigma_{r}^{2}(t_{0})=k^{\\operatorname{T}}C_{\\tilde{r}}k+\\operatorname{Var}(\\gamma_{2}(t_{0})).$$<PARAGRAPH_SEPARATOR>Using an interpolation scheme where only grid points within a Markov neighbourhood are used, only terms of $C(r(\\bar{\\tilde{t}}_{i}),r(\\tilde{t}_{j}))$ where $\\tilde{\\pmb{t}}_{i}\\sim\\tilde{\\pmb{t}}_{j}$ must be calculated to obtain the first term of (27). To do this we use the efficient algorithm of Rue and Martino (2007) implemented in GMRFLib, where marginal variances and covariances for neighbours are calculated with $N\\log(N)^{2}$ operations.<PARAGRAPH_SEPARATOR># 3.3.1. Alternative prediction error variances for the bilinear interpolation<PARAGRAPH_SEPARATOR>The independence assumptions (12)–(13) make $\\gamma_{2}(\\pmb{t})$ behave as a nugget effect. This gives a term $\\mathrm{Var}(\\gamma_{2}(\\pmb{t}_{0}))$ in the prediction error variance (27).<PARAGRAPH_SEPARATOR>In many practical problems, predictions on a grid are wanted. Since at lattice locations $\\mathrm{Var}(\\gamma_{2}(t_{0}))=0$ , the additional variance term does not affect the prediction error variance as long as the GMRF is defined on that same grid. Thus, for these problems, bilinear interpolation is only used to incorporate data (that are typically not collected on a grid). Quality of predictions and prediction error variances on the grid are both better by this method than if each data point is simply assigned to the closest grid point as in nearest neighbour interpolation, see results in Section 4. However, should predictions and prediction error variances be calculated even at nonlattice locations, a deficiency with assumptions (12)–(13) is that the prediction error variance will be overestimated. This is because $\\operatorname{Var}(\\gamma_{2}(\\cdot))$ is defined directly from the variance of the underlying field Y, which has larger variance than the prediction errors $\\hat{Y}-Y$ . As the prediction error variance at lattice locations is well tuned to what it would be with the full Gaussian model, it follows that the prediction error variance will be overestimated at locations within cells.<PARAGRAPH_SEPARATOR>However, if high-quality prediction error variances are wanted at nonlattice locations, a heuristically sound alternative could be to calculate the prediction error variance as the weighted average of the prediction error variances at the grid points surrounding $\\scriptstyle t_{0}$ ,<PARAGRAPH_SEPARATOR>$$\\boldsymbol{\\sigma}_{r}^{2}(t_{0})=\\sum_{j=1}^{N}k_{j}\\boldsymbol{\\sigma}_{r}^{2}(\\tilde{t}_{j})=\\boldsymbol{k}^{\\mathrm{T}}\\boldsymbol{V}_{\\tilde{r}},$$<PARAGRAPH_SEPARATOR>where $V_{\\tilde{r}}{=}\\mathrm{diag}(C_{\\tilde{r}})$ is a diagonal matrix containing the prediction error variances on the grid. This alternative, although quite ad hoc, stems from prediction error variances being well tuned at grid points to those of a Gaussian model, and in the same way prediction error variance being continuous, just as with a continuous Gaussian model.<PARAGRAPH_SEPARATOR>For approximations with nearest neighbour interpolation where the weight $k_{j}=1$ for the closest grid point $\\tilde{\\pmb{t}}_{j}$ , the used approximation (14) produces $\\mathrm{Var}(\\gamma_{2}(t))=0$ everywhere, and thus (27) and (28) are equal.<PARAGRAPH_SEPARATOR># 4. Simulation study"
  },
  {
    "qid": "statistic-posneg-412-7-1",
    "gold_answer": "TRUE. The Hellinger distance between two normal densities is given by $d_{H}^{2}(p_{a_{1},\\sigma_{1}},p_{a_{2},\\sigma_{2}})=2\\left(1-\\sqrt{\\frac{2\\sigma_{1}\\sigma_{2}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}}}e^{-(a_{1}-a_{2})^{2}/4(\\sigma_{1}^{2}+\\sigma_{2}^{2})}\\right)$, which is always greater than or equal to $2(1-e^{-(a_{1}-a_{2})^{2}/4(\\sigma_{1}^{2}+\\sigma_{2}^{2})})$ as shown in Lemma 1.",
    "question": "Does the Hellinger distance $d_{H}^{2}(p_{a_{1},\\sigma_{1}},p_{a_{2},\\sigma_{2}})$ always satisfy $d_{H}^{2}(p_{a_{1},\\sigma_{1}},p_{a_{2},\\sigma_{2}})\\geqslant2(1-e^{-(a_{1}-a_{2})^{2}/4(\\sigma_{1}^{2}+\\sigma_{2}^{2})})$?",
    "question_context": "Adaptive Regression Estimation via Hellinger Distance and K-L Divergence\n\nThe adaptation schemes given in this paper allow one to combine advantages of any countable collection of regression procedures (in terms of $L_{2}$ risk) without requiring any restrictive properties on the procedures. This provides more flexibility in estimating a regression function. Thus estimation procedures (including adaptive ones as mentioned above) designed under various (possibly completely different) assumptions can be combined at the same time, significantly increasing the chance of capturing the true characteristics of the unknown regression function."
  },
  {
    "qid": "statistic-posneg-1746-1-0",
    "gold_answer": "TRUE. Explanation: The condition $$ n\\mathrm{D}\\mathrm{co}\\boldsymbol{\\mathbf{\\mathit{x}}}_{n}(\\mathbf{\\mathit{X}},\\mathbf{\\mathit{Y}})\\geq F_{{\\boldsymbol{\\chi}}_{1}^{2}-1}^{-1}(1-\\alpha) $$ is equivalent to checking if the p-value, which is computed as $1 - F_{{\\boldsymbol{\\chi}}_{1}^{2}-1}(n\\cdot C)$, is less than $\\alpha$. This is because the inverse cumulative distribution function (CDF) $F^{-1}(1-\\alpha)$ gives the critical value such that the probability of observing a value greater than or equal to this critical value under the null hypothesis is $\\alpha$. Thus, if the test statistic $n\\cdot C$ is greater than or equal to this critical value, the p-value will be less than $\\alpha$, leading to the rejection of the null hypothesis.",
    "question": "Is the condition for rejecting the null hypothesis of independence in the distance correlation chi-square test given by $$ n\\mathrm{D}\\mathrm{co}\\boldsymbol{\\mathbf{\\mathit{x}}}_{n}(\\mathbf{\\mathit{X}},\\mathbf{\\mathit{Y}})\\geq F_{{\\boldsymbol{\\chi}}_{1}^{2}-1}^{-1}(1-\\alpha) $$ equivalent to checking if the p-value is less than $\\alpha$?",
    "question_context": "Chi-Square Test of Distance Correlation for Independence\n\nAlgorithm 1 The Distance Correlation Chi-Square Test for Independence <PARAGRAPH_SEPARATOR> <html><body><table><tr><td>1  b+a  ( x)} = ( )  s and nd [n]}.</td></tr><tr><td>Output: The bias-corrected distance correlation C and its p- value p.</td></tr><tr><td>functionFAsTTEsT(X,Y)</td></tr><tr><td>C = Dcorn(X, Y); >thebias-correcteddistance</td></tr><tr><td>correlation</td></tr><tr><td>p= 1 - Fx²-1(n· C); > reject the null when p < α</td></tr><tr><td>endfunction</td></tr><tr><td></td></tr></table></body></html> <PARAGRAPH_SEPARATOR> Theorem 1. The distance correlation chi-squared test that rejects independence if and only if <PARAGRAPH_SEPARATOR> $$ n\\mathrm{D}\\mathrm{co}\\boldsymbol{\\mathbf{\\mathit{x}}}_{n}(\\mathbf{\\mathit{X}},\\mathbf{\\mathit{Y}})\\geq F_{{\\boldsymbol{\\chi}}_{1}^{2}-1}^{-1}(1-\\alpha) $$ <PARAGRAPH_SEPARATOR> is a valid and universally consistent test for sufficiently large $n$ and sufficiently small Type I error level $\\alpha$ . <PARAGRAPH_SEPARATOR> In practice, $n\\geq20$ suffices, and the validity empirically holds for any $\\alpha\\le0.05$ . <PARAGRAPH_SEPARATOR> # 2.2. Fast Statistic Computation"
  },
  {
    "qid": "statistic-posneg-203-1-1",
    "gold_answer": "TRUE. If the dependence function $D(z) = 1$ for all $z \\in [0,1]$, then the GP $W(x,y) = 1 + x + y$ corresponds to the case where $(U,V) = (U, -1-U)$, and the Pickands coordinates $(Z,C)$ are degenerate. This explains why $(U,V)$ does not realize in a neighborhood of $(0,0)$ under this condition.",
    "question": "Does the condition $D(z) = 1$ for all $z \\in [0,1]$ imply that $(U,V)$ will not realize in a neighborhood of $(0,0)$?",
    "question_context": "Distribution of Pickands Coordinates in Bivariate Extreme Value Models\n\nThe paper models a bivariate extreme value distribution with Pickands coordinates $(Z,C) = (V/(U+V), U+V)$, where $(U,V)$ is a random vector with df $H$ having support in $(-\\infty,0]^2$. The transformation $T(x,y) := \\left(\\frac{y}{x+y}, x+y\\right)$ is introduced to define the Pickands coordinates. The density of $(Z,C)$ is derived under the assumption that $H$ has continuous partial derivatives of second order near $(0,0)$."
  },
  {
    "qid": "statistic-posneg-70-0-0",
    "gold_answer": "TRUE. The Gamma Process (GP) prior specified for the baseline cumulative hazard Λ₀Yₖ(w) indeed ensures that the hazard rate over any finite interval follows a Gamma distribution. This is a key property of the GP: if Λ₀Yₖ(w) follows a GP, then the hazard rate over any interval [u_j, u_{j+1}) is Gamma-distributed, centered around the prior hazard rate over that interval, with dispersion controlled by α_Yₖ. This property is explicitly mentioned in the context and is fundamental to the flexible, nonparametric modeling of the hazard function.",
    "question": "Is the statement 'The Gamma Process prior for the baseline cumulative hazard ensures that the hazard rate over any finite interval follows a Gamma distribution centered around the prior hazard rate' true based on the model specifications provided?",
    "question_context": "Bayesian Semiparametric Model for Sequential Treatment Effects\n\nThe paper presents a Bayesian semiparametric model for estimating the effects of sequential treatments, focusing on survival probabilities under dynamic treatment rules. The model incorporates flexible baseline hazard specifications using Gamma Process priors and proportional hazard models. Key components include the identification of potential outcomes, sequential conditional exchangeability, and the use of the g-formula to express survival probabilities in terms of observed data."
  },
  {
    "qid": "statistic-posneg-14-0-0",
    "gold_answer": "FALSE. The condition βD_high - D_low > (1/2)(1-β)Y is not directly derived from the formula for 1+r_S < 1. The economic intuition in the paper's model suggests that a large deleveraging shock forces borrowers to cut spending significantly, but the condition provided does not accurately reflect the equilibrium condition for a negative natural rate.",
    "question": "Is the condition for a negative natural rate derived as βD_high - D_low > (1/2)(1-β)Y consistent with the formula for 1+r_S < 1? Briefly explain the economic intuition derived from the paper's model.",
    "question_context": "Central Limit Theorems for Non-Linear Functionals of Gaussian Fields\n\nThe paper models a deleveraging shock where the debt limit falls from D_high to D_low. In the flexible price endowment economy, the short-run natural rate is 1+r_S = ( (1/2)Y + D_low ) / ( β(1/2)Y + βD_high ). A negative rate (r_S < 0) occurs if βD_high - D_low > (1/2)(1-β)Y. In the sticky price model at the ZLB, output is Ŷ_S = Γ - [ χ_b / (1-χ_b(μ+κγ_D)) ] * D̂, leading to paradoxes like the Paradox of Flexibility."
  },
  {
    "qid": "statistic-posneg-212-0-1",
    "gold_answer": "FALSE. Consistency: The formula h(k) shows that the threshold should vary with k to maintain a constant ARL_0. Using a fixed h contradicts this relationship and can lead to inconsistent control limits. Statistical Rationale: The fixed h simplifies implementation but sacrifices the theoretical balance between sensitivity to small and large shifts. The design choice trades off optimal detection performance for practical convenience, which may reduce the chart's effectiveness for certain shift sizes.",
    "question": "The VSI ACUSUM control chart uses a fixed threshold h for signaling, regardless of the reference value k_i. Is this approach consistent with the formula h(k) = (ln(1 + 2k^2 ARL_0 + 2.332k))/(2k) - 1.166, and what is the statistical rationale behind this design choice?",
    "question_context": "Adaptive CUSUM Control Chart with Variable Sampling Intervals\n\nThe paper discusses the design and properties of an Adaptive CUSUM (ACUSUM) control chart with variable sampling intervals (VSI). The ACUSUM chart uses an EWMA operator to estimate the mean shift δ in real-time, adjusting the reference value k_i = δ̂_i/2 for optimal detection. The VSI feature allows the sampling interval to vary based on the current CUSUM statistic, improving detection efficiency. The paper presents various formulas for the CUSUM statistic, EWMA estimator, and control limits, and discusses the impact of initial parameters on the chart's performance."
  },
  {
    "qid": "statistic-posneg-394-0-1",
    "gold_answer": "FALSE. The context clearly indicates that the likelihood surface has no maximum in these cases, but rather a saddle point. This reflects an indeterminacy in the problem when $\\phi$ and $\\psi$ are unknown, as highlighted by Solari's findings and other heuristic arguments.",
    "question": "Does the likelihood surface for the linear functional relationship model always have a maximum when the variances $\\phi$ and $\\psi$ are unknown?",
    "question_context": "Likelihood Surface Analysis for Linear Functional Relationship Estimation\n\nSoLAR1 (1969) convincingly demonstrates that there is no maximum-likelihood solution to the problem of determining the slope of a linear functional relationship when the observational variates $x_{i},y_{i}$ are normally distributed with unknown means $\\xi_{i},\\beta\\xi_{i}$ and unknown variances $\\phi,\\psi$. <PARAGRAPH_SEPARATOR> Although Solari claims her result is mainly of academic interest, there are some practical aspects that seem worth noting. <PARAGRAPH_SEPARATOR> Firstly, her result focuses attention on the desirability of considering the whole of the likelihood surface. What has previously been regarded as the maximumlikelihood estimator for the model considered by Solari is given by <PARAGRAPH_SEPARATOR> $$ {\\displaystyle\\tilde{\\beta}=\\pm(\\sum{y_{i}^{2}}/\\sum{x_{i}^{2}})^{\\sharp}}, $$ <PARAGRAPH_SEPARATOR> where the sign is chosen so that $\\beta\\sum x_{i}y_{i}>0$ .This estimator of $\\beta$ often gives a line looking very like one fitted by eye to the data. It must be admitted that if it is easy to fit a line by eye, the classical regression estimators will usually do nearly as well. Denoting the latter by $\\begin{array}{r}{\\hat{\\beta}_{1}=\\sum y_{i}x_{i}/\\sum x_{i}^{2}}\\end{array}$ and $\\begin{array}{r}{\\widehat{\\beta}_{2}=\\sum y_{i}^{2}/\\sum y_{i}x_{i},}\\end{array}$ we get the well-known result that $\\tilde{\\beta}$ is the geometric mean of ${\\hat{\\beta}}_{1}$ and ${\\widehat{\\beta}}_{\\mathfrak{a}}$ Using arguments similar to those given by Sprent (1969, Section 3.3) it can be shown that if $\\textstyle\\sum{\\xi_{i}^{2}}/n$ converges to a finite limit $V$ then as $\\pmb{n}$ tends to infinity $\\widehat{\\beta}_{1},\\widehat{\\beta}_{2}$ converge to $\\beta V/(V+\\phi)$ and $(\\beta^{2}V+\\psi)/$ $(\\beta V)$ respectively. The former is less than $\\beta$ and the latter is greater than $\\beta$ .If $\\phi/V$ and $\\psi/\\beta^{2}V$ are both small $\\hat{\\beta}_{2}/\\hat{\\beta}_{1}$ will be close to unity so that $\\tilde{\\beta}$ should then be a fairly reasonable estimator of $\\beta$ . It is of course well known that $\\tilde{\\beta}$ is not in general a consistent estimator of $\\beta_{:}$ , but as Sprent (1969) has pointed out the inconsistency may not be very great in circumstances where the practical man is justified in fitting a line by eye; namely, when the points are well spaced out, the errors are small and the slope of the line is not too near either zero or infinity. <PARAGRAPH_SEPARATOR> Solari's findings have some relevance in the following cases : <PARAGRAPH_SEPARATOR> (i) if we relax the assumption that the departures of $x_{i},y_{i}$ from the unknown true values $\\xi_{i},\\beta\\xi_{i}$ are uncorrelated, and allow them to have a fixed covariance $\\theta_{;}$ then providing $\\theta^{2}\\neq\\phi\\psi.$ , the so-called maximum-likelihood estimators of $\\beta,\\phi,\\psi$ and $\\pmb{\\xi}$ are the same as those given in Solari's paper; <PARAGRAPH_SEPARATOR> (i) if we relax the assumption that the variances of the $x_{i},y_{i}$ are the same for all $i_{:}$ replacing $\\phi,\\psi$ by $\\phi_{i};\\not\\psi_{i};\\:i=1,2,...,n,$ then we get the estimators of $\\beta$ and $\\pmb{\\xi}$ given by  Solari; those for $\\phi_{i}$ and $\\psi_{i}$ become $2\\phi_{i}=x_{i}^{2}-x_{i}y_{i}/\\widetilde\\beta$ and $\\breve{2}\\psi_{i}=y_{i}^{2}-\\tilde{\\beta}x_{i}y_{i}$ where $\\tilde{\\beta}$ is given by (1). <PARAGRAPH_SEPARATOR> The first of the above cases might be regarded as a mathematical curiosity as it seemsunlikelythat $\\theta$ wouldbeknown when $\\phi,\\psi$ are not known. However, the uncorrelated case is really only the special one wherein it is assumed that $\\theta$ takesthe fixedvalue zero. <PARAGRAPH_SEPARATOR> Solari develops her arguments by considering a monotonic decreasing function of the likelihood, namely, <PARAGRAPH_SEPARATOR> $$ g({\\boldsymbol{\\beta}},{\\boldsymbol{\\phi}},{\\boldsymbol{\\psi}},{\\boldsymbol{\\xi}};{\\textbf{x}},{\\textbf{y}})=\\log{(\\phi{\\boldsymbol{\\psi}})}+\\frac{1}{n\\phi}\\sum(x_{i}-{\\boldsymbol{\\xi}}_{i})^{2}+\\frac{1}{n\\psi}\\sum(y_{i}-\\beta{\\boldsymbol{\\xi}}_{i})^{2}, $$ <PARAGRAPH_SEPARATOR> and corresponding functions arising from the models specified in (i) and (i) above maybewritten <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{g_{1}(\\beta,\\phi,\\psi,\\xi;\\theta,{\\bf x},{\\bf y})=\\log{(\\dot{\\phi}\\psi-\\theta^{2})}}\\ &{\\qquad+\\displaystyle\\frac{1}{\\phi\\psi-\\theta^{2}}\\{\\psi\\Sigma{(x_{i}-\\xi_{i})}^{2}-2\\theta\\Sigma{(x_{i}-\\xi_{i})}{(y_{i}-\\beta\\xi_{i})}+\\phi\\Sigma{(y_{i}-\\beta\\xi_{i})}^{2}\\}}\\end{array} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ g_{2}({\\boldsymbol{\\beta}},\\dot{\\Phi},\\Psi,\\xi;\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}})=\\sum\\log\\phi_{i}\\psi_{i}+\\sum\\frac{(x_{i}-\\xi_{i})^{2}}{\\phi_{i}}+\\Sigma\\frac{(y_{i}-\\beta\\xi_{i})^{2}}{\\psi_{i}}, $$ <PARAGRAPH_SEPARATOR> Although (2), (3), (4) all lead to the same so-called maximum-likelihood estimator of $\\beta$ , arguments similar to those given by Solari show that a saddle point rather than a maximum of the likelihood surface is involved ; thus there are no maximum-likelihood estimators in any of these cases. <PARAGRAPH_SEPARATOR> When theratio $\\lambda=\\psi/\\phi$ is known the maximum-likelihood estimator of $\\beta$ is easily shown to be <PARAGRAPH_SEPARATOR> $$ \\widehat{\\beta}=\\frac{\\sum y_{i}^{2}-\\lambda\\sum x_{i}^{2}+\\sqrt{\\{(\\sum y_{i}^{2}-\\lambda\\sum x_{i}^{2})^{2}+4\\lambda(\\sum x_{i}y_{i})^{2}\\}}}{2\\sum x_{i}y_{i}}, $$ <PARAGRAPH_SEPARATOR> a result due to Lindley (1947). <PARAGRAPH_SEPARATOR> If we put $\\lambda=\\sum y_{i}^{2}/\\sum x_{i}^{2}$ , (5) reduces to the estimator given by (1), and for this value of $\\bar{\\lambda,\\bar{\\beta}}$ represents a maximum. In general, however, ${\\widetilde{\\beta}}\\neq{\\widetilde{\\beta}}$ and $\\Vec{\\beta}$ depends upon $\\lambda,$ whereas $\\tilde{\\beta}$ associated with the saddle point arising from the models leading to (2), (3) and (4) is independent of $\\lambda.$ <PARAGRAPH_SEPARATOR> The importance of Solari's result is not that it shows maximum likelihood to fail as a principle, but that it shows the likelihood surface to have no maximum in these cases. This refects an indeterminacy of the problem that has been shown in other ways. Perhaps the best known of these is the heuristic geometric argument advanced by Kendall and Stuart (1967, Section 29.16), whereby it is shown that without some knowledgeof $\\phi,\\psi$ there is little that can safely be said about the existence of a linear relationship.If existence is conceded confidence limits for $\\beta$ can be obtained with a knowledge of $\\lambda$ only using methods due to Creasy (1956)."
  },
  {
    "qid": "statistic-posneg-1646-0-0",
    "gold_answer": "TRUE. Explanation: When $\\alpha=0$ and $\\beta=1$, the logit model simplifies to $P(x)=\\{1+\\exp{[-x]}\\}^{-1}$. This function is symmetric around $x=0$ because $P(-x) = 1 - P(x)$. For example, $P(0) = 0.5$, $P(1) \\approx 0.731$, and $P(-1) \\approx 0.269$, demonstrating the symmetry.",
    "question": "In the logit model $P(x)=\\{1+\\exp{[-\\alpha+\\beta x)]}\\}^{-1}$, setting $\\alpha=0$ and $\\beta=1$ implies that the probability $P(x)$ is symmetric around $x=0$. Is this statement true?",
    "question_context": "Quantal Response Curve Estimation Using Logit Model\n\nThis paper studies methods given by Wetherill (1963) for estimating general percentage points of & quantal response curve. A new method of estimation is proposed which is both simple to compute and highly eficient. The use of this with the Up and Down Transformed Response Rule (UDTR) is investigated, by exact calculation of probability distributions and by empirical sampling trials. The use of the UDTR to estimate the slope of a response curveisoutlined. <PARAGRAPH_SEPARATOR> # 1. INTRODUCTION <PARAGRAPH_SEPARATOR> Wetherill (1963) congidered the following problem. Random variables $\\pmb{Y}(\\pmb{x})$ can be observed which take on the values l or O with probabilities $P(x)$ and $\\scriptstyle{\\mathbf{1}}-P({\\pmb{x}})$ respectively. The quantity $\\pmb{\\mathscr{x}}$ is assumed to be under the control of the experimenter and is referred to as the level at which the observation $\\pmb{Y}(\\pmb{x})$ is taken. The experimenter may require point and interval estimates for the level of $\\pmb{x}$ $L_{p}$ defined by <PARAGRAPH_SEPARATOR> $$ \\operatorname*{Pr}\\{Y(L_{p})=1\\}=P(L_{p})=p. $$ <PARAGRAPH_SEPARATOR> In this paper we consider some simple but highly effcient methods for point estimation of $\\pmb{L_{p}}$ . For examples of some practical situations where such problems arise, see Wetherill (1963) and the discussion on that paper, and Wetherill $\\pmb{\\&}$ Levitt (1966). <PARAGRAPH_SEPARATOR> It will be convenient to assume the logit model for $\\scriptstyle P(x)$ <PARAGRAPH_SEPARATOR> $$ P(x)=\\{1+\\exp{[-\\alpha+\\beta x)]}\\}^{-1}. $$ <PARAGRAPH_SEPARATOR> It is conjectured that the method of estimation proposed will be reasonably robust to the model for $\\pmb{P}(\\pmb{x})$ $\\S10$ <PARAGRAPH_SEPARATOR> Following the approach adopted in Wetherill (1963), liberal use was made of empirical sampling, using the IBM 7094 at Bell Laboratories, and these trials were made for the standard form of (1), with ${\\pmb{\\alpha}}={\\pmb{0}}$ &nd $\\pmb\\beta=1$ <PARAGRAPH_SEPARATOR> # 2. DEFINITIONS"
  },
  {
    "qid": "statistic-posneg-1837-0-1",
    "gold_answer": "FALSE. The IBias values are function-specific. While gSCAD has lower IBias for β0(t), the differences are not uniform across all functions. For instance, GAL1 has comparable or better IBias for β2(t) (0.01 vs. 0.01) and β3(t) (0.3 vs. 0.02) at τ=0.25. The bias-variance trade-off must be evaluated holistically, considering that GALγ often compensates with lower IVar (e.g., 90.2 vs. 98.0 for β0(t) at τ=0.25). The optimal method depends on the specific application's tolerance for bias versus variance.",
    "question": "The integrated squared bias (IBias) for β0(t) is reported as 3.3 for GAL1 at τ=0.25 and 2.1 for gSCAD. Does this mean gSCAD provides uniformly better bias performance across all coefficient functions?",
    "question_context": "Variable Selection in Quantile Varying Coefficient Models\n\nThe paper investigates the performance of the proposed GALγ procedure in variable selection for quantile varying coefficient models, comparing it with the group SCAD method. Two simulated examples are used: Example 1 with 23 predictors and Example 2 with 100 predictors. The models involve time-varying coefficients βk(t) and errors εij(τ) adjusted for quantile regression. The performance is evaluated based on variable selection accuracy, integrated squared bias (IBias), and integrated variance (IVar)."
  },
  {
    "qid": "statistic-posneg-416-0-0",
    "gold_answer": "FALSE. While β2 = 0 indicates no differential subgroup effect on the hazard rate (i.e., the hazard ratio does not vary by subgroup), it does not necessarily imply the absence of subgroups. The subgroups could still exist (ξi = 0 or 1) but have identical hazard ratios (β2 = 0). The logistic component P(ξi=1|Xi,Zi) = exp(γ⊤Xi)/[1+exp(γ⊤Xi)] may still show meaningful subgroup classification based on Xi, even if β2 = 0. The test for β2 = 0 specifically checks for subgroup effects on the hazard, not subgroup existence per se.",
    "question": "In the Logistic-Cox mixture model, the subgroup effect is determined by the parameter β2 in the hazard function λ(t)exp{(β1 + β2ξi)⊤Zi}. If β2 = 0, does this imply that there are no subgroups in the population?",
    "question_context": "Logistic-Cox Mixture Model for Subgroup Analysis in Survival Data\n\nOften the outcome variable in clinical trials, public policy or marketing studies is the duration time to a certain event, such as the recovery time from some disease, or the time to purchase of a new product. The duration time data, also known as the time-to-event data, are usually subject to right censoring, which complicates the statistical analysis. The collection of statistical methods for dealing with time-to-event data is called survival analysis (Fleming & Harrington, 1991). When interest lies in the relationship between covariates such as treatment indicator and duration time, the most popular regression model in survival analysis is the Cox proportional hazards model proposed by Cox (1972). The model assumes that the conditional hazard of the duration time is the product of an unspecified baseline hazard and a function of the covariates. Cox (1972, 1975) suggested a partial likelihood approach to estimate model parameters. The approach can deal with the right censoring, and its large sample properties were later studied by Anderson & Gill (1982). Because there exists a non-parametric baseline hazard in the model assumption, while the interested regression parameter is of finite dimension, the Cox model is a typical semiparametric model. There are several other semiparametric regression models in survival analysis, such as the proportional odds model, the additive hazard model, the accelerated failure time model and the linear transformation model. One may find a review of all these models in Guo & Zeng (2014). For subgroup analysis with right censored time-to-event data, Negassa et al. (2005) developed a tree model to estimate the subgroup effects. However, they assume that there exist subgroups. To the best of our knowledge, no literature has discussed confirmatory statistical test for detecting subgroup with right-censored data. Based on the most commonly used Cox model, we propose a Logistic-Cox mixture model for subgroup identification, estimation and prediction. Specifically, suppose that we have a random sample with $n$ subjects receiving either one of the two specific treatments. Let $T_{i}$ , $i=1,\\ldots,n$ be the event times of interest, $\\xi_{i}\\in\\{0,1\\}$ the unobservable subgroup indicator, $\\mathbf{Z}_{i}$ a $p_{1}$ -dimensional observable covariates vector associated with the subgroup effect and $\\mathbf{X}_{i}$ a $p_{2}$ -dimensional observable covariates vector associated with the subgroup membership. Let $\\lambda_{T_{i}|\\xi_{i},\\mathbf{Z}_{i},\\mathbf{X}_{i}}\\left(t|\\xi_{i},\\mathbf{Z}_{i},\\mathbf{X}_{i}\right)$ be the conditional hazard function of $T_{i}$ given $\\xi_{i},\\mathbf{Z}_{i}$ and $\\mathbf{X}_{i}$ . We assume that the conditional hazard satisfies $$ \\lambda_{T_{i}|\\xi_{i},\\mathbf{Z}_{i},\\mathbf{X}_{i}}\\left(t|\\xi_{i},\\mathbf{Z}_{i},\\mathbf{X}_{i}\right)=\\lambda(t)\\exp\\left\\{(\beta_{1}+\beta_{2}\\xi_{i})^{\top}\\mathbf{Z}_{i}\right\\} $$ where $\\lambda$ is an unspecified baseline hazard function and $\beta_{1},\beta_{2}\\in\\mathbb{R}^{p_{1}}$ are unknown regression coefficients. We also assume that $$ \\mathsf{P}(\\xi_{i}=1|\\mathbf{X}_{i},\\mathbf{Z}_{i})=\\frac{\\exp(\\gamma^{\top}\\mathbf{X}_{i})}{1+\\exp(\\gamma^{\top}\\mathbf{X}_{i})}, $$ where $\\gamma\\in\\mathbb{R}^{p_{2}}$ is an unknown parameter. The first element of $\\mathbf{X}_{i}$ is set to be 1, and the first element of $\\mathbf{Z}_{i}$ is the treatment indicator. Note that although $\\mathbf{X}_{i}$ , regarded as baseline covariates, may share the same components in $\\mathbf{Z}_{i}$ and it contains no treatment indicator nor any posttreatment covariate. We call (1) combined with (2) a Logistic-Cox mixture model which can be viewed as an extension of the Logistic-normal mixture model proposed by Shen & He (2015). Under this model, the regression parameter $\beta_{2}$ corresponds to the subgroup effect. One could also treat the proposed model as a member of the ‘mixture of experts’ models (Yuksel et al., 2012), for which the ‘gating function’ is logisitic and the ‘expert function’ is decided by the Cox model (1). Based on the proposed model, we develop a likelihood ratio-based test for detecting the existence of the subgroups, that is, to test if $\beta_{2}=0$ , with $T_{i}$ being possibly right censored. The expectation–maximization (EM) iteration is utilized to raise the testing power. To improve the finite sample performance of the proposed method, we further design a model-based bootstrap approach to implement the test. When there exist subgroups, one can also utilize the proposed model to estimate the subgroup effect and predict the subgroup membership. Our work extends that of Shen & He (2015) in several ways. Firstly, the proposed LogisticCox model is a semiparametric mixture model, while the model of Shen & He (2015) is a parametric one. The algorithm and some theoretical derivations developed in their work cannot be extended trivially here, and some substantial development is needed. In fact, there exists systematic research on the parametric mixture models, such as Jiang & Tanner (1999), Wong & Li (2001), Lo et al. (2001) and Muthén & Asparouhov (2009). By contrast, much less literature has studied semiparametric mixture models in survival analysis, except that Rosen & Tanner (1999) studied estimation for mixtures of proportional hazard models. Secondly, our proposed method is capable of correcting the bias created by right censoring in time-to-event data, while Shen & He (2015)’s approach can only deal with data with complete observation."
  },
  {
    "qid": "statistic-posneg-1549-0-2",
    "gold_answer": "FALSE. The paper reports that Fisher's test for the difference between the z-transformations of the estimated correlations (r1 = 0.65, z1 = 0.78 and r3 = 0.25, z3 = 0.25) yields an insignificant result. The chance of obtaining a difference between correlation estimates larger than observed is about 0.2, which is not statistically significant.",
    "question": "Is Fisher's z-transformation test for the difference between correlation estimates (r1 and r3) found to be significant in the paper?",
    "question_context": "Testing Differences in Covariance Estimates Between Two Samples\n\nThe paper discusses testing the significance of the difference between covariance estimates from two samples (hogs and gilts) using various statistical methods, including Fisher's z-transformation and a t-test for regression coefficients. The analysis involves comparing correlation estimates and regression coefficients, with a focus on the ratio of covariance estimates and its significance under different correlation assumptions."
  },
  {
    "qid": "statistic-posneg-1155-0-1",
    "gold_answer": "FALSE. The MLE for θ in the uniform (0, θ) distribution case is not minimax with respect to squared error loss. The paper shows that the MLE has infinite maximum risk with respect to squared error loss, implying that it is not minimax. The analysis involves the Chapman-Robbins-Kiefer inequality, which demonstrates that the risk tends to infinity as θ increases.",
    "question": "Is the MLE for θ in the uniform (0, θ) distribution case minimax with respect to squared error loss?",
    "question_context": "Admissibility and Minimaxity of Maximum Likelihood Estimators for Integer Parameters\n\nMAxIMum likelihood estimation of the normal mean restricted to integers was first considered by Hammersley (1950). Khan (1973) proved the admissibility and minimaxity of such estimators with respect to zero--one loss, and inadmissibility and nonminimaxity with respect to squared error loss. In the present note, we consider some important distributions where ranges of variables depend on parameters to be estimated, the parameter space being a subset of integers. Maximum likelihood estimators (m.l.e.'s) for the parameters are obtained in these cases, and their admissibility and minimaxity are studied with respect to zero-one and squared error losses. Finaly, in the case of the normal mean restricted to integers, a proof of the admissibility and minimaxity of the m.1.e. with respect to zero-one loss is provided. The proof turns out to be much shorter and simpler than Khan's."
  },
  {
    "qid": "statistic-posneg-1426-0-0",
    "gold_answer": "TRUE. The leave-one-out cross-validation criterion can indeed be calculated using the formula $$ \\sum_{i=1}^{n}\\frac{\\{y_{i}-\\hat{\\mu}(\\mathbf{x}_{i})\\}^{2}}{(1-h_{i i})^{2}}, $$ where $h_{i i}$ is the ith diagonal element of the hat matrix $\\hat{\\bf H}$. This leverages the fact that in linear models, the leave-one-out prediction can be derived analytically from the full model fit, avoiding the need for n separate model fits. The denominator $(1-h_{i i})^{2}$ accounts for the leverage of the ith observation, adjusting the residual accordingly.",
    "question": "Is it true that in linear models, the leave-one-out cross-validation criterion can be calculated without refitting the model n times, using the formula involving the hat matrix diagonal elements h_ii?",
    "question_context": "Cross-Validation Methods for Tuning Parameter Estimation\n\nAlternatively, extrapolation may also be handled by defining extra B-splines outside the range of the training data as in Currie et al. (2003), but I will not use this approach in the present paper. <PARAGRAPH_SEPARATOR> # 3.5. Estimating tuning parameters by permuted leave-k-out cross-validation <PARAGRAPH_SEPARATOR> Alternative methods for choosing tuning parameters includes Akaikes information criterion, (ordinary) cross-validation or generalized cross-validation, see for instance Eilers and Marx (1996) or Koenker and Mizera (2004), but here we restrict ourselves to (ordinary) cross-validation. The most popular variant is leave-one-out cross-validation, i.e. the tuning parameters are estimated by minimizing <PARAGRAPH_SEPARATOR> $$ \\sum_{i=1}^{n}\\{y_{i}-{\\hat{\\mu}}_{(-i)}(\\mathbf{x}_{i})\\}^{2}, $$ <PARAGRAPH_SEPARATOR> where $\\hat{\\mu}_{(-i)}$ is the regression function estimated without the ith observation. In linear models, this can equivalently be calculated as <PARAGRAPH_SEPARATOR> $$ \\sum_{i=1}^{n}\\frac{\\{y_{i}-\\hat{\\mu}(\\mathbf{x}_{i})\\}^{2}}{(1-h_{i i})^{2}}, $$ <PARAGRAPH_SEPARATOR> i.e. without any need to recalculate ${\\hat{\\mu}}n$ times. Here $h_{i i}$ is the ith diagonal element of the so-called hat matrix $\\hat{\\bf H}$ (see Appendix A). This expression is used in Marx and Eilers (1998), since P-splines are linear in the B-basis. But (12) does not cover extrapolation, i.e. $\\mathbf{X}_{i}$ lying outside the range of $\\mathbf{X}_{(-i)}$ , as discussed in Section 3.4. If we assume that (11) is calculated with extrapolation performed as described in the previous section, (12) will not be exactly equivalent to (11). <PARAGRAPH_SEPARATOR> The so-called V-fold cross-validation is a common alternative to leave-one-out crossvalidation. It saves computer time, and several authors (e.g. Breiman and Spector, 1992) have noted that it often yields better results than the leave-one-out variant. The training data are divided into $\\mathrm{v}$ groups with approximately $n/V$ observations in each group, and the response values in each group are predicted by the model estimated from the remaining $(V-1)$ groups. Denote the groups by $v$ , $(v{=}1,\\ldots,V)$ ), and let the subscript $(-v)$ symbolize estimates without the $v$ th group. The criterion to be minimized is then <PARAGRAPH_SEPARATOR> $$ \\sum_{v=1}^{V}\\sum_{i\\in v}\\{y_{i}-\\hat{\\mu}_{(-v)}(\\mathbf{x}_{i})\\}^{2}. $$ <PARAGRAPH_SEPARATOR> In $V.$ -fold cross-validation about $k\\approx n/V$ observations are left out for prediction, and the term leave- $\\cdot k$ -out may also be used to emphasize that more than one observation may be left out. In the context of variable subset selection in a linear model, Shao (1993) showed that when $n\\rightarrow\\infty$ , the proportion to be left out should go to 1, the opposite of what happens in leave-one-out cross-validation! Thus $V$ should decrease when $n$ increases, whereas the number $k$ of observations to be left out should increase faster than $n$ . <PARAGRAPH_SEPARATOR> There is no unique way to divide the training data into $V$ groups. Assume that one divides the data into $V$ groups by putting the first $k$ observations in the first group, the next $k$ observations in the second group etcetera. Then, if the data were permuted, the group assignment would be different, and the cross-validated sum of squares (13) would differ. An obvious extension of leave- $\\cdot k$ -out cross-validation is then to perform this several times for various permutations of the data, and taking the average over (13). In this way all response values $y_{i}$ will be predicted by more than one estimated model, and the variance of the optimization criterion will be reduced. I will call this permuted leave- $\\cdot k$ -out cross-validation, see Shao (1993) and Pan (1999) who suggested a similar method called bootstrap-smoothed cross-validation. <PARAGRAPH_SEPARATOR> # 4. Simulation experiment"
  },
  {
    "qid": "statistic-posneg-322-3-0",
    "gold_answer": "TRUE. The condition is correct as derived in Lemma 2 of the paper. Under Assumptions (H1), (H2), (III.1), (I.1), (I.2), (I.3), and with the scaling sequence δk,n,y satisfying 1≪δk,n,y≪mins∈{0,1,k}δ̃s,n,y, the tail ratio estimator ĉk,y converges to ck,y at the rate δk,n,y, ensuring δk,n,y|ck,y−ĉk,y|→ℙ0 as n→+∞.",
    "question": "Is the condition for the consistency of the tail ratio estimator given by δk,n,y|ck,y−ĉk,y|→ℙ0 as n→+∞ correct under the assumptions of the paper?",
    "question_context": "Estimation of Extreme Multivariate Expectiles with Functional Covariates\n\nThe paper presents a semi-parametric method for estimating functional multivariate L1-expectiles at extreme risk levels (functional L1-MEEs). It establishes the consistency with rate of the approximated loss function using empirical kernel-based estimators for the tail index, tail ratio, and upper tail dependence function. The method couples the loss function estimation with a gradient descent algorithm (e.g., BFGS-family) and provides consistency with rate for the approximated optimum problem solving functional L1-MEEs."
  },
  {
    "qid": "statistic-posneg-1025-0-3",
    "gold_answer": "FALSE. While the Fine & Gray model has lower MSE for the correct (cloglog) specification (Table 4), Table 3 shows it exhibits substantially larger bias than the pseudo-value methods when the true model is logistic (link function misspecification). The pseudo-value methods are more robust to link function choice, though they are generally more variable (higher MSE). The paper notes the Fine & Gray model 'is not overly robust to the choice of the link function.'",
    "question": "The Fine & Gray proportional hazards model outperforms the pseudo-value approach in all scenarios studied in the Monte Carlo simulation. Is this claim supported by the results?",
    "question_context": "Pseudo-Value Approach in Multistate Modeling and Competing Risks Analysis\n\nThe paper presents a general approach to multistate model probability estimation using pseudo-values, with applications to regression models for cumulative incidence functions in competing risks. The method involves defining pseudo-values based on unbiased estimators of state probabilities and modeling their dependence on covariates via generalized linear models. The approach is validated through Monte Carlo studies comparing different working covariance matrices and link functions."
  },
  {
    "qid": "statistic-posneg-1568-2-4",
    "gold_answer": "FALSE. The paper derives separate expressions for Φ+(w) (for w ≥ 0) and Φ−(w) (for w ≤ 0), indicating that the distribution is not symmetric. The forms of these functions are different, and the paper does not suggest any symmetry in the distribution.",
    "question": "Is the distribution function Φ(w) symmetric around w = 0?",
    "question_context": "Distribution of the Ratio of Covariance Estimates in Normal Bivariate Populations\n\nThe paper discusses the distribution function for the ratio of covariance estimates from two independent random samples drawn from normal bivariate populations. The problem is generalized to derive the distribution function for the ratio of sums of products, which is a more convenient statistic than the covariance ratio itself. The mathematical proof involves transforming variables and integrating over ranges to obtain the distribution functions for positive and negative ratios."
  },
  {
    "qid": "statistic-posneg-251-0-2",
    "gold_answer": "FALSE. The paper presents A_1 as the ratio of two determinants involving M(1), M(2), M(3), and the roots λ_1, λ_2, λ_3. However, the derivation assumes specific properties of the roots and initial conditions that are not explicitly verified, leading to potential inaccuracies in the general case. The correctness of this formula depends on the specific form of the recurrence relation and the roots, which may not hold universally.",
    "question": "Is the formula for A_1 given by the ratio of two determinants correctly derived from the initial conditions M(1), M(2), and M(3)?",
    "question_context": "Cumulant Evaluation for Probability Distributions on a Line\n\nThe paper discusses the evaluation of cumulants for probability distributions arising from points on a line with multiple characters. The moment-generating function (m.g.f.) is given by a recurrence relation, and the cumulants are derived using the roots of the characteristic equation and their derivatives."
  },
  {
    "qid": "statistic-posneg-639-0-1",
    "gold_answer": "FALSE. The paper explicitly states that the exact distribution of this statistic under the null hypothesis is difficult to derive theoretically. While the upper tail of the distribution resembles an $F(1, r-1)$ distribution, simulations show that the critical values correspond to approximately $0.8P/(r-1)$% points of the $F(1, r-1)$ distribution, not the exact $F$-distribution. This approximation is necessary due to the selection of the least residual sum of squares ($RSS_{M}$), which complicates the theoretical distribution.",
    "question": "Does the variance ratio $\\frac{RSS_{C} - RSS_{M}}{RSS_{M}/(r-1)}$ follow an exact $F(1, r-1)$ distribution under the null hypothesis of no spurious observations?",
    "question_context": "Detection of Spurious Values in Factorial Experiments\n\nThe substitute value, 81, obtained for treatment combination a in Table 2 by minimizing the residual mean square is the same as that calculated in Section 1 by averaging the magnitudes of the first- and second-order 'interaction' effects. The other replacement values could have been calculated similarly, and it is straightforward to prove the equivalence of the two approaches in the general case. Thus for a complete ${\\mathfrak{2}}^{4}$ experiment, the signs and magnitude of the second- and third-order 'interaction' effects could be used to highlight a spurious response value for a main-effects and first-order interactions model and would minimize the mean square residual (of higher-order interactions). However, the method we have adopted of identifying that treatment combination which leads to the least residual mean square when its response value is regarded as missing, is more convenient computationally. <PARAGRAPH_SEPARATOR> # 2.1. Significance Test for Spurious Values <PARAGRAPH_SEPARATOR> Each of the residual mean squares in Table 2 is based on three degrees of freedom and we have to decide whether the least of them, 34.83, represents a statistically significant reduction from the original residual mean square of 538·125, with four degrees of freedom. As a test statistic we have computed the variance ratio <PARAGRAPH_SEPARATOR> $$ \\frac{R S S_{C}-R S S_{M}}{R S S_{M}/(r-1)}, $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $R S S_{C}$ is the residual sum of squares when the complete set of original data is used to fit the given model, $\\pmb{r}$ is the degrees of freedom associated with this residual and $R S S_{M}$ is the least of the residual sums of squares when each datum in turn is regarded as missing. <PARAGRAPH_SEPARATOR> The distribution of this statistic even under a null hypothesis of no real spurious observation is difficult to derive theoretically. However, it seems reasonable to expect that it will have the general shape of an ${\\pmb F}\\cdot$ distribution with 1 and $(r-1)$ degrees of freedom in its upper tail, provided allowance is made for the use of a least residual sum of squares. <PARAGRAPH_SEPARATOR> To proceed further, critical levels for the ratio (1) have been computed by direct numerical simulation. For a $\\mathfrak{L}^{3}$ experiment and a main-effects model, eight response values were generated from a standard Normal distribution by a pseudo-random procedure and the spurious values algorithm was applied as far as the calculation of the ratio (1). This was done 100 times and the cumulative distribution of ratio values was constructed. The shape of the upper part of this distribution did agree with the shape of $F(1,3)$, except that the ratio beyond which $\\pmb{P}$ per cent of the values lay corresponded to that beyond which approximately $\\ddagger^{P}$ per cent of the $F(1,3)$ distribution lies (for $P<15)$. <PARAGRAPH_SEPARATOR> Further simulations of this type were carried out for nine other experimental designs and models, as listed in Table 3: a main-effect model of a three-level factor includes both linear and quadratic effects. From these it was possible to deduce the general rule that for an experiment-wise error rate of $\\pmb{P}$ per cent the ratio (1) should be compared with the upper $0{\\cdot}8P/(r{-}1)$ per cent point of the $F\\cdot$ distribution with 1 and $(r-1)$ degrees of freedom. This rule and the 0·8 factor are clearly only approximate, but they are sufficiently accurate for practical use, since any error only amounts to making a small change in the outliers insurance premium. An indication of the accuracy is given in Table 3 for $P=5$ and 10, but it must be realized that part of the lack of agreement is due to the sampling variation in the simulated experiments. <PARAGRAPH_SEPARATOR> Applying the rule to the data of Table 1, the ratio (1) is <PARAGRAPH_SEPARATOR> $$ {\\frac{2152\\cdot5-104\\cdot5}{104\\cdot5/3}}=58\\cdot8. $$ <PARAGRAPH_SEPARATOR> The upper ${\\bf0\\cdot8\\times5/3}$ and ${0\\cdot8\\times1/3}$ per cent points of the $F(1,3)$ distribution are 27.7 and ${\\bf85\\cdot7}$ respectively. We may therefore judge the response value of 145 as a rogue, significant at the (overall) 5 per cent level. <PARAGRAPH_SEPARATOR> TABLE3 Comparison of results from simulated experiments with significance tail area from empirical rule <PARAGRAPH_SEPARATOR> <html><body><table><tr><td colspan='2'></td><td rowspan='2'>r</td><td colspan='2'>Tail area in simulation beyond critical F-ratio derived from rule</td></tr><tr><td>Experimental design</td><td>Fitted model</td><td>P=5</td><td>P=10</td></tr><tr><td>23</td><td>Main effects</td><td rowspan='2'>4 4</td><td>0·075</td><td>0·14</td></tr><tr><td>2×32</td><td>Main effects and</td><td>0·045</td><td>0·10</td></tr><tr><td>24</td><td colspan='2'>first-order interactions Main effects and</td><td>0·03</td><td>0·075</td></tr><tr><td>22×3</td><td colspan='2'>first-order interactions Main effects</td><td>0·04</td><td>0·095</td></tr><tr><td>42</td><td colspan='2'>Main effects</td><td>0·05</td><td>0·10</td></tr><tr><td>2</td><td colspan='2'>Main effects</td><td>0·03</td><td>0·125</td></tr><tr><td>2x32</td><td colspan='2'>Main effects</td><td>0-06</td><td>0·125</td></tr><tr><td>2x3x4</td><td colspan='2'>Main effects</td><td>0-05</td><td>0-105</td></tr><tr><td>33</td><td colspan='2'>Main effects</td><td>20 0·05</td><td>0·12</td></tr><tr><td>25</td><td colspan='2'>Main effects</td><td>26 0·04</td><td>0.08</td></tr></table></body></html> <PARAGRAPH_SEPARATOR> # 2.2. Partially Identifiable Cases <PARAGRAPH_SEPARATOR> In C. Daniel's unpublished method of identifying outliers, described in Section 1, the number of possible combinations of sign associated with the four interaction effects is ${\\mathfrak{L}}^{4}$. Since each of the eight response values may be high or low, there is just sufficient information to identify a particular treatment combination as giving rise to a spurious response value. In general, a necessary condition for a cell to be uniquely detected is <PARAGRAPH_SEPARATOR> $$ 2^{r}\\geqslant2N, $$ <PARAGRAPH_SEPARATOR> i.e. <PARAGRAPH_SEPARATOR> $$ r\\geqslant1+\\log_{2}N, $$ <PARAGRAPH_SEPARATOR> where $N$ is the total number of response values in the experiment. Thus the ability to pick out a particular value as an outlier depends both on the size of the experiment and on the model fitted. Condition (2) is satisfied in all except the second of the examples listed in Table 3. <PARAGRAPH_SEPARATOR> To illustrate the situation when (2) does not hold, suppose that the eight response values in Table 1 had arisen, not from a ${\\mathfrak{Z}}^{3}$ experiment, but as a half-fraction of a ${\\mathfrak{L}}^{4}$ design with the defining contrast $D=A B C$. When a main-effects model is fitted, giving $r=3$, the sign combinations of large 'interaction' effects arise from the response values as shown in Table 4. <PARAGRAPH_SEPARATOR> Thus each combination of signs occurs twice and we are unable to identify which of the two response values might be an outlier. For example, the combination $(-,-,+)$ could arise because ad $(=145)$ is high or because bc $(=180)$ is high."
  },
  {
    "qid": "statistic-posneg-418-0-0",
    "gold_answer": "TRUE. The context explicitly states that $L_v(t) = W_{1,Z^L}(t)$ with $Z_i^L(t)$ defined as shown. This equivalence is derived from the general framework of trend statistics, where $L_v(t)$ is a special case of $W_{q,Z}(t)$ with $q(t) \\equiv 1$ and $Z_i(t) = Z_i^L(t)$.",
    "question": "Is the statement 'The Jonckheere-type statistic $L_v(t)$ is equivalent to $W_{1,Z^L}(t)$ where $Z_i^L(t)$ is defined as $v_i(t) - \\sum_{j=1}^i v_j(t) \\frac{Y_j.(t)}{\\sum_{m=j}^s Y_m.(t)}$' correct based on the provided context?",
    "question_context": "Jonckheere-Type Trend Statistics in Survival Analysis\n\nThe paper discusses Jonckheere-type trend statistics for survival analysis, focusing on three forms: $\\sum L_i$, $\\sum\\sum U_{ij}$, and $\\sum R_i$. These statistics are rewritten as special cases of a general trend statistic $W_{q,Z}(t)$, which involves weighted log-rank statistics comparing different groups or pooled groups. The paper explores the asymptotic distributions of these statistics under contiguous alternatives and discusses their dependence on group sizes and censoring distributions."
  },
  {
    "qid": "statistic-posneg-630-6-0",
    "gold_answer": "FALSE. Explanation: The condition for Poisson approximation to the binomial distribution requires q to be small and n to be large, but the data shows significant uncertainty in both q and n. The probable error of q (±0.0670) is much larger than the estimated q (0.0034), suggesting q might not be reliably small. Additionally, the large uncertainty in n (±3449.103) relative to the estimated n (177.1711) indicates that n might not be reliably large. Therefore, the conditions for Poisson approximation are not clearly satisfied based on the given data.",
    "question": "Is the condition for approximating the binomial distribution with a Poisson distribution (small q and large n) satisfied in Bortkewitsch's data, given q = 0.0034 ± 0.0670 and n = 177.1711 ± 3449.103?",
    "question_context": "Poisson Approximation to Binomial Distribution in Small Probability Events\n\n<html><body><table><tr><td>Number ofDeaths</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>Totals</td></tr><tr><td>Number of Corps</td><td>109</td><td>65</td><td>22</td><td>3</td><td>1</td><td>200</td></tr></table></body></html><PARAGRAPH_SEPARATOR>Whence :<PARAGRAPH_SEPARATOR>and the binomial is :<PARAGRAPH_SEPARATOR>$$200({\\cdot}996,557+{\\cdot}003,443)^{1777107}.$$<PARAGRAPH_SEPARATOR>This is the frst of Bortkewitsch's illustrations for which his hypothcsis that $\\pmb q$ small and $\\pmb{\\mathscr{n}}$ large is really justified by his data. For :<PARAGRAPH_SEPARATOR>$$\\begin{array}{l}{\\scriptstyle{\\mathcal{q}}=\\cdot0034\\pm\\cdot06{\\mathrm{7}}0,}\\ {\\scriptstyle{n}=17{\\mathrm{7}}\\cdot1{\\mathrm{711}}\\pm3449\\cdot103.}\\end{array}$$<PARAGRAPH_SEPARATOR>The probable error of $\\pmb q$ for $\\pmb q$ really zero is $\\pm\\cdot0674.$<PARAGRAPH_SEPARATOR>* Of course immensely better general total fits are obtained by using the sums of the actual 8 or 11 binomials than by the Poisson-Exponential sum or the single binomial, but the results in that case involve 16 or 22 non-significant constants.<PARAGRAPH_SEPARATOR>The actual results as given by the binomial and the Poisson-Exponential are :"
  },
  {
    "qid": "statistic-posneg-702-0-1",
    "gold_answer": "TRUE. Under the null hypothesis β=0, the mean of y_j simplifies to 1/(α + βx_j) = 1/α, implying that the observations y_j are independently exponentially distributed with a common mean 1/α, independent of the x_j values.",
    "question": "Is the null hypothesis β=0 equivalent to assuming that the observations y_j are independently exponentially distributed with means 1/α?",
    "question_context": "Exponential Regression Analysis and Hypothesis Testing\n\nSuppose that we want to test the observations $y_{1},...,y_{n}$ for regression on the fixed quantities $x_{1},...,x_{n}$ .It is worth outlining briefly the procedure to be followed if $y_{1},...,y_{n}$ are assumed exponentially distributed. In the absence of specific reason to the contrary, it is sensible to consider a simple regression model chosen for mathematical convenience. This is that the $y_{j}$ are independently exponentially distributed with means $1/(\\alpha+\\beta x_{j})$ . The null hypothesis is that $\\beta=0$ <PARAGRAPH_SEPARATOR> The likelihood is <PARAGRAPH_SEPARATOR> $$ \\exp{(-\\alpha\\Sigma y_{j}-\\beta\\Sigma x_{j}y_{j})}\\Pi(\\alpha+\\beta x_{j}), $$ <PARAGRAPH_SEPARATOR> the sufficient statistic is $(\\Sigma y_{j},\\Sigma x_{j}y_{j})$ and the “exact’ test is based on the conditional distribution of $\\Sigma{}x_{j}y_{j}$ given $\\Sigma{y}_{j}$ which is independent of the nuisance parameter $\\upalpha$ Vincent (1961) has discussed this test in the different context of tests on sample estimates of variance. <PARAGRAPH_SEPARATOR> An interesting special case arises when $x_{j}=j$ ; if the $x_{j}$ are interpreted as intervals between events in a point proces, the nuil hypothesis is that we have a Poisson process, whereas under the alternative process there is a trend with the serial number of the event. This can be compared with the model and test considered by Cox (1955) of a time-dependent Poisson process in which the probability rate of occurrence at time $\\boldsymbol{\\mathscr{r}}$ is $\\alpha e^{\\beta\\tau}$ .  The two significant tests of the null hypothesis $\\beta=0$ are almost identical; the models are, of course, different when $\\beta\\neq0$ , although equivalent in a deterministic approximation. <PARAGRAPH_SEPARATOR> Another special case is the two-sample problem of Section 3 obtained when the $x_{i}$ takesvalues $1,0$ Only. <PARAGRAPH_SEPARATOR> Turning now to the test based on scores, we rank the $y_{j}$ replace them by the appropriate scores and consider the test statistic <PARAGRAPH_SEPARATOR> $$ v=\\Sigma x_{j}t_{j}, $$ <PARAGRAPH_SEPARATOR> where the random variable $t_{j}$ is the score attached to the jth observation. Under the null hypothesis, all permutations of the scores $\\{t_{r n}\\}$ are equally likely. <PARAGRAPH_SEPARATOR> We shall not develop the theory of $v$ beyond the simplest normal approximation, for which we need the mean and variance of $v$ over all permutations of $\\{t_{r n}\\}$ .Now $E(v)$ is bilinear in $\\{x_{j}\\}$ and $\\{t_{r n}\\}$ and is symmetric in the two sets separately. Hence <PARAGRAPH_SEPARATOR> $$ E(v)=a_{n}s_{1n}\\Sigma x_{j}=a_{n}n\\Sigma x_{j}, $$ <PARAGRAPH_SEPARATOR> where $a_{n}$ depends only on $n$ . Consideration of the special case $x_{1}=\\ldots=x_{n}=1$ shows that $a_{n}=1/n$ and that <PARAGRAPH_SEPARATOR> $$ E(v)=\\Sigma x_{j}. $$ <PARAGRAPH_SEPARATOR> Similarly, we can show from considerations of degree and invariance that <PARAGRAPH_SEPARATOR> $$ \\mathrm{var}(v)=b_{n}\\Sigma(x_{j}-\\bar{x})^{2}K_{2n}, $$ <PARAGRAPH_SEPARATOR> where $b_{n}$ depends only on $n$ . If we consider the special case $x_{1}=1$ ， $x_{2}=...=x_{n}=0$ we find on substituting into (10) that $b_{n}=1$ <PARAGRAPH_SEPARATOR> Thus the large-sample test is to take <PARAGRAPH_SEPARATOR> $$ (v-\\Sigma x_{j})/\\{\\Sigma(x_{j}-\\bar{x})^{2}K_{2n}\\}^{\\frac{1}{2}} $$ <PARAGRAPH_SEPARATOR> as normally distributed with mean zero and unit variance. <PARAGRAPH_SEPARATOR> In special cases, as in Section 3, it may be possible to obtain a better approximation fairly easily."
  },
  {
    "qid": "statistic-posneg-449-0-0",
    "gold_answer": "FALSE. The context mentions visual examination of images and knowledge of semi-regular walking patterns as support for the assumption, but does not provide concrete empirical evidence or statistical tests to confirm the equality of joint distributions across different time points. The assumption is based on qualitative observations rather than rigorous quantitative validation.",
    "question": "Is the assumption that the joint distribution of $(\\nu_{i j}(s),\\nu_{i j}(s-u))$ and $(\\nu_{i j}(s^{\\prime}),\\nu_{i j}(s^{\\prime}-u))$ are equal as long as enough seconds are observed, supported by the empirical evidence provided in the context?",
    "question_context": "Binary Classification of Walking Patterns via Functional Regression\n\nFor each subject, $i$ , the collection of three-dimensional vectors $\\{\\nu_{i j}(s-u),\\nu_{i j}(s),u\\}$ for all inter­ vals $1,\\ldots,J_{i}$ and all lags $u=1,...,S-1$ is a three-dimensional image representation of the vector magnitude time series. We refer to this as the empirical joint distribution of acceleration and lag acceleration. The number of observations for subject $i$ in this image is equal to $\\begin{array}{r}{\\sum_{u=1}^{S-1}{(S-u)J_{i}}=J_{i}\\sum_{u=1}^{S-1}{u}=J_{i}\\frac{S\\left(S-1\\right)}{2},}\\end{array}$ . <PARAGRAPH_SEPARATOR> Figure 2 provides the intuition into how this image is constructed. The first row of four panels corresponds to the first second, $j=1$ , while the second row corresponds to the second, $j=2$ , of data for Subject 19 in the IU data. The columns correspond to four different lags, $u=1$ , 15, 30, 45, centiseconds, respectively. Each panel in the first column contains 99 pairs of observations $\\{\\nu_{i j}(s-1),\\nu_{i j}(s)\\}$ , for $s=1,...,S-1$ . Note the strong correlation between the obser­ vations, as the values of acceleration do not change too much for 1 centisecond. The shape of the resulting point cloud resembles that of a bivariate Normal distribution with high correlation for this time lag. Each panel in the second column contains 85 pairs of observations $\\{\\nu_{i j}(s-15),\\nu_{i j}(s)\\}_{;}$ , for $s=1,...,S-15$ . Compared to the first column, the point clouds exhibit lower correlation and exhibit structure beyond simple linear association. The three-dimensional image predictor is the union of all these panels over all the seconds, $J_{i},$ and all lags, $\\boldsymbol{u}$ , for one in­ dividual. We refer to this image as the empirical joint distribution of acceleration and lag acceleration. <PARAGRAPH_SEPARATOR> This joint distribution is the foundation of both prediction approaches we employ. When we predict subject $i_{0}$ the outcome is $Y_{i j}^{i_{0}}$ , where $Y_{i j}^{i_{0}}=1$ if $i=i_{0}$ and 0 otherwise, i.e. the outcome is an indicator that data belongs to study participant $i_{0}$ . The predictors are $\\{\\nu_{i j}(s-u),\\nu_{i j}(s).$ , $u:u\\in{1,2,...,S-1;s\\in u+1,...,S}\\}$ . Thus, at the conceptual level, we changed the problem of identifying individuals from their high density accelerometry data recorded during walking into a binary regression of the type <PARAGRAPH_SEPARATOR> $$ Y_{i j}^{i_{0}}\\mid\\{\\nu_{i j}(s-u),\\nu_{i j}(s),u:u\\in{1,2,\\dots,S-1};s\\in u+1,\\dots,S\\}, $$ <PARAGRAPH_SEPARATOR> where there are $J_{i_{0}}$ indicators equal to one (data observed from study participant $i_{0}$ ) and $\\textstyle\\big(\\sum_{i=1}^{n}J_{i}\\big)-J_{i_{0}}$ indicators of zeroes (data for remaining study participants). One approach to this problem is to reduce the complexity of the predictor space via ‘image partitioning,’ which summa­ rizes the three-dimensional image into several predictors. The second approach is to consider the model $Y_{i j}^{i_{0}}\\sim\\mathrm{Bernoulli}\\{p_{i j}^{i_{0}}\\}$ where probabilities are modelled as a tri-variate functional regression model <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\mathrm{logit}\\{p_{i j}^{i_{0}}\\}=\\int_{s,u}F\\{\\nu_{i j}(s-u),\\nu_{i j}(s),u\\}\\mathrm{d}s\\mathrm{d}u.}\\end{array} $$ <PARAGRAPH_SEPARATOR> In this case. parsimony is controlled by assuming that $F(\\cdot,\\cdot,\\cdot)$ is smooth. In the following sec­ tion, we provide more details for both these approaches. Implicit in both approaches is the as­ sumption that the joint distribution of $(\\nu_{i j}(s),\\nu_{i j}(s-u))$ and $(\\nu_{i j}(s^{\\prime}),\\nu_{i j}(s^{\\prime}-u))$ are equal as long as the $\\nu_{i j}(s)$ , $\\nu_{i j}(s-u)$ are pooled over large enough $j_{:}$ i.e. as long as enough seconds are observed. Visual examination of the images derived from the joint distributions (i.e. the fingerprints shown in Figure 6 for different subsets of seconds) and knowledge of the semi-regular pattern of walking support this assumption."
  },
  {
    "qid": "statistic-posneg-462-0-1",
    "gold_answer": "TRUE. The formula is the correct inverse CDF of the generalized Pareto distribution for threshold exceedances, where $(\\sigma^{*}, \\gamma^{*})$ are fiducial samples. This follows from the GPD's survival function $P(Y > y) = (1 + \\frac{\\gamma}{\\sigma}(y-a))^{-1/\\gamma}$, solving for $y$ at probability $1-\\beta$ yields the given expression. The fiducial approach propagates uncertainty in $(\\sigma, \\gamma)$ through their joint fiducial distribution to estimate extreme quantiles.",
    "question": "Does the fiducial quantile estimator $q^{*}=a+\\frac{\\sigma^{*}}{\\gamma^{*}}\\left((1-\\beta)^{-\\gamma^{*}}-1\\right)$ correctly represent the inverse distribution function for the $\\beta$-percentile under the generalized Pareto model?",
    "question_context": "Generalized Fiducial Inference for Extreme Value Quantiles in Solar Flare Brightness Data\n\nThe instrument AIA captures images of the Sun in eight different wavelengths every $12\\mathsf{s e c}$ , providing more than 70,000 high-resolution images per day; see Figure 7 for two examples. The image size is $4096\\times4096$ pixels, which amounts to a total of 1.5 terabytes compressed data per day. An uncompressed and preprocessed version of the data can be obtained from Schuh et al. (2013). Here, each image was partitioned into $64\\times64$ squared and equi-sized sub-images, each consists of $64\\times64$ pixels. For each sub-image, 10 summary statistics were computed, such as the average and the standard deviation of the pixel values. <PARAGRAPH_SEPARATOR> A solar flare is a sudden eruption of high-energy radiation from the Sun’s surface, which could cause disturbances on communication and power systems on the Earth. In those images captured by AIA, such solar flares are characterized by extremely bright spots; see the right panel of Figure 7 for an example. <PARAGRAPH_SEPARATOR> Wandler and Hannig (2012) provided a solution for estimating extremes using GFI. Their approach is based on modeling values over large threshold using a generalized Pareto distribution (Pickands III 1975). The data-generating equation is <PARAGRAPH_SEPARATOR> $$ Y_{i}=a+\\frac{\\sigma}{\\gamma}\\left(U_{i}^{-\\gamma}-1\\right),i=1,\\ldots,n, $$ <PARAGRAPH_SEPARATOR> where $U_{i}$ are iid $U(0,1)$ , $^{a}$ is a known threshold, $\\sigma$ is a scale parameter, and $\\gamma$ is a shape parameters. The corresponding <PARAGRAPH_SEPARATOR> likelihood and Jacobian (3) are <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\displaystyle f({\\bf{y}};\\sigma,\\gamma)=\\sigma^{-n}\\prod_{i=1}^{n}\\left(1+\\frac{\\gamma}{\\sigma}(x_{i}-a)\\right)^{-1-1/\\gamma}I_{(a,\\infty)}(x_{i}),}}\\ {{\\displaystyle J(\\gamma,\\sigma,\\gamma)=\\gamma^{-2}D\\left(\\begin{array}{c c}{{x_{1}-a}}&{{(1+\\frac{\\gamma}{\\sigma}(x_{1}-a))\\log(1+\\frac{\\gamma}{\\sigma}(x_{1}-a))}}\\ {{\\vdots}}&{{\\vdots}}\\ {{x_{n}-a}}&{{(1+\\frac{\\gamma}{\\sigma}(x_{n}-a))\\log(1+\\frac{\\gamma}{\\sigma}(x_{n}-a))}}\\end{array}\\right).}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Wandler and Hannig (2012) provided an MCMC algorithm for sampling from the fiducial distribution of the generalized Pareto parameters for small datasets. A sample from fiducial distribution for a $\\beta$ -percentile is then obtained by plugging the fiducial samples $(\\sigma^{*},\\gamma^{*})$ into the inverse distribution function <PARAGRAPH_SEPARATOR> $$ q^{*}=a+\\frac{\\sigma^{*}}{\\gamma^{*}}\\left((1-\\beta)^{-\\gamma^{*}}-1\\right). $$ <PARAGRAPH_SEPARATOR> For this solar dataset, we used the averaged pixel values (summary statistics P2) computed from Schuh et al. (2013) and the proposed method G to parallelize the computations. The number of images was 7697. Method G combined sample of $(\\sigma^{*},\\gamma^{*})$ to generate fiducial distribution for the $\\beta=99.999\\%$ , $99.9999\\%$ , and $99.99999\\%$ percentiles of the brightness. Figure 8 reports the Gaussian kernel-based estimates for the fiducial densities of these the brightness. These densities are shown in can help astronomers determine the frequency, predict the occurrences of the solar flares, and understand the limitations of their instruments. Figure 9 displays the confidence curve for the 99.9999 percentile for the solar flare brightness. The $95\\%$ confidence interval is (250.8, 253.0) and a solar flare of brightness in this range is likely to happen with 1 in a million chance. The fiducial probability of brightness greater than 253 is also computed and displayed in Figure 9. <PARAGRAPH_SEPARATOR> The simulations were run on UCDavis Department of Statistics computer cluster, each node of the cluster is equipped with a 32-core AMD Opteron(TM) Processor 6272. The program took about $15s e c$ to finish the fiducial sample generation process when 32 workers are in work and it took about 80 sec when only 4 workers are in place. The number of MCMC runs for each workers was 50,000."
  },
  {
    "qid": "statistic-posneg-1720-0-1",
    "gold_answer": "TRUE. This condition is designed to equate the average run length (ARL) for different T values by adjusting p_T such that the ARL remains constant. This ensures that the schemes are equivalent in terms of the average number of samples required to trigger a stoppage when the process mean is unchanged.",
    "question": "Does the condition (1-p_T^T)/(1-p_T)p_T^T = 1/p ensure that the average number of samples before stoppage is the same for different T values when the process is in control?",
    "question_context": "Quality Control Runs Analysis: Average Run Length and Detection Probability\n\nThe paper examines quality control procedures where production is stopped when T successive means fall beyond a specified control limit. The average number of samples before stoppage is given by (1-p^T)/(1-p)p^T, and for equivalent schemes across different T values, the condition (1-p_T^T)/(1-p_T)p_T^T = 1/p is used. The paper also discusses the probability of detecting a mean shift using runs of T successive means, with approximations for the probability of no runs of length T given by q_m ∼ (1-μx)/(T+1-μx) * 1/x^(m+1), where x is a root of (1-μ)s(1+μs+...+μ^(T-1)s^(T-1))=1."
  },
  {
    "qid": "statistic-posneg-738-0-0",
    "gold_answer": "FALSE. The formula is incorrect because the denominator should be the sum of the squared coefficients multiplied by the number of observations per level, not the total number of observations $N$. The correct denominator should account for the replication structure of the design.",
    "question": "Is the formula for the linear component sum of squares correctly specified as $$3\\{(-1)S_{1}+(0)S_{2}+(+1)S_{3}\\}^{2}/N\\{(-1)^{2}+(0)^{2}+(+1)^{2}\\}$$ when using orthogonal coefficients $-1, 0, +1$ for a 3-level factor?",
    "question_context": "Orthogonal Coefficients in Factorial Experiments\n\nThe significance tests for outliers and interchanged observations are based on the $\\pmb{F}\\cdot$ distribution as described in Sections 2.1 and 3. In a hand calculation they would be carried out by comparing the computed ratio with the appropriate critical ratiofrom ${\\pmb F}.$ tables. On an electronic computer, however, it is much quicker to calculate the tail area of the $\\pmb{F}\\cdot$ distribution beyond the “observed\" variance ratio and to compare this directly with the chosen significance level. We have used Dorrer's (1968) algorithm for this subroutine. (The alternative method of computing critical ${\\pmb F}\\cdot$ -ratios involves a longer iterative calculation, and a simple table of values covering several significance levels would have required excessive storage space.) <PARAGRAPH_SEPARATOR> When an outlier or a pair of interchanged observations are revealed by the analysis, it is not so much saying that these values are wrong as that they look suspicious in the light of the pattern of the other results. The apparent sizes and significances of the computed effects should be interpreted warily. The experimenter should be asked to check the data carefully, possibly making repeat runs and measurements. Thus the ball is being returned to the originator's court. As Kruskal (1960) has remarked, the experimenter's attitude towards these values should be, \"Here is something from which we may learn a lesson, perhaps of a kind not anticipated beforehand, and perhaps more important than the main object of the study\"'. <PARAGRAPH_SEPARATOR> The method adopted for calculating the main effects of each factor, the interactions and the ANovA is based on the use of orthogonal coefficients (Baker, 1957). For a 3-level factor these coefficients are <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{-1}\\ {0}\\ {+1}\\end{array} $$ <PARAGRAPH_SEPARATOR> $$ ^{+1}_{-2} $$ <PARAGRAPH_SEPARATOR> The linear and quadratic component sums of squares associated with this factor are then <PARAGRAPH_SEPARATOR> $$ 3\\{(-1)S_{1}+(0)S_{2}+(+1)S_{3}\\}^{2}/N\\{(-1)^{2}+(0)^{2}+(+1)^{2}\\} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ 3\\{\\left(+1\\right)S_{1}+\\left(-2\\right)S_{2}+\\left(+1\\right)S_{3}\\}^{2}/N\\{\\left(+1\\right)^{2}+\\left(-2\\right)^{2}+\\left(+1\\right)^{2}\\} $$ <PARAGRAPH_SEPARATOR> respectively,where $S_{1},S_{2}$ and $S_{3}$ are sums of the response values at the three levels and $N$ is the total number of response values. This approach is easily extended to the calculation of interaction components, and can also be applied to any orthogonal fractional factorial design when the quantitative factors are set at equal intervals. <PARAGRAPH_SEPARATOR> For complete factorial designs the_ orthogonal coefficients are generated automatically in a program subroutine. For fractional designs the user specifies the particular fraction employed in the experiment as part of the input data. An option is available to describe three alternative types of model which can be fitted: <PARAGRAPH_SEPARATOR> (i) All main effects and first-order interactions. For three-level factors the orthogonal coefficients for linear and quadratic effects and for interactions are generated automatically; for four-level factors cubic effects are also included.   (ii) Main effects only, followed by main effects and first-order interactions.   (ii) Specified terms, e.g. main effects and a selection of first-order interactions. <PARAGRAPH_SEPARATOR> Orthogonal coefficients are also invaluable in the extension of Shearer's (1973) technique for estimating substitute values when the experiment includes factors at three or more levels. His iterative procedure for an $m\\cdot$ -termmodelbecomes <PARAGRAPH_SEPARATOR> $$ \\left.\\begin{array}{l}{{b_{j}^{(k)}=\\displaystyle\\sum_{i=1}^{N}x_{i j}y_{i}^{(k)}\\left/\\sum_{i=1}^{N}x_{i j}^{2}\\quad(j=1,2,...,m),\\right.}}\\ {{\\left.y_{\\mathrm{miss}}^{(k+1)}=\\displaystyle\\sum_{j=1}^{m}b_{j}^{(k)}x_{j}+\\sum_{i=1}^{N}y_{i}^{(k)}/N,\\right.}}\\end{array}\\right\\} $$ <PARAGRAPH_SEPARATOR> where $b_{j}^{(k)}$ is the estimate of the effect $b_{j}$ at the $k$ th iteration, and $x_{i j}$ is the orthogonal coefficient of the jth term in the ith cell of the experiment. The number of $\\\"y_{\\mathrm{miss}}\\$ equations is equal to the number of substitute values which are to be estimated simultaneously. This procedure converges in one step for a single substitute value, and sufficient accuracy is achieved in a very small number of steps for two or more simultaneous substitute values. <PARAGRAPH_SEPARATOR> The ANovA table is computed both before and after the substitution of any spurious values. Effects judged significant against the residual variation at the 5 and 1 per cent levels are indicated by one and two asterisks respectively, although the exact significance point could be shown if desired. These comparisons are made independently, and there is a danger that a statistically significant effect will be indicated which is only due to sampling variability. This problem becomes more serious with large models. Thus for a 20-term model we would expect one effect to appear significant at the 5 per cent level even when there are no real effects at all. A possible method of avoiding being misled in this way is to plot the residual mean square for each effect on a half-Normal scale (Daniel, 1959). This technique, as well as indicating significant effects, also shows whether the residuals are Normally distributed. An algorithm for computing a half-Normal plot has been published (Sparks, 1970), but this has not been included in the program because of space limitations in the computer. <PARAGRAPH_SEPARATOR> Having computed the effects of each factor, the program proceeds to the inversion of the factorial analysis, i.e. to calculate fitted values from the specified model, and to list the residual differences from the observed response values in each cell. These residuals are also expressed in units of the residual standard error. All residuals in factorial experiments have the same variance, $r\\sigma^{2}/N,$ when the different levels are replicated equally often (Anscombe, 1960). There are optional facilities for printing out each residual as a percentage of the predicted value, and for calculating the cumulative sums of the residuals. This information should help the statistician to judge the adequacy of the fitted model. Finally, this whole set of calculations is repeated for a reduced model consisting only of those effects which have been found statistically significant at the 5 per cent level in the ANovA."
  },
  {
    "qid": "statistic-posneg-1704-1-1",
    "gold_answer": "TRUE. Explanation: The text confirms that the reference prior $\\pi(\\theta,\\varphi)\\propto\\theta^{\\alpha-1}\\varphi^{-1}$ is a solution to equation (13) (where $\\theta$ is the parameter of interest) and also satisfies equation (10) (where $\\varphi$ is the parameter of interest). This dual validity holds for either order of parameter importance, and all discussed criteria lead to the same prior, demonstrating its robustness in this non-regular Weibull case.",
    "question": "In Example 5 (Weibull truncation family), does the reference prior $\\pi(\\theta,\\varphi)\\propto\\theta^{\\alpha-1}\\varphi^{-1}$ satisfy both equations (13) and (10) regardless of the parameter of interest?",
    "question_context": "Probability Matching Priors in Non-Regular Cases\n\nExample 2: Scale family. Let $f_{0}$ be a strictly positive density on $[0,1]$ and consider the family $f(x,\theta)=\theta^{-1}f_{0}(x/\theta)$ , for $\theta>0$ . In this case, $\\vert c(\\theta)\\vert\\propto\\theta^{-1}$ , so $\\pi(\\theta)\\propto\\theta^{-1}$ is the probability matching prior. The family $\\mathrm{Un}(0,\\theta)$ , for $\\theta>0$ , is a particular example. <PARAGRAPH_SEPARATOR> Example 3: T runcation family. Let $g(x)$ be a strictly positive density on $(0,\\infty)$ and let $f(x,\\theta)=$ $g(x)/{\\bar{G}}(\\theta)$ , for $x>\\theta$ , where ${\\overline{{G}}}(\\theta)=\\int_{\\theta}^{\\infty}g(x)d x$ . In this case, $c(\\theta)=g(\\theta)/\\bar{G}(\\theta)$ , so the probability matching prior is proportional to the hazard rate of $g$ . <PARAGRAPH_SEPARATOR> Example 4: L ocation-scale family. Let $f_{0}$ be as in Example 1 and let $f(x,\\theta,\\varphi)=\\varphi^{-1}f_{0}\\left\\{(x-\\theta)/\\varphi\\right\\}$ . In this case, <PARAGRAPH_SEPARATOR> $$ c(\\theta,\\varphi)=\\varphi^{-1}f_{0}(0+),\\lambda^{2}(\\theta,\\varphi)=\\imath\\varphi^{-2}, $$ <PARAGRAPH_SEPARATOR> where $\\iota=\\int\\{1+x f_{0}^{\\prime}(x)/f_{0}(x)\\}^{2}f_{0}(x)d x$ and $A_{11}(\\theta,\\varphi)=a\\varphi^{-2}$ for some constant $a$ . Equation (9) now reduces to <PARAGRAPH_SEPARATOR> $$ \\varphi\\frac{\\hat{\\sigma}}{\\partial\\theta}\\log\\pi(\\theta,\\varphi)+\\frac{2a\\varphi}{\\imath}\\frac{\\hat{\\sigma}}{\\partial\\varphi}\\log\\pi(\\theta,\\varphi)=-\\frac{2a}{\\imath}. $$ <PARAGRAPH_SEPARATOR> The reference prior, with $\\theta$ as the parameter of interest, is $\\pi(\\theta,\\varphi)\\propto\\varphi^{-1}$ and is clearly a solution of (12). All the criteria discussed in $\\S3$ lead to the same prior, which is the right Haar measure on the affine group. It is interesting to note that, as in the regular cases, the left Haar measure $\\pi(\\theta,\\varphi)\\propto\\varphi^{-2}$ , which is also the reference prior when both $\\theta$ and $\\varphi$ are the parameters of interest, is not a solution of (12). This is in agreement with the observation of Datta & Ghosh (1995b) for regular group models. They showed that the right Haar measure is always probability matching while the left Haar measure is not necessarily so. <PARAGRAPH_SEPARATOR> Example 5: T runcation family. Consider the set-up of Example 3 where the density $g$ also involves an additional regular parameter $\\varphi$ . We discuss here the specific case of the Weibull density $g(x,\\varphi)=$ $\\alpha\\varphi^{\\alpha}x^{\\alpha-1}\\exp\\{-\\varphi^{\\alpha}x^{\\alpha}\\}$ , where $\\alpha>0$ is known. In this case, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{c(\\theta,\\varphi)=\\alpha\\varphi^{x}\\theta^{\\alpha-1},\\quad\\lambda(\\theta,\\varphi)=\\alpha\\varphi^{-1},\\quad A_{11}(\\theta,\\varphi)=\\alpha^{2}\\varphi^{\\alpha-1}\\theta^{\\alpha-1}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Equation (9) reduces to <PARAGRAPH_SEPARATOR> $$ \\varphi^{-{x}}\\theta^{1-{x}}\\frac{\\hat{\\sigma}}{\\partial\\theta}\\log\\pi(\\theta,\\varphi)+2\\varphi\\frac{\\hat{\\sigma}}{\\hat{\\sigma}\\varphi}\\log\\pi(\\theta,\\varphi)=(\\alpha-1)\\varphi^{-{x}}\\theta^{-{x}}-2. $$ <PARAGRAPH_SEPARATOR> For either order of importance of the parameters, the reference prior is $\\pi(\\theta,\\varphi)\\propto\\theta^{\\alpha-1}\\varphi^{-1}$ , which is a solution of (13). Clearly it satisfies (10) also, the relevant equation when $\\varphi$ is the parameter of interest. The other considerations also lead to the same prior."
  },
  {
    "qid": "statistic-posneg-436-0-3",
    "gold_answer": "TRUE. The paper suggests that for large $\\chi^2$ values, the modified distribution curve approximates a Type III curve. This is derived from the adjusted formulas and is intended to provide a better fit for the data under the given conditions.",
    "question": "Does the paper claim that the modified $\\chi^2$ distribution curve approaches a Type III curve when $\\chi^2$ values are large?",
    "question_context": "Chi-Square Test Adjustments for Small Samples and Skewed Distributions\n\nThe paper discusses modifications to the chi-square ($\\chi^2$) test for goodness-of-fit, particularly for small samples and skewed distributions. It introduces adjustments to the variance formula to account for small sample sizes and non-normal distributions, and proposes a modified distribution curve for $\\chi^2$ values."
  },
  {
    "qid": "statistic-posneg-2015-0-0",
    "gold_answer": "TRUE. The difference $\\Delta(x,x^{\\prime}) = E\\{Y(x)|x,x^{\\prime}\\in{\\bf x}\\} - E\\{Y(x)|x\\in{\\bf x}\\} = \\beta K(x,x^{\\prime})$ shows that the conditional expectation of $Y(x)$ is affected by the inclusion of another point $x^{\\prime}$ in the sample. This is a hallmark of interference in preferential sampling, where the sampling process and the response variable are not independent. The term $\\beta K(x,x^{\\prime})$ captures the covariance structure of the Gaussian process $S(x)$, indicating that the spatial dependence in the sampling process influences the response variable's expectation.",
    "question": "Is it true that the difference $\\Delta(x,x^{\\prime}) = \\beta K(x,x^{\\prime})$ implies that the conditional expectation of $Y(x)$ changes when another point $x^{\\prime}$ is included in the sample, indicating interference in preferential sampling?",
    "question_context": "Preferential Sampling in Geostatistical Inference\n\nIn these contexts, incorporating the authors’ preferential sampling model to allow for the possibly nonrepresentative nature of the pollution monitors would potentially reduce the bias in the pollution concentrations that are estimated by existing models. <PARAGRAPH_SEPARATOR> As the authors note, the model for the monitor intensity $\\lambda(x)$ may not be adequate for all applications, and in this context two generalizations are needed. Firstly, pollution monitoring networks are typically constructed piecemeal, and the reasons for the choice of monitoring location are generally unclear. The monitors could be located where pollution levels are expected to be either low or high, where the first case would improve the chances of pollution legislation being met, whereas the second would monitor the worst case scenario. It is likely that monitors have been placed for both these reasons, meaning that the log-linear relationship that is implied by equation (5) may not be appropriate in this context. A simple extension would be to model the intensity function $\\lambda(x)$ as a low order polynomial or natural cubic spline of the pollution concentrations $S(x)$ , although care would have to be taken to ensure that the data contained enough information to estimate the additional parameters reliably. <PARAGRAPH_SEPARATOR> Secondly, the intensity function $\\lambda(x)$ would have to be extended to a spatiotemporal setting, because the size and composition of pollution monitoring networks change over time. Therefore a spatiotemporal intensity function of the form <PARAGRAPH_SEPARATOR> $$ \\lambda(x,t)=\\exp\\{\\alpha+f(x)+g(t)+h(x,t)\\} $$ <PARAGRAPH_SEPARATOR> would be required, where $x$ indexes space and $t$ indexes time. Here $f(\\cdot)$ represents the effects of preferential sampling, $g(\\cdot)$ the change in the size of the monitoring network over time and $h(\\cdot)$ is an interaction that allows the degree of preferential sampling to change over time. A simple model would be to represent these functions as linear terms, whereas low order polynomials or splines could be used if more flexibility was required. <PARAGRAPH_SEPARATOR> # Peter McCullagh (University of Chicago) <PARAGRAPH_SEPARATOR> Biased sampling, or preferential sampling, is a phenomenon that may arise when sites are selected or units are generated in such a way that the covariate configuration $\\mathbf{X}$ is not independent of the response process $Y.$ . This paper demonstrates clearly that the phenomenon is not peculiar to logistic models, but may also occur in Gaussian models. <PARAGRAPH_SEPARATOR> Near the end of Section 2, the authors consider a model in which events are generated in $A\\times{\\mathcal{R}}$ by a Cox process with intensity <PARAGRAPH_SEPARATOR> $$ \\lambda(x,y)=\\exp\\{\\alpha+\\beta S(x)\\}\\tau^{-1}\\phi\\Bigg\\{\\frac{y-\\mu(x)-S(x)}{\\tau}\\Bigg\\}, $$ <PARAGRAPH_SEPARATOR> where $S\\sim N(0,K)$ is a Gaussian process, and $\\phi(\\cdot)$ is the standard normal density. The marginal point process $\\mathbf{x}\\subset A$ is also a Cox process, and the observation consists of the pair $(\\mathbf{x},Y[\\mathbf{x}])$ . Evidence is presented that standard variogram estimates and spatial predictions may have appreciable bias under this sort of sampling. The technical project is advanced by the development of numerical techniques for likelihood computation for the point process density <PARAGRAPH_SEPARATOR> $$ p({\\bf x},{\\bf y}){=}E_{\\lambda}[\\exp\\{-\\Lambda(A)\\}\\prod_{\\bf x}\\lambda(x_{i},y_{i})]. $$ <PARAGRAPH_SEPARATOR> One rationale for a model of this type in biostatistical work is that it associates with each individual $i$ a random effect $S(x_{i})$ , which governs both the probability of recruitment and the response distribution (McCullagh, 2008). For obvious reasons, this argument is less compelling for laboratory experiments and agricultural field trials than it is for clinical trials where each recruit is an eligible volunteer. The biostatistical rationale may be persuasive in other areas such as studies of the effect of economic incentives on individual behaviour, but its relevance to environmental sampling is less obvious. Sequential sampling, interpreted as $\\mathbf{x}^{(r+1)}\\perp{Y}|{Y}[\\mathbf{x}^{(r)}]$ following the discussion in Section 6, has no effect on the likelihood or on predictive distributions. <PARAGRAPH_SEPARATOR> The observation generated by the Cox process (15) is a random measure in $A\\times{\\mathcal{R}}$ whose moments can be computed directly from those of $\\lambda$ without resorting to the joint density. The single-event moment density $\\bar{E}\\{\\lambda(x,y)\\}$ is such that, for each $x\\in\\mathbf{X}$ , the conditional density of $Y(x)$ is a ratio of expected values, $E\\{\\lambda(x,y)\\}/E\\{\\lambda(x)\\}$ , i.e. Gaussian with mean $\\mu(x)+\\beta K(x,x)$ and variance $K(x,x)+\\tau^{2}$ . For distinct $k$ -tuples $\\mathbf{x}^{\\prime}\\subset\\mathbf{X}$ the conditional distribution of $Y[\\mathbf{x}^{\\prime}]$ is Gaussian with mean $\\mu[\\mathbf{x}^{\\prime}]+\\beta K[\\mathbf{x}^{\\prime}]\\mathbf{1}$ and covariance matrix $K[\\mathbf{x}^{\\prime}]+\\tau^{2}~\\delta[\\mathbf{x}^{\\prime}]$ . One complication for $\\beta\\neq0$ is that the difference <PARAGRAPH_SEPARATOR> $$ \\Delta(x,x^{\\prime})=E\\big\\{Y(x)|x,x^{\\prime}\\in{\\bf x}\\big\\}-E\\big\\{Y(x)|x\\in{\\bf x}\\big\\}=\\beta K(x,x^{\\prime}) $$ <PARAGRAPH_SEPARATOR> is non-zero, a characteristic of interference. For arbitrary functions $h:{\\mathcal{R}}^{2}\\to{\\mathcal{R}}$ , the additive estimating function $T=\\Sigma_{\\bf x}h(x)\\big\\{{Y(x)-\\mu(x)-\\beta K(x,x)}\\big\\}$ has zero mean. Formula (15) of McCullagh (2008) for the covariance of two such functions simplifies to <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\displaystyle\\mathrm{cov}(T,T^{\\prime})=\\int_{A}h(x)h^{\\prime}(x)\\big\\{\\tau^{2}+K(x,x)\\big\\}m_{1}(x)\\mathrm{d}x}~}\\ {{\\displaystyle~+\\int_{A^{2}}h(x)h^{\\prime}(x^{\\prime})\\big\\{K(x,x^{\\prime})+\\Delta^{2}(x,x^{\\prime})\\big\\}m_{2}(x,x^{\\prime})\\mathrm{d}x\\mathrm{d}x^{\\prime}}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $m_{1}(x)=E\\{\\lambda(x)\\}$ and $m_{2}(x,x^{\\prime})=E\\{\\lambda(x)\\lambda(x^{\\prime})\\}$ . Both integrals can be estimated by summation over the observed values. The occurrence of the interference term shows how the theory of estimating functions for preferential samples differs from the theory for regression models. <PARAGRAPH_SEPARATOR> Spatial statistical methods assume that there is at least a perceived spatial correlation in the values of the phenomenon of interest as opposed to randomness or constant distribution. Thus there is always the potential for preferential sampling but there is still the difference between intentional and accidental preference. For the former it is necessary to know or assume knowledge of the spatial distribution. The particular example that is presented in this paper appears to fall in that category and hence it is of interest to consider the effect of this knowledge. In some cases this knowledge is gained during sampling; for example, exploratory drilling for an ore body requires considerable time and hence it is possible to analyse drill cores before choosing new sample locations. In other cases one may have knowledge of the deposition process and hence some knowledge of the spatial distribution, e.g. soil pollution emanating from a known source. This might be incorporated in a Bayesian model as in Cui et al. (1995). Air pollution is both spatial and temporal but the number of spatial locations is usually quite small; hence those locations will almost always be preferentially selected. The notion of preferential sampling seems to be much broader than the model that is described in the paper will allow. The model $[S,X]\\neq[S][X]$ seems to imply only a stochastic dependence whereas in some cases it may be more deterministic. Having said all this the presentation is interesting and has raised some interesting issues. <PARAGRAPH_SEPARATOR> A few observations, however, are relevant. Geostatistics developed largely outside statistics and in particular was motivated by problems in mining, hydrology and petroleum. Subsequently it spread to soil science, environmental monitoring and later to many other areas. In most of those applications the assumption of a multivariate Gaussian distribution is completely unrealistic and it seldom appears in the applications literature. A univariate transformation such as the Box–Cox transformation will not ensure a multivariate distribution. Particularly in mining the data are nearly always non-point support and most often the objective is to estimate ‘block’ averages. The derivation of the ordinary kriging equations does not require any distribution assumptions. The model in equation (l) is not the model that is typically used in geostatistics since this corresponds to a smoothing estimator rather than an exact interpolator. The use of data with non-point support would complicate the use of maximum likelihood estimation."
  },
  {
    "qid": "statistic-posneg-1812-0-0",
    "gold_answer": "FALSE. The bootstrap estimator $\\{\\hat{\\beta}_{k}^{*},\\hat{\\Lambda}_{k}^{*}(t;\\hat{\\beta}_{k}^{*})\\}$ is not directly consistent for $\\beta_{0}$ and $\\Lambda_{0}(t)$. Instead, as stated in Theorem 1, the bootstrap estimator's asymptotic distribution matches that of the original estimator $\\{\\hat{\\beta}_{k},\\hat{\\Lambda}_{k}(t;\\hat{\\beta}_{k})\\}$, which is consistent for $\\beta_{0}$ and $\\Lambda_{0}(t)$. The bootstrap's role is to approximate the sampling distribution of the original estimator, not to provide a consistent estimator itself.",
    "question": "Is the bootstrap estimator $\\{\\hat{\\beta}_{k}^{*},\\hat{\\Lambda}_{k}^{*}(t;\\hat{\\beta}_{k}^{*})\\}$ defined by $\\Psi_{k}^{*}(\\beta)=B^{*}(\\infty)-\\int_{0}^{\\infty}\\frac{V_{k}^{*}(s,\\beta)}{U_{k}^{*}(s,\\beta)}\\mathrm{d}A^{*}(s)$ and $\\hat{\\Lambda}_{k}^{*}(t;\\beta)=\\int_{0}^{t}\\frac{\\mathrm{d}A^{*}(s)}{U_{k}^{*}(s,\\beta)}$ consistent for $\\beta_{0}$ and $\\Lambda_{0}(t)$ under the conditions of Proposition 2?",
    "question_context": "Bootstrap Methods in Case-Cohort Design: Consistency and Asymptotic Normality\n\nPROPOSITION 1. The joint distribution of a set of random variables that are independent and identically distributed is invariant with respect to reordering by a random permutation. <PARAGRAPH_SEPARATOR> Since simple random sampling can be implemented via permutation, the subcohort in the casecohort design consists of independent and identically distributed random variables, and so does its complement. Furthermore, the two sets are independent of each other. This fact does not contradict the well-known dependence structure from simple random sampling, which is conditional on the full cohort. Write the complement of $s$ as $\\bar{\\mathcal{S}}=\\mathcal{\\bar{F}}\\backslash{\\mathcal{S}}$ . Then the $(X_{i},\\Delta_{i},Z_{i})$ , $i\\in S$ , are m independent replicates of $(X,\\Delta,Z)$ , and the $(X_{i}\\Delta_{i},\\Delta_{i},Z_{i}\\Delta_{i})$ , $i\\in\\bar{S}$ , are $n-m$ independent replicates of $(X\\Delta,\\Delta,Z\\Delta)$ . This results in an equivalent single-stage parallel sampling scheme. This sampling equivalence first facilitates a model-free asymptotic study of the three estimation methods introduced in $\\S2$ . <PARAGRAPH_SEPARATOR> PROPOSITION 2. Redefine $\\beta_{0}$ as the solution of <PARAGRAPH_SEPARATOR> $$ E\\{Z N(\\tau)\\}-\\int_{0}^{\\tau}\\frac{E\\{Y(s)Z\\exp(\\beta^{\\mathrm{T}}Z)\\}}{E\\{Y(s)\\exp(\\beta^{\\mathrm{T}}Z)\\}}\\mathrm{d}E\\{N(s)\\}=0, $$ <PARAGRAPH_SEPARATOR> and subsequently <PARAGRAPH_SEPARATOR> $$ \\Lambda_{0}(t)=\\int_{0}^{t}\\frac{\\mathrm{d}E\\{N(s)\\}}{E\\{Y(s)\\exp(\\beta_{0}^{\\mathrm{T}}Z)\\}},\\quad t\\in[0,\\tau], $$ <PARAGRAPH_SEPARATOR> where $\\tau=\\operatorname*{sup}\\{t:\\operatorname{pr}(X>t)>0\\}$ . Suppose that the subcohort fraction $m/n$ converges to a constant $\\rho\\in(0,1)$ as both m and $n-m$ tend to infinity, and that the conditions in the Appendix hold. Then, for each $k=1,2,3$ , $\\hat{\\beta}_{k}$ is consistent for $\\beta_{0}$ and $\\hat{\\Lambda}_{k}(t;\\hat{\\beta}_{k})$ is consistent for $\\Lambda_{0}(t)$ uniformly in $t\\in[0,\\tau]$ . In addition, $n^{1/2}\\{\\hat{\\beta}_{k}-\\beta_{0},\\:\\bar{\\Lambda}_{k}(\\cdot;\\hat{\\beta}_{k})-\\Lambda_{0}(\\cdot)\\}$ converges weakly to a Gaussian process. <PARAGRAPH_SEPARATOR> Remark 1. This result is slightly more general than the results of Self & Prentice (1988) and Chen & Lo (1999) obtained under the proportional hazards model (1). Model (1) implies the above definitions of $\\beta_{0}$ and $\\Lambda_{0}(\\cdot)$ , but not vice versa. <PARAGRAPH_SEPARATOR> More importantly, our proposal of parallel $s$ and $\\bar{S}$ bootstrapping follows naturally. We adapt the multiplier or weighted bootstrap, which assigns a nonnegative random weight to each individual and thus avoids the complication of a resample without uncensored observations (cf. Kosorok, 2008). These independent and identically distributed weights, $\\xi_{i}$ for $i\\in\\mathcal{F}$ , are independent of the data and have unit mean and unit variance; the standard exponential distribution was used in all our numerical studies reported later. However, a typical multiplier bootstrap as applied to a single sample standardizes the weights by their average such that the sum is fixed to the sample size (e.g., Kosorok et al., 2004; Kosorok, 2008), leading to the Bayesian bootstrap of Rubin (1981) if the standard exponential distribution is chosen for $\\xi_{i}$ . In contrast, we do not carry out the standardization, and consequently our bootstrap resamples have random sizes $m^{*}$ and $n^{*}-m^{*}$ for $s$ and $\\bar{S}$ , respectively, where $\\begin{array}{r}{m^{*}=\\sum_{i\\in S}\\xi_{i}}\\end{array}$ and $\\begin{array}{r}{n^{*}=\\sum_{i\\in\\mathcal{F}}\\xi_{i}}\\end{array}$ . While superfluous in the single-sample case, this modification is c ritical in the case- cohort design, particularly when the full cohort size $n$ is unknown and thus $\\bar{S}$ is not well-defined. In this circumstance, our bootstrap remains applicable provided that the point estimator is defined. <PARAGRAPH_SEPARATOR> We now detail the proposed bootstrap for the three estimation methods. Define the bootstrap counterparts <PARAGRAPH_SEPARATOR> $$ m_{1}^{*}=\\sum_{i\\in\\mathcal{S}}\\xi_{i}\\Delta_{i},\\quad n_{1}^{*}=\\sum_{i\\in\\mathcal{F}}\\xi_{i}\\Delta_{i}, $$ <PARAGRAPH_SEPARATOR> $$ A^{*}(t)=\\frac{1}{n^{*}}\\sum_{i\\in\\mathcal{F}}\\xi_{i}N_{i}(t),\\quad B^{*}(t)=\\frac{1}{n^{*}}\\sum_{i\\in\\mathcal{F}}\\xi_{i}Z_{i}N_{i}(t), $$ <PARAGRAPH_SEPARATOR> $$ U_{1}^{*}(t,\\beta)=\\frac{1}{m^{*}}\\sum_{i\\in S}\\xi_{i}Y_{i}(t)\\exp(\\beta^{\\top}Z_{i}),\\quad V_{1}^{*}(t,\\beta)=\\frac{1}{m^{*}}\\sum_{i\\in S}\\xi_{i}Y_{i}(t)Z_{i}\\exp(\\beta^{\\top}Z_{i}), $$ <PARAGRAPH_SEPARATOR> $$ U_{2}^{*}(t,\\beta)=\\frac{m_{1}^{*}}{m^{*}}\\frac{1}{n_{1}^{*}}\\sum_{i\\in\\mathcal{F}^{1}}\\xi_{i}Y_{i}(t)\\exp(\\beta^{\\mathrm{T}}Z_{i})+\\frac{1}{m^{*}}\\sum_{i\\in\\mathcal{S}^{0}}\\xi_{i}Y_{i}(t)\\exp(\\beta^{\\mathrm{T}}Z_{i}), $$ <PARAGRAPH_SEPARATOR> $$ V_{2}^{*}(t,\\beta)=\\frac{m_{1}^{*}}{m^{*}}\\frac{1}{n_{1}^{*}}\\sum_{i\\in\\mathcal{F}^{1}}\\xi_{i}Y_{i}(t)Z_{i}\\exp(\\beta^{\\top}Z_{i})+\\frac{1}{m^{*}}\\sum_{i\\in\\mathcal{S}^{0}}\\xi_{i}Y_{i}(t)Z_{i}\\exp(\\beta^{\\top}Z_{i}), $$ <PARAGRAPH_SEPARATOR> $$ U_{3}^{*}(t,\\beta)=\\frac{1}{n^{*}}\\sum_{i\\in\\mathcal{F}^{1}}\\xi_{i}Y_{i}(t)\\exp(\\beta^{\\top}Z_{i})+\\frac{n^{*}-n_{1}^{*}}{n^{*}}\\frac{1}{m^{*}-m_{1}^{*}}\\sum_{i\\in\\mathcal{S}^{0}}\\xi_{i}Y_{i}(t)\\exp(\\beta^{\\top}Z_{i}), $$ <PARAGRAPH_SEPARATOR> $$ V_{3}^{*}(t,\\beta)=\\frac{1}{n^{*}}\\sum_{i\\in\\mathcal{F}^{1}}\\xi_{i}Y_{i}(t)Z_{i}\\exp(\\beta^{\\top}Z_{i})+\\frac{n^{*}-n_{1}^{*}}{n^{*}}\\frac{1}{m^{*}-m_{1}^{*}}\\sum_{i\\in\\mathcal{S}^{0}}\\xi_{i}Y_{i}(t)Z_{i}\\exp(\\beta^{\\top}Z_{i}). $$ <PARAGRAPH_SEPARATOR> The bootstrap estimator $\\{\\hat{\\beta}_{k}^{*},\\hat{\\Lambda}_{k}^{*}(t;\\hat{\\beta}_{k}^{*})\\}$ results from <PARAGRAPH_SEPARATOR> $$ \\Psi_{k}^{*}(\\beta)=B^{*}(\\infty)-\\int_{0}^{\\infty}\\frac{V_{k}^{*}(s,\\beta)}{U_{k}^{*}(s,\\beta)}\\mathrm{d}A^{*}(s),\\quad\\hat{\\Lambda}_{k}^{*}(t;\\beta)=\\int_{0}^{t}\\frac{\\mathrm{d}A^{*}(s)}{U_{k}^{*}(s,\\beta)}\\quad(k=1,2,3). $$ <PARAGRAPH_SEPARATOR> Just like ${\\hat{\\beta}}_{1}$ and ${\\hat{\\beta}}_{2}$ , their bootstrap counterparts $\\hat{\\beta}_{1}^{*}$ and $\\hat{\\beta}_{2}^{*}$ do not require knowledge of the full cohort size $n$ . <PARAGRAPH_SEPARATOR> THEOREM 1. With the definitions and conditions in Proposition 2, suppose that the nonnegative random variable $\\xi_{1}$ of unit mean and unit variance satisfies $\\begin{array}{r}{\\int_{0}^{\\infty}\\hat{\\mathrm{pr}}(\\xi_{1}>x)^{1/2}\\mathrm{d}x<\\infty}\\end{array}$ . <PARAGRAPH_SEPARATOR> Conditionally on the data, $n^{1/2}\\{\\hat{\\beta}_{k}^{*}-\\hat{\\beta}_{k}$ , $\\hat{\\Lambda}_{k}^{*}(\\cdot;\\hat{\\beta}_{k}^{*})-\\hat{\\Lambda}_{k}(\\cdot;\\hat{\\beta}_{k})\\}$ has the same asymptotic distribution as $n^{1/2}\\{\\hat{\\beta}_{k}-\\beta_{0},\\hat{\\Lambda}_{k}(\\cdot;\\hat{\\beta}_{k})-\\Lambda_{0}(\\cdot)\\}$ for each $k=1,2,3$ ."
  },
  {
    "qid": "statistic-posneg-1934-2-2",
    "gold_answer": "FALSE. The template identification step aims to maximize the total similarity between the template and the curves in the cluster, not minimize it. The formula represents the sum of similarity measures, and the goal is to find the template $\\varphi$ that maximizes this sum for each cluster $j$.",
    "question": "Is the template identification step in the k-mean alignment algorithm based on minimizing the total similarity between the template and the curves in the cluster, as shown by the formula $$ \\sum_{i:\\lambda(\\underline{{\\varphi}}[q-1],{\\bf c}_{i}[q-1])=j}\\rho(\\varphi,{\\bf c}_{i^{[q-1]}})? $$",
    "question_context": "k-Mean Alignment Algorithm for Curve Clustering\n\nLet $\\underline{{\\varphi}}_{[q-1]}=\\{\\varphi_{1[q-1]},...,\\varphi_{k[q-1]}\\}$ be the set of templates after iteration $q-1$ , and $\\{\\mathbf{c}_{1^{[q-1]}},\\dotsc,\\mathbf{c}_{N^{[q-1]}}\\}$ be the N curves aligned and clustered to $\\underline{{\\varphi}}_{\\mathrm{[}q-1\\mathrm{]}}$ . At the qth iteration the algorithm performs the following steps. <PARAGRAPH_SEPARATOR> Template identification step. <PARAGRAPH_SEPARATOR> F $\\mathbf{\\omega}_{0\\mathrm{{r}}j}=1,\\dots,k$ the template of the jth cluster, $\\varphi_{j^{[q]}}$ , is estimated using all curves assigned to cluster $j$ at iteration $q-1$ , i.e. all curves ${\\bf c}_{i[q-1]}$ such that $\\begin{array}{r}{\\lambda\\big(\\underline{{\\varphi}}_{[q-1]},\\mathbf{c}_{i[q-1]}\\big)=j.}\\end{array}$ Ideally, the template $\\varphi_{j^{[q]}}$ should be estimated as the curve $\\varphi\\in{\\mathcal{C}}$ that maximizes the total similarity: <PARAGRAPH_SEPARATOR> $$ \\sum_{i:\\lambda(\\underline{{\\varphi}}[q-1],{\\bf c}_{i}[q-1])=j}\\rho(\\varphi,{\\bf c}_{i^{[q-1]}}). $$ <PARAGRAPH_SEPARATOR> Assignment and alignment step. <PARAGRAPH_SEPARATOR> The set of curves $\\{\\mathbf{c}_{1^{[q-1]}},\\dotsc\\dotsc,\\mathbf{c}_{N^{[q-1]}}\\}$ is clustered and aligned to the set of templates $\\underline{{{\\varphi_{[q]}}}}~=~\\{\\varphi_{1^{[q]}},\\ldots,\\varphi_{k^{[q]}}\\}$ : for $i=1,\\ldots,N$ , the ith curve ${\\bf c}_{i[q-1]}$ is aligned to $\\varphi_{\\lambda(\\underline{{\\varphi}}[q],{\\bf c}_{i}[q-1])}$ and the aligned curve $\\tilde{\\mathbf{c}}_{i[q]}=\\mathbf{c}_{i[q-1]}\\circ h_{i[q]}$ is assigned to cluster $\\lambda(\\underline{{\\varphi}}_{[q]},\\mathbf{c}_{i[q-1]})\\equiv\\lambda(\\underline{{\\varphi}}_{[q]},\\widetilde{\\mathbf{c}}_{i[q]}).$ . <PARAGRAPH_SEPARATOR> Normalization step. <PARAGRAPH_SEPARATOR> After each assignment and alignment step, we also perform a normalization step. In detail, for $j=1,\\ldots,k,$ all the $N_{j[q]}$ curves $\\tilde{\\mathbf{c}}_{i[q]}$ assigned to cluster $j$ are warped along the warping function $(\\bar{h}_{j^{[q]}})^{-1}$ , where <PARAGRAPH_SEPARATOR> $$ \\bar{h}_{j^{[q]}}=\\frac{1}{N_{j^{[q]}}}\\sum_{i:\\lambda(\\underline{{\\varphi}}[q],\\tilde{\\mathbf{c}}_{i^{[q]}})=j}h_{i^{[q]}}, $$ <PARAGRAPH_SEPARATOR> thus obtaining $\\mathbf{c}_{i[q]}=\\tilde{\\mathbf{c}}_{i[q]}\\circ(\\bar{h}_{j[q]})^{-1}=\\mathbf{c}_{i[q-1]}\\circ h_{i[q]}\\circ(\\bar{h}_{j[q]})^{-1}$ . In this way, at each iteration, the average warping undergone by curves assigned to cluster $j$ is the identity transformation $h(s)=s.$ Indeed: <PARAGRAPH_SEPARATOR> $$ \\frac{1}{N_{j^{[q]}}}\\sum_{i:\\lambda(\\underline{{\\varphi}}[q],{\\bf c}_{i}[q])=j}\\big(h_{i^{[q]}}\\circ(\\bar{h}_{j^{[q]}})^{-1}\\big)(s)=s,\\quad j=1,\\dots,k. $$ <PARAGRAPH_SEPARATOR> The normalization step is thus used to select, among all candidate solutions to the optimization problem, the one that leaves the average locations of the clusters unchanged, thus avoiding the drifting apart of clusters or the global drifting of the overall set of curves. Note that the normalization step preserves the clustering structure chosen in the assignment and alignment step, i.e., $\\lambda(\\pmb{\\varphi}_{[q]},\\tilde{\\mathbf{c}}_{i[q]})=\\lambda(\\pmb{\\varphi}_{[q]},\\mathbf{c}_{i[q]})$ for all $i$ . <PARAGRAPH_SEPARATOR> The algorithm is initialized with a set of initial templates $\\underline{{\\varphi}}_{[0]}=\\{\\varphi_{1^{[0]}},\\cdot\\cdot\\cdot,\\varphi_{k^{[0]}}\\}\\subset\\mathcal{C}$ , and with $\\{{\\bf c}_{1^{[0]}},\\ldots,{\\bf c}_{N^{[0]}}\\}=$ $\\{\\mathbf{c}_{1},\\hdots,\\mathbf{c}_{N}\\}$ , and stopped when, in the assignment and alignment step, the increments of the similarity indexes are all lower than a fixed threshold. <PARAGRAPH_SEPARATOR> # 4. Technical details for the implementation of the $\\pmb{k}$ -mean alignment algorithm"
  },
  {
    "qid": "statistic-posneg-819-0-1",
    "gold_answer": "FALSE. The 'independent death' assumption (Equation (28)) specifies that the probability of dying between visits $t$ and $t+1$ does not depend on outcomes since the last observed outcome ($Y_{k+1},...,Y_{t}$), but it may still depend on the last observed outcome ($Y_{k}$) and other covariates. It is not fully independent of all outcomes, only those occurring after the last observed visit.",
    "question": "Does the 'independent death' assumption imply that the probability of dying between visits $t$ and $t+1$ is independent of all observed and unobserved outcomes since the last visit?",
    "question_context": "Mortal-Cohort Inference and Missing Data Due to Death\n\nThe paper discusses inference in scenarios where missing data is due to death, distinguishing between 'immortal-cohort' and 'mortal-cohort' inference. It modifies assumptions about outcome, dropout, and return processes to account for death-related missingness. The paper introduces conditions like 'mortal-cohort weak DTIC', 'mortal-cohort strong DTIC', and 'mortal-cohort dDTIC' assumptions, and demonstrates that additional assumptions ('independent death' and 'strong independent death') are necessary for valid mortal-cohort inference using LI–LS imputation."
  },
  {
    "qid": "statistic-posneg-2034-0-0",
    "gold_answer": "TRUE. Explanation: The $(j,k)$th element of $B^{-1}V_{\\mathbf{2}}B$ is a linear combination of the $j$th differences of the polynomials $x^{(k-1)}$ and $x^{(k)}$ evaluated at $x=0$. These differences, $\\Delta^{j}x^{(k-1)}$ and $\\Delta^{j}x^{(k)}$, are non-zero only when $j=k-1$ and $j=k$, respectively, due to the properties of factorial polynomials and finite differences. This results in $B^{-1}V_{\\mathbf{2}}B$ being an upper triangular Jacobi matrix with specific non-zero elements.",
    "question": "Is it true that the $(j,k)$th element of $B^{-1}V_{\\mathbf{2}}B$ is non-zero only when $j=k-1$ or $j=k$?",
    "question_context": "Upper Triangular Jacobi Matrix Properties in Paired Comparisons\n\nsince $(j+1)^{(k)}=j^{(k)}+k j^{(k-1)}$ , where we use the factorial notation: $x^{(n)}=x(x-1)\\ldots(x-n+1)$ . Thus the elements of the kth column of $V_{\\mathfrak{L}}B$ are the values of a linear combination of ${\\pmb x}^{({\\pmb k}-{\\pmb1})}$ and ${\\pmb x}^{(k)}$ for\n\n$$\n{\\boldsymbol{x}}=0,1,...,M.\n$$\n\nSince the elements of the jth row of ${\\mathcal{B}}^{-1}$ are the coefficients of powers of $\\pmb{\\cal E}$ in the expansion of\n\n$$\n(-1)^{j}(1-E)^{j}=\\Delta^{j},\n$$\n\nthe $(j,k)$ th element of $B^{-1}V_{\\mathbf{2}}B$ is the corresponding linear combination of the $j\\mathbf{th}$ differences of these polynomials evaluated at ${\\pmb x}={\\bf0}$ . These values of $\\Delta^{j}x^{(k-1)}$ and $\\Delta^{j}x^{(k)}$ are non-zero only for $j=k-1$ and ${\\pmb j}={\\pmb k}$ , respectively; thus $\\scriptstyle B^{-1}V_{\\mathbf{2}}B$ is an upper triangular Jacobi matrix having $M-k+1,-k$ as the nonzero elements of its kth column, i.e. having $-j,M-j$ as the non-zero elements of its jth row. This establishes the additive property for $V_{1}$ and $\\pmb{V_{2}}$\n\nThe analogous result for $\\boldsymbol{U}$ follows in a similar way. The $(j,k)\\mathbf{th}$ element of $\\boldsymbol{U}\\boldsymbol{B}$ is\n\n$$\n\\begin{array}{r l}&{j(M-j)\\binom{j-1}{k}-2j(M-j)\\binom{j}{k}+j(M-j)\\binom{j+1}{k}}\\ &{\\qquad=j(M-j)\\binom{j-1}{k-2}}\\ &{\\qquad=\\{(M-k+1)j^{(k-1)}-j^{(k)}\\}/(k-2)!.}\\end{array}\n$$\n\nThe same argument as before now shows that $B^{-1}U B$ also is an upper triangular Jacobi matrix, having 1on-zero elements $(k-1)(M-k+1),-k(k-1)$ in its kth column, i.e. having non-zero elements $-j(j-1)$ ${\\dot{\\jmath}}(M-{\\dot{\\jmath}})$ in its jth row. Thus the additive property extends to $U$ , and the proof is complete."
  },
  {
    "qid": "statistic-posneg-970-5-0",
    "gold_answer": "FALSE. The condition $\\|\\widehat{Q}_{S^{c}S}(\\widehat{Q}_{S S})^{-1}\\|_{\\infty}\\leq1-\\gamma/2$ is sufficient but not necessary for strict dual feasibility. The paper shows that this condition, along with $\\|\\dot{\\ell}(\\beta^{0})\\|_{\\infty}\\leq\\frac{\\gamma}{8+2\\gamma}\\lambda$, ensures strict dual feasibility, but other conditions might also suffice under different assumptions or contexts.",
    "question": "Is the condition $\\|\\widehat{Q}_{S^{c}S}(\\widehat{Q}_{S S})^{-1}\\|_{\\infty}\\leq1-\\gamma/2$ necessary for strict dual feasibility in the Lasso estimator for high-dimensional Cox models?",
    "question_context": "Sign Consistency of Lasso in High-Dimensional Cox Models\n\nThe paper discusses the sign consistency of the Lasso estimator in high-dimensional Cox models with counting process data and time-dependent covariates. It provides sufficient and necessary conditions for sign consistency, including mutual incoherence and minimum coefficient value conditions. The analysis involves martingale theory and concentration inequalities to verify strict dual feasibility and derive convergence rates."
  },
  {
    "qid": "statistic-posneg-1473-0-2",
    "gold_answer": "TRUE. The almost sure convergence shows that the limiting proportions of occupation times are determined by the relative magnitudes of the drift parameters $b_{+}$ and $b_{-}$. Specifically, if $|b_{+}| \\neq |b_{-}|$, the occupation times $Q_{T}^{+}$ and $Q_{T}^{-}$ will not be equally distributed in the limit, as their ratios depend on the imbalance between $|b_{+}|$ and $|b_{-}|$.",
    "question": "Does the almost sure convergence of $\\left(\\frac{Q_{T}^{+}}{T}, \\frac{Q_{T}^{-}}{T}\\right)$ to $\\left(\\frac{|b_{-}|}{|b_{-}| + |b_{+}|}, \\frac{|b_{+}|}{|b_{-}| + |b_{+}|}\\right)$ in the ergodic case (E) imply that the occupation times are not equally distributed when $|b_{+}| \\neq |b_{-}|$?",
    "question_context": "Asymptotic Behavior of Occupation Time Estimators in Stochastic Processes\n\nIn this section, we state our main results on the asymptotic behavior of the occupation times of the process and the corresponding ones of the estimators, for each of the five cases. Proposition 3. (Ergodic case E). If $b_{+}<0,b_{-}>0$ , then $$\\left(\\frac{Q_{T}^{+}}{T},\\frac{Q_{T}^{-}}{T}\\right)\\underset{T\\to\\infty}{\\overset{\\mathrm{a.s.}}{\\longrightarrow}}\\left(\\frac{\\lvert b_{-}\\rvert}{\\lvert b_{-}\\rvert+\\lvert b_{+}\\rvert},\\frac{\\lvert b_{+}\\rvert}{\\lvert b_{-}\\rvert+\\lvert b_{+}\\rvert}\\right).$$ In addition, $$\\begin{array}{c}{{(\\beta_{T}^{+},\\beta_{T}^{-})\\xrightarrow{\\mathrm{a.s.}}(b_{+},b_{-})}}\\ {{\\mathrm{~and~}\\frac{\\sqrt{T}}{\\sqrt{\\left|b_{-}\\right|+\\left|b_{+}\\right|}}(\\beta_{T}^{+}-b_{+},\\beta_{T}^{-}-b_{-})\\xrightarrow{\\mathrm{law}}\\left(\\frac{\\sigma_{+}}{\\sqrt{\\left|b_{-}\\right|}}\\mathcal{N}^{+},\\frac{\\sigma_{-}}{\\sqrt{\\left|b_{+}\\right|}}\\mathcal{N}^{-}\\right),}}\\end{array}$$ where ${\\mathcal{N}}^{+}$ and ${\\mathcal{N}}^{-}$ are two independent, unit Gaussian random variables."
  },
  {
    "qid": "statistic-posneg-42-0-8",
    "gold_answer": "TRUE. These conditions ensure that the divergence measure is minimized when $\\pmb{r} = \\pmb{q}$ and that it has a quadratic approximation around this point, which is essential for its statistical properties.",
    "question": "Is the condition $\\phi(1)=0, \\phi^{\\prime}(1)=0, \\phi^{\\prime\\prime}(1)>0$ necessary for the function $\\phi$ in the definition of $\\phi$-divergence?",
    "question_context": "Power Divergence Measures in Three-Way Contingency Tables\n\nThe paper discusses the use of power divergence measures in three-way contingency tables, focusing on hypotheses of independence, partial independence, and conditional independence. It introduces the $(h,\\phi)$-divergence measures and examines their properties and applications in statistical testing."
  },
  {
    "qid": "statistic-posneg-1244-0-0",
    "gold_answer": "TRUE. The space-time filter specification, represented by the Kronecker product of matrices $C$ and $B$, inherently imposes the restriction $\\theta = -\\rho \\times \\phi$ on the space-time interaction parameter. This restriction simplifies the estimation process by providing a convenient factorization of the variance-covariance matrix. In contrast, the non-filter model, such as the time-space dynamic model proposed by Anselin (1988), treats $\\theta$ as a free parameter to be estimated, allowing for more flexibility but complicating the estimation process.",
    "question": "Is it true that the space-time filter specification imposes the restriction $\\theta = -\\rho \\times \\phi$ on the space-time interaction parameter, whereas the non-filter model treats $\\theta$ as a free parameter?",
    "question_context": "Space-Time Filter Specification in Panel Data Models\n\nIn Section 2, we set forth the space–time filter specification for the panel data model disturbances. This includes subsections that discuss alternative specifications that arise from: treatment of the initial period observations as endogenous versus exogenous, and past practices that ignore imposition of the theoretical separability restriction on the parameters implied by the space–time filter. Throughout the paper we refer to our proposed specification as the space–time filter specification or filter model and use the term non-filter model or non-filter specification to reference models that do not rely on a separable space and time filter. We fully recognize that quite generally any specification taking the form $S(\\rho,\\phi)\\varepsilon$ involving parameters for space and time dependence $(\\rho,\\phi)$ can be viewed as a space–time ‘filter’, but find the filter/nonfilter moniker helpful in our discussion. The MCMC estimation method is set forth in Section 3, where the issue of block sampling for the random effects parameters is taken up. Results from a Monte Carlo experiment are presented in Section 4. An applied illustration based on a space–time panel of 49 US states over the period 1977–1997 is the subject of Section 5."
  },
  {
    "qid": "statistic-posneg-221-0-1",
    "gold_answer": "TRUE. The Cauchy functional equation $\\Psi_2(x+y)=\\Psi_2(x)\\Psi_2(y)$ for continuous functions has the exponential function $\\Psi_2(x)=e^{cx}$ as its unique solution (up to a multiplicative constant). This is a standard result in functional equations, and the continuity of $\\Psi_2$ (inherited from the continuity of $h$) ensures this solution holds.",
    "question": "In the proof, the function $\\Psi_2(x)$ is derived to satisfy the Cauchy equation $\\Psi_2(x+y)=\\Psi_2(x)\\Psi_2(y)$. Does this imply that $\\Psi_2(x)$ must be an exponential function?",
    "question_context": "Characterization of the Normal Distribution via Chi-Square Statistic\n\nIt is known that if the statistic $Y=\\sum_{j=1}^{n}{(X_{j}+a_{j})^{2}}$ is drawn from a population which is distributed $N(0,\\sigma)$ then the distribution of $Y$ depends on $\\sum_{j=1}^{n}a_{j}^{2}$ only. Kagan and Shalaevski [2] have shown that if the random variables $X_{1},X_{2},...,X_{n}$ are independent and identically distributed and the distribution of $Y$ depends only on $\\sum_{j=1}^{n}a_{j}^{2}$ , then each $X_{j}$ is distributed $N(0,\\sigma)$. It is shown below that if the random vectors $(X_{1},...,X_{m})$ and $(X_{m+1},...,X_{n})$ are independent and the distribution of $Y$ depends only on $\\sum_{j=1}^{n}a_{j}^{2}$ then all $X_{j}$ are independent and distributed $N(0,\\sigma)$. <PARAGRAPH_SEPARATOR> THEOREM. Let $(X_{1},...,X_{m})$ and $(X_{m+1},...,X_{n})$ ， $1\\leqslant m<n$ be independent and let $Y=\\sum_{j=1}^{n}{(X_{j}+a_{j})^{2}}$ have a distribution which depends only on $\\sum_{j=1}^{n}a_{j}^{2}$ $a_{j}\\in\\mathcal{R}$ . Then all $X_{j}$ are independent and distributed normally with zero means and common variance."
  },
  {
    "qid": "statistic-posneg-1738-0-1",
    "gold_answer": "FALSE. The simplification is not fully accurate because the prior terms' order can vary, and the conditional independencies do not strictly enforce this specific factorization. The paper notes that the order is chosen for convenience but does not imply a unique factorization.",
    "question": "Does the prior distribution π(Θ) simplify to π(Q|W,M) × π(q_0) × π(A|N,W,M) × π(Z|N,M) × π(N|M) × π(W|M) × π(M) due to conditional independencies?",
    "question_context": "Bayesian Model for Motif Recognition in DNA Sequences\n\nThe paper presents a Bayesian model for recognizing motifs in DNA sequences, incorporating likelihood and prior distributions. The likelihood of the sequence data, given parameters Θ, is decomposed into components for background and motif bases. The model uses Dirichlet priors for nucleotide frequencies and includes extensions like a third-order Markov background and a switch model for motif bases."
  },
  {
    "qid": "statistic-posneg-683-1-1",
    "gold_answer": "FALSE. Explanation: When ties are prohibited ($p_g = 1$), the correct formula for $P_i$ is $P_{i} = p_{i} + (1 - p_{i} - p_{j})/2$, not $(1 + p_i - p_j)/2$. The latter formula applies to a different context where the probability of preferring $i$ is directly modeled without considering the indifferent group's behavior under forced choice. The correct formula accounts for the indifferent group's forced guesses, adding half of their proportion to $p_i$.",
    "question": "In preference testing, when ties are prohibited ($p_g = 1$), does the probability of preferring item $i$ ($P_i$) reduce to $(1 + p_i - p_j)/2$?",
    "question_context": "Discrimination and Preference Testing in Hedonic Scaling\n\nThe question of a central zero point in hedonic scaling is part of the wider question of whether or not, in straight pair comparison, ties should be allowed. The answer is: it all depends. If we are concerned with testing discriminability, or if the null hypothesis that items A and B are indistinguishable' is being checked, ties are best avoided. The question put to the subject should be 'Which. .. .? Answer one or the other even if you feel you are guessing', rather than, \\*Which, if either. .. .?' On the other hand, when preferences in a population are sought, the answer 'Neither’ should be allowed. Some further remarks on this topic follow; they mostly stem from a recent investigation (Gridge$\\mathtt{m a n^{12}}.$ 一 <PARAGRAPH_SEPARATOR> If the subject is judging the relative intensity of two taste stimuli, the test is purely of discrimination, for we may assume that, having detected the difference, he will not mistake its direction. When, on occasion, he does not detect the difference, he has to guess. Assume an empirical probability $\\pmb{\\phi}_{s}$ of true discriminability. Then the left-hand side equation (l) can be written: <PARAGRAPH_SEPARATOR> $$ \\displaystyle{\\mathbf{P}(i>j)=(1\\pm\\mathbf{\\hat{p}_{s}})/2} $$ <PARAGRAPH_SEPARATOR> For a specific pair in which $C_{i}{>}C_{j},$ we have <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{P_{i}=\\P(i{>}j)=(1+\\pmb{\\rho}_{s})/2\\biggr\\}}\\ {P_{j}=\\P(j{>}i)=(1-\\pmb{\\rho}_{s})/2\\biggr\\}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Now if ties are allowed, the subject will not invariably record ‘no apparent difference' when he fails to discriminate; sometimes (and not necessarily consciously) he will guess. We therefore. postulate a probability $\\pmb{\\phi}_{\\pmb{g}}$ of his guessing (and, of course, about half of his guesses will be correct). Then, writing $P_{\\mathbf{0}}$ for the probability of a tie, we have: <PARAGRAPH_SEPARATOR> $$ \\left.\\begin{array}{l}{P_{i}=\\phi_{s}+\\slash p_{s}(1-\\slash p_{s})/2}\\ {P_{j}=\\slash p_{g}(1-\\slash p_{s})/2}\\ {P_{0}=1-P_{i}-P_{j}=(1-\\slash p_{g})(1-\\slash p_{s})}\\end{array}\\right\\} $$ <PARAGRAPH_SEPARATOR> Equations (8) have as a special case ${p}_{g}=1$ , which means that guessing is obligatory, and which is represented by equations (7). In general, theory shows that ties are best banned in discrimination work provided that $\\phi_{s}$ itself (when non-zero) is independent of the design. But the design is rarely without effect on $\\phi_{s}$ .This is not surprising. Normally, the simpler the design, and the more straightforward the subject's task, the larger $\\pmb{\\mathscr{p}}_{s}$ will be—-a circumstance that is apt to offset the theoretical advantages of the no-tie system. <PARAGRAPH_SEPARATOR> # Ties in Preference Tasting <PARAGRAPH_SEPARATOR> When preference is at issue, three classes of subject can be distinguished: <PARAGRAPH_SEPARATOR> Those (a proportion $\\pmb{\\mathscr{p}}_{i,\\pmb{\\mathscr{N}}}$ who prefer $\\textit{i}$ Those (a proportion $\\scriptstyle{p_{j}}.$ who prefer $j$ The rest (who have no preference) <PARAGRAPH_SEPARATOR> But not all of the third (the “indifferent') class will say they have no preference; some of them, some of the time, will guess, and we assume that there is a probability $\\scriptstyle{p_{g}}$ that any member of this class, on a particular occasion, will in fact state a pseudopreference. We must further suppose that such decisions will not be influenced by extraneous circumstances or characteristics of the compared items, so that there is an equal chance of choosing $\\textit{i}$ or $j$ in the absence of preference. To take an exaggerated example: it would be methodologically wrong to put chocolates $\\mathbf{\\nabla}_{i}\\mathbf{\\cdot}_{i}$ in a handsome white pack embellished with a pretty girl, and to put $\\mathfrak{f}^{}$ in an ill-made red pack covered with a hammer-and-sickle motif. This does not however imply that all extraneous differences should be equalised; it may be best to.retain them and “balance them out'. Thus we might put half the $\\mathbf{\\nabla}_{i}^{\\star}$ chocolates in white packs and the other half in red packs, and ditto with the $\\mathfrak{f}^{}$ chocolates; then the two main pairings (white: red and red: white) or perhaps all four possible pairings (white: red and red: white, red: red, and white: white) should be equally distributed among trials. Then, ties being permitted, the composition of the three kinds of answer will be: <PARAGRAPH_SEPARATOR> And if ties are prohibited (which means that $\\scriptstyle{p_{g}}$ is constrained to unity), wehave <PARAGRAPH_SEPARATOR> $$ \\left.\\begin{array}{r l}&{P_{i}=\\rlap/p_{i}+(1-p_{i}-p_{j})p_{g}/2}\\ &{P_{j}=p_{j}+(1-p_{i}-p_{j})p_{g}/2}\\ &{P_{\\mathbf{0}}=1-P_{i}-P_{j}=(1-p_{i}-p_{j})(1-p_{g})}\\end{array}\\right\\} $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{P_{i}=(1+{p_{i}}-{p_{j}})/2}\\ {P_{j}=1-P_{i}=(1-{p_{i}}+{p_{j}})/2}\\end{array} $$ <PARAGRAPH_SEPARATOR> Now, often and perhaps mostly, it is helpful not only to test $\\mathrm{H}_{0}\\left({\\pmb{\\mathit{p}}}_{i}={\\pmb{\\mathit{p}}}_{j}\\right)$ but to be able to estimate $\\scriptstyle{\\boldsymbol{p}}_{i}$ and $\\phi_{j}$ .In other words the size of the 'indifferent group, a proportion $1-\\phi_{i}-\\phi_{j}$ of the whole, is a matter of consequence. (If $\\mathbf{\\nabla}_{i}\\mathbf{\\cdot}_{}$ and $\\mathfrak{f}^{}$ are experimental products a decision on whether either or both should be marketed would turn on such information.) Therefore ties should be allowed in answers. Even so, it does not necessarily follow how estimates of the class sizes should be arrived at. We present the choice of chocolates to a sample of $\\mathcal{N}$ subjects, and we observe that a number $r_{i}$ say that they prefer $\\mathbf{\\epsilon}_{i}\\mathbf{\\cdot}\\mathbf{\\sigma} $ , and a number $r_{j}$ that they prefer $\\mathfrak{f}^{}$ , so that $\\dot{r}_{i}/\\dot{\\mathcal{N}}=\\hat{P}_{i},$ and $r_{j}^{\\mathrm{~\\scriptsize~1~}}/N=\\hat{P}_{j}$ . But all we have here are two independent equations for thrce parameters, so that no solution is possible. It is to the credit of G. E. Ferris8 to have couched this problem in clear statistical terms, to have seen that when replication is introduced a solution is possible, and so to have put forward the $^{\\bullet}k$ visit method'. <PARAGRAPH_SEPARATOR> # The $k$ -visit Method <PARAGRAPH_SEPARATOR> Let each subject state his preference, in each pair comparison, on $k$ $(k>1)$ different occasions. (Incidentally, if there are extraneous differences, such as varied packs, they can now be balanced out within each subject's $k$ trials.) Any complete ( $k\\cdot$ -fold) result falls into one of $3^{k}$ categories, whose expectations can readily be found from the fundamental assumptions and equations. Estimation of the three parameters then becomes possible (although $\\scriptstyle{\\pmb{\\mathscr{p}}}_{\\pmb{\\mathscr{g}}}$ is of no practical interest). For the simplest case, $k=2$ , Ferris gives the maximum-likelihood solutions and their standard errors."
  },
  {
    "qid": "statistic-posneg-1470-0-1",
    "gold_answer": "TRUE. A larger cutoff $K$ increases the threshold for the difference in deviance between a tree and its pruned subtree. This means that more terminal nodes will be pruned, leading to a smaller estimated context tree and thus a smaller model dimension. This is consistent with the economic intuition that a higher threshold for pruning results in a more parsimonious model.",
    "question": "Does a larger cutoff value $K$ lead to more aggressive pruning of the context tree, resulting in a smaller model dimension?",
    "question_context": "Variable Length Markov Chains: Pruning and Cutoff Determination\n\nFitting a VLMC can be done with a version of the tree structured context algorithm (Rissanen 1983) which is detailed in Section 6.1. The variable length memory is usually represented with an estimated context tree. Thus, we are fitting tree-structured models where every terminal (and some internal) node represents a state in the Markov chain and is equipped with corresponding transition probabilities. Our context algorithm grows a large tree and prunes it back subsequently. The pruning part requires specification of a tuning parameter, the so-called cutoff. The cutoff $K$ is a threshold value when comparing a tree with its subtree by pruning away one terminal node; the comparison is made with respect to the difference of deviance from the two trees. A large cutoff has a stronger tendency for pruning and yields smaller estimated context trees, that is, a smaller dimension of the model.<PARAGRAPH_SEPARATOR>The value of the cutoff $K$ has a heuristic meaning on the scale of $\\chi^{2}$ -quantiles. As a threshold for differences of deviances, an asymptotic $\textstyle{\\frac{1}{2}}\\chi_{\nu}^{2}$ distributionwith $\nu=\\mathrm{card}(\\mathcal{X})-1$ is associated to every individual decision about pruning a terminal node (under the nullhypothesis of equal models, i.e., pruning). Therefore, we often specify the cutoff $K$ on the scale of percentages (quantiles),<PARAGRAPH_SEPARATOR>$$K=K(\\alpha)=\\frac{1}{2}\\chi_{\nu,\\alpha}^{2}=\\frac{1}{2}\\mathrm{qchisq}(1-\\alpha,\nu),\nu=\\mathrm{card}(\\chi)-1.$$<PARAGRAPH_SEPARATOR>For the DNA case, $\nu=3$ , and hence $K(\\alpha)=3.91$ and 5.67 for $\\alpha=5\\%$ or $1\\%$ , respectively. Note that we use the notation ${\\hat{\tau}}\\equiv\tau_{\\hat{c}}$ for the fitted context tree.<PARAGRAPH_SEPARATOR># 3.1 IMPLEMENTATION IN R<PARAGRAPH_SEPARATOR>We provide an R package VLMC, also available from the Comprehensive R Archive Network (CRAN 1997 ff.). After starting R, we load the package into the running R session, load the BNRF1 gene data (which comes with the VLMC package as well), and look at its first 50 values:"
  },
  {
    "qid": "statistic-posneg-259-0-2",
    "gold_answer": "FALSE. The condition $h^{\\prime\\prime}(\\xi)=0$ and $\\rho h^{\\prime}(\\xi)\\leqslant0$ is necessary but not sufficient on its own to conclude that the distribution is ${\\mathrm{TP}}_{2}$. The theorem states that the distribution is ${\\mathrm{TP}}_{2}$ if and only if for each value of $\\xi>0$, the conditions $h^{\\prime\\prime}(\\xi)=0$ and $\\rho h^{\\prime}(\\xi)\\leqslant0$ hold. However, other conditions involving $h^{\\prime\\prime}(\\xi)$ and $K_{\\rho}(t)$ must also be considered, as detailed in the proof.",
    "question": "In Theorem 3.1, is the condition $h^{\\prime\\prime}(\\xi)=0$ and $\\rho h^{\\prime}(\\xi)\\leqslant0$ sufficient to conclude that the bivariate elliptically symmetric distribution is ${\\mathrm{TP}}_{2}$?",
    "question_context": "Positive Dependence Properties of Elliptically Symmetric Distributions\n\nTHEOREM 2.2  Let $\\pmb{X}=(X_{1},...,X_{p})^{\\prime}$ have  joint  p.d.f. $f({\\pmb x})=g({\\pmb x}^{\\prime}{\\pmb x}),$ $-\\infty<\\pmb{x}<\\infty$ ,， where $g(u)>0$ for all $u\\geqslant0$ and is twice dijfferentiable. Define $W_{i}=|X_{i}|$ $i=1,...,p$ .Then $h(w)$ , the joint $p.d.f.$ of $W_{1},...,W_{p}$ ,is $T P_{2}$ in pairs $\\pmb{j}\\pmb{f}$ and only $i f,$ ln $g(u)$ is convex for $\\pmb{u}\\geqslant0$"
  },
  {
    "qid": "statistic-posneg-168-1-2",
    "gold_answer": "TRUE. The variance formula shows no dependence on the kernel order $k$, only on the bandwidth $b$, sample size $n$, design density $f_X(x)$, and conditional variance $\\sigma^2(x)$. This is because the variance is primarily determined by the effective number of points in the local neighborhood (controlled by $b$), not by the kernel's moment properties. The $o_p([nb]^{-1})$ term also confirms this order-independence. Thus, kernel order affects bias but not variance in this framework.",
    "question": "Is the variance term $\\operatorname{var}[{\\hat{m}}_{k}(x)|\\mathbb{X}]={\\frac{\\sigma^{2}(x)V_{0}}{n b f_{X}(x)}}+o_{p}([n b]^{-1})$ independent of the kernel order $k$, as claimed in the text?",
    "question_context": "Kernel Regression Bias and Variance Analysis with Higher-Order Kernels\n\n(A1) $x\\in$ interior of $\\operatorname{supp}(f_{X})$ ; $f_{X}(x)>0$ ;   \n(A2) $K(\\cdot)\\in\\mathrm{Lip}[-1,1],K(\\cdot)$ is of order $(0,k)$ as defined in (3), $k=2^{j}$ , for some $j\\geq1$ .   \n(A3) $\\begin{array}{c}{{V_{l}=\\displaystyle\\int K^{2}(u)u^{l}d u<\\infty,B_{l}=\\displaystyle\\int K(u)u^{l}d u<\\infty~\\mathrm{for}~l=1...,8;}}\\ {{X\\sim f_{X}(x)\\in\\mathrm{Lip}({\\bf R}),Y|X\\sim f_{Y|X}(y|x),f_{X Y}(x,y)=f_{Y|X}(y|x)f_{X}(x),}}\\end{array}$ $f_{X}^{(k-2)}(x)\\neq0,u_{\\ell}(x)=\\int y^{\\ell}f_{X Y}(x,y)d\\mu(y)<\\infty\\mathrm{~for~}l=1,...,4$ where $\\mu(y)$ is the measure appropriate for the binary or continuous response case, and $\\sigma^{2}(x)(=\\operatorname{var}(Y|X=x))$ is bounded and continuous on $\\operatorname{supp}(f_{X})$ ;   \n(A4) $b\\to0$ and $n b\\rightarrow\\infty$ as $n\\to\\infty$ ;   \n(A5) $m(x),f_{X}(x),u_{i}(x),i=1,...,4,\\in{\\bf C}^{k}({\\bf R}).m(\\cdot)$ and $m^{(2)}(\\cdot)$ are bounded on $\\operatorname{supp}(f_{X})$ :\n<PARAGRAPH_SEPARATOR>\nThe conditional bias and variance for $\\widehat{\\pmb{m}}_{k}(\\boldsymbol{x},\\boldsymbol{b})$ are given below. The bias portion extends results previously obtained for kernels of  obrder $(0,2)$ in Ruppert & Wand (1994) and is proved in the appendix. The variance term does not depend on the order of the kernel and is included for completeness.\n<PARAGRAPH_SEPARATOR>\nIn the following, let\n<PARAGRAPH_SEPARATOR>\n$$\n\\begin{array}{l}{{\\displaystyle\\beta(x,k)=B_{k}\\left[\\frac{1}{f_{X}(x)}\\sum_{l=2}^{k}\\frac{m^{(l)}(x)f_{X}^{(k-l)}(x)}{l!~(k-l)!}\\right.}}\\ {{\\displaystyle\\left.-\\frac{f_{X}^{(k-1)}(x)}{f_{X}(x)f_{X}^{(k-2)}(x)(k-1)}\\sum_{l=2}^{k-1}\\frac{m^{(l)}(x)f_{X}^{(k-1-l)}(x)}{l!~(k-1-l)!}\\right].}}\\end{array}\n$$\n<PARAGRAPH_SEPARATOR>\nLemma 1 Under assumptions (A1)–(A5) of this section,\n<PARAGRAPH_SEPARATOR>\n$$\nE[\\widehat{m}_{k}(x,b)|\\mathbb{X}]-m(x)=b^{k}\\beta(x,k)+o_{p}(b^{k})\n$$\n<PARAGRAPH_SEPARATOR>\nand\n<PARAGRAPH_SEPARATOR>\n$$\n\\operatorname{var}[{\\hat{m}}_{k}(x)|\\mathbb{X}]={\\frac{\\sigma^{2}(x)V_{0}}{n b f_{X}(x)}}+o_{p}([n b]^{-1}),\n$$\n<PARAGRAPH_SEPARATOR>\nwhere $\\mathbb{X}=(X_{1},....,X_{n})$ .\n<PARAGRAPH_SEPARATOR>\nThe proof is given in the appendix. Note that the bias term increases in complexity as the order of the kernel increases. For example, when $k=2$ , $\\beta(x,2)={\\textstyle{\\frac{1}{2}}}m^{(2)}(x)B_{2}$ and for $k=4$ , the bias depends on $m^{(2)}(x),m^{(3)}(x)$ and $m^{(4)}(x)$ . The leading terms of the conditional MSE are\n<PARAGRAPH_SEPARATOR>\n$$\n\\mathrm{AMSE}(x,b)=b^{2k}[\\beta(x,k)]^{2}+\\frac{\\sigma^{2}(x)V_{0}}{n b f_{X}(x)},\n$$\n<PARAGRAPH_SEPARATOR>\n$\\copyright$ Board of the Foundation of the Scandinavian Journal of Statistics 2003.\n<PARAGRAPH_SEPARATOR>\nand minimizing (9) with respect to the bandwidth $^b$ gives an optimal bandwidth at each point $x$ of\n<PARAGRAPH_SEPARATOR>\n$$\nb^{*}(x)=\\left(\\frac{\\sigma^{2}(x)V_{0}}{\\beta(x,k)^{2}f_{X}(x)2k}\\right)^{1/(2k+1)}n^{-1/(2k+1)}.\n$$\n<PARAGRAPH_SEPARATOR>\nSubstituting (10) into (9), we obtain the optimal MSE:\n<PARAGRAPH_SEPARATOR>\n$$\n\\begin{array}{r l}&{\\mathrm{AMSE}(x,b^{*}(x))=n^{-2k/(2k+1)}\\left(\\cfrac{\\sigma^{2}(x)V_{0}}{\\beta(x,k)^{2}f_{X}(x)2k}\\right)^{2k/(2k+1)}\\beta(x,k)^{2}}\\ &{\\qquad+\\left[\\cfrac{\\sigma^{2}(x)V_{0}}{n f_{X}(x)}\\right]^{2k/(2k+1)}[\\beta(x,k)^{2}2k]^{1/(2k+1)}.}\\end{array}\n$$\n<PARAGRAPH_SEPARATOR>\nThe rate of convergence for the optimal MSE is faster with higher order kernels. The quantity $b^{*}(x)$ in (10) cannot be used as a bandwidth because it contains unknown quantities, but it suggests an optimal rate for the bandwidth $b\\sim n^{-1/(2k+1)}$ and provides a target quantity for consistent data-dependent bandwidth procedures."
  },
  {
    "qid": "statistic-posneg-1403-4-2",
    "gold_answer": "FALSE. Remark 3 explicitly states that the Spike MUSIC estimator is efficient (achieves Cramér–Rao bound) at high SNR when the noise matrix is standard Gaussian. The result is specifically tied to Gaussian noise assumptions.",
    "question": "The Spike MUSIC estimator achieves the Cramér–Rao bound at high SNR ($\\omega_{i}^{2}\\to\\infty$) only when the noise matrix is non-Gaussian.",
    "question_context": "Spike MUSIC Estimator Variance Analysis\n\nRemark 2. If $\\sqrt{n}X$ is standard Gaussian and if $c_{n}=N/n$ satisfies ${\\sqrt{n}}(c_{n}-c)\\to0$ , then Assumption A8 is satisfied. Indeed, call $m_{n}(z)$ the Stieltjes Transform of the Marčenko–Pastur distribution, i.e., the analytic continuation of (7), when $c$ is replaced with $c_{n}$ , and let $\\pi_{n}$ be the associated probability measure. For $z\\in\\mathbb{C}-[\\lambda_{-},\\lambda_{+}],$ function $f(x)=(x-z)^{-1}$ is analytic outside the support of $\\pi_{n}$ for $n$ large, and [2, Theorem 1.1] can be applied to show that $\\sqrt{n}(\\alpha_{n}(z)-m_{n}(z))\\stackrel{\\mathcal{P}}{\\rightarrow}0$ When $\\sqrt{n}(c_{n}-c)\\to\\bar{0}$ , it is furthermore clear that ${\\sqrt{n}}(m_{n}(z)-{\\dot{m}}(z))\\to{\\bar{0}}$ . <PARAGRAPH_SEPARATOR> The main result of this section is the following. <PARAGRAPH_SEPARATOR> Theorem 4. Let Assumptions A1–A8 hold true. Then the estimates $\\hat{\\varphi}_{k,n}$ satisfy <PARAGRAPH_SEPARATOR> $$ \\boldsymbol{n}^{3/2}\\left[\\begin{array}{c}{\\hat{\\varphi}_{1,n}-\\varphi_{1}}\\ {\\vdots}\\ {\\hat{\\varphi}_{r,n}-\\varphi_{r}}\\end{array}\\right]\\xrightarrow{\\mathcal{D}}\\mathcal{N}\\left(0,\\left[\\begin{array}{c c c}{\\sigma_{1}^{2}I_{j_{1}}}&&\\ &{\\ddots}&\\ &&{\\sigma_{s}^{2}I_{j_{s}}}\\end{array}\\right]\\right) $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ \\sigma_{i}^{2}=\\frac{6}{c^{2}D^{2}}\\left(\\frac{m^{\\prime}(\\rho_{i})-m(\\rho_{i})^{2}}{c m(\\rho_{i})^{2}}+\\omega_{i}^{2}\\left(m(\\rho_{i})+\\rho_{i}m^{\\prime}(\\rho_{i})\\right)\\right),\\quad1\\leq i\\leq s. $$ <PARAGRAPH_SEPARATOR> When $\\sqrt{n}X$ is standard Gaussian, plugging the r.h.s. of (7) into this expression leads after some derivations to the following corollary. <PARAGRAPH_SEPARATOR> Corollary 2. If $\\sqrt{n}X$ is standard Gaussian and if ${\\sqrt{n}}(c_{n}-c)\\to0$ , the convergence (15) holds true with <PARAGRAPH_SEPARATOR> $$ \\sigma_{i}^{2}=\\frac{6}{c^{2}D^{2}}\\frac{\\omega_{i}^{2}+1}{\\omega_{i}^{4}-c}. $$ <PARAGRAPH_SEPARATOR> This corollary calls for some comments. <PARAGRAPH_SEPARATOR> Remark 3 (Efficiency at High SNR). Recalling that $\\omega_{i}^{2}>{\\sqrt{c}}$ is the condition for the existence of a corresponding isolated eigenvalue (Corollary 1), we observe that the estimator variance for $\\varphi_{k}$ goes to infinity as the corresponding $\\omega_{i}^{2}$ decreases to $\\sqrt{c}$ . At the other extreme, this variance behaves like $6c^{-2}D^{-2}\\omega_{i}^{-2}$ as $\\omega_{i}^{2}\\to\\infty$ . It is useful to notice that this asymptotic variance coincides with the Cramér–Rao bound for estimating $\\varphi_{k}$ [28]. In other words, the Spike MUSIC estimator is efficient at high SNR when the noise matrix is standard Gaussian."
  },
  {
    "qid": "statistic-posneg-945-3-0",
    "gold_answer": "TRUE. The context explicitly references Keribin (2000) as showing that BIC consistently estimates the number of mixture components under certain regularity conditions. Additionally, Leroux (1992) is cited for showing that BIC does not underestimate the number of components, and Lopes and West (2004) provide empirical support for BIC's performance in model selection, reinforcing the claim's validity within the specified conditions.",
    "question": "Is the statement 'The BIC consistently estimates the number of mixture components under certain regularity conditions' supported by the provided context?",
    "question_context": "Bayesian Information Criterion (BIC) in Model Selection\n\nNote that Figs. 1–3 all arise from real analyzes using the AECM algorithm detailed herein. Furthermore, the convergence criteria in Eq. (4) is necessarily at least as strict as the lack of progress criteria given in Eq. (5). This follows from the fact that $0\\leq a^{(k)}<1$ in a neighborhood of the maximum value and so $1/(1-a^{(k)})\\geq1.$ Hence, $l_{\\infty}^{(k+1)}-l^{(k)}\\geq l^{(k+1)}-l^{(k)}$ in a neighborhood of the maximum. <PARAGRAPH_SEPARATOR> # 4.4. Model selection <PARAGRAPH_SEPARATOR> For the MCLUST and PGMM techniques, the Bayesian information criterion (BIC, Schwartz, 1978) is used to select the ‘best’ member of the family for given data. The BIC can also be used to compare PGMM and MCLUST models. For a model with parameters $\\Phi$ , the BIC is given by <PARAGRAPH_SEPARATOR> $$ \\mathrm{BIC}=2l(\\mathbf{x},\\hat{\\boldsymbol{\\Phi}})-\\nu\\log n, $$ <PARAGRAPH_SEPARATOR> where $l(\\mathbf{x},\\hat{\\Phi})$ is the maximized log-likelihood, $\\hat{\\Phi}$ is the maximum likelihood estimate of $\\Phi,\\nu$ is the number of free parameters in the model and $n$ is the number of observations. <PARAGRAPH_SEPARATOR> Leroux (1992) shows that the BIC does not underestimate the number of components in a mixture model and Keribin (2000) shows that BIC consistently estimates the number of mixture components under certain regularity conditions. Lopes and West (2004) provide a simulation study where they show that the BIC has excellent performance in selecting the number of factors in a factor analysis model, even when the sample size is small. <PARAGRAPH_SEPARATOR> # 5. Parallel implementation"
  },
  {
    "qid": "statistic-posneg-1416-0-1",
    "gold_answer": "FALSE. The paper clearly states that the SRRR estimator β̂ₘ(k) is biased for k > 0. The bias is given by Bias(β̂ₘ(k)) = -kMₖβ, where Mₖ = (X'V⁻¹X + R'W⁻¹R + kIₚ)⁻¹. This bias arises due to the introduction of the ridge parameter k, which shrinks the estimates towards zero. The bias is a trade-off for reduced variance, which is a common characteristic of ridge regression estimators.",
    "question": "The SRRR estimator β̂ₘ(k) is unbiased for all values of k > 0. Is this statement correct?",
    "question_context": "Stochastic Restricted Ridge Regression (SRRR) Estimator\n\nThe paper discusses the problem of multicollinearity in multiple regression models and introduces the Stochastic Restricted Ridge Regression (SRRR) estimator. This estimator combines the ideas of ridge regression and stochastic linear restrictions to address multicollinearity. The model is given by y = Xβ + ε, where y is the dependent variable, X is the matrix of explanatory variables, β is the vector of regression coefficients, and ε is the error term with E(ε) = 0 and Var(ε) = σ²V. The SRRR estimator is derived as β̂ₘ(k) = (X'V⁻¹X + R'W⁻¹R + kIₚ)⁻¹(X'V⁻¹y + R'W⁻¹r), where k > 0 is the ridge parameter, R and r are stochastic restrictions, and W is the variance matrix of the restrictions."
  },
  {
    "qid": "statistic-posneg-1151-0-0",
    "gold_answer": "FALSE. While the condition helps in distinguishing significant predictors from insignificant ones, it is not strictly necessary for the model's identification. The model can still perform variable selection and estimation without this condition, but the condition simplifies the interpretation by clearly separating the predictors into significant and insignificant groups based on $p_0$.",
    "question": "Is the condition $\\beta_{0j}(t)\\neq0$ for $j\\le p_0$ and $\\beta_{0j}(t)\\equiv0$ for $j>p_0$ necessary for the model to identify significant and insignificant predictors?",
    "question_context": "Robust Shrinkage Estimation and Selection for Functional Regression\n\nThe random error $\\epsilon$ is assumed to be independent of the covariate Xj(t), $1\\le j\\le p$ . Moreover, assume that $\\beta_{0j}(t)\\neq0$ , for $j\\le p_0$ and $\\beta_{0j}(t)\\equiv0$ , for $j>p_0$ with $1\\leq p_0\\leq p$ . Then the true model has $p_0$ significant and $p-p_0$ insignificant predictors. Let $\\{(X_{i1}(t),\\ldots,X_{i p}(t),Y_i)$ , $i=1,\\ldots,n\\}$ be the i.i.d. copies of $(\\mathbf{X}_1(t),\\ldots,\\mathbf{X}_p(t),Y)$ and $(x_{i1}(t),\\dots,x_{i p}(t),y_i)$ be the observation of $(X_{i1}(t),\\dots,X_{i p}(t),Y_i)$ . <PARAGRAPH_SEPARATOR> # 2.1. Functional principal component analysis <PARAGRAPH_SEPARATOR> Assume that $E\\mathbf{X}_j(t)=0,1\\leq j\\leq p $ . Define the covariance function of $\\mathbf{\\boldsymbol{X}}_j(t)$ by $K_j(s,t)=\\mathrm{cov}\\{\\mathbf{X}_j(s),\\mathbf{X}_j(t)\\},j=1,\\ldots,p.$ Then by Mercer’s Theorem, we can obtain the spectral decomposition <PARAGRAPH_SEPARATOR> $$ K_j(s,t)=\\sum_{k=1}^{\\infty}\\kappa_{j k}\\phi_{j k}(s)\\phi_{j k}(t) $$ <PARAGRAPH_SEPARATOR> where $\\begin{array}{l}{{\\kappa_{j1}>\\kappa_{j2}>\\cdots>0}}\\end{array}$ are eigenvalues of the operator associated with $K_j(s,t)$ , and $\\phi_{j k}$ ’s are the corresponding eigenfunctions. Then empirical covariance function <PARAGRAPH_SEPARATOR> $$ \\hat{K}_j(s,t)=\\frac{1}{n}\\sum_{i=1}^{n}x_{i j}(s)x_{i j}(t)-\\bar{x}_j(s)\\bar{x}_j(t) $$ <PARAGRAPH_SEPARATOR> can be used to estimate $K_j(s,t)$ , where $\\begin{array}{r}{\\bar{x}_j(s)=\\sum_{i=1}^{n}x_{i j}(s)/n,\\bar{x}_j(t)=\\sum_{i=1}^{n}x_{i j}(t)/n}\\end{array}$ . Also we can obtain that <PARAGRAPH_SEPARATOR> $$ \\hat{K}_j(s,t)=\\sum_{k=1}^{\\infty}\\hat{\\kappa}_{j k}\\hat{\\phi}_{j k}(s)\\hat{\\phi}_{j k}(t), $$ <PARAGRAPH_SEPARATOR> where $\\hat{\\kappa}_{j1}\\geq\\hat{\\kappa}_{j2}\\geq\\dots\\underline{{{\\vert\\kappa\\vert}}}_{\\vert\\vert}$ . Usually $\\hat{\\int}\\hat{\\phi}_{j k}\\phi_{j k}>0$ is assumed to get rid of uncertainty of signs. Since $\\{\\hat{\\phi}_{j k},k=1,2,...\\}$ is a set of bases spanned space by $\\{x_{i j}(t),1\\le i\\le n\\}$ , we see that at most $n$ eigenvalues among $\\{\\hat{\\kappa}_{j k}$ , $k=1,2,\\ldots\\}$ are strictly positive. <PARAGRAPH_SEPARATOR> According to Karhunen–Loéve expansion, $\\begin{array}{r}{{\\bf X}_j(t)=\\sum_{k=1}^{\\infty}\\xi_{j k}\\phi_{j k}(t)}\\end{array}$ , where $\\xi_{j k}$ is the principal component scores and the sample $X_{i j}(t)$ can be expressed as <PARAGRAPH_SEPARATOR> $$ X_{i j}(t)=\\sum_{k=1}^{\\infty}\\xi_{i,j k}\\phi_{j k}(t), $$ <PARAGRAPH_SEPARATOR> where $\\begin{array}{r}{\\xi_{i,j k}=\\int X_{i j}\\phi_{j k}}\\end{array}$ , $i=1,\\ldots,n$ are i.i.d. copy of $\\xi_{j k}$ . It can be verified that $E\\xi_{i,j k}=0$ , $E\\xi_{i,j k}^2=\\kappa_{j k}$ and $E\\xi_{i,j k_1}\\xi_{i,j k_2}=0,k_1\\neq$ $k_2$ . From (6) we also have covariance operator expansion <PARAGRAPH_SEPARATOR> $$ \\mathrm{Cov}\\{\\mathbf{X}_{j_1}(s),\\mathbf{X}_{j_2}(t)\\}=\\sum_{k_1,k_2=1}^{\\infty}\\kappa_{k_1,k_2}^{j_1,j_2}\\phi_{j_1k_1}(s)\\phi_{j_2k_2}(t), $$ <PARAGRAPH_SEPARATOR> which measures the correlation between different functional predictors, where $\\kappa_{k_1,k_2}^{j_1,j_2}=E\\xi_{i,j_1k_1}\\xi_{i,j_2k_2}$ . Based on the orthonormal basis $\\{\\phi_{j k}(t),k=1,2,\\ldots\\},\\beta_{0j}(t)$ can be expressed as <PARAGRAPH_SEPARATOR> $$ \\beta_{0j}(t)=\\sum_{k=1}^{\\infty}b_{j k}\\phi_{j k}(t). $$ <PARAGRAPH_SEPARATOR> Then for i.i.d. observations $(Y_i,X_{i1}(t),\\dots,X_{i p}(t)),i=1,\\dots,n,$ from model (2), we have <PARAGRAPH_SEPARATOR> $$ Y_i=\\sum_{j=1}^{p}\\sum_{k=1}^{\\infty}\\xi_{i,j k}b_{j k}+\\epsilon_i,\\quad1\\leq i\\leq n. $$ <PARAGRAPH_SEPARATOR> With samples estimate $\\begin{array}{r}{\\hat{\\xi}_{i,j k}=\\int x_{i j}\\hat{\\phi}_{j k}}\\end{array}$ of $\\xi_{i,j k}$ and truncating parameters $L_j$ $,1\\le j\\le p$ , we have <PARAGRAPH_SEPARATOR> $$ Y_i\\approx\\sum_{j=1}^{p}\\sum_{k=1}^{L_j}\\hat{\\xi}_{i,j k}b_{j k},\\quad1\\le i\\le n, $$ <PARAGRAPH_SEPARATOR> where truncating parameter $L_j$ means that only $L_j$ principal components are utilized. We denote the true parameter after truncation by $B_0=(\\ensuremath{\\mathbf{b}}_1^T,\\dots,\\ensuremath{\\mathbf{b}}_p^T)$ with $\\mathbf{b}_j=(b_{j1},\\dots,b_{j L_j})^T$ , $1\\le j\\le p$ . <PARAGRAPH_SEPARATOR> Let $V=(\\pmb{\\mathrm{v}}_1^T,\\dots,\\pmb{\\mathrm{v}}_p^T)$ with $\\mathbf{v}_j=(v_{j1},\\ldots,v_{j L_j})^T$ and $\\|\\mathbf{v}_j\\|_\\infty=\\operatorname*{max}\\{|v_{j1}|,\\dots,|v_{j L_j}|\\},$ the $L_\\infty$ norm of $\\mathbf{v}_j$ . Since robustness is a major concern, we consider the LAD loss function $\\rho(t)=|t|$ here, motivated by the LAD-Lasso (Wang et al., 2007). Then the object function can be formulated as <PARAGRAPH_SEPARATOR> $$ Q(V)=\\sum_{j=1}^{n}\\rho\\left(Y_i-\\sum_{j=1}^{p}\\sum_{k=1}^{L_j}\\hat{\\xi}_{i,j k}v_{j k}\\right)+n\\sum_{j=1}^{p}P_{\\lambda_j}(\\lVert\\mathbf{v}_j\\rVert_\\infty), $$ <PARAGRAPH_SEPARATOR> where $\\lambda_j$ is the tuning parameter, $P_\\lambda(t)=\\lambda t$ being the penalty term. Note that the penalty term is the combination of $L_1$ norm and sup-norm $\\|\\cdot\\|_\\infty$ . We use sup-norm here instead of group lasso penalty, due to the following two considerations. First, the combination of sup-norm and $L_1$ encourages the group selection in ${\\bf{v}}_j^{\\prime}{\\boldsymbol{s}}$ (Zhao et al., 2009). Second, sup-norm along with the LAD loss leads to efficient linear program to solve (9) as shown in Section 2.3, see also Bang and Jhun (2012). <PARAGRAPH_SEPARATOR> The estimate of $B_0$ can be defined as $\\hat{B}=(\\hat{\\ensuremath{\\mathbf{b}}}_1^T,\\dots,\\hat{\\ensuremath{\\mathbf{b}}}_p^T)^T$ and $\\hat{\\mathbf{b}}_j=(\\hat{b}_{j1},\\dots,\\hat{b}_{j L_j})^T$ , such that <PARAGRAPH_SEPARATOR> $$ {\\hat{B}}=\\operatorname{argmin}_V Q(V). $$ <PARAGRAPH_SEPARATOR> The proposed method above can be called functional LAD Lasso (FLL). <PARAGRAPH_SEPARATOR> Given $\\hat{\\xi}_{i,j k}$ ’s and $L_j^{\\prime}{\\mathsf{S}},$ , the optimization problem in (9) is similar to the quantile estimate of Bang and Jhun (2012) with $\\tau=1/2$ for linear model. Therefore, both methods share the similarity in computing as shown in Section 2.3. The key difference is on the theoretical analysis aspect. Here $\\hat{\\xi}_{i,j k},\\cdot,\\boldsymbol{L_j}^{\\prime}\\boldsymbol{s}$ are estimated from data, while Bang and Jhun (2012) considered the linear model with predictors fully observed, which leads to significant difference in both the assumptions and the conclusions. In addition, Bang and Jhun (2012) only consider only the case of fixed $p$ , while we consider both fixed and diverging $p$ for FLL. Both FLL in (9) and LAD-Lasso of Wang et al. (2007) adopt the LAD loss function, but FLL has the natural group structure while LAD-Loss use $L_1$ penalty. <PARAGRAPH_SEPARATOR> Comparing FLL with the method of Lian (2013), both methods adopt the FPC basis. Lian (2013) used the least square loss that is not robust, while FLL uses the LAD loss that leads to robust estimate. In addition, Lian (2013) used group-SCAD penalty, while FLL use the sup-norm penalty, which leads to efficient algorithm, see Section 2.3. Finally we establish the theoretical results on FLL for both fixed and divergent $p$ , while Lian (2013) considered only the case of fixed $p$ . In fact, the result of Lian (2013) is a special case of our results, as shown in Section 3."
  },
  {
    "qid": "statistic-posneg-2121-11-1",
    "gold_answer": "TRUE. Preferential sampling based on the variability of Y(x) is likely to introduce more bias because it systematically samples areas with higher variability, which can distort the relationship between Y* and Z* in the health effect model. Non-preferential uniform sampling, on the other hand, provides a more representative sample of the entire domain, leading to less biased estimates.",
    "question": "Is preferential sampling based on the variability of Y(x) (gradient (Y)) expected to introduce more bias in the health effect estimation compared to non-preferential uniform sampling?",
    "question_context": "Geostatistical Inference Under Preferential Sampling\n\nIn air pollution epidemiology, we use geostatistical methods to predict exposures at subject locations based on monitoring data (Jerrett et al., 2005; Szpiro et al., 2009a). This introduces measurement error that can adversely affect inference (Kim et al., 2009). Recent work has introduced correction methods (Szpiro et al., 2009b; Gryparis et al., 2009), but these are based on uniform sampling. If the sampling is preferential then the implications for health effect inference are further complicated. It is plausible to account for preferential sampling in the geostatistical model or as part of the measurement error correction. Both are important directions for future research. <PARAGRAPH_SEPARATOR> The preferential sampling modelled by Diggle and his colleagues does not account for all of the relevant considerations in air pollution monitoring. Data are often derived from regulatory monitors sited for policy objectives. Sometimes additional data are collected, and a major consideration in siting is the covariate design space (Cohen et al., 2009). Preferential sampling in regions with high variability has also been proposed (Kanaroglou et al., 2005). <PARAGRAPH_SEPARATOR> We simulate effects of preferential sampling on health effect estimation. The exposure model is <PARAGRAPH_SEPARATOR> $$ \\textstyle Y=\\mu+S(x)+Z $$ <PARAGRAPH_SEPARATOR> with intercept $\\mu{=}1.36$ and exponential variogram with range 90, sill 3.76 and nugget 1.34. On a $(500\\times400)$ - unit domain (discretized to $50\\times50$ cells), we sample $Y$ at 200 locations and use kriging based on maximum likelihood to predict the exposure $Y^{*}$ at $N^{*}=300$ locations where the health outcome $Z^{*}$ is measured. Assume <PARAGRAPH_SEPARATOR> $$ Z^{\\ast}=\\beta_{0}+Y^{\\ast}\\beta_{1}+\\varepsilon $$ <PARAGRAPH_SEPARATOR> with $\\varepsilon$ independent and identically distributed and $\\beta_{1}=-0.322$ the parameter of interest. Consider five sampling schemes: <PARAGRAPH_SEPARATOR> (a) baseline, non-preferential uniform sampling; (b) intensity $(S)$ , preferential sampling based on $S(x)$ (as in the paper with $\\alpha=1$ and $\\beta=2$ ); (c) intensity $(Y)$ , preferential sampling based on $Y(x)$ ; (d) gradient $(S)$ , preferential sampling based on the variability of $S(x)$ (variance of 30 nearest neighbours); (e) gradient $(Y)$ , preferential sampling based on the variability of $Y(x)$ ."
  },
  {
    "qid": "statistic-posneg-1549-0-1",
    "gold_answer": "FALSE. The paper states that the t-test for the difference between the regression coefficients (bπ = 9.56 and bα = 5.38) returns an altogether insignificant result, with the chance of obtaining a difference equal to or larger than that observed being greater than 0.5. This indicates no statistically significant difference between the regression coefficients.",
    "question": "Does the paper conclude that the difference between the regression coefficients (bπ and bα) is statistically significant based on the t-test results?",
    "question_context": "Testing Differences in Covariance Estimates Between Two Samples\n\nThe paper discusses testing the significance of the difference between covariance estimates from two samples (hogs and gilts) using various statistical methods, including Fisher's z-transformation and a t-test for regression coefficients. The analysis involves comparing correlation estimates and regression coefficients, with a focus on the ratio of covariance estimates and its significance under different correlation assumptions."
  },
  {
    "qid": "statistic-posneg-1832-0-2",
    "gold_answer": "TRUE. The transition probability \\( q_{it}(x, x') \\) is defined to be non-zero only when \\( x_{-i}' = x_{-i} \\), meaning the sampler only proposes changes to the \\( i \\)-th component while keeping the others fixed. This ensures the sampler respects the conditional structure of the Gibbs sampling framework, maintaining correctness.",
    "question": "The transition probability function \\( q_{it}(x, x') \\) in the Metropolis-approximate Gibbs sampler is non-zero only if \\( x_{-i}' = x_{-i} \\). Is this condition necessary for the correctness of the sampler?",
    "question_context": "Metropolis-Within-Gibbs Sampler Performance and Approximate Gibbs Sampling\n\nThe paper discusses the computational burden of various implementations of the Gibbs sampler, including the Metropolis-within-Gibbs method. It compares performance metrics like run time and function evaluations for unimodal and bimodal trivariate normal distributions. The Metropolis-approximate Gibbs sampler is introduced as a variant that uses approximate conditional distributions and corrects them via Metropolis-Hastings steps to ensure convergence to the desired joint distribution."
  },
  {
    "qid": "statistic-posneg-1153-4-1",
    "gold_answer": "TRUE. The rank equality $\\operatorname{rank}=p-1$ in a $p\\times p$ covariance matrix implies exactly one zero eigenvalue. The proof explicitly constructs $\\theta$ as the eigenvector corresponding to this zero eigenvalue (via $\\theta^{\\mathrm{{T}}}\\mathsf{c o v}\\{\\cdot\\}\\theta=0$), while demonstrating that all other eigenvectors $\\theta_{1}\\perp\\theta$ yield $\\theta_{1}^{\\mathrm{{T}}}\\operatorname{cov}\\{\\cdot\\}\\theta_{1}>0$. This confirms the positive definiteness of the remaining $p-1$ dimensions, making $\\theta$ the unique direction of degeneracy in the conditional covariance matrix.",
    "question": "Does the equality $\\operatorname{rank}[\\operatorname{cov}\\{X-E(X\\mid\\theta^{\\operatorname{T}}X)\\}]$=p-1$ in Lemma 1 imply that $\\theta^{\\mathrm{{T}}}\\mathsf{c o v}\\{X-E(X|\\theta^{\\mathrm{{T}}}X)\\}\\theta=0$ is the only zero eigenvalue?",
    "question_context": "Identification in Extended Partially Linear Single-Index Models\n\nAssumption 6. We require that $K(v)$ is supported on the interval $(-1,1)$ and is a symmetric probability density function with a bounded derivative. Furthermore, the Fourier transformation of $K(v)$ is absolutely integrable. <PARAGRAPH_SEPARATOR> Assumption 1 is needed in order that the symmetric kernel in Assumption 6 leads to a bias of $O(h^{2})$ in kernel smoothing. Higher-order bias can be achieved by imposing stronger conditions on the smoothness of functions in Assumption 1 and the kernel function (Fan & Gijbels, 1996, p. 62). Assumption 2 is imposed only for the purpose of simplicity of proof. It can be weakened to $\\alpha(k)=$ ${\\cal O}(k^{-\\imath})$ for some $\\iota>0$ . However, many time series models satisfy Assumption 2, including nonparametric , autoregressive conditional heteroscedasticity, models (Masry & Tjøstheim, 1995) and model (2·3) under certain assumptions. Assumption 3 is imposed to permit estimation of functions on the region ${\\mathcal{A}}^{p}$ . Assumption 4 is a commonly used assumption for dependent data; see Masry & Tjøstheim (1995) for example. Assumption 5 is assumed so that the Chebyshev inequality can be applied. The second part of Assumption 6 is needed so that we may use the moment inequality of Kim & Cox (1995). Similar assumptions can be found for example in Hardle et al. (1993), Kim & Cox (1995) and Ichimura (1993). <PARAGRAPH_SEPARATOR> Proof of T heorem 1. Suppose that there exist other values $\\beta_{1},~\\theta_{1}$ and $\\phi_{1}(u)$ satisfying the same conditions. Then <PARAGRAPH_SEPARATOR> $$ \\beta^{\\mathrm{{T}}}{\\boldsymbol{x}}+\\phi(\\theta^{\\mathrm{{T}}}{\\boldsymbol{x}})\\equiv\\beta_{1}^{\\mathrm{{T}}}{\\boldsymbol{x}}+\\phi_{1}(\\theta_{1}^{\\mathrm{{T}}}{\\boldsymbol{x}}). $$ <PARAGRAPH_SEPARATOR> Taking second derivatives, we have <PARAGRAPH_SEPARATOR> $$ \\phi^{\\prime\\prime}(\\theta^{\\mathrm{{T}}}x)\\theta\\theta^{\\mathrm{{T}}}\\equiv\\phi_{1}^{\\prime\\prime}(\\theta_{1}^{\\mathrm{{T}}}x)\\theta_{1}\\theta_{1}^{\\mathrm{{T}}}. $$ <PARAGRAPH_SEPARATOR> If $\\theta\\neq\\theta_{1}$ , then we can find $\\theta_{2}$ such that $\\theta_{2}^{\\mathrm{T}}\\theta\\neq0$ and $\\theta_{2}^{\\mathrm{T}}\\theta_{1}=0$ . Therefore <PARAGRAPH_SEPARATOR> $$ (\\theta_{2}^{\\mathrm{{T}}}\\theta)\\phi^{\\prime\\prime}(\\theta^{\\mathrm{{T}}}x)\\equiv0. $$ <PARAGRAPH_SEPARATOR> This contradicts $\\phi^{\\prime\\prime}(\\theta^{\\mathrm{{T}}}x)\\not\\equiv0$ and therefore we have $\\theta=\\theta_{1}$ . From (A·2) and noting that $\\theta=\\theta_{1}$ , we further have $\\phi^{\\prime\\prime}(u)\\equiv\\phi_{1}^{\\prime\\prime}(u)$ , which means that <PARAGRAPH_SEPARATOR> $$ \\phi_{1}(u)=c_{0}+c_{1}u+\\phi(u). $$ <PARAGRAPH_SEPARATOR> Combining (A·3) with (A·1), we have <PARAGRAPH_SEPARATOR> $$ {\\boldsymbol{\\beta}}^{\\mathrm{{T}}}{\\boldsymbol{x}}=\\beta_{1}^{\\mathrm{{T}}}{\\boldsymbol{x}}+c_{0}+c_{1}\\theta^{\\mathrm{{T}}}{\\boldsymbol{x}}. $$ <PARAGRAPH_SEPARATOR> Thus ${c_{0}}=0,{c_{1}}=0$ and $\\beta=\\beta_{1}$ . <PARAGRAPH_SEPARATOR> L 1. Suppose that $X$ has a joint positive density function on an open convex subset in $\\mathbb{R}^{p}$ and $\\|\\theta\\|=1$ . T hen rank $[\\operatorname{cov}\\{X-E(X\\mid\\theta^{\\operatorname{T}}X)\\}]$=p-1 . <PARAGRAPH_SEPARATOR> Proof. First note that $\\theta^{\\mathrm{{T}}}\\mathsf{c o v}\\{X-E(X|\\theta^{\\mathrm{{T}}}X)\\}\\theta=0$ , and therefore <PARAGRAPH_SEPARATOR> $$ \\operatorname{rank}[\\operatorname{cov}\\{X-E(X\\mid\\theta^{\\operatorname{T}}X)\\}]\\leqslant p-1. $$ <PARAGRAPH_SEPARATOR> Next, we need only prove that, for any $\\theta_{1}=(\\theta_{11},\\ldots,\\theta_{1p})^{\\mathrm{{T}}}\\perp\\theta$ and $\\|\\theta_{1}\\|=1$ , <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\theta_{1}^{\\mathrm{{T}}}\\operatorname{cov}\\{X-E(X|\\theta^{\\mathrm{{T}}}X)\\}\\theta_{1}\\mp0.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Without loss of generality, we may assume that $\\operatorname{cov}(X_{t})=\\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{p})$ . We have <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\operatorname{cov}\\{\\theta_{1}^{\\mathrm{\\scriptscriptstyleT}}X-E(\\theta_{1}^{\\mathrm{\\scriptscriptstyleT}}X|\\theta^{\\mathrm{\\scriptscriptstyleT}}X)\\}=\\displaystyle\\sum_{i=1}^{p}\\theta_{1i}^{2}\\lambda_{i}-E[E\\{\\theta_{1}^{\\mathrm{\\scriptscriptstyleT}}X-E(\\theta_{1}^{\\mathrm{\\scriptscriptstyleT}}X)|\\theta^{\\mathrm{\\scriptscriptstyleT}}X\\}]^{2}}\\ &{\\quad\\geqslant\\displaystyle\\sum_{i=1}^{p}\\theta_{1i}^{2}\\lambda_{i}-E(E[\\{\\theta_{1}^{\\mathrm{\\scriptscriptstyleT}}X-E(\\theta_{1}^{\\mathrm{\\scriptscriptstyleT}}X)\\}^{2}|\\theta^{\\mathrm{\\scriptscriptstyleT}}X])}\\ &{\\quad=0.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Equality holds only when $\\theta_{1}^{\\mathrm{{T}}}X$ is a function of $\\theta^{\\mathrm{{T}}}X$ , which contradicts the assumptions. <PARAGRAPH_SEPARATOR> Proof of T heorem 2. First note that $\\beta_{0}$ and $\\theta_{0}$ is the minimum point of $S(\\theta,\\beta)$ and secondly $\\beta_{\\theta_{0}}$ is perpendicular to $\\theta_{0}$ from Lemma 1 and Theorem 5·6 of Schott (1997). By the identity of the model, Theorem 2 follows. $\\sqsupset$ <PARAGRAPH_SEPARATOR> Proof of T heorem 3. This follows directly from Example 3·3 of An & Huang (1994)."
  },
  {
    "qid": "statistic-posneg-763-0-2",
    "gold_answer": "TRUE. The context states that if $(\\hat{\\pmb{\\alpha}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\gamma}}_{t}^{\\mathrm{ls}})$ is an unbiased consistent estimator of $(\\pmb{\\alpha}_{t},\\pmb{\\beta}_{t},\\pmb{\\gamma}_{t})$, then the estimate of $E(Y_{t})$ obtained via the compensator method will also be unbiased and consistent. This is a direct consequence of the properties of unbiased and consistent estimators.",
    "question": "Is the estimating the compensator method guaranteed to produce an unbiased estimate of $E(Y_{t})$ if $\\hat{\\pmb{\\alpha}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\gamma}}_{t}^{\\mathrm{ls}}$ are unbiased and consistent estimators of $\\pmb{\\alpha}_{t},\\pmb{\\beta}_{t},\\pmb{\\gamma}_{t}$?",
    "question_context": "Linear Increment Model Estimation and Imputation Methods\n\nIn conclusion, the least-squares estimators are unbiased for the parameters of the LI model if weak DTIC holds and either (i) $\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}}$ is fixed at the true value of $\\beta_{t}$ , or (ii) there is no measurement error. Otherwise, they may not be. Henceforth, the only value at which we consider fixing $\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}}$ is $I$ . This is the case of most practical interest: one could fix $\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}}={\\pmb I}$ if one believed that expected increment $E(\\Delta Y_{t}\\mid\\mathcal{F}_{t-1})$ did not depend on $Y_{t-1}$ (i.e. $\\boldsymbol\\beta_{t}=\\boldsymbol I$ ). Otherwise, one could treat $\\beta_{t}$ as unknown and estimate it. It seems unlikely one would wish to fix $\\beta_{t}$ at a value other than $I$ .\n\n# 2.4. Estimating the compensator\n\nFrom Equation (2), $E(Y_{t}\\mid X,Y_{1})=\\alpha_{t}+\\beta_{t}^{\\top}E(Y_{t-1}\\mid X,Y_{1})+\\gamma_{t}^{\\top}X$ , and so\n\n$$\nE(Y_{t}\\mid X,Y_{1})=\\alpha_{t}+\\gamma_{t}^{\\top}X+\\left\\{\\sum_{j=2}^{t-1}\\left(\\prod_{k=j+1}^{t}\\beta_{k}^{\\top}\\right)(\\alpha_{j}+\\gamma_{j}^{\\top}X)\\right\\}+\\prod_{j=2}^{t}\\beta_{j}^{\\top}Y_{1}\n$$\n\nwhere $\\textstyle\\prod_{j=a}^{b}\\beta_{j}^{\\top}$ means $\\beta_{b}^{\\top}\\beta_{b-1}^{\\top}\\ldots\\beta_{a}^{\\top}$ . Thus, $E(Y_{i t}\\mid X_{i},Y_{i1})$ can be estimated for each individual $i$ in the dataset by replacing $\\pmb{\\alpha}_{t},\\pmb{\\beta}_{t}$ and $\\gamma_{t}$ in Equation (6) with $\\hat{\\pmb{\\alpha}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}}$ and $\\hat{\\gamma}_{t}^{\\mathrm{ls}}$ . Denote this estimate as $Y_{i t}^{\\mathrm{cpr}}$ . This is the estimating the compensator method. The overall mean $E(Y_{t})$ can then be estimated as $N^{-1}\\sum_{i=1}^{N}Y_{i t}^{\\mathrm{cpr}}$ . If $(\\hat{\\pmb{\\alpha}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\gamma}}_{t}^{\\mathrm{ls}})$ is an unbiased consistent estimator of $(\\pmb{\\alpha}_{t},\\pmb{\\beta}_{t},\\pmb{\\gamma}_{t})$ , then this estimate of $E(Y_{t})$ will also be unbiased and consistent.\n\nReturning to Example 1, if $\\hat{\\alpha}_{2}^{\\mathrm{ls}}=0.585$ and $\\hat{\\beta}_{2}^{\\mathrm{ls}}=0.267$ are used in Equation (6), then $E(Y_{2})$ is calculated to equal $\\hat{\\alpha}_{2}^{\\mathrm{ls}}=0.585$ , whereas its true value is zero.\n\n# 2.5. LI–LS imputation\n\nA&G propose an alternative to estimating the compensator when there is no measurement error. We call this method ‘LI-LS imputation’, because it is based on the LI model of Equation (2) and the least-squares (LS) estimators $\\hat{\\pmb{\\alpha}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}},\\hat{\\pmb{\\gamma}}_{t}^{\\mathrm{ls}}$ of $\\pmb{\\alpha}_{t},\\pmb{\\beta}_{t}$ and $\\gamma_{t}$ . (Later, we shall introduce other LI imputation methods that differ from LI-LS imputation only in that they use alternative estimators of $\\pmb{\\alpha}_{t},\\pmb{\\beta}_{t}$ and $\\gamma_{t}$ .) LI–LS imputation uses an actual outcome when it is observed and imputes a missing outcome as the most recently observed outcome updated by the expected increments. That is, letting $Y_{t}^{\\mathrm{est}}$ denotes the value of $Y_{t}$ in the imputed dataset, we have $Y_{t}^{\\mathrm{est}}=Y_{t}$ if $R_{0t}=1$ and $\\begin{array}{r}{Y_{t}^{\\mathrm{est}}=\\hat{\\alpha}_{t}^{\\mathrm{ls}}+(\\hat{\\beta}_{t}^{\\mathrm{ls}})^{\\top}Y_{t-1}^{\\mathrm{est}}+(\\hat{\\gamma}_{t}^{\\mathrm{ls}})^{\\top}X}\\end{array}$ if $R_{0t}=0$ .\n\nA&G show that if Equation (2) holds, there is no measurement error, strong DTIC holds and\n\n$$\nE\\{R_{0,t}(Y_{t-1}^{\\mathrm{est}}-Y_{t-1})\\mid X,Y_{1}\\}={\\bf0}\n$$\n\nthen $E(Y_{t}^{\\mathrm{est}}\\mid Y_{1},X)=E(Y_{t}\\mid Y_{1},X)$ . It follows that $N^{-1}\\sum_{i=1}^{N}Y_{i t}^{\\mathrm{est}}$ is an unbiased estimator of $E(Y_{t})$ and that if a linear regression model for $Y$ with any or all of $X$ and $t$ as covariates is fitted to the imputed dataset $\\{Y_{i t}^{\\mathrm{est}}:i=1,\\dots,N$ I $t=1,\\ldots,T\\}$ using least squares, then the parameter estimators of this model are consistent.\n\nThe following theorem shows that LI–LS imputation can also be used when there is measurement error, provided that $\\beta_{t}=I$ and $\\hat{\\pmb{\\beta}}_{t}^{\\mathrm{ls}}$ is fixed at $I$ .\n\nTheorem 2. Suppose that the increments model of Equation (2), the strong DTIC assumption of Equation (4) and Equation (7) hold, that ${\\boldsymbol{\\beta}}_{t}={\\boldsymbol{I}}$ , and that $\\hat{\\pmb{\\beta}}_{t}^{l s}$ is fixed at $I$ . Then, $E(Y_{t}^{e s t}\\mid$ $Y_{1},X)=E(Y_{t}\\mid Y_{1},X)$ ."
  },
  {
    "qid": "statistic-posneg-261-1-2",
    "gold_answer": "TRUE. The paper uses induction to generalize the result to m dimensions, assuming the hazard gradient is increasing for dimension (m-1). The proof involves showing that η′(x₁) > 0 for the m-dimensional case, leveraging the properties of the (m-1)-dimensional hazard gradient.",
    "question": "Is the general result for the multivariate normal hazard gradient proven by induction, assuming the result holds for dimension (m-1)?",
    "question_context": "Multivariate Normal Hazard Gradient Analysis\n\nThe paper discusses the hazard gradient of multivariate normal distributions, focusing on the increasing hazard rate property. It provides detailed derivations for bivariate, trivariate, and general multivariate cases, using conditional distributions and correlation matrices."
  },
  {
    "qid": "statistic-posneg-1764-3-0",
    "gold_answer": "TRUE. The justification follows from Lemma 5, where $u = \\frac{g(x)}{g(y)} - 1$ and $p|u| \\leqslant p\\frac{L_{g}}{g_{\\mathrm{min}}}\\|x-y\\|^{\\alpha}$. Given $\\|x-y\\| \\leqslant h$ and $p h^{\\alpha} \\to 0$, for sufficiently large $n$, $p|u| \\leqslant \\ln 2$. Applying Lemma 4(i), $\\left|(1+u)^{p} - 1\\right| \\leqslant 2p|u| \\leqslant 2\\frac{L_{g}}{g_{\\mathrm{min}}}p h^{\\alpha}$, which proves the claim.",
    "question": "Is the claim that $\\left|\\left(\\frac{g(x)}{g(y)}\\right)^{p}-1\\right|\\leqslant2\\frac{L_{g}}{g_{\\mathrm{min}}}p h^{\\alpha}$ under the conditions $p h^{\\alpha}\\to0$ and $\\|x-y\\|\\leqslant h$ mathematically justified based on the provided lemmas?",
    "question_context": "Frontier Estimation via Kernel Regression: Asymptotic Normality and Convergence\n\nThe context discusses the asymptotic properties of kernel regression estimators for frontier estimation. It includes lemmas and proofs related to the convergence and asymptotic normality of estimators such as $\\widehat{g}_{n}(x)$, $\\widehat{r}_{n}(x)$, and $\\widehat{\\varphi}_{n}(x)$. Key formulas involve bounds on deviations, convergence in probability, and distributional limits."
  },
  {
    "qid": "statistic-posneg-917-2-3",
    "gold_answer": "FALSE. The paper explicitly states that τ = log(N) does not guarantee screening consistency in this setting. It requires stronger penalties like τ = log(N)loglog(p) to satisfy the conditions τ = o(N^(1/2)) and log(p) = o(τ) necessary for screening consistency when p grows with n.",
    "question": "Does the paper assert that setting τ = log(N) (BIC-type penalty) is sufficient to guarantee screening consistency in high-dimensional settings where p grows with n?",
    "question_context": "GEE-Assisted Forward Regression with Score Information Criterion for Model Selection\n\nand $\\mathcal{M}^{(k)}=\\mathcal{M}^{(1)}\\cup\\{\\hat{l}_{1},...,\\hat{l}_{k-1}\\}$ for $k=2,\\ldots,m(p-1)$ with $\\hat{l}_{k}$ being the predictor chosen for inclusion in the $k$ -step. Note that at each step in the forward regression path, we only require estimates from the most recent GEE fit to determine which candidate predictor to include next, as judged by the score statistic. This contrasts to, say, using the Wald or some form of a pseudo-likelihood ratio statistic, which would require estimating the parameters for each of the alternative models. Finally, in the special case where IEEs are used to estimate candidate models, the score statistic is modified as follows: for step 2 in Algorithm 1, we set $S\\{\\hat{\\alpha}(\\mathcal{M})\\}=X_{\\mathcal{M}^{c}}^{\\top}\\hat{W}(\\mathcal{M})\\hat{A}^{-1}(\\mathcal{M})\\{y-$ $\\hat{\\pmb{\\mu}}(\\mathcal{M})\\}$ and ${\\mathcal{Z}}\\{{\\hat{\\pmb{\\alpha}}}({\\mathcal{M}})\\}=X_{{\\mathcal{M}}^{c}}^{\\top}{\\hat{W}}({\\mathcal{M}}){\\hat{\\pmb{A}}}^{-1}({\\mathcal{M}}){\\hat{\\pmb{W}}}({\\mathcal{M}})X_{{\\mathcal{M}}^{c}}$ The remainder of the forward regression procedure remains the same. <PARAGRAPH_SEPARATOR> # 4.1. Score Information Criterion <PARAGRAPH_SEPARATOR> To select a model on the forward regression path, as well as make use of an early stopping rule, we propose minimizing a modified version of the score information criterion (SIC, Stoklosa, Gibb, and Warton 2014). At model $\\mathcal{M}^{(k)}\\in\\mathbb{M}$ , define the SIC as <PARAGRAPH_SEPARATOR> $$ \\mathrm{SIC}(\\mathcal{M}^{(k)})=-\\sum_{k^{\\prime}=1}^{k}\\mathcal{U}_{\\hat{l}_{k^{\\prime}}}\\{\\hat{\\alpha}(\\mathcal{M}^{(k^{\\prime})})\\}+\\tau\\mathrm{dim}(\\mathcal{M}^{(k)}), $$ <PARAGRAPH_SEPARATOR> where $\\tau{}>0$ is a model complexity penalty. We select the best model as $\\begin{array}{r}{\\hat{\\mathcal{M}}~=~\\arg\\operatorname*{min}_{\\mathcal{M}^{(k)}}\\mathrm{SIC}(\\mathcal{M}^{(k)})}\\end{array}$ . The SIC is a reasonable choice as it builds on the score statistics computed as part of constructing the GEE-assisted forward regression path in Algorithm 1, and thus requires little added computation. The first component in SIC is the cumulative sum of the score statistics as we progress along the forward regression path, while the second component in SIC penalizes for the corresponding increase in the degree of model complexity. While Stoklosa, Gibb, and Warton (2014) considered AIC- (which uses a penalty of 2) and BIC- (which uses a penalty of $\\log(N)$ type model complexity penalties, here we use stronger penalties that are appropriate in the setting where $\\boldsymbol{p}$ grows with $n$ . Specifically, in supplementary material A we show that under the same setting used to established slope consistency with $p~=~o(N^{1/3})$ , and assuming the conditions $\\tau~=~o(N^{\\dot{1}/2})$ and $\\log(p)~=~o(\\tau)$ , then GEE-assisted forward regression in combination with SIC is screening consistent for the true spatial GLLVM. That is, if the underlying spatial GLLVM is sparse with only some of the elements in each of the $\\beta_{j}\\mathrm{^s}$ being truly nonzero (note the number and location of the truly nonzero coefficients can differ across the $m$ responses), then with probability tending to one GEEassisted forward regression using SIC is guaranteed to select a model containing at least all of these important predictors. Moreover, this screening consistency holds even if the structure of the working correlation matrix $\\pmb R$ is misspecified. <PARAGRAPH_SEPARATOR> In the simulations and application, we set $\\tau=\\log(N)\\operatorname{l}$ og $\\log(p)$ (similar to Wang, Li, and Leng 2009). Note the conditions imposed on $\\tau$ mean that choosing $\\tau=\\log(N)$ for the model complexity penalty, as characterizes the BIC, does not guarantee screening consistency. Indeed, in the simulations below we also tested two other choices of the model complexity parameter in $\\tau{}=2$ (as characterizes the AIC) and $\\tau~=~\\log(N)$ , and perhaps not surprisingly found both performed worse than our proposed choice of $\\tau~=~\\log(N)\\log\\log(p)$ . At the same time however, we stress this particular choice of $\\tau$ is by no means definitive; other, perhaps more data driven choices are possible (see for instance Bai, Rao, and Wu 1999, in a different context using information criteria), although they would likely increase detract from the computational efficiency of the proposed approach. We offer a more detailed discussion regarding the choice of $\\tau$ in Supplementary Material C. <PARAGRAPH_SEPARATOR> Finally, as seen in Algorithm 1, note that SIC can also be used as an early stopping rule to further reduce computation time. Specifically, in Step 3 of Algorithm 1, we only include a new predictor and proceed on the forward solution path if it reduces the current value of SIC. Otherwise, we terminate the solution path early, and select the last fitted candidate GEE. We employ this approach in both our simulations and application below. It is important to emphasize that the use of an early stopping rule based on SIC is theoretically justified, in the sense that it asymptotically still guarantees screening consistency. We provide further discussion of this, as well as empirical studies to support the SIC-based early stopping rule, in supplementary material B."
  },
  {
    "qid": "statistic-posneg-1228-0-0",
    "gold_answer": "FALSE. The context explicitly states that if the linearity assumption does not hold, certain transformations are needed to fit the proposed setting, and a more viable computational method needs to be developed. This implies that the model cannot be directly applied without modifications if the linearity assumption is violated.",
    "question": "The covariance expression for $X_{lt}$ and $X_{ls}$ is derived under the assumption that the log-spectra and covariates have a linear relationship. If this linearity assumption does not hold, the proposed model can still be applied without any modifications. True or False?",
    "question_context": "Locally Stationary Time Series with Covariate-Dependent Spectra\n\nWe have assumed a linear relationship between the log-spectra and the covariates. If the linearity assumption does not hold, certain transformations are needed to fit the proposed setting, whereas a more viable computational method needs to be developed. <PARAGRAPH_SEPARATOR> From (5.1), we have the following covariance expression for a given $l\\in\\{1,\\ldots,n\\}$ , and for $t,s=1,\\ldots,T$ : <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{\\displaystyle{\\mathrm{cov}(X_{l t},X_{l s})=\\frac{1}{T}\\sum_{k=1}^{T}\\exp\\Biggl[W_{l t}\\Biggl\\{\\beta\\biggl(\\frac{k}{T},\\frac{t}{T}\\biggr)+\\beta\\biggl(\\frac{k}{T},\\frac{s}{T}\\biggr)\\Biggr\\}\\Biggr]}}\\ {{\\mathrm{}\\times\\exp\\Biggl\\{\\frac{2\\pi k(t-s)}{T}i\\Biggr\\}.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Let $U_{T}(\\omega)$ be a count measure on [0, 1] with jump $1/T$ at each $k=1,\\ldots,T$ and let $\\beta_{d}(\\omega,u)$ be a smooth function connecting $\\{k/T,t/T,\\beta(k/T,t/T)\\}$ . Equation (A.1) can be rewritten as <PARAGRAPH_SEPARATOR> $$ \\mathrm{cov}(X_{l t},X_{l s})=\\int_{0}^{1}\\exp\\Biggl[W_{l t}\\left\\{\\beta_{d}\\Biggl(\\omega,\\frac{t}{T}\\Biggr)+\\beta_{d}\\Biggl(\\omega,\\frac{s}{T}\\Biggr)\\right\\}\\Biggr]\\exp\\{2\\pi\\omega(t-s)i\\}d U_{T}(\\omega). $$ <PARAGRAPH_SEPARATOR> It then follows from the Karhunen Theorem (Grenander and Rosenblatt 1984, p. 29) that there exists an orthogonal stochastic process $Z_{l}(\\omega)$ on [0, 1], such that <PARAGRAPH_SEPARATOR> $$ X_{l t}=\\int_{0}^{1}\\exp\\left\\{\\frac{W_{l t}}{2}\\beta_{d}\\biggl(\\omega,\\frac{t}{T}\\biggr)\\right\\}\\exp(2\\pi\\omega t i)d Z_{l}(\\omega). $$ <PARAGRAPH_SEPARATOR> This also shows that $X_{l t}$ is a locally stationary process with spectrum $f_{d}(\\omega,u,W_{l u})$ being $\\exp\\{W_{l u}\\pmb\\beta_{d}(\\omega,u)\\}$ . The condition (1) follows from the definition of $\\beta_{d}(\\omega,u)$ . By the smoothness assumption of $f(\\omega,u,W_{l u})$ , $|f_{d}(\\omega,u,W_{l u})-f(\\omega,u,W_{l u})|<c(1/T)$ with $c$ the Lipschitz constant on $[0,1]\\times[0,1]$ , which concludes the proof for condition (2)."
  },
  {
    "qid": "statistic-posneg-2049-5-2",
    "gold_answer": "FALSE. Explanation: The paper recommends restricting design selection to the set $\\mathcal{D}$ of perpetually connected designs whenever $\\mathcal{D}$ is not empty, implying that there may be cases where $\\mathcal{D}$ is empty. The existence of perpetually connected designs depends on the specific parameters (number of treatments, periods, and subjects) and is not guaranteed in all scenarios.",
    "question": "Is the set $\\mathcal{D}$ of perpetually connected designs guaranteed to be non-empty for any given number of treatments, periods, and subjects?",
    "question_context": "Robust Assessment of Two-Treatment Higher-Order Crossover Designs\n\nQuite apart from the use of the weighted coefficients procedure of Proposition 2 for formulating sampling variances of the treatment contrasts for direct and carry-over effects, there is another property of these weighted values which is helpful when comparing the effects of missing values on connected eventual designs. The output in Section 4.2 shows that these weights change as different observations are lost, and this is useful information for assessing the influence of those observations that remain. <PARAGRAPH_SEPARATOR> The literature on the optimality and efficiency of cross-over designs includes the recent contributions by Low et al. (1999), Majumdar et al. (2008), Zhou and Majumdar (2012) and Zheng (2013), which study the possibility of subject drop-out and yield some interesting and useful results. The approach of the present paper differs from these works in two ways. Firstly, wherever possible the main objective is to define and then eliminate from consideration all eventual designs that could be disconnected due to drop-out over the third and subsequent periods. The second objective is to identify locally efficient designs, i.e. designs with good properties of design efficiency from the set $\\mathcal{D}$ of perpetually connected designs which remain. For example Zheng (2013, page 83) recommends a design for $t=2$ treatments over $p=6$ periods for $s=14$ subjects, designated $d9$ therein, and this is displayed here as Design 5.2.1. <PARAGRAPH_SEPARATOR> Design 5.2.1. Zheng two-treatment design d9 <PARAGRAPH_SEPARATOR> This design has high efficiency and robustness, conditional on the assumption made by Zheng (2013) that there is no subject drop-out in the first four periods and that drop-out is more likely in the final period compared to period 5 in the ratio of 3:2. However, Design 5.2.1 is not perpetually connected so it follows that there will be some disconnected eventual designs. In particular, let the subjects (columns) of the displayed design be labelled consecutively from 1 to 14. If all oddnumbered subjects and subject 2 leave the study at period 3 we get Design 5.2.2: <PARAGRAPH_SEPARATOR> Design 5.2.2. Zheng design $d9$ with drop-out <PARAGRAPH_SEPARATOR> The eventual design, Design 5.2.2, is disconnected. When this occurs, then no unbiased linear estimator of $\\tau_{A}-\\tau_{B}$ or $\\rho_{A}-\\rho_{B}$ is available. Whilst this form of drop-out behaviour may seem extreme, the question of most concern in this case is that the eventual design has 52 remaining measurements, 26 of treatment A and 26 of treatment $B$ , but the structure of this eventual design is such that unbiased linear estimates of these two contrasts cannot be realized. <PARAGRAPH_SEPARATOR> This situation can be avoided when design selection is restricted to the set $\\mathcal{D}$ of perpetually connected designs, which is the procedure recommended by the authors whenever $\\mathcal{D}$ is not empty. A perpetually connected design is given by Design 5.2.3 which consists of three copies of dual block AABBBA and BBAAAB and four copies of dual block ABBAAB and BAABBBA."
  },
  {
    "qid": "statistic-posneg-963-0-3",
    "gold_answer": "FALSE. Explanation: Proposition 1 states that if $a=\\sum_{i}Z_{i}\\delta_{i}$, then $Y_{(k+1-a)}>\\theta>Y_{(k-a)}$. This condition holds for any $a$ (not just $a=0$) as long as $a$ is the correct count of displacements. The proposition explicitly accounts for the $a$ displacements by adjusting the order statistics to reflect the exact number of subjects with $Y_{i}>\\theta$ and $\\theta>Y_{i}$.",
    "question": "Is it correct to say that the condition $Y_{(k+1-a)}>\\theta>Y_{(k-a)}$ in Proposition 1 holds only if $a=0$?",
    "question_context": "Inference on Treatment Effects in Experiments with Discrete Pivot\n\nFix an integer $k$ , so that $y_{C(k)}$ is the unobserved $k/N$ quantile of responses to control. Let $\\theta$ be a value strictly between $y_{C(k)}$ and $y_{C(k+1)}$ , so that $y_{C(k+1)}>\\theta>y_{C(k)}$ , and say that subject $i$ has a ‘displacement’ around $\\theta$ if $y_{T i}>\\theta>y_{C i}$ . The observed data will reveal whether such a $\\theta$ exists. Let $Y_{i}=Z_{i}y_{T i}+(1-Z_{i})y_{C i}$ be the observed response from subject $i$ and let the order statistics of the $Y_{i}^{\\mathrm{{*}}}\\mathbf{s}$ be $Y_{(N)}\\geqslant Y_{(N-1)}\\geqslant\\ldots\\geqslant Y_{(1)}$ . Write $r_{C i}=1$ if $y_{C i}>\\theta$ and $r_{C i}=0$ otherwise, and write $r_{T i}=1$ if $y_{T i}>\\theta$ and $r_{T i}=0$ otherwise. Then there is a displacement if $\\delta_{i}=r_{T i}-r_{C i}=1$ and no displacement if $\\delta_{i}=r_{T i}-r_{C i}=0$ . The number of displacements attributable to treatment is $\\begin{array}{r}{A=\\sum_{i}Z_{i}\\delta_{i}}\\end{array}$ . Under the null hypothesis of no treatment effect, $H_{0}:y_{T i}=y_{C i}$ , for $i=1,\\ldots,N_{\\astrosun}$ , it follows that $\\delta_{i}=0$ for $i=1,\\ldots,N$ . <PARAGRAPH_SEPARATOR> A traditional choice of $k$ is $k/N=\\textstyle{\\frac{1}{2}}$ , somewhat analogous to the control median test of Gart (1963) and Gastwirth (1968). In some applications, it may be interesting to ask whether the treatment caused the response to be abnormally high, rather than higher than typical, in which case $k/N=0{\\cdot}95$ might be appropriate. <PARAGRAPH_SEPARATOR> The attributable effect, <PARAGRAPH_SEPARATOR> $$ A=\\sum_{i=1}^{N}Z_{i}(r_{T i}-r_{C i})=\\sum_{i=1}^{N}Z_{i}\\delta_{i}, $$ <PARAGRAPH_SEPARATOR> is the number of subjects who were caused to have responses above the $k/N$ quantile of potential control responses because they were exposed to the treatment, and would have had responses below that quantile had they been exposed to the control instead. For instance, if $A/m=50\\%$ for $k/N=95\\%$ , then half of the treated subjects were caused to have responses above the $95\\%$ quantile of responses that would have been seen had all subjects received the control. <PARAGRAPH_SEPARATOR> # 3·2. Inference about displacement eVects in experiments <PARAGRAPH_SEPARATOR> The structure here is similar to $\\S2$ , but $y_{C(k)}$ is not observed, so $r_{T i}$ and $r_{C i}$ cannot be calculated for any subject $i.$ This uncertainty about $y_{C(k)}$ is not a problem, however. <PARAGRAPH_SEPARATOR> P 1. If $\\begin{array}{r}{a=\\sum_{i}Z_{i}\\delta_{i}}\\end{array}$ , then $Y_{(k+1-a)}>\\theta>Y_{(k-a)}.$ . <PARAGRAPH_SEPARATOR> Proof. There are exactly $N-k$ subjects with $y_{C i}>\\theta$ , and since $y_{T i}\\geqslant y_{C i}$ it follows that these $N-k$ subjects all have $Y_{i}>\\theta$ . Since $\\begin{array}{r}{a=\\sum_{i}Z_{i}\\delta_{i}}\\end{array}$ there are exactly $a$ other subjects, not included among the $N-k$ subjects, with $Y_{i}=y_{T i}>\\theta>y_{C i}$ . For the remaining $k-a$ subjects, $\\theta>y_{T i}\\geqslant y_{C i}$ , so $\\theta>Y_{i}$ . Thus there are exactly $N-k+a$ subjects with $Y_{i}>\\theta$ and exactly $k-a$ subjects with $\\theta>Y_{i}$ . This means that <PARAGRAPH_SEPARATOR> $$ Y_{(N)}\\geqslant Y_{(N-1)}\\geqslant\\ldots\\geqslant Y_{(k+1-a)}>\\theta>Y_{(k-a)}\\geqslant\\ldots\\geqslant Y_{(1)}. $$ <PARAGRAPH_SEPARATOR> Proposition 1 is used in the following way. To test the hypothesis $H_{0}:\\delta=\\delta_{0}$ , calculate the observed order statistics, $Y_{(N)}\\geqslant\\ldots\\geqslant Y_{(1)}$ , and the hypothesised attributable effect, $\\begin{array}{r}{A_{0}=\\sum_{i}Z_{i}\\delta_{i}}\\end{array}$ . If $Y_{(k+1-A_{0})}=Y_{(k-A_{0})}$ , then there cannot be $A_{0}$ displacements around a $\\theta$ such that $y_{C(k+1)}>\\theta>y_{C(k)}$ , so reject $H_{0}$ with type one error rate of zero. Assume therefore from now on that $Y_{(k+1-A_{0})}>Y_{(k-A_{0})}$ , in which case, if the hypothesis is true, then $Y_{(k+1-A_{0})}>\\theta>Y_{(k-A_{0})}$ . Call the hypothesis compatible with the observed data if $\\delta_{0i}=0$ for all $i$ such that either $Z_{i}=0$ and $Y_{i}>Y_{(k-A_{0})}$ or $Z_{i}=1$ and $Y_{(k-A_{0})}\\geqslant Y_{i}$ ; otherwise, call the hypothesis incompatible. If the hypothesis is incompatible, reject it with certainty. If it is compatible, then a treated subject, $Z_{i}=1$ , has $r_{T i}=1$ if $Y_{i}>Y_{(k-A_{0})}$ and $r_{T i}=0$ if $Y_{(k-A_{0})}\\geqslant Y_{i}$ , while a control subject, $Z_{i}=0$ , has $r_{C i}=1$ if $Y_{i}>Y_{(k-A_{0})}$ and $r_{C i}=0$ if $Y_{(k-A_{0})}\\geqslant Y_{i}$ . Write $I(E)=1$ if event $E$ occurs, $I(E)=0$ otherwise, and compute Table 4. Under the null hypothesis, Table 4 equals Tables 2 and 3, and the same test may be performed. Note that Table 4 always has row totals $N-k$ and $k$ . <PARAGRAPH_SEPARATOR> Table 4. T he observed table for testing an attributable displacement eVect $H_{0}:\\delta=\\delta_{0}$ <PARAGRAPH_SEPARATOR> <html><body><table><tr><td>Response</td><td>Treated</td><td>Control</td></tr><tr><td>1</td><td>∑Z;I(Y; > Y(k-Ao)- Ao</td><td>∑(1 - Zi)I(Y> Yk-Ao))</td></tr><tr><td>0</td><td></td><td></td></tr><tr><td>Total</td><td>m</td><td>N-m</td></tr></table></body></html>"
  },
  {
    "qid": "statistic-posneg-1861-0-1",
    "gold_answer": "TRUE. The paper clearly states that a brute force implementation of distance covariance has a time complexity of O(n²). This is due to the need to compute pairwise distances for all elements in the dataset, which scales quadratically with the number of observations.",
    "question": "The paper claims that the brute force algorithm for computing distance covariance has a time complexity of O(n²). Is this claim correct given the context?",
    "question_context": "Fast Algorithm for Distance Covariance Computation\n\nThe paper introduces a fast algorithm for computing sample distance covariance, which measures dependence between two random variables. The squared distance covariance is defined as the weighted L2 distance between the joint characteristic function and the product of marginal characteristic functions. The paper proposes an O(n log n) algorithm to compute the sample distance covariance for univariate random variables, leveraging sorting and merge techniques to optimize computations."
  },
  {
    "qid": "statistic-posneg-759-0-0",
    "gold_answer": "FALSE. While the condition $E_{\\pi_{1}}|\\ell p/\\ell_{1}p_{1}|^{2+\\epsilon}<\\infty$ is necessary for the consistency and asymptotic normality of the importance sampling estimator $\\widetilde{\\eta}_{n}$, it is not sufficient on its own. The condition must hold for both the weight function $\\ell p/\\ell_{1}p_{1}$ and the weighted function $f\\ell p/\\ell_{1}p_{1}$, as specified in the context. Additionally, the Markov chain $\\Phi$ must satisfy certain ergodicity and minorization conditions (as referenced in Corollary 1) to ensure the estimator's validity.",
    "question": "Is the condition $E_{\\pi_{1}}|\\ell p/\\ell_{1}p_{1}|^{2+\\epsilon}<\\infty$ sufficient to ensure the consistency of the importance sampling estimator $\\widetilde{\\eta}_{n}$ when the two Bayesian models differ only in the prior?",
    "question_context": "Importance Sampling in Bayesian Analysis with Multiple Markov Chains\n\nImportance sampling can be very useful in Bayesian analysis when we wish to see the effect of a change in the prior distribution or the likelihood function on a posterior expectation. Here we focus on the application of single-chain importance sampling estimators in such a situation. Let $x$ and $y$ denote parameters and observed data, respectively. We think of $\\pi$ and $\\pi_{1}$ as two different posterior densities with $\\pi(x)=\\ell(x)p(x)/m$ , and $\\pi_{1}(x)=\\ell_{1}(x)p_{1}(x)/m_{1},$ , where $\\ell(x)$ and $\\ell_{1}(x)$ denote two different likelihood functions, $p(x)$ and $p_{1}(x)$ denote two different prior densities, and $m$ and $m_{1}$ are unknown normalizing constants. (When the dependence of the likelihood function on the data needs to be noted we will write $\\ell(x;y)$ instead of $\\ell(x)$ .) We imagine a situation where an MCMC algorithm for $\\pi$ is yet to be developed, but we have available a Markov chain with invariant distribution $\\pi_{1}$ , say $\\Phi=\\{X_{i},i=0,1,...\\}$ , which satisfies the assumptions of Corollary 1. Suppose we are interested in approximating the expectation $E_{\\pi}f=\\textstyle\\int_{\\mathbb{X}}f(x)\\pi(x)\\mu(d x)$ , where $f:\\mathsf{X}\\to\\mathbb{R}$ is a $\\pi$ -integrable function. According to Equation (1.2), the ratio estimator of $\\eta$ based on the Markov chain $\\Phi$ from $\\pi_{1}$ is given by $$ \\widetilde{\\eta}_{n}=\\sum_{i=1}^{n}\\frac{f(X_{i})\\ell(X_{i})p(X_{i})}{\\ell_{1}(X_{i})p_{1}(X_{i})}\\Bigg/\\sum_{i=1}^{n}\\frac{\\ell(X_{i})p(X_{i})}{\\ell_{1}(X_{i})p_{1}(X_{i})}. $$ If the two Bayesian models differ only in the prior, that is, if $\\ell=\\ell_{1}$ , then $$ \\widetilde{\\eta}_{n}=\\sum_{i=1}^{n}\\frac{f(X_{i})p(X_{i})}{p_{1}(X_{i})}\\bigg/\\sum_{i=1}^{n}\\frac{p(X_{i})}{p_{1}(X_{i})}, $$ and the cancellation of the potentially very complicated likelihood gives a convenient simplification. According to Corollary 1, if there exists $\\epsilon>0$ such that $$ E_{\\pi_{1}}|\\ell p/\\ell_{1}p_{1}|^{2+\\epsilon}<\\infty\\quad\\mathrm{and}\\quad E_{\\pi_{1}}|f\\ell p/\\ell_{1}p_{1}|^{2+\\epsilon}<\\infty, $$ then we can estimate $E_{\\pi}f$ with $\\widetilde{\\eta}_{n}$ , and a valid asymptotic standard error for this estimate is given by $\\hat{\\underline{{{\\tau}}}}/R^{1/2}$ ."
  },
  {
    "qid": "statistic-posneg-1923-0-1",
    "gold_answer": "FALSE. Explanation: The distance measure D(x,𝒳ₖ|β)² = (1/p)∑ᵢβᵢ(xᵢ - X̄ₖᵢ)² is a weighted Euclidean distance and does not inherently assume normality. It is a general distance-based classifier that can be applied regardless of the underlying distribution of the data. The use of the sample mean X̄ₖ is a way to summarize the central tendency of 𝒳ₖ, but it does not impose distributional assumptions. The classifier's performance may vary depending on the data's distribution, but the method itself is distribution-free.",
    "question": "The distance measure D(x,𝒳ₖ|β)² = (1/p)∑ᵢβᵢ(xᵢ - X̄ₖᵢ)² uses the sample mean X̄ₖ. Does this imply that the classifier assumes the data within each population Πₖ is normally distributed?",
    "question_context": "Distance-Based Classification with Weighted Features\n\nThe paper discusses two approaches to constructing classifiers for assigning a new data vector X to one of two populations, Π₀ or Π₁. The first approach uses a logit model where the probability pr(Π₀|X) is modeled as a logistic function of α + βᵀX. The second approach is distance-based, using measures D(x,𝒳ₖ|β) to compute the distance from x to training samples 𝒳ₖ, and then forming a ratio R(X|β) to decide classification. Two examples of distance measures are provided, both weighted by a parameter vector β with non-negative components summing to 1. The classifier assigns X to Π₀ if R(X|β) exceeds a threshold r, and to Π₁ otherwise."
  },
  {
    "qid": "statistic-posneg-1126-0-1",
    "gold_answer": "TRUE. Fig. 2(a) shows a clear banded structure in $\\hat{A}$ with zero entries outside the band, making it easier to interpret as it reflects a specific ordering (e.g., spatial). In contrast, Fig. 2(b) shows $\\tilde{A}$ lacks any discernible structure, with scattered non-zero entries, making it harder to interpret economically or statistically.",
    "question": "The paper argues that the banded coefficient matrix $\\hat{A}$ is more interpretable than the sparse matrix $\\tilde{A}$ because it has a structured form. Based on Fig. 2, is this claim justified?",
    "question_context": "High-dimensional Banded vs. Sparse Vector Autoregressions: Estimation and Interpretation\n\nThe paper compares banded and sparse autoregressive models for high-dimensional time series data. The sparse model is estimated via lasso by minimizing the objective function: $$\\sum_{t=2}^{n}\\|y_{t}-A y_{t-1}\\|^{2}+\\sum_{i,j=1}^{p}\\lambda_{i}|a_{i j}|,$$ where $\\lambda_{1},\\ldots,\\lambda_{p}$ are tuning parameters estimated by five-fold cross-validation. The prediction accuracy of the sparse model is comparable to banded autoregressive models but slightly worse for two-step-ahead prediction. The banded coefficient matrix $\\hat{A}$ is found to be more interpretable than the sparse matrix $\\tilde{A}$ due to its structured form."
  },
  {
    "qid": "statistic-posneg-1225-0-1",
    "gold_answer": "TRUE. The proof shows that independence of $(X_{\\pi_{1}},X_{\\pi_{2}})$ is equivalent to the condition $(X_{1},X_{2})\\in\\mathrm{IC}_{0}$, as it ensures the joint probabilities factorize into the product of marginal probabilities for all measurable sets $A$ and $B$.",
    "question": "Does the condition $(X_{1},X_{2})\\in\\mathrm{IC}_{0}$ imply that $(X_{\\pi_{1}},X_{\\pi_{2}})$ are independent random variables, where $\\pi$ is a random permutation?",
    "question_context": "Invariant Correlation and Quasi-Independence in Random Vectors\n\nLemma 2. For a random vector $(X,Y)\\sim H$ with the marginals $F_{\\cdot}$ , $(X,Y)$ is quasi- $_r$ -Fréchet if and only if (10) holds. <PARAGRAPH_SEPARATOR> Proof of Theorem 1. We first show the ‘‘only if’’ part. As $(X,Y)\\in\\mathrm{IC}_{0}$ , we have $\\operatorname{Cov}(g(X),g(Y))=0$ for any admissible $g$ . For any $A\\in{\\mathcal{B}}(\\mathbb{R})$ , taking $g(x)=\\mathbb{1}_{A}(x)$ , we have $\\operatorname{Cov}(g(X),g(Y))=\\operatorname{Cov}(\\mathbb{1}_{A}(X),\\mathbb{1}_{A}(Y))=0$ . For any $A,B\\in{\\mathcal{B}}(\\mathbb{R})$ , let $\\begin{array}{r}{g(\\boldsymbol{x})=\\mathbb{1}_{A}(\\boldsymbol{x})+\\mathbb{1}_{B}(\\boldsymbol{x}),}\\end{array}$ which leads to <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathrm{Cov}(g(X),g(Y))=\\mathrm{Cov}\\left(\\mathbb{1}_{A}(X)+\\mathbb{1}_{B}(X),\\mathbb{1}_{A}(Y)+\\mathbb{1}_{B}(Y)\\right)=\\mathrm{Cov}\\left(\\mathbb{1}_{A}(X),\\mathbb{1}_{B}(Y)\\right)+\\mathrm{Cov}\\left(\\mathbb{1}_{B}(X),\\mathbb{1}_{A}(Y)\\right)}\\ &{\\quad\\quad\\quad=\\mathbb{P}(X\\in A,Y\\in B)+\\mathbb{P}(X\\in B,Y\\in A)-\\mathbb{P}(X\\in A)\\mathbb{P}(Y\\in B)-\\mathbb{P}(X\\in B)\\mathbb{P}(Y\\in A).}\\end{array} $$ <PARAGRAPH_SEPARATOR> As $\\operatorname{Cov}(g(X),g(Y))=0$ , we have that $(X,Y)$ satisfies (8) for all $A,B\\in{\\mathcal{B}}(\\mathbb{R})$ . Hence, $(X,Y)$ is quasi-independent by Lemma 1. Next, we show that quasi-independence implies $\\operatorname{Cov}(g(X),g(Y))=0$ for any admissible $g$ . As quasi-independence implies (8), by taking $A=B$ , we have $\\mathbb{P}(X\\in A,Y\\in A)=\\mathbb{P}(X\\in A)\\mathbb{P}(Y\\in A)$ for any $A\\in{\\mathcal{B}}(\\mathbb{R})$ . Hence, $\\operatorname{Cov}(g(X),g(Y))=0$ for any indicator functions $g=\\mathbb{1}_{A}$ . <PARAGRAPH_SEPARATOR> First, let $\\begin{array}{r}{g(x)=\\sum_{i=1}^{n}c_{i}\\mathbb{1}_{A_{i}}(x)}\\end{array}$ be an admissible simple function where $A_{1},\\ldots,A_{n}\\subseteq\\mathbb{R}$ are disjoint measurable sets, and $c_{1},\\ldots c_{n}\\in\\mathbb{R}$ are real numbers. For the simple function $g$ , we have <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\displaystyle\\mathsf{C o v}(g(X),g(Y))=\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j}\\mathrm{Cov}\\left(\\mathbb{1}_{A_{i}}(X),\\mathbb{1}_{A_{j}}(Y)\\right)=\\sum_{1\\leqslant i<j\\leqslant n}\\left(c_{i}c_{j}\\mathrm{Cov}\\left(\\mathbb{1}_{A_{i}}(X),\\mathbb{1}_{A_{j}}(Y)\\right)+c_{j}c_{i}\\mathrm{Cov}\\left(\\mathbb{1}_{A_{j}}(X),\\mathbb{1}_{A_{j}}(Y)\\right)\\right.}\\ &{\\displaystyle\\left.=\\sum_{1\\leqslant i<j\\leqslant n}c_{i}c_{j}\\left(\\mathbb{P}\\left(X\\in A_{i},Y\\in A_{j}\\right)-\\mathbb{P}\\left(X\\in A_{i}\\right)\\mathbb{P}\\left(Y\\in A_{j}\\right)+\\mathbb{P}\\left(X\\in A_{j},Y\\in A_{i}\\right)-\\mathbb{P}\\left(X\\in A_{j}\\right)\\mathbb{P}\\left(Y\\in A_{i}\\right)\\right)=}\\end{array} $$ <PARAGRAPH_SEPARATOR> Therefore, we have $\\operatorname{Cov}(g(X),g(Y))=0$ for all admissible simple functions $g$ . <PARAGRAPH_SEPARATOR> Second, let $g$ be an admissible non-negative function. Admissibility of $g$ implies that $\\mathbb{E}[g(X)]<\\infty$ , $\\mathbb{E}[g(Y)]<\\infty$ and $\\mathbb{E}[g(X)g(Y)]<$ $\\infty$ . For a non-negative admissible function $g$ , there exists a sequence of non-negative simple functions $\\{g_{n}\\}_{n\\geqslant1}$ such that $g_{n}\\uparrow g$ pointwise. Thus, we can get <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathrm{Cov}\\left(g(X),g(Y)\\right)=\\mathrm{Cov}\\left(\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X),\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(Y)\\right)=\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X)\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(Y)\\right]-\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X)\\right]\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(Y)\\right]}\\ &{=\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X)g_{n}(Y)\\right]-\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(X)\\right]\\mathbb{E}\\left[\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}g_{n}(Y)\\right]=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}\\left[g_{n}(X)g_{n}(Y)\\right]-\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}\\left[g_{n}(X)\\right]\\mathbb{E}\\left[g_{n}(Y)\\right]}\\ &{=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\left\\{\\mathbb{E}\\left[g_{n}(X)g_{n}(Y)\\right]-\\mathbb{E}\\left[g_{n}(X)\\right]\\mathbb{E}\\left[g_{n}(Y)\\right]\\right\\}=\\underset{n\\rightarrow\\infty}{\\operatorname*{lim}}\\mathrm{Cov}(g_{n}(X),g_{n}(Y))=0.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Therefore, we have $\\operatorname{Cov}(g(X),g(Y))=0$ for all admissible non-negative function $g$ . <PARAGRAPH_SEPARATOR> Finally, let $g$ be an admissible function. Define $g_{+}=\\operatorname*{max}(g,0)$ and $g_{-}=-\\operatorname*{min}(g,0)$ . It is clear that $g_{+}$ and $g_{-}$ are non-negative admissible functions and $g=g_{+}-g_{-}$ . Let $G=g_{+}+g_{-}$ . We have that $G$ is also a non-negative admissible function. Hence, we get $\\operatorname{Cov}(g_{+}(X),g_{+}(Y))=0_{\\cdot}$ , $\\operatorname{Cov}(g_{-}(X),g_{-}(Y))=0$ and $\\operatorname{Cov}(G(X),G(Y))=0,$ , which imply that $\\operatorname{Cov}(g_{+}(X),g_{-}(Y))+\\operatorname{Cov}(g_{-}(X),g_{+}(Y))=0~$ . As a result, we have <PARAGRAPH_SEPARATOR> $$ \\operatorname{Cov}\\left(g(X),g(Y)\\right)=\\operatorname{Cov}(g_{+}(X)-g_{-}(X),g_{+}(Y)-g_{-}(Y))=0. $$ <PARAGRAPH_SEPARATOR> Therefore, we can conclude $(X,Y)\\in\\mathrm{IC}_{0}$ . □ <PARAGRAPH_SEPARATOR> Proof of Proposition 2. For all $A,B\\in{\\mathcal{B}}(\\mathbb{R})$ , we have <PARAGRAPH_SEPARATOR> $$ \\mathbb{P}(X_{\\pi_{1}}\\in A,X_{\\pi_{2}}\\in B)=\\frac{1}{2}\\mathbb{P}(X_{1}\\in A,X_{2}\\in B)+\\frac{1}{2}\\mathbb{P}(X_{1}\\in B,X_{2}\\in A). $$ <PARAGRAPH_SEPARATOR> Since $X_{1}$ and $X_{2}$ have the same distribution, independence of $(X_{\\pi_{1}},X_{\\pi_{2}})$ is equivalent to <PARAGRAPH_SEPARATOR> $$ \\mathbb{P}(X_{\\pi_{1}}\\in A,X_{\\pi_{2}}\\in B)=\\mathbb{P}(X_{\\pi_{1}}\\in A)\\mathbb{P}(X_{\\pi_{2}}\\in B)=\\mathbb{P}(X_{1}\\in A)\\mathbb{P}(X_{2}\\in B)=\\mathbb{P}(X_{1}\\in B)\\mathbb{P}(X_{2}\\in A). $$ <PARAGRAPH_SEPARATOR> Hence, independence of $(X_{\\pi_{1}},X_{\\pi_{2}})$ is equivalent to <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\operatorname{\\mathbbP}(X_{1}\\in A,X_{2}\\in B)+\\operatorname{\\mathbbP}(X_{1}\\in B,X_{2}\\in A)=\\operatorname{\\mathbbP}(X_{1}\\in A)\\operatorname{\\mathbbP}(X_{2}\\in B)+\\operatorname{\\mathbbP}(X_{1}\\in B)\\operatorname{\\mathbbP}(X_{2}\\in A),}\\end{array} $$ <PARAGRAPH_SEPARATOR> which is also equivalent to $(X_{1},X_{2})\\in\\mathrm{IC}_{0}$ by Theorem 1. □"
  },
  {
    "qid": "statistic-posneg-63-1-2",
    "gold_answer": "FALSE. The text explicitly states that the hanning estimator can be at a disadvantage when p_NN(u) has significant structure (e.g., cyclic Cox process with lags 3, 7, 10,...), where bias outweighs variance reduction. For the Neyman-Scott process, the hanning estimator performs slightly better, but for the unpunctuality process, its performance depends on the smoothing effect. The claim of universal superiority is false, as estimator performance is context-dependent, particularly with respect to bias-variance trade-offs.",
    "question": "Is the hanning estimator (p̂⁽²⁾_NN) universally superior to other estimators in terms of mean-squared error for all types of processes simulated?",
    "question_context": "Nonparametric Estimation of Intensities for Sample Point Processes\n\nThe paper discusses nonparametric estimation of second-order product moments for various point processes, including exponential moving average (EMA) processes, cyclic Cox processes, Neyman-Scott cluster processes, and unpunctuality processes. It compares Poisson mean-squared errors (MSEs) with Monte Carlo MSEs for different estimators, highlighting the performance under varying conditions of clustering and regularity compared to Poisson processes."
  },
  {
    "qid": "statistic-posneg-1762-0-2",
    "gold_answer": "TRUE. The ZGZ-test assumes asymptotic normality, which holds only when the test statistic's skewness vanishes (i.e., d → ∞). Under moderate/high correlation, the skewness persists (Condition C4), making normal approximation inadequate. Theorem 1 and Remark 2 highlight this limitation, whereas the proposed test accounts for skewness via the 3-c matched χ²-approximation.",
    "question": "The ZGZ-test of Zhou et al. (2017) is valid only when the data are nearly uncorrelated because it relies on normal approximation, which ignores the skewness of the test statistic under moderate/high correlation.",
    "question_context": "High-Dimensional Multi-Sample Heteroscedastic MANOVA Test\n\nThe paper discusses a new test for high-dimensional multi-sample heteroscedastic one-way MANOVA problems, comparing it against existing methods like the ZGZ-test and ZZG-test. The proposed test uses a 3-c matched χ²-approximation to better control size and power under various correlation structures."
  },
  {
    "qid": "statistic-posneg-1078-1-0",
    "gold_answer": "FALSE. The exact maximum likelihood method is generally more accurate and provides closer estimates to the true values, but this is particularly true for larger sample sizes. For small sample sizes, the differences between the exact and conditional methods may be less pronounced, and the exact method may not always outperform the conditional method due to higher variability in estimates.",
    "question": "Is it true that the exact maximum likelihood method generally provides closer estimates to the true values than the conditional method, especially for small sample sizes?",
    "question_context": "Exact Maximum Likelihood Estimation of Structured VARMA Models\n\nThis section illustrates the proposed algorithm by fitting models that have been used in the literature. These models are fitted by the conditional method (Reinsel, 1997), and the exact maximum likelihood method. For the SCM model, a two-stage method is used as explained above. In the following, the symbol $\\because X^{\\prime}$ , indicates a coefficient to be estimated, $\\mathrm{\\nabla}{\\cdot}0^{\\mathrm{}}{}$ or ‘1’, that the corresponding coefficient is identically equal to 0 or 1, respectively, and $\\mathbf{\\hat{\\Pi}}^{\\epsilon}\\ast\\mathbf{\\hat{\\Pi}}^{\\epsilon}$ means that the coefficient is either a fixed constant different from 0 and 1, or is dependent on another coefficient of the model."
  },
  {
    "qid": "statistic-posneg-1532-0-1",
    "gold_answer": "TRUE. The adjusted estimator $\\widehat{O R}_{\\text{adj}}$ adds $1/(2n)$ to each cell probability, which is of order $O(n^{-1})$. This adjustment is analogous to the nonparametric adjustment $\\varepsilon(x)\\sim (nh)^{-1}$ in the local odds ratio context, as both adjustments are scaled by the inverse of the effective sample size (global for Haldane's estimator and local for the kernel-based estimator).",
    "question": "Does the adjusted estimator $\\widehat{O R}_{\\text{adj}}$ proposed by Haldane (1955) use an adjustment of $O(n^{-1})$ to reduce bias in the classical odds ratio estimator?",
    "question_context": "Nonparametric Estimation of Local Odds Ratio with Kernel Methods\n\nThe paper discusses the estimation of local odds ratios (OR) in contingency tables with continuous covariates using kernel methods. The local OR is defined as $O R(x)=\\frac{p_{11}(x)p_{22}(x)}{p_{12}(x)p_{21}(x)}$ for $x\\in S_X$, where $p_{ij}(x)$ are conditional probabilities. The paper proposes amended estimators to address bias and variance issues in finite samples, particularly when some $\\widehat{p}_{ij}^h(x)$ are close to zero. The estimators involve adding a small value $\\varepsilon(x)$ to each $\\widehat{p}_{ij}^h(x)$ to improve statistical properties."
  },
  {
    "qid": "statistic-posneg-85-0-0",
    "gold_answer": "FALSE. The context states that the Breiman method using repeated cross-validation is the most accurate overall for the given sample sizes and distributions, but it also mentions that for datasets with low Bayes risk, either a local bootstrap 0.632 estimate or a modified Breiman method using bootstrap estimates is more accurate. Therefore, the claim is not universally true across all scenarios described.",
    "question": "Is the claim that the Breiman method using repeated cross-validation global error rate is the most accurate overall for the given sample sizes and distributions supported by the empirical results described in the context?",
    "question_context": "Empirical Bayes Estimation in Classification Trees\n\nThe resubstitution method for estimating probability in the partitions (terminal nodes) of classification trees is inaccurate. One alternate is an empirical Bayes approach of Breiman, for which we give new justification. We used Monte Carlo simulations to compare existing and several new methods for estimating probability in the partitions. We evaluated the methods using 14 data distributions, with 1 to 8 dimensions (features), continuous and categorical variables, noise variables, sample sizes from 50 to 1,000 data points, and Bayes risk ranging from 0.01 to 0.4. For these sample sizes and distributions, the Breiman method using the repeated cross-validation global error rate is the most accurate overall, as measured by mean absolute error and mean squared error. For datasets with low Bayes risk (less than 0.1 and less than half the Bayes risk of the “no data” rule), either a local bootstrap 0.632 estimate or Breiman’s method modified to use a bootstrap estimate of the global misclassification rate is most accurate, although the Breiman plus repeated cross-validation method is closely competitive for such distributions. That techniques that rely upon the bootstrap should be better at adjusting small to moderate biases than large ones is consistent with Section 11.7 of Breiman et al. (1984). These methods are applicable to other adaptive partitioning classification algorithms."
  },
  {
    "qid": "statistic-posneg-1494-4-1",
    "gold_answer": "FALSE. The high $p$-value (0.66) indicates failure to reject the null hypothesis of equal variances, suggesting no significant variance change. However, the paper argues that the variance chart's signal is likely an artifact of the mean shift (induced median increase of $0.622$), not a true variance shift. The Monte Carlo simulation under constant variance ($\\hat{\\sigma}_1 \\approx \\hat{\\sigma}_2$) reproduced similar ARL values, corroborating that the signal arose from mean effects. Thus, the $F$-test's result aligns with the conclusion that the variance signal is spurious, but it does not imply the overall detection (driven by mean change) is insignificant.",
    "question": "Does the bootstrap $F$-test's $p$-value of 0.66 for equality of variances between segments $\\{X_{1},\\dotsc,X_{214}\\}$ and $\\{X_{215},\\dotsc,X_{235}\\}$ imply that the observed signal in the variance chart is statistically insignificant?",
    "question_context": "CUSUM Sensitivity Analysis: Impact of Reference Constants on Signal Detection\n\nThe $W^{2}$ -CUSUM (right-hand panel in Fig. 1) signals at $n=238$ , shortly after the $W$ -CUSUM. The standard deviation estimates from the segments $\\{X_{1},\\dotsc,X_{214}\\}$ and $\\{X_{215},\\dotsc,X_{235}\\}$ are very similar, namely $\\hat{\\sigma}_{1}~=~0.61$ and ${\\hat{\\sigma}}_{2}=0.68$ A bootstrap $F$ -test for equality of variances in these two segments yields a $p$ -value of 0.66. Thus, on the available evidence, the signal from the variance chart is most likely a result of the substantial mean change. Further substantiation of this conclusion comes from a Monte Carlo simulation in which data were generated from the density estimate in Fig. 3, shifted to the right by an amount 0.027 to make the resulting density have mean zero. Repeated sampling from this distribution ensures a constant standard deviation. An increase of $0.622\\left(=0.595+0.027\right)$ was induced in the median after $\tau=214$ observations. The estimate of the ARL $E[N-\tau\\vert N>\tau]$ resulting from 10,000 such trials was 22. This is of the same order of magnitude as the excess $N-\\hat{\tau}=235-214=21$ in the observed data and confirms the likely reaction by the dispersion CUSUM to the mean shift.<PARAGRAPH_SEPARATOR>The impact of the choices $\\zeta=0.15$ and $\\zeta=0.20$ on the CUSUMs can be assessed if the first 50 observations, say, are treated as in-control Phase I data. These have a standard deviation of $\\hat{\\sigma}=0.45$ , which is somewhat less than the original estimate of 0.6. The default bandwidth for a Gaussian kernel density estimate made on these Phase I data is $b=0.22$ . Then, using (8), a computation gives $\\hat{\theta}_{0}=1.03$ so that the suggested reference constant for a target shift of $\\delta_{1}=0.5$ would be<PARAGRAPH_SEPARATOR>$$\\zeta=1.03\\times0.5/2\\approx0.25.$$<PARAGRAPH_SEPARATOR>For the $W^{2}$ -CUSUM, analogous computations give<PARAGRAPH_SEPARATOR>$${\\hat{\\theta}}_{1}=1.12,$$<PARAGRAPH_SEPARATOR>which suggests $\\zeta=0.23$ as reference constant, very close to the value that was actually used. Running the CUSUMs on the Phase II observations $X_{51}$ , $X_{52}$ , . . . with this new reference constant has no material effect on the results: The $W$ -CUSUM then signals at $n=50+180=230$ and the changepoint is again estimated to be at $n=50+164=214.$<PARAGRAPH_SEPARATOR>For $b=0.44$ and $b~=~0.11$ , respectively double and one half the default bandwidth, the corresponding estimated reference values for the W -CUSUM are $\\zeta=0.21$ and $\\zeta=0.28$ . Again, when these are used, the CUSUM results are almost identical to those found at $\\zeta=0.15$ . This points to the fact that the performance of the CUSUM is not overly sensitive to misestimation of $\theta_{0}$ and consequent misestimation of the ''optimal'' reference value $\\zeta=\\theta_{0}\\delta_{1}$ ."
  },
  {
    "qid": "statistic-posneg-540-0-2",
    "gold_answer": "FALSE. While the condition 𝔼_X∼π₁[|g'(X)|^(2+δ)] < ∞ is necessary for controlling the tails of g'(X), the finiteness of V(t) also depends on the behavior of the other terms in the expectation, including the dynamics of X_t(X,P) and a'(√ν t; X, P). The additional condition 𝔼_ν∼μ[ν^(1+δ/2)] < ∞ is required to ensure that the product ν g'(X) P g'(√ν X_t) a'(√ν t; X, P) has a finite expectation, which is crucial for the covariance function V(t) to be well-defined.",
    "question": "Is the condition 𝔼_X∼π₁[|g'(X)|^(2+δ)] < ∞ for some δ > 0 sufficient to ensure the finiteness of the covariance function V(t)?",
    "question_context": "Gaussian Process Limit in Hamiltonian Monte Carlo for Product Targets\n\nThe paper discusses the behavior of Hamiltonian Monte Carlo (HMC) for product targets with a potential defined as a sum of component functions, each with its own length scale. The scaled dot product of momentum and gradient of the potential is shown to converge to a stationary Gaussian process as the number of components tends to infinity. The expected number of apogees (local maxima in the potential along the path) per unit time is derived, highlighting the relationship between the target's length scales and the integration time."
  },
  {
    "qid": "statistic-posneg-744-1-0",
    "gold_answer": "TRUE. The reasoning is based on the shape-preserving properties of Bernstein polynomials, as established by Carnicer and Peña (1993). The constraints on the weights (non-decreasing up to $k^{*}$ and non-increasing thereafter) ensure that the resulting density estimate $f_{m}(x,\\omega)$ inherits the unimodal shape. This is because the Bernstein polynomial basis is optimally shape-preserving, meaning the unimodal constraint on the weights translates directly to a unimodal density estimate.",
    "question": "Is the claim that the Bernstein polynomial density estimate $f_{m}(x,\\omega)$ satisfies the unimodality condition (iii) if the weights $\\omega_{k}$ are non-decreasing up to $k^{*}$ and non-increasing thereafter, correct?",
    "question_context": "Unimodal Density Estimation Using Bernstein Polynomials\n\nWe begin by assuming $X_{i}\\stackrel{i i d}{\\sim}f(\\cdot)$ , for $i=1$ , 2, . . . , n, and it is known that $f(\\cdot)$ is a unimodal continuous density function. In other words, we assume that there exists an , such that is non-decreasing on and is non-increasing on $(x^{*},\\infty)$ . Our goal is to construct an estimate of $\\overleftrightarrow{f}(\\cdot),\\overhat{f}(\\cdot)$ , which satisfies the following conditions: (i) ${\\hat{f}}(x)\\geq0$ , for all $x\\in\\mathbb{R}$ , (ii) $\\textstyle\\int{\\hat{f}}(x)d x=1$ , (iii) ${\\hat{f}}(x)$ is unimodal, i.e. there exists an $\\hat{x}^{*}\\in\\mathbb{R}$ , such that ${\\hat{f}}(x)$ is non-decreasing on $(-\\infty,\\hat{x}^{*})$ and ${\\hat{f}}(x)$ is non-increasing on $(\\hat{x}^{*},\\infty)$. It is also desirable that the proposed density estimate, $\\hat{f}(\\cdot)$ , satisfies a set of asymptotic properties (e.g. consistency). Without loss of generality, we first consider densities with support [0, 1], and then extend our methodology to more general supports. We begin by considering a Bernstein polynomial of order $m-1$ to estimate $f(x)$, $B_{m}(x,f)=\\sum_{k=1}^{m}f\\left(\\frac{k-1}{m-1}\\right)\\binom{m-1}{k-1}x^{k-1}(1-x)^{m-k}$. However, it is clear that $B_{m}(x,f)$ , defined in (2), is not a proper density function. We therefore consider a re-scaled version of $B_{m}(x,f)$, $f_{m}(x)=\\sum_{k=1}^{m}f\\left(\\frac{k-1}{m-1}\\right)m\\left({m-1\\atop k-1}\\right)x^{k-1}(1-x)^{m-k}\\left/\\sum_{k=1}^{m}f\\left(\\frac{k-1}{m-1}\\right),\\right.$, which is a proper density function for any value $m$ . This motivates us to consider the following class of density estimates, $f_{m}(x,\\omega)=\\sum_{k=1}^{m}\\omega_{k}f_{b}(x;k,m-k+1)$, where $\\boldsymbol{\\omega}=(\\omega_{1},\\omega_{2},\\dots,\\omega_{m})^{T}$ is a vector of weights of size $m$ , and $f_{b}(\\cdot)$ is the Beta density function with shape parameters $k$ and $m-k+1$ . Notice that $f_{m}(x,\\omega)$ will obtain all the desired properties of $\\hat{f}(\\cdot)$ if the vector of weights, $\\omega$ , satisfies the following constraints: (a) $\\omega_{k}\\geq0$ , for $k=1,\\ldots,m,$ (b) $\\textstyle\\sum_{k=1}^{m}\\omega_{k}=1$ , and (c) $\\omega_{1}\\leq\\omega_{2}\\leq\\cdot\\cdot\\cdot\\leq\\omega_{k^{*}}\\geq\\omega_{k^{*}+1}\\geq\\cdot\\cdot\\cdot\\geq\\omega_{m}$ , for some $k^{*}\\in\\{1,2,\\ldots,m\\}$ , i.e. the $\\omega_{\\boldsymbol{k}}\\mathbf{\\dot{s}}$ are non-decreasing for $k\\leq k^{*}$ and non-increasing for $k\\geq k^{*}$."
  },
  {
    "qid": "statistic-posneg-1425-2-0",
    "gold_answer": "FALSE. The restriction θ = -φ × ρ is not necessary for estimability using Bayesian MCMC methods. The non-filter specification explicitly relaxes this restriction by introducing θ as a free parameter, and the paper demonstrates that MCMC can handle this more general case without requiring integration over random effects, even for large N and T. The restriction is a simplifying assumption, not a computational requirement.",
    "question": "In the space-time filter specification, the restriction θ = -φ × ρ is imposed to simplify the model. Is this restriction necessary for the model to be estimable using Bayesian MCMC methods?",
    "question_context": "Bayesian MCMC Estimation in Space-Time Filter Panel Data Models\n\nThe paper discusses the application of a space-time filter to panel data models, focusing on Bayesian MCMC estimation methods. It presents various specifications for the error terms and likelihood functions, comparing filtered and non-filtered approaches. The models incorporate spatial and temporal dependencies through parameters like ρ (spatial autoregressive), φ (temporal autoregressive), and θ (space-time interaction). The Bayesian framework uses conjugate priors for parameters and handles large N and T samples efficiently by avoiding complex matrix inversions."
  },
  {
    "qid": "statistic-posneg-224-0-1",
    "gold_answer": "TRUE. The MTCJ copula's conditional quantile function $C_{V|U}^{-1}(\\alpha|u;\\delta)\\sim(\\alpha^{-\\delta/(1+\\delta)}-1)^{-1/\\delta}u$ shows linear dependence on $u$ as $u\\to0$. This matches the 'strongly linear' case ($\\eta=1$) where $F_{Y|X}^{-1}(\\alpha|x)\\sim x$ as $x\\to-\\infty$. The linear term $u$ dominates the asymptotic behavior, indicating strong tail dependence, consistent with the MTCJ copula's known lower tail dependence properties.",
    "question": "Does the MTCJ copula's conditional quantile function $C_{V|U}^{-1}(\\alpha|u;\\delta)\\sim(\\alpha^{-\\delta/(1+\\delta)}-1)^{-1/\\delta}u$ imply strongly linear dependence in the lower tail?",
    "question_context": "Conditional Quantile Functions in Bivariate Copula Models\n\n1. Strongly linear: $\\eta=1$ and $k_{\\alpha}=1$ . $F_{Y|X}^{-1}(\\alpha|x)$ goes to infinity linearly, and it does not depend on $\\alpha$ . It has stronger dependence than bivariate normal.   \n2. Weakly linear: $\\eta=1$ , $k_{\\alpha}$ can depend on $\\alpha$ and $0<k_{\\alpha}<1$ . $F_{Y|X}^{-1}(\\alpha|x)$ goes to infinity linearly and it depends on $\\alpha$ . It has comparable dependence with bivariate normal.   \n3. Sublinear: $\\dot{\\mathbf{\\eta}}0<\\eta<\\dot{1.\\:}F_{Y|X}^{-1}(\\alpha|x)$ goes to infinity sublinearly. The dependence is weaker than bivariate normal.   \n4. Asymptotically constant: $\\eta=0.F_{Y|X}^{-1}(\\alpha|x)$ converges to a finite constant. Asymptotically it behaves like independent.\n<PARAGRAPH_SEPARATOR>\nFig. 3 shows the conditional quantile functions for bivariate copulas with different $\\eta$ in the upper and lower tails. Example 4.1 to 4.2 derive the conditional quantile functions for bivariate Mardia–Takahasi–Clayton–Cook–Johnson (MTCJ) and Gumbel copulas. Note that $\\eta$ is constant over $\\alpha$ for several commonly used parametric bivariate copula families. However, there are cases where $\\eta$ depends on $\\alpha$ . For example, the boundary conditional distribution of the bivariate Student-t copula has mass at both 0 and 1; depending on the value of $\\alpha$ , $C_{V|U}^{-1}(\\alpha|u)$ could go to either 0 or 1, as $u\\to0$\n<PARAGRAPH_SEPARATOR>\nExample 4.1 (MTCJ Lower Tail). The bivariate MTCJ copula CDF is\n<PARAGRAPH_SEPARATOR>\n$$\nC(u,v;\\delta)=(u^{-\\delta}+v^{-\\delta}-1)^{-1/\\delta},\\quad0<u<1,0<v<1,\\delta>0.\n$$\n<PARAGRAPH_SEPARATOR>\nThe conditional quantile function is\n<PARAGRAPH_SEPARATOR>\n$$\n\\begin{array}{r}{C_{V|U}^{-1}(\\alpha|u;\\delta)=[(\\alpha^{-\\delta/(1+\\delta)}-1)u^{-\\delta}+1]^{-1/\\delta}\\sim(\\alpha^{-\\delta/(1+\\delta)}-1)^{-1/\\delta}u,\\quad u\\to0.}\\end{array}\n$$\n<PARAGRAPH_SEPARATOR>\nTake the log of both sides, $-\\log C_{V|U}^{-1}(\\alpha|u;\\delta)\\sim\\log u.$ . By Proposition 4.1, we have $F_{Y|X}^{-1}(\\alpha|x)\\sim x$ , as $x\\to-\\infty$ To apply the next proposition to get the same conclusion, the generator is the gamma Laplace transform $\\psi(s)=(1+s)^{-1/\\delta}$ .\n<PARAGRAPH_SEPARATOR>\nExample 4.2 (Gumbel Lower Tail). The bivariate Gumbel copula CDF is\n<PARAGRAPH_SEPARATOR>\n$$\nC(u,v;\\delta)=\\exp\\{-[(-\\log u)^{\\delta}+(-\\log v)^{\\delta}]^{1/\\delta}\\},\\quad0<u<1,0<v<1,\\delta>1.\n$$\n<PARAGRAPH_SEPARATOR>\nThe conditional CDF is\n<PARAGRAPH_SEPARATOR>\n$$\nC_{V|U}(v|u;\\delta)=u^{-1}\\exp\\bigl\\{-[(-\\log u)^{\\delta}+(-\\log v)^{\\delta}]^{1/\\delta}\\bigr\\}\\biggl[1+\\biggl(\\frac{-\\log v}{-\\log u}\\biggr)^{\\delta}\\biggr]^{1/\\delta-1}.\n$$\n<PARAGRAPH_SEPARATOR>\nThe conditional quantile function $C_{V\\vert U}^{-1}(\\alpha\\vert u;\\delta)$ does not have a closed-form expression; it has the following asymptotic expansion:\n<PARAGRAPH_SEPARATOR>\n$$\n-\\log C_{V|U}^{-1}(\\alpha|u;\\delta)\\sim(-\\delta\\log\\alpha)^{1/\\delta}(-\\log u)^{1-1/\\delta},\\quad u\\to0.\n$$\n<PARAGRAPH_SEPARATOR>\nBy Proposition 4.1, we have $F_{Y|X}^{-1}(\\alpha|x)\\sim-(-2\\delta\\log\\alpha)^{1/(2\\delta)}|x|^{1-1/\\delta}$ , as $x\\to-\\infty$ . To apply the next proposition to get the same conclusion, the generator is the positive stable Laplace transform $\\begin{array}{r}{\\psi(s)=\\exp\\{-s^{-1/\\delta}\\}}\\end{array}$ .\n<PARAGRAPH_SEPARATOR>\nFor Archimedean and survival Archimedean copulas, the following proposition provides some links between tail dependence behavior and tail conditional distribution and quantile functions. The proof of the proposition is included in the supplementary material.\n<PARAGRAPH_SEPARATOR>\nProposition 4.2. Given the generator or Laplace transform (LT) $\\psi$ of an Archimedean copula, we assume the following.\n<PARAGRAPH_SEPARATOR>\n1. For the upper tail of $\\psi$ , $a s s\\rightarrow+\\infty$ ,\n<PARAGRAPH_SEPARATOR>\n$$\n\\psi(s)\\sim T(s)=a_{1}s^{q}\\exp(-a_{2}s^{r})\\quad\\mathrm{and}\\quad\\psi^{\\prime}(s)\\sim T^{\\prime}(s),\n$$\n<PARAGRAPH_SEPARATOR>\nwhere $a_{1}>0$ , $r=0$ implies $a_{2}=0$ and $q<0$ , and $r>0$ implies $r\\leq1$ and $q$ can be $0$ negative or positive. 2. For the lower tail of $\\psi$ , as $s\\to0^{+}$ , there is $M\\in(k,k+1)$ such that\n<PARAGRAPH_SEPARATOR>\n$$\n\\psi(s)=\\sum_{i=0}^{k}(-1)^{i}h_{i}s^{i}+(-1)^{k+1}h_{k+1}s^{M}+o(s^{M}),\\quad s\\to0^{+},\n$$\n<PARAGRAPH_SEPARATOR>\nwhere $h_{0}=1$ and $0<h_{i}<\\infty$ for $i=1,\\ldots,k+1.$ If $0<M<1$ , then $k=0$ .\n<PARAGRAPH_SEPARATOR>\nThen we have the following.\n<PARAGRAPH_SEPARATOR>\n• (Lower tail) If $v\\in(0,1)$ and $\\alpha\\in(0,1)$ are fixed, then as $u\\to0$ ,\n<PARAGRAPH_SEPARATOR>\n$$\nC_{V|U}(v|u)\\sim\\left\\{\\begin{array}{l l}{{1+\\left(q-1\\right)\\psi^{-1}(v)\\left(u/a_{1}\\right)^{-1/q}\\to1}}&{{i f r=0,}}\\ {{1-a_{2}^{1/r}r\\psi^{-1}(v)\\left(-\\log u\\right)^{1-1/r}\\to1}}&{{i f0<r<1,}}\\ {{\\mathrm{const}\\in\\left(0,1\\right)}}&{{i f r=1.}}\\end{array}\\right.\n$$\n<PARAGRAPH_SEPARATOR>\n$$\nC_{V|U}^{-1}(\\alpha|u)\\sim\\left\\{\\begin{array}{l l}{\\left(\\alpha^{1/(q-1)}-1\\right)^{q}\\cdot u\\rightarrow0}&{~i f r=0,}\\ {\\exp\\left[-\\left(-r^{-1}\\log\\alpha\\right)^{r}(-\\log u)^{1-r}\\right]\\rightarrow0}&{~i f0<r<1,}\\ {\\mathrm{const}\\in(0,1)}&{~i f r=1.}\\end{array}\\right.\n$$\n<PARAGRAPH_SEPARATOR>\nThe cases $r=0$ , $0<r<1$ and $r=1$ correspond to lower tail dependence, intermediate dependence and quadrant independence respectively.\n<PARAGRAPH_SEPARATOR>\n• (Upper tail) If $v\\in(0,1)$ and $\\alpha\\in(0,1)$ are fixed, then as $u\\rightarrow1$ ,\n<PARAGRAPH_SEPARATOR>\n$$\nC_{V|U}(v|u)\\sim\\left\\{\\begin{array}{l l}{-\\frac{\\psi^{\\prime}(\\psi^{-1}(v))}{h_{1}^{1/M}M}(1-u)^{(1-M)/M}\\to0}&{\\quad i f0<M<1,}\\ {\\mathrm{const}\\in(0,1)}&{\\quad i f M>1.}\\end{array}\\right.\n$$\n<PARAGRAPH_SEPARATOR>\n$$\n\\begin{array}{r l}&{C_{V|U}^{-1}(\\alpha|u)\\sim\\left\\{\\begin{array}{l l}{\\displaystyle1-\\left(\\alpha^{1/(M-1)}-1\\right)^{M}(1-u)\\to1\\quad}&{~i f0<M<1,}\\ {\\mathrm{const}\\in(0,1)\\quad}&{~i f M>1.}\\end{array}\\right.}\\end{array}\n$$\n<PARAGRAPH_SEPARATOR>\nThe cases $0~<~M~<~1$ and $M>1$ correspond to upper tail dependence and intermediate dependence/quadrant independence, respectively. Note that we do not cover the case of $M=1$ for the upper tail because it involves a slowly varying function.\n<PARAGRAPH_SEPARATOR>\nCombined with Proposition 4.1, the above proposition states that, for the lower tail, the three cases $r=0$ , $0<r<1$ and $r=1$ correspond to strongly linear, sublinear and asymptotic constant conditional quantile functions respectively; for the upper tail, the two cases $0<M<1$ and $M>1$ correspond to strongly linear and asymptotic constant conditional quantile functions respectively.\n<PARAGRAPH_SEPARATOR>\nTable 1 Simulation results for two explanatory variables. The table shows the root-mean-square error (RMSE), logarithmic score (LogS), quadratic score (QS), interval score (IS), and integrated Brier score (IBS) in different simulation cases. The arrows in the header indicate that lower RMSE, IS, and IBS; and higher LogS and QS are better. The numbers in parentheses are the corresponding standard errors."
  },
  {
    "qid": "statistic-posneg-1550-0-1",
    "gold_answer": "TRUE. The Lasso requires the $\\theta$-uniform irrepresentable condition, which is stronger than the stable nullspace property required by TBP. Therefore, TBP achieves sign recovery under a weaker condition on $X$ and $S^{0}$ compared to the Lasso.",
    "question": "Is it false that the Lasso requires a weaker condition on the design matrix $X$ for exact sign recovery compared to TBP?",
    "question_context": "Model Selection with Lasso-Zero: Sign Recovery and False Discovery Trade-offs\n\nThe following result provides a lower bound for the probability of correct sign recovery. <PARAGRAPH_SEPARATOR> Theorem 1. Under the linear model (1) and Assumptions (A)– (D), there exist universal constants $c,c^{\\prime},c^{\\prime\\prime}>0$ such that for any $\\rho\\in(0,1)$ , <PARAGRAPH_SEPARATOR> provided that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{s^{0}<\\displaystyle\\frac{c^{\\prime\\prime}\\nu^{2}}{(1+\\rho^{-1})^{2}}\\displaystyle\\frac{n}{\\log p},}\\ {\\left\\vert\\beta^{0}\\right\\vert_{\\mathrm{min}}>C_{\\rho}\\displaystyle\\frac{2\\sqrt{2}\\sigma\\sqrt{p}}{\\nu(\\sqrt{p/n}-1)},}\\end{array} $$ <PARAGRAPH_SEPARATOR> awchheireev $\\begin{array}{l}{{C_{\\rho}={\\frac{2(3+\\rho)}{1-\\rho}}}}\\end{array}$ 2(13+ρρ ) . Consequently, if p/n → C > 1, TBP ency with $\\beta_{\\mathrm{min}}^{0}~=~\\Omega(\\sqrt{n})$ $s^{0}=$ $O(n/\\log n)$ . <PARAGRAPH_SEPARATOR> The proof is provided in Appendix A. Its main argument is that, no matter what $\\mathrm{sign}(\\beta_{S^{0}}^{0})$ is, TBP recovers the signs of $\\beta^{0}$ under some beta-min condition and the stable nullspace property: <PARAGRAPH_SEPARATOR> $$ \\beta\\in\\ker X\\implies\\left\\lVert\\beta_{S^{0}}\\right\\rVert_{1}\\leq\\rho\\left\\lVert\\beta_{\\overline{{S^{0}}}}\\right\\rVert_{1},\\qquad\\rho\\in(0,1). $$ <PARAGRAPH_SEPARATOR> Let us compare this to the theory of the Lasso. It is known (Zhao and Yu 2006) that the Lasso achieves sign consistency under the $\\theta$ -irrepresentable condition <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\left\\lVert X\\frac{T}{S^{0}}X_{S^{0}}(X_{S^{0}}^{T}X_{S^{0}})^{-1}\\mathrm{sign}(\\beta_{S^{0}}^{0})\\right\\rVert_{\\infty}<\\theta,}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\theta\\in\\quad(0,1)$ . For sign consistency independently of $\\mathrm{sign}(\\beta_{S^{0}}^{0})$ , the $\\theta$ -uniform irrepresentable condition— requiring (10) for all $\\mathrm{sign}(\\beta_{S^{0}}^{0})$ —should be assumed. It is known (see Bühlmann and van de Geer 2011, Theorem 7.2) that the $\\theta$ -uniform irrepresentable condition is stronger than the $(L,S^{0})$ -compatibility condition for any $L\\in(1,\\theta^{-1})$ , which states that there exists a $\\phi>0$ such that <PARAGRAPH_SEPARATOR> $$ \\big\\|\\beta_{\\overline{{S^{0}}}}\\big\\|_{1}\\leq L\\big\\|\\beta_{S^{0}}\\big\\|_{1}\\implies\\big\\|\\beta_{S^{0}}\\big\\|_{1}^{2}\\leq\\frac{\\big|S^{0}\\big|}{n\\phi^{2}}\\|X\\beta\\|_{2}^{2}. $$ <PARAGRAPH_SEPARATOR> Clearly, (11) implies (9) for $\\rho=L^{-1}$ , so the $\\theta$ -uniform irrepresentable condition implies the stable nullspace property with any constant $\\rho~\\in~(\\theta,1)$ . Hence, TBP achieves sign recovery under a weaker condition on $X$ and $S^{0}$ compared to the Lasso."
  },
  {
    "qid": "statistic-posneg-1612-0-0",
    "gold_answer": "FALSE. The Koksma–Hlawka inequality is correctly stated as bounding the absolute error by the product of the total variation of the function and the discrepancy of the point set. However, the discrepancy measure used in QMC integration is not the Kolmogorov–Smirnov statistic as implied, but rather a specific measure of uniformity (star discrepancy) tailored to the integration domain. The inequality is valid for functions with bounded variation in the sense of Hardy and Krause, not for all integrable functions.",
    "question": "Is the Koksma–Hlawka inequality, which states that the absolute error of the QMC integration approximation is bounded by the product of the total variation of the function and the discrepancy of the point set, a correct representation of the theoretical foundation for QMC integration?",
    "question_context": "Quasi-Monte Carlo Integration in Generalized Linear Mixed Models\n\nIn this paper, we propose to use the QMC approximation to solve the integration problem in the GLMM. In principle, the QMC approach generates integration nodes that are scattered uniformly on the integration domain. These nodes, called quasi-random numbers, tend to fill the space as fully as possible. The approach is also related to the computer space-filling technique (Bates et al., 1996). This paper is structured as follows. In Section 2, we briefly review the definition of the GLMM and highlight the problem of deriving the likelihood. In Section 3, we introduce the principle of the QMC approximation and discuss the generation of quasi-random numbers. In Section 4, the QMC approach is used to approximate the integrated likelihood and then calculate the maximum likelihood estimates (MLE) of fixed effects and variance components in the GLMM. In Section 5, the proposed approach is used to analyze the infamous salamander mating data, which involves six 20-dimensional integrals in the likelihood. In Section 6, simulation studies that have the same design protocol as the salamander data are conducted. Numerical comparisons are made to Gibbs sampling (Karim and Zeger, 1992) and the PQL estimation (Breslow and Clayton, 1993). In Section 7, discussions on some related issues are given. Technical details on the second-order derivatives of the log-likelihood are deferred to the Appendix."
  },
  {
    "qid": "statistic-posneg-1926-0-1",
    "gold_answer": "FALSE. Proposition 4.1 correctly establishes the dual representation. The attractor C_A* is indeed expressible both as an extreme value copula (with dependence function A* and log-generator) and as an Archimax copula (with original A and powered log-generator φ*). This equivalence is verified through the regular variation condition φ(1-1/t) ∈ RV_{-m} and the convergence proof involving the limit of s_n in the proposition's derivation.",
    "question": "Does Proposition 4.1 incorrectly state that the maximum attractor C_A* of an Archimax copula C_φ,A with φ(1-1/t) ∈ RV_{-m} can be represented equivalently with generators A* and φ(t) = log(1/t) or with A and φ*(t) = logᵐ(1/t)?",
    "question_context": "Archimax Copulas and Their Extreme Value Attractors\n\nThe main purpose of this paper, therefore, is to propose a scheme for building large families of analytically tractable bivariate distributions that are reasonably convenient to simulate and that belong to the domain of attraction of a predetermined extreme value distribution for maxima. A class of distributions that meets this requirement is introduced in Sect. 2. It can accomodate arbitrary marginals and encompasses all bivariate maximum extreme value distributions, the frailty models of Oakes (1989) as well as the Archimedean system of copulas introduced by Genest and MacKay (1986a, 1986b), which itself includes many well-known families of bivariate distributions (cf., e.g., Chap. 4 of Nelsen, 1999). Examples of this new class of so-called Archimax distributions are exhibited in Sect. 3. In Sect. 4, the maximum and minimum attractors of class members are determined, under fairly general conditions. In Sect. 5, a distributional result is presented which leads to effective simulation algorithms; it also provides insight into the dependence structure of Archimax distributions, as described in Sect. 6. Finally, this tool is exploited in Sect. 7 to explore briefly the small-sample behavior of the threshold method of Joe et al. (1992) in a variety of simulated conditions."
  },
  {
    "qid": "statistic-posneg-540-0-0",
    "gold_answer": "TRUE. The second term in the product, √(𝔼[ν²]/𝔼[ν]²), represents the coefficient of variation of the squared inverse length scales (ν). This term increases with the relative variability in the length scales of the target components, indicating that the expected number of apogees is indeed influenced by this variability. The first term, T√𝔼[ν], relates the time interval to the overall length scale of the target, while the second term adjusts for the heterogeneity in component scales.",
    "question": "Is the expected number of apogees per unit time, given by 𝔼[N(T)] ∝ T√𝔼[ν] × √(𝔼[ν²]/𝔼[ν]²), dependent on the relative variability of the squared inverse length scales of the target components?",
    "question_context": "Gaussian Process Limit in Hamiltonian Monte Carlo for Product Targets\n\nThe paper discusses the behavior of Hamiltonian Monte Carlo (HMC) for product targets with a potential defined as a sum of component functions, each with its own length scale. The scaled dot product of momentum and gradient of the potential is shown to converge to a stationary Gaussian process as the number of components tends to infinity. The expected number of apogees (local maxima in the potential along the path) per unit time is derived, highlighting the relationship between the target's length scales and the integration time."
  },
  {
    "qid": "statistic-posneg-1713-0-0",
    "gold_answer": "FALSE. Explanation: The context explicitly states that T* (not T) is the robust statistic, maintaining its asymptotic χ² distribution under broader conditions (independence of Y and Z with finite variances for s₁(Y) and t₁(Z)). T relies on the model M's covariance structure Ω(λ,φ), making it sensitive to misspecification outside M. The distinction arises because T* uses sample covariance matrices (S_{s₁s₁}, etc.), which are distribution-free under independence, whereas T depends on model-based Ω estimates.",
    "question": "Is the statement 'The Rao score statistic T is robust to deviations from the model M as long as Y and Z are independent' true or false based on the provided context?",
    "question_context": "Rao Score Tests for Independence in Exponential Family Models\n\nIn this section we investigate certain tests of independence which can be included in the framework of Rao score tests. Let\n\n$$\n\\begin{array}{r}{{\\boldsymbol{x}}=(y,z),\\quad\\theta=(\\lambda,\\varphi),\\quad u(\\boldsymbol{x})=(s(y)^{\\mathrm{T}},t(z)^{\\mathrm{T}})^{\\mathrm{T}}}\\end{array}\n$$\n\nand suppose the model $M(\\theta)$ now takes the form\n\n$$\nf(y,z;\\lambda,\\varphi)\\propto\\exp\\left\\{\\lambda^{\\mathrm{T}}s(y)+\\varphi^{\\mathrm{T}}t(z)\\right\\}\n$$\n\nwith respect to a product measure $\\pmb{\\nu}(d x)=\\pmb{\\sigma}(d y)\\pmb{\\tau}(d z)$ . Thus under $M(\\theta),y$ and $z$ are independent. It is convenient here to denote the dimensions of $s(y)$ and $t(z)$ by $\\pmb{p}$ and $\\pmb q,$ respectively.\n\nNext  partition $\\begin{array}{r}{s(\\boldsymbol{y})=\\{s_{1}(\\boldsymbol{y})^{\\mathsf{T}},s_{2}(\\boldsymbol{y})^{\\mathsf{T}}\\}^{\\mathsf{T}},}\\end{array}$ where $\\mathfrak{s}_{1}(y)$ is a ${\\pmb p}_{1}$ -vector and $t(z)=$ $\\{t_{1}(z)^{\\mathsf{T}},t_{2}(z)^{\\mathsf{T}}\\}^{\\mathsf{T}}$ where $t_{1}(z)$ is a $q_{1}$ -vector. Let $v(x)=s_{1}(y)\\otimes t_{1}(z)$ a ${\\pmb{p}}_{1}{\\pmb q}_{1}$ -vector representing the interaction between $s_{1}(y)$ and $t_{1}{\\left(z\\right)}$ . We use ${\\pmb v}({\\pmb x})$ to test for departures from M. Note that not all of the interactions in the pq-vector $s(y)\\bigotimes t(z)$ need be used to form ${\\pmb v}({\\pmb x})$ . For example, to test for independence in the bivariate normal distribution for $(y,z)$ , we would set $\\qquads_{1}(y)=y_{\\mathrm{(}}$ $s_{2}(y)=y^{2}$ $t_{1}(z)=z_{1}$ $t_{2}(z)=z^{2}$ and $v\\left(x\\right)=s_{1}(y)t_{1}(z)=y z,$ with $y\\in R,z\\in R.$\n\nThe independence between $\\mathbf{Y}$ and $z$ under $\\mathcal{M}$ simplifies many of the calculations for Rao's score statistic. For example, if we let $\\mu_{s}(\\lambda)=E\\{s(Y)\\}$ and $\\mu_{t}(\\varphi)=E\\{t(Z)\\}.$ , it is easy to see that $E\\{s(Y)\\otimes t(Z)\\}=\\mu_{s}(\\lambda)\\otimes\\mu_{t}(\\varphi)$ . Similarly the $\\left(p+q\\right)$ - dimensional covariance matrix of $\\{s(Y)^{\\mathsf{T}},t(Z)^{\\mathsf{T}}\\}^{\\mathsf{T}}$ can be partitioned as\n\n$$\n\\Omega(\\lambda,\\varphi)=\\left[\\begin{array}{c c}{{\\Omega_{s s}(\\lambda)}}&{{0}}\\\\ {{0}}&{{\\Omega_{\\iota\\iota}(\\varphi)}}\\end{array}\\right],\n$$\n\nwhere each diagonal block depends on only one parameter.\n\nNext let $\\left(y_{i},z_{i}\\right)\\left(i=1,\\ldots,n\\right)$ be a random sample from $M(\\lambda,\\varphi)$ . Denote the sample mean vector and sample covariance matrix of $\\{(s(y_{i})^{\\mathsf{T}},t(z_{i})^{\\mathsf{T}})^{\\mathsf{T}}\\}$ by $(\\bar{\\bar{s}}^{\\mathsf{T}},\\bar{t}^{\\mathsf{T}})^{\\mathsf{T}}$ and $s,$ where $s$ is defined with a divisor of $\\pmb{n}$ rather than $n-1$ . The maximum likelihood estimates satisfy $\\mu_{s}(\\hat{\\lambda})=\\bar{s},\\mu_{t}(\\hat{\\varphi})=\\bar{t}.$ After a little algebra, it can be shown that Rao's score statistic takes the form\n\n$$\nT=\\nmid\\mathrm{tr}\\{\\Omega_{s_{1}s_{1}}(\\hat{\\lambda})^{-1}S_{s_{1}t_{1}}\\Omega_{t_{1}t_{1}}(\\hat{\\varphi})^{-1}S_{t_{1}s_{1}}\\},\n$$\n\nwhere $S_{s_{1}t_{1}}$ is the $\\textstyle p_{1}\\times q_{1}$ block of $s$ correspondingto $\\mathfrak{s}_{1}(y)$ and $t_{1}(z)$ , etc. Under $\\mathcal{M},T$ has an asymptotic $\\chi_{p_{1}q_{1}}^{2}$ distribution.\n\nA statistic similar to $T$ is\n\n$$\nT^{*}=n\\mathrm{~tr~}\\{S_{s_{1}s_{1}}^{-1}S_{s_{1}t_{1}}S_{t_{1}t_{1}}^{-1}S_{t_{1}s_{1}}\\},\n$$\n\nwhich is asymptotically equivalent to $T$ under $\\mathcal{M}.$ Note that $T^{*}$ can be written as $\\pmb{n}$ times the sum of squared sample canonical correlations; see, e.g. Mardia et al. (1979, Ch. 10). In the terminology of Kent (1982b), $T^{*}$ is more ‘robust' than $T$ because $T^{*}$ continues to have an asymptotic $\\chi_{p_{1}q_{1}}^{2}$ distribution when sampling outside the family $\\mathcal{M}_{\\sf}$ provided $\\pmb{Y}$ and $Z$ are still independent and provided the variances of $\\mathfrak{s}_{1}(Y)$ and $t_{1}(Z)$ are finite. For applications of (5·2) to testing problems in directional data analysis, see Mardia & Puri (1978) and Jupp & Mardia (1980).\n\nIn some applications $\\pmb{T}$ and $T^{*}$ reduce to the same statistic. If $\\mathfrak{s}_{2}(y)$ containsthe squares and cross-products of the components of $\\mathfrak{s}_{1}(y)$ , then $\\Omega_{s_{1}s_{1}}(\\hat{\\lambda})=S_{s_{1}s_{1}};$ similarly for $t_{2}(z)$ . As an example suppose we are testing for dependence between $\\pmb{Y}$ and $z$ in a $(p_{1}+q_{1})$ -dimensional normal distribution. In the notation of $\\S4$ we have\n\n$$\ns_{1}(y)=y=y_{(1)},\\quad s_{2}(y)=y_{(2)},\\quad t_{1}(z)=z=z_{(1)},\\quad t_{2}(z)=z_{(2)}.\n$$"
  },
  {
    "qid": "statistic-posneg-2086-2-0",
    "gold_answer": "TRUE. According to the paper, when $\\gamma > 1$, the distribution $H$ is relatively flat, leading to faster convergence. The stopping time $t(k) \\sim k \\log k$ is sufficient to ensure that $\\lim_{k \to \\infty} P\\{\tau(0) > t(k)\\} = 0$, indicating polynomial-time convergence to equilibrium.",
    "question": "Is it true that for $\\gamma > 1$, the Gibbs sampler chain converges to equilibrium in polynomial time $t(k) \\sim k \\log k$?",
    "question_context": "Gibbs Sampler Convergence Analysis in Markov Chain Monte Carlo\n\nThe model (Cassandro et al., 1991) is based on a graph with $\\pmb{k}$ sites and two colours $\\{+1,-1\\}$ The space of all possible configurations is denoted by $\\Omega=\\{x_{1},x_{2},...,x_{k};x_{i}\\in\\{+1,-1\\}\\}$ Consider the random distribution $\\pi(x)=\\{1/Z(\\beta)\\}\\exp\\{-\\beta H(x)\\}$ where each $H(x)$ is drawn independently from\n\n$$\n\\begin{array}{r}{H(x)=0\\qquad\\mathrm{with~probability~}1-k^{-\\gamma}\\qquad}\\ {=-\\log k\\qquad\\mathrm{with~probability~}k^{-\\gamma},\\qquad}\\end{array}\n$$\n\nand then held fixed. The parameter $\\gamma>0$ controls the roughness of $H.$ Let $X(0),X(1),...,X(t),...$ be a Gibbs sampler chain, started uniformly in $\\pmb{\\Omega}$ .At each step $t$ a single randomly chosen site $i(t)$ is updated. A simple way to run the Gibbs sampler is to put $x_{i(t)}$ equal to $+1$ or $^{-1}$ if a [0, 1]-uniform random number $\\pmb{u}(t)$ is larger or smaller respectively than $\\{1+\\exp(\\beta X_{i(t)}(t-1)[H\\{X^{i(t)}(t-1)\\}-$ $H\\{X(t-1)\\}])\\}^{-1}$ , where $X^{i(t)}(t-1)$ is the configuration obtained from $X(t-\\mathrm{i})$ by flipping the variable at site $i(t)$\n\nIn the coupling method, a second Gibbs sampler chain. $Y(t)$ is run with the same random numbers $i(t)$ and ${\\pmb u}(t)$ as for $X(t)$ . However, $Y(t)$ is started at equilibrium. Once $X(t)$ and $Y(t)$ meet, the two trajectories continue together, which means that $X(t)$ has also reached equilibrium. Let $\\tau(0)=\\operatorname*{inf}\\{t\\geqslant0$ $d(X(t),Y(t))=0\\}$ where $d(x,y)=\\Sigma_{i=1}^{k}\\mathbf{1}_{x_{i}\\neq y_{i}}$ is the Hamming distance. The coupling inequality\n\n$$\n{\\frac{1}{2}}\\sum_{x\\in\\Omega}\\mid P_{X(0)}\\{X(t)=x\\}-\\pi(x)\\mid\\leqslant P\\{\\tau(0)>t\\}\n$$\n\nleads us to study the tail distribution of $\\pmb{\\tau}(0)$ to bound above the rate of convergence (in total variation or law) of the Gibbs sampler chain. The probability on the right-hand side is on the coupled product space. The aim is to determine a stopping time $t=t(k)$ , depending on the number of sites, which guarantees, at least, that $\\begin{array}{r}{\\operatorname*{lim}_{k\\to\\infty}[P\\{\\tau(0)>t(k)\\}]=0}\\end{array}$ . We expect convergence to be fast if $\\gamma$ is large so that $\\pmb{H}$ is rather flat. It is easy to show (Cassandro et al., 1991) that, for $\\gamma>1$ and $t(k)\\sim k\\log k$ this limit holds. The situation is drastically different when $\\gamma<1$ (Frigessi and den Hollander, 1992): here it takes much longer even to bring the two trajectories close together. Let $\\pmb{\\tau}(M)=\\mathbf{inf}\\{t\\geqslant0$ $d(X(t)$ $Y(t)){=}M\\}$ If $\\beta>\\gamma,\\gamma<1$ , and $\\delta<\\frac{1}{2}$ , then $\\begin{array}{r}{\\operatorname*{lim}_{k\\to\\infty}[P\\{\\tau(\\delta k)>t(k)\\}]=0}\\end{array}$ only if $t(k)\\sim\\exp(c k^{1-\\gamma})$ as $k{\\rightarrow}{\\infty}$ ， for some positive constant c. This indicates that a time exponential in $\\pmb{k}$ is needed to reach equilibrium at high values of $\\beta$ if $\\pmb{H}$ tends to be rough. In the intermediate regime $(\\beta<\\gamma,\\gamma<1)$ the two trajectories come close together quickly (in polynomial time in $\\pmb{k}$ but need an exponential time again to couple."
  },
  {
    "qid": "statistic-posneg-128-2-1",
    "gold_answer": "FALSE. While the condition $E(\\tilde{X}_{r}|X_{r})=X_{r}$ implies that the distorting effects of $U$ average out to zero in expectation, it does not mean that the effects are completely eliminated. The condition ensures that the mean of the adjusted covariates $\\tilde{X}_{r}$ given $X_{r}$ equals $X_{r}$, but individual observations may still be affected by $U$. The context states that the effects 'average out to 0', not that they are entirely removed.",
    "question": "The condition $E(\\tilde{X}_{r}|X_{r})=X_{r}$ ensures that the distorting effects of the confounding variable $U$ on the covariates $X_{r}$ are completely eliminated. Is this interpretation correct?",
    "question_context": "Covariate-Adjusted Regression and Confounding Variable Analysis\n\nCondition A5. As $n\\to\\infty$ , $n^{-1}X^{\\mathrm{T}}X\\to\\mathcal{X}$ in probability, where the limiting $\\left(p+1\\right)\\times\\left(p+1\\right)$ matrix $\\mathcal{X}$ is nonsingular. <PARAGRAPH_SEPARATOR> Condition A6. The function $h(u)=\\int x g(x,u)d x$ is uniformly Lipschitz, where $g(.,.)$ is the joint density function of $\\tilde{X}$ and $U.$ . <PARAGRAPH_SEPARATOR> These are mild conditions that are satisfied in most practical situations. Bounded covariates are standard in asymptotic theory for least squares regression, as are Conditions A2 and A5 (Lai et al., 1979). The identifiability conditions, Condition A4, are equivalent to <PARAGRAPH_SEPARATOR> $$ E(\\tilde{Y}|X)=E(Y|X),\\quad E(\\tilde{X}_{r}|X_{r})=X_{r}. $$ <PARAGRAPH_SEPARATOR> This means that the confounding of $Y$ by $U$ does not change the mean regression function, and the distorting effects of the confounding variable $U$ average out to 0."
  },
  {
    "qid": "statistic-posneg-806-3-1",
    "gold_answer": "TRUE. The condition shows that the probability of $\\hat{H}^{'}$ being less than or equal to $C_{3}$ on $\\mathcal{I}$ decreases faster than any polynomial in $n$ as $n\\to\\infty$. This implies that $\\hat{H}^{'}$ is strictly increasing on $\\mathcal{I}$ with high probability, as the probability of it not being strictly increasing (i.e., being less than or equal to $C_{3}$) becomes negligible for large $n$.",
    "question": "Does the condition $P\\left[\\operatorname*{inf}_{x\\in\\mathcal{I}}\\{\\hat{H}^{'}(x)\\}\\leqslant C_{3}\\right]=O(n^{-\\lambda})$ imply that $\\hat{H}^{'}$ is strictly increasing on $\\mathcal{I}$ with high probability as $n\\to\\infty$?",
    "question_context": "High Order Data Sharpening for Density Estimation\n\nTo examine more explicitly the effects of the method of estimation on wiggliness of the resulting estimator, we kept track of the average number of extrema and inflection points for each sample size, estimation method and bandwidth. Ignoring for the time being the locally parametric estimators, there was a consistent ranking for any given bandwidth, sample size and true density. The second-order kernel estimator $\\hat{f}$ had fewest wiggles, followed by the sharpened estimators $\\tilde{f_{4}}$ and $\\tilde{f_{6}}$ , followed finally by the higher order kernel estimators $\\check{f}_{4}$ and $\\check{f}_{6}$ . Of course, these measures were decreasing functions of $h$ , but the ranking never varied. The locally parametric estimators were less consistent relative to the others, but the local log-linear estimator $\\hat{f}_{\\mathrm{lin}}$ always fell between $\\hat{f}\\mathrm{and}\\check{f}_{4}$ , whereas the local log-quadratic estimator $\\hat{f}_{\\mathrm{quad}}^{-}$ always had more wiggles than $\\tilde{f_{6}}$ but fewer than $\\check{f}_{6}$ . <PARAGRAPH_SEPARATOR> # 4. Theoretical results <PARAGRAPH_SEPARATOR> Assume that $f$ is bounded away from 0 and has $r$ bounded derivatives in an interval $\\mathcal{T}=[a,b]$ , that $K$ is a symmetric, compactly supported density with $r-3$ continuous derivatives and that $h=h(n)\\to0$ and $\\log(n)/n h^{r}$ is bounded. Define $\\hat{f}_{r}$ and $\\tilde{f}_{r}$ by equations (2.5) and (2.7) respectively. Let $\\hat{g}_{r}$ denote either $\\hat{f}_{r}$ or $\\tilde{f_{r}}$ . Then, for each interval $\\mathcal{I}=[c,d]$ for which $a<c<d<b$ , <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\underset{x\\in\\mathcal{I}}{\\operatorname*{sup}}|E\\{\\hat{g}_{r}(x)\\}-f(x)|=O\\{h^{r}+(n h)^{-1}\\}}\\\\{\\underset{x\\in\\mathcal{I}}{\\operatorname*{sup}}|n h\\operatorname*{var}\\{\\hat{g}_{r}(x)\\}-f(x)\\sigma_{r}^{2}|\\rightarrow0,}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\sigma_{r}^{2}>0$ . In particular, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{\\displaystyle\\sigma_{4}^{2}=\\int\\left\\{K(v)-\\frac{1}{2}\\kappa_{2}\\int K^{\\prime}(u+v)K^{\\prime}(u)\\mathrm{d}u\\right\\}^{2}\\mathrm{d}v},~}\\\\{{\\displaystyle\\sigma_{6}^{2}=\\int\\left[K(v)-\\int K^{\\prime}(u+v)\\left\\{\\frac{1}{2}\\kappa_{2}K^{\\prime}(u)+\\left(\\frac{1}{24}\\kappa_{4}-\\frac{1}{2}\\kappa_{2}^{2}\\right)K^{(3)}(u)\\right\\}\\mathrm{d}u\\right]^{2}\\mathrm{d}v}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> The above conditions are also sufficient for equation (2.2). <PARAGRAPH_SEPARATOR> Result (4.1) may be refined to $E\\{\\hat{g}_{r}(x)\\}-f(x)=h^{r}\\beta_{r}(x)+O\\{(n h)^{-1}\\}+o(h^{r})$ , where $\\beta_{r}$ denotes a continuous functional on $\\mathcal{I}$ . In particular, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{\\beta_{4}=\\bigg(\\frac{\\kappa_{4}}{24}-\\frac{\\kappa_{2}^{2}}{2}\\bigg)f^{(4)}+\\frac{\\kappa_{2}^{2}}{2}\\frac{f^{(3)}f^{\\prime}}{f}+\\frac{\\kappa_{2}^{2}}{2}\\frac{f^{\\prime\\prime2}}{f}-\\frac{7\\kappa_{2}^{2}}{8}\\frac{f^{\\prime\\prime}f^{\\prime2}}{f^{2}}+\\frac{\\kappa_{2}^{2}}{4}\\frac{f^{\\prime4}}{f^{3}},}}\\\\{{\\beta_{6}=\\bigg(\\frac{\\kappa_{6}}{720}-\\frac{\\kappa_{4}\\kappa_{2}}{12}+\\frac{3\\kappa_{2}^{3}}{8}\\bigg)f^{(6)}+\\bigg(\\frac{\\kappa_{4}\\kappa_{2}}{24}-\\frac{3\\kappa_{2}^{3}}{8}\\bigg)\\frac{f^{(5)}f^{\\prime}}{f}+\\bigg(\\frac{\\kappa_{4}\\kappa_{2}}{12}-\\frac{7\\kappa_{2}^{3}}{8}\\bigg)\\frac{f^{(4)}f^{\\prime\\prime}}{f}}}\\\\{{\\ -\\bigg(\\frac{\\kappa_{4}\\kappa_{2}}{16}-\\frac{3\\kappa_{2}^{3}}{4}\\bigg)\\frac{f^{(4)}f^{\\prime2}}{f^{2}}+\\bigg(\\frac{\\kappa_{4}\\kappa_{2}}{24}-\\frac{\\kappa_{2}^{3}}{2}\\bigg)\\frac{(f^{(3)})^{2}}{f}-\\bigg(\\frac{\\kappa_{4}\\kappa_{2}}{12}-\\frac{9\\kappa_{2}^{3}}{4}\\bigg)\\frac{f^{(3)}f^{\\prime\\prime}}{f^{2}}}}\\\\{{\\ +\\bigg(\\frac{\\kappa_{4}\\kappa_{2}}{24}-\\frac{55\\kappa_{2}^{3}}{48}\\bigg)\\frac{f^{(3)}f^{\\prime3}}{f^{3}}+\\frac{\\kappa_{2}^{3}}{2}\\frac{f^{\\prime\\prime3}}{f^{2}}-\\frac{35\\kappa_{2}^{3}}{16}\\frac{f^{\\prime\\prime2}f^{\\prime2}}{f^{3}}+\\frac{3\\kappa_{2}^{3}}{2}\\frac{f^{\\prime\\prime}f^{\\prime }}} $$ <PARAGRAPH_SEPARATOR> Outline proofs of results (4.1)±(4.3) will be given in Appendix A. <PARAGRAPH_SEPARATOR> The definition of $\\hat{\\gamma}_{r}$ , and hence of $\\hat{f}_{r}$ , requires the function $\\hat{H}_{r}$ to be monotone. We claim that under very mild conditions this is true with high probability. Indeed, let us assume that $f$ is bounded away from 0 on the interval $\\mathcal{T}=[a,b]$ and has a bounded derivative there. (On this occasion we require no more than one derivative of $f.$ ) Suppose also that $K$ has $r+1$ bounded derivatives, $h\\rightarrow0$ and $n h^{3+\\varepsilon}\\to\\infty$ for some $\\varepsilon>0$ . Let $\\mathcal{I}=[c,d]$ where $a<c<d<b $ . Then it may be shown from Bernstein's inequality that for constants $C_{1}$ , $C_{2}>0$ , and all $\\lambda>0$ , <PARAGRAPH_SEPARATOR> $$ P\\left[\\operatorname*{inf}_{x\\in\\mathcal{I}}\\left\\{\\hat{f}(x)\\right\\}\\leqslant C_{1}\\right]=O(n^{-\\lambda}), $$ <PARAGRAPH_SEPARATOR> $$ P\\bigg\\{h^{j-1}\\operatorname*{sup}_{x\\in\\mathcal{I}}\\big|\\hat{f}^{(j)}(x)\\big|>C_{2}\\bigg\\}=O(n^{-\\lambda}) $$ <PARAGRAPH_SEPARATOR> for $1\\leqslant j\\leqslant r+1$ . It may be shown from these results that, for some $C_{3}>0$ , <PARAGRAPH_SEPARATOR> $$ P\\left[\\operatorname*{inf}_{x\\in\\mathcal{I}}\\{\\hat{H}^{'}(x)\\}\\leqslant C_{3}\\right]=O(n^{-\\lambda}). $$ <PARAGRAPH_SEPARATOR> Therefore, the probability that $\\hat{H}^{'}$ is strictly increasing on $\\mathcal{I}$ converges to 1 faster than the inverse of any polynomial in $n$ , as $n\\to\\infty$ . <PARAGRAPH_SEPARATOR> # Acknowledgements <PARAGRAPH_SEPARATOR> We are grateful to two reviewers and the Joint Editor for their helpful comments on the first version of our paper."
  },
  {
    "qid": "statistic-posneg-91-0-1",
    "gold_answer": "TRUE. The exclusion restriction assumption states that the response probabilities for always-takers and never-takers do not depend on the assigned treatment but only on the received treatment. Since always-takers always receive the experimental treatment and never-takers always receive the standard treatment, regardless of their assigned treatment, their response probabilities are equal across assigned treatment groups. This is explicitly stated in the context as $\\pi_{i A}^{(1)}=\\pi_{i A}^{(0)}$ and $\\pi_{i N}^{(1)}=\\pi_{i N}^{(0)}$ for $i=1,2$.",
    "question": "Does the exclusion restriction assumption imply that the response probabilities for always-takers and never-takers are equal across assigned treatment groups, i.e., $\\pi_{i A}^{(1)}=\\pi_{i A}^{(0)}$ and $\\pi_{i N}^{(1)}=\\pi_{i N}^{(0)}$ for $i=1,2$?",
    "question_context": "Proportion Ratio Estimation in Non-Compliance RCT with MAR Outcomes\n\nIn this paper, we focus the discussion on estimation of the $P R$ under a non-compliance RCT with outcomes missing at random (MAR). Using an approach similar to that proposed elsewhere (Brunner and Neumann, 1985; Bernhard and Compagnone, 1989), we derive in Sections 2.1–2.3 the maximum likelihood estimator (MLE) for the $P R$ under various assumptions, including the model in which there is no missingness, the model in which missingness depends on only assigned treatments, and the model in which missingness depends on both assigned and received treatments. In Section 2.4, we develop three asymptotic interval estimators in closed form for the $P R$ under the model in which missingness depends on assigned and received treatments. We use Monte Carlo simulation to study the performance of point estimators developed in Section 3.1 and the performance of the three asymptotic interval estimators in Section 3.2. We note that the point estimator for missing completely at random (MCAR) can be subject to a serious bias under various models with MAR. By contrast, we note that the point estimators developed in this paper can perform well when the number of patients per assigned treatment is large. Further, we find that the asymptotic interval estimator using Wald’s statistic tends to lose accuracy with respect to the coverage probability, while the interval estimator using the logarithmic transformation (Katz et al., 1978; Fleiss, 1981; Lui, 2002) tends to lose precision with respect to the average length. Finally, we note that an interval estimator, formed by an ad hoc combination of the previous two estimators, can perform well in almost all situations considered here. We apply the data taken from a multiple risk factor intervention trial for reducing the mortality of coronary heart disease (Multiple Risk Factor Intervention, 1982) in Section 3.3 to illustrate the bias of the point estimator for MCAR, and the use of the above point and interval estimators under the assumed MAR."
  },
  {
    "qid": "statistic-posneg-1421-1-3",
    "gold_answer": "TRUE. Under the conditions specified in Theorem 2, including polynomial ergodicity of the Markov chain and appropriate growth rates for the batch size bn, the multivariate batch means estimator Σn is strongly consistent, meaning Σn → Σ with probability 1 as n → ∞. This ensures reliable estimation of the covariance matrix for constructing confidence regions.",
    "question": "Is the multivariate batch means estimator Σn strongly consistent under the conditions that the batch size bn → ∞ and n/bn → ∞ as n → ∞?",
    "question_context": "Multivariate Sequential Termination Rules and Effective Sample Size in Markov Chain Monte Carlo\n\nThe paper discusses multivariate sequential termination rules for Markov Chain Monte Carlo (MCMC) simulations, focusing on constructing asymptotically valid confidence regions. It introduces a fixed-volume sequential stopping rule that terminates the simulation when the volume of the confidence region, adjusted by a relative metric, falls below a specified tolerance. The paper also defines a multivariate effective sample size (ESS) and provides a lower bound for ESS based on the desired precision and confidence level."
  },
  {
    "qid": "statistic-posneg-703-0-0",
    "gold_answer": "FALSE. The recurrence relation for \\( s_{2n} \\) should account for the square of the new term \\( t_{1n} = 1/n \\) added to the triangular scheme. The correct recurrence relation should be \\( s_{2n} = s_{2,n-1} + (2/n)s_{1,n-1} + (1/n)^2 \\), as the square of the new term \\( t_{1n} \\) must be included in the sum of squares.",
    "question": "Is the recurrence relation for the second moment \\( s_{2n} = s_{2,n-1} + (2/n)s_{1,n-1} + 1/n \\) correctly derived from the triangular scheme of expected order statistics \\( t_{rn} \\)?",
    "question_context": "Properties of Exponential Ordered Scores and Their Moments\n\nThe present paper investigates some tests based on exponential scores. Of course, in practice, formal tests of significance are likely to be only part of the whole analysis. An important thing, for instance, is likely to be graphical analysis of the survivor curves of the separatesetsof data. <PARAGRAPH_SEPARATOR> # 2. THE EXPONENTIAL SCORES <PARAGRAPH_SEPARATOR> In this Section we examine some properties of the finite population consisting of the expected order statistics for a random sample of size $n$ from the distribution $e^{-y}$ Denoteby $t_{r n}$ the expected value of the rth observation in increasing order of magnitude. It is well known that <PARAGRAPH_SEPARATOR> $$ t_{r n}=1/n+...+1/(n-r+1)\\quad(r=1,...,n), $$ <PARAGRAPH_SEPARATOR> so that in particular <PARAGRAPH_SEPARATOR> $$ \\scriptstyle t_{1n}=1/n,\\scriptstyle\\left.\\begin{array}{l}{{\\scriptstyle1}}\\\\ {{\\scriptstyle t_{2n}=1/n+1/(n-1),}}\\\\ {{\\scriptstyle\\vdots}}\\\\ {{\\scriptstyle t_{n n}=1/n+1/(n-1)+\\ldots+1.}}\\end{array}\\right\\} $$ <PARAGRAPH_SEPARATOR> For $\\pmb{n}$ not too large, these are easily calculated from tables of reciprocals. <PARAGRAPH_SEPARATOR> The moments of the finite population (2) are found by first calculating the power sums <PARAGRAPH_SEPARATOR> $$ s_{i n}=\\sum_{r=1}^{n}t_{r n}^{i}. $$ <PARAGRAPH_SEPARATOR> Now the triangular scheme (2) for sample size $_{n}$ is derived from that for sample size $n{-}1$ by adding the first column of $1/n^{\\mathfrak{s}}$ . There follow easily recurrence relations such as the following: <PARAGRAPH_SEPARATOR> $$ \\left.\\begin{array}{l}{{s_{1n}=s_{1,n-1}+1,\\quad s_{2n}=s_{2,n-1}+\\left(2/n\\right)s_{1,n-1}+1/n,}}\\\\ {{s_{3n}=s_{3,n-1}+\\left(3/n\\right)s_{2,n-1}+\\left(3/n^{2}\\right)s_{1,n-1}+1/n^{2}.}}\\end{array}\\right\\} $$ <PARAGRAPH_SEPARATOR> These can be solved subject to the initial condition $s_{i1}=1$ . In particular, <PARAGRAPH_SEPARATOR> $$ s_{1n}=n,s_{2n}=2n-\\sum_{r=1}^{n}(1/r)=2n-t_{n n}. $$ <PARAGRAPH_SEPARATOR> After some calculation, we have, for Fisher's $K$ parameters  for  the  finite population, <PARAGRAPH_SEPARATOR> $$ K_{1n}=1,K_{2n}=\\frac{n-t_{n n}}{n-1}=1-\\frac{\\log n+\\gamma-1}{n}+O\\biggl(\\frac{\\log n}{n^{2}}\\biggr), $$ <PARAGRAPH_SEPARATOR> where $\\gamma=0{\\cdot}5772$ is Euler's constant. <PARAGRAPH_SEPARATOR> The leading term in the asymptotic expansion for each cumulant is the corresponding cumulant of the unit exponential distribution. <PARAGRAPH_SEPARATOR> Table 1 gives some values of $K_{2n}$ and of the approximation $1-(\\log n+\\gamma-1)/n$"
  },
  {
    "qid": "statistic-posneg-2012-0-0",
    "gold_answer": "TRUE. The paper states that if (X,Y) ≤_{sst} (S,T), then (X_{1:n}, X_{2:n}, …, X_{n:n}, Y_{[1:n]}, Y_{[2:n]}, …, Y_{[n:n]}) ≤_{st} (S_{1:n}, S_{2:n}, …, S_{n:n}, T_{[1:n]}, T_{[2:n]}, …, T_{[n:n]}). This implies that the stochastic order is preserved for each individual order statistic pair (X_{r:n}, Y_{[r:n]}) and (S_{r:n}, T_{[r:n]}).",
    "question": "If (X,Y) ≤_{sst} (S,T), does it imply that (X_{r:n}, Y_{[r:n]}) ≤_{st} (S_{r:n}, T_{[r:n]}) for all r=1,2,…,n?",
    "question_context": "Stochastic Ordering of Order Statistics in Two-Sample Comparisons\n\nThe paper discusses the stochastic ordering of order statistics in two-sample comparisons. It examines whether stochastic ordering between random vectors (X,Y) and (S,T) implies similar ordering between their corresponding order statistics (X_{r:n}, Y_{[r:n]}) and (S_{r:n}, T_{[r:n]}). The paper presents theorems and proofs for various stochastic orders, including the usual stochastic order (≤_{st}), the strong stochastic order (≤_{sst}), and the likelihood ratio order (≤_{lr})."
  },
  {
    "qid": "statistic-posneg-1886-0-1",
    "gold_answer": "FALSE. While Lemma 9 claims uniqueness by solving $t a^{t-1} = 1$ (where $t = \\frac{\\alpha'}{\\alpha}$ and $a = (s_0')^2$), the proof assumes $a > 1$ implies $t = 1$ is the only solution. However, this overlooks potential edge cases or alternative domains where multiple solutions might exist, especially if $a$ or $t$ constraints are relaxed. The uniqueness heavily depends on strict adherence to $s_0 > 1$ and $\\alpha \\in (0, \\frac{1}{2})$, which may not universally hold.",
    "question": "Does the Lambert W function-based solution for $\\alpha$ in Proposition 3 guarantee uniqueness under the given domain constraints?",
    "question_context": "Statistical Estimation and Convergence Analysis in Parameter Estimation\n\nThe paper presents detailed proofs for various lemmas and propositions related to statistical estimation, particularly focusing on the convergence and properties of estimators. The context includes mathematical derivations and bounds for functions and their derivatives, leveraging tools like the mean value theorem and Lambert W function."
  },
  {
    "qid": "statistic-posneg-1535-0-0",
    "gold_answer": "FALSE. The prediction interval $I_{Y}(t_{0})$ is constructed under the assumption that the covariance parameters are known exactly. When plug-in estimates $\\hat{\\boldsymbol{\\theta}}$ are used for the covariance parameters in $\\Sigma^{-1}$ and $\\omega$, the prediction error variance $\\sigma_{r}^{2}(t_{0})$ does not account for the additional uncertainty introduced by estimating these parameters. As a result, the actual coverage of the interval may deviate from the nominal $(1-\\alpha)$ level. This is why methods like bootstrap calibration (as mentioned in Sjöstedt-de Luna and Young, 2003) are needed to adjust for this uncertainty and achieve the desired coverage.",
    "question": "Is the prediction interval $I_{Y}(t_{0})=[\\hat{Y}(t_{0})\\pm z_{\\alpha/2}\\sigma_{r}(t_{0})]$ guaranteed to have the exact $(1-\\alpha)$ coverage when using plug-in estimates for the covariance parameters?",
    "question_context": "Ordinary Kriging Prediction Intervals and Variance\n\nLinear kriging is often divided into simple kriging (known mean), ordinary kriging (unknown but constant mean) and universal kriging (the mean is an unknown linear combination of known functions), depending on the mean value specification. For clarity of the following discussion we describe the benefits of a Markov approximation for ordinary kriging, although the principles apply also for universal kriging, see Section 7.<PARAGRAPH_SEPARATOR>Let therefore<PARAGRAPH_SEPARATOR>$$Z(t)=\\mu+\\gamma(t)+\\varepsilon(t),\\quad t=(t_{1},t_{2})\\in{\\cal D}\\subset\\mathbb{R}^{2}$$<PARAGRAPH_SEPARATOR>be a stochastic field in the area $D$ with mean value $\\mu$ , zero-mean intermediate-scale variation $\\gamma(\\cdot)$ and white noise error $\\varepsilon(\\cdot)$ with variance $\\sigma_{\\varepsilon}^{2}$ . Given observations $(Z(t_{1}),\\dots,Z(t_{n}))^{\\mathrm{T}}=Z$ , a common task is to predict $Y(t)=\\mu+\\gamma(t)$ , often at a grid of locations, and to calculate the prediction error variance at each such location.<PARAGRAPH_SEPARATOR>In ordinary kriging linear predictors are used, i.e.<PARAGRAPH_SEPARATOR>$${\\hat{Y}}(t_{0})=\\sum_{i=1}^{n}a_{i}Z(t_{i})=a^{\\mathrm{T}}Z,$$<PARAGRAPH_SEPARATOR>where $\\pmb{a}=(a_{1},\\dots,a_{n})^{\\mathrm{T}}$ are weights such that $\\textstyle\\sum_{i=1}^{n}a_{i}=1$ , in order for the predictor to be unbiased.<PARAGRAPH_SEPARATOR>Let $\\Sigma=(\\mathrm{Cov}(Z(t_{i}),Z(t_{j})))_{i,j}=\\mathrm{Cov}(Z)$ de note the covariance matrix of the observations, and let the vecto<PARAGRAPH_SEPARATOR>$$\\boldsymbol{\\omega}=(\\mathrm{Cov}(Z(t_{1}),Y(t_{0})),\\dots,\\mathrm{Cov}(Z(t_{n}),Y(t_{0})))^{\\mathrm{T}}=\\mathrm{Cov}(Z,Y(t_{0}))$$<PARAGRAPH_SEPARATOR>contain the covariances between observations and the value at an arbitrary location $\\pmb{t}_{0}$ , where we want to predict the process.<PARAGRAPH_SEPARATOR>The best linear predictor is found by minimizing the mean squared prediction error. The resulting weights (Cressie, 1993, p. 123) are<PARAGRAPH_SEPARATOR>$${\\pmb a}={\\pmb\\Sigma}^{-1}\\left(\\omega+{\\bf1}_{n}\\left(\\frac{{\\bf1}-{\\bf1}_{n}^{\\mathrm T}{\\pmb\\Sigma}^{-1}\\omega}{{\\bf1}_{n}^{\\mathrm T}{\\pmb\\Sigma}^{-1}{\\bf1}_{n}}\\right)\\right),$$<PARAGRAPH_SEPARATOR>where $\\mathbf{1}_{n}=(\\mathbf{1},\\ldots,\\mathbf{1})^{\\mathrm{T}}$ is a column vector of size $n$ .<PARAGRAPH_SEPARATOR>The unconditional kriging variance, i.e. the variance of the prediction error $r(t)=Y(t)-\\hat{Y}(t)$ , is (Cressie, 1993, pp. 123, 128)<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}&{\\sigma_{r}^{2}(t_{0})=\\sigma_{\\gamma}^{2}(t_{0})+\\cfrac{1-\\mathbf{1}_{n}^{\\mathrm{T}}\\Sigma^{-1}\\omega}{\\mathbf{1}_{n}^{\\mathrm{T}}\\Sigma^{-1}\\mathbf{1}_{n}}-\\left(\\omega+\\mathbf{1}_{n}\\left(\\cfrac{\\mathbf{1}-\\mathbf{1}_{n}^{\\mathrm{T}}\\Sigma^{-1}\\omega}{\\mathbf{1}_{n}^{\\mathrm{T}}\\Sigma^{-1}\\mathbf{1}_{n}}\\right)\\right)^{\\mathrm{T}}\\Sigma^{-1}\\omega}\\ &{\\qquad=\\sigma_{\\gamma}^{2}(t_{0})-\\omega^{\\mathrm{T}}\\Sigma^{-1}\\omega+\\cfrac{(1-\\mathbf{1}_{n}^{\\mathrm{T}}\\Sigma^{-1}\\omega)^{2}}{\\mathbf{1}_{n}^{\\mathrm{T}}\\Sigma^{-1}\\mathbf{1}_{n}},}\\end{array}$$<PARAGRAPH_SEPARATOR>where $\\sigma_{\\gamma}^{2}(\\pmb{t}_{0})$ is the variance of the intermediate-scale variation at $t_{0}$ .<PARAGRAPH_SEPARATOR>Using the quantiles of the predictors, prediction intervals can be obtained. E.g. if $Z(t)$ is Gaussian, so is the predictor, and thus the $1-\\alpha$ prediction interval is obtained as<PARAGRAPH_SEPARATOR>$$I_{Y}(t_{0})=[\\hat{Y}(t_{0})\\pm z_{\\alpha/2}\\sigma_{r}(t_{0})],$$<PARAGRAPH_SEPARATOR>where $z\\alpha$ is the $(1-\\alpha)$ -quantile of the standard normal distribution. Alternatively the conditional prediction error variance, calculated without the last term of (4), could be used for prediction intervals, see Sjöstedt-de Luna and Young (2003).<PARAGRAPH_SEPARATOR>Any covariance function (i.e. any nonnegative definite function) is valid for kriging. To be able to estimate model parameters, however, most applications use a model where the covariance function is stationary (though possibly anisotropic) and of standard form, such as exponential, spherical, Gaussian, pure nugget or a linear combination of such models, see Goovaerts (1997, p. 88).<PARAGRAPH_SEPARATOR>If a parametric model, $\\operatorname{Cov}(Z(t_{1}),Z(t_{2}))=C_{Z}(t_{1},t_{2};\\theta)$ , for the covariance function is given, where $\\theta$ is a vector of unknown parameters, a common procedure is as follows: Estimate $\\theta$ and then predict $Y(\\pmb{t}_{0})$ with (2) and (5) replacing $\\theta$ by a plug-in estimate $\\hat{\\boldsymbol{\\theta}}$ in $\\Sigma^{-1}$ and $\\omega$ . When plug-in of estimated covariance parameters is used, the prediction error variance (4) does not reflect the uncertainty of the estimate and the plug-in prediction interval will therefore not have the prescribed coverage. Sjöstedt-de Luna and Young (2003) handle this with bootstrap calibration."
  },
  {
    "qid": "statistic-posneg-307-5-1",
    "gold_answer": "FALSE. While ensuring $\\Sigma' \\neq 0$ is necessary to avoid a degenerate case where the sampler cannot change the hidden states, it is not sufficient alone to guarantee irreducibility. The Markov chain must also satisfy other conditions, such as the proper construction of the state space model to avoid dependencies that could lead to reducibility. For example, in an AR(2) process, alternative state space representations might be needed to ensure irreducibility, as mentioned in the context.",
    "question": "Does the condition $\\Sigma' \\neq 0$ imply that the Markov chain in the sampling algorithm is always irreducible?",
    "question_context": "State Space Model Sampling and Conditional Distributions in Traffic Volume Analysis\n\nThe context describes a sampling algorithm for a state space model used in analyzing aggregated traffic volume. It details the conditional distributions of the states $X_t$ given their neighbors, leveraging properties of the multivariate normal distribution. Key formulas include the joint distribution of $(X_{t+1}, X_t)$ given $X_{t-1}$, and the conditional distribution of $X_t$ given $X_{t-1}$ and $X_{t+1}$. The text also discusses the importance of ensuring the covariance matrix $\\Sigma'$ is non-zero to maintain the irreducibility of the Markov chain and avoid issues in the sampling process."
  },
  {
    "qid": "statistic-posneg-617-0-1",
    "gold_answer": "TRUE. This is proven in Lemma A1, where the decomposition of g(E) into g(A′∩E) + g(B′∩E) allows the sum to be expressed in terms of Möbius inversions over partitions of D. The alternating signs and the properties of non-empty sets ensure the sum cancels out to zero.",
    "question": "Does the condition g(E) = g(A′∩E) + g(B′∩E) for all E ⊆ D imply that the sum ∑_{E⊆D} (-1)^{|D\\E|} g(E) = 0?",
    "question_context": "Log-mean-linear Models for Binary Data: Independence and Interaction Analysis\n\nThe paper discusses log-mean-linear models for binary data, focusing on the implications of independence and interaction terms. Key formulas include the decomposition of the log-mean-linear parameter γ and the recursive condition for independence. The proof involves induction on the cardinality of sets A and B, leveraging properties of the Möbius inversion and the function g(E) = logμ_E."
  },
  {
    "qid": "statistic-posneg-1409-2-0",
    "gold_answer": "FALSE. The context states that a 10-fold value for $\\rho$ increased the level of $\\alpha_{t}$ to some extent, indicating sensitivity to prior specifications for $\\rho$. However, other parts of the model were insensitive to substantial changes in $\\rho$ and $\\sigma$.",
    "question": "Is the statement 'The model was insensitive to prior specifications for the drift $\\rho$ and diffusion coefficient $\\sigma$' supported by the context?",
    "question_context": "Bayesian Predictive Probabilities in Ear Infection Modeling\n\nEffect of prior specifications. Extensive tests with different prior values for the main parameters indicated that the model was insensitive to prior specifications. As the values of the drift $\\rho$ and diffusion coefficient $\\sigma$ were fixed (estimated) a priori, it was of interest to test whether large perturbations in these values would change the results. A 10-fold value for $\\rho$ , compared with its a priori estimated value, increased the level of $\\alpha_{t}$ to some extent, but the agedependent shape remained the same. Other parts of the model were insensitive to substantial changes in $\\rho$ and $\\sigma$ . The model was also robust against different prior values of carriage duration $d.$ . Although the prior model (different variances in $N(0,\\sigma^{2}))$ for the regression parameters $\\beta_{1}$ and $\\beta_{2}$ in the ear infection model had no effect on the results, careful tuning of their proposal variances was needed. <PARAGRAPH_SEPARATOR> Model fit. The latent parts of the joint model, augmented by missing variables, complicate model assessment because obviously it should be based on some observable quantity. Figure 10 shows the overall fit (TTT-plot) of the ear infection model in terms of the AOM counting process $N_{t}$ and its cumulative posterior intensity $\\int\\mu_{s}\\mathrm{d}s$ , the latter of which depends on both of the latent processes $X$ and $Y.$ . The joint model slightly underestimates the number of PncAOM before the age of 1 year but after that fits very well. <PARAGRAPH_SEPARATOR> In Bayesian analysis, model assessment is often based on predictive distributions. The probabilistic agreement of replicated or new data with the estimated model can be studied in this way. In our case, the relevant measures are the following individual predictive probabilities of ear infection <PARAGRAPH_SEPARATOR> $$ P(N_{i}(\\tau_{k}+d_{k})-N_{i}(\\tau_{k})\\in I|\\mathcal{F}_{\\tau_{k}}^{i}) $$ <PARAGRAPH_SEPARATOR> where $I\\subset\\{1,2,...\\}$ within the infection intervals $(\\tau_{k},\\tau_{k}+d_{k}]$ . Alternatively, in Fig. 10, the cumulative predictive intensity could be used instead of the estimated one. The conditioning history $\\mathcal{F}_{\\tau_{k}}^{i}=\\sigma(H_{[0,T_{-i}]}^{-i}\\cup H_{[0,\\tau_{k}]}^{i})$ is a union of the complete observed history of all other individuals except the ith in the whole follow-up interval $[0,T]$ , and the observed history $H_{[0,t^{*}]}^{i}$ of the ith individual up to some prediction time $t^{*}$ (cf. Arjas & <PARAGRAPH_SEPARATOR> ![](images/179abbe515d1b5ffcaf85bdc82c9137c2179774c5ccca209c349be4f5be92844.jpg)  Fig. 10. TTT-plot: Observed $(N_{T})$ versus estimated $(\\lceil\\mu_{t}\\mathrm{d}t)$ number of Pnc ear infections. <PARAGRAPH_SEPARATOR> Andreev, 2000). When calculating cross-validation predictive probabilities, the observations of the ith child are erased from some chosen prediction time $t^{*}$ until the end of the observation interval $(t^{*},T]$ . The predictive probabilities are then expectations of (14) given $\\mathcal{\\ F}_{t*}^{i}$ , and the possible sample paths of all processes of the ith individual are approximated by MCMC simulation."
  },
  {
    "qid": "statistic-posneg-1777-0-1",
    "gold_answer": "FALSE. The paper clarifies that AGHQ with $m=3$ points does not provide the same error rate as second-order Laplace approximation. Instead, it shows that AGHQ requires at least $m=4$ points per dimension to achieve an error rate of $O(p^{-2})$, which is comparable to second-order Laplace approximation.",
    "question": "Does the paper claim that adaptive Gauss-Hermite quadrature with $m=3$ points per dimension provides the same error rate as second-order Laplace approximation?",
    "question_context": "Error Rate Analysis of Adaptive Gauss-Hermite Quadrature in Latent Variable Models\n\nThe paper discusses the error rate of adaptive Gauss-Hermite quadrature (AGHQ) approximation in latent variable models. It corrects the previously derived error rate by Liu & Pierce (1994), showing that the correct error rate for AGHQ is $O(p^{-\\lfloor(m+2)/3\\rfloor})$ rather than $O(p^{-\\lfloor m/3+1\\rfloor})$. The paper also compares AGHQ with Laplace approximation, demonstrating that AGHQ with $m=4,5,6$ points per dimension has an error rate of $O(p^{-2})$, similar to second-order Laplace approximation."
  },
  {
    "qid": "statistic-posneg-1170-0-0",
    "gold_answer": "TRUE. The approximation is derived by inverting the linear relationship $1/{\\mathrm{var}}({\\tilde{\\sigma}})=1{\\cdot}99n-2{\\cdot}54$, which simplifies to $\\operatorname{var}\\left({\\tilde{\\sigma}}\\right) \\approx \\sigma^{2}/(2n-2.5)$. This approximation is valid for $n > 20$ and is equivalent to using a root mean square estimator with $(n-1.25)$ degrees of freedom, as indicated in the paper.",
    "question": "Is the approximation formula $\\operatorname{var}\\left({\\tilde{\\sigma}}\\right)=\\sigma^{2}/(2n-2\\cdot5)$ derived from the linear relationship $1/{\\mathrm{var}}({\\tilde{\\sigma}})=1{\\cdot}99n-2{\\cdot}54$ accurate for sample sizes greater than 20?",
    "question_context": "Variance Estimation in Normal Population from Doubly Censored Samples\n\nThe paper discusses the efficiency of the estimator $\tilde{\\sigma}$ for normal population standard deviation under various censoring conditions. It provides simplified equations for the estimator and its variance in uncensored samples, and introduces approximation procedures for variance estimation. The reciprocal of the variance of $\tilde{\\sigma}$ is shown to be nearly linear in sample size, leading to a simple approximation formula. Comparisons are made with the unbiased root mean square estimator, and methods for handling censored samples are detailed."
  },
  {
    "qid": "statistic-posneg-617-0-2",
    "gold_answer": "FALSE. The proof explicitly states that the inductive step is valid for |A∪B| = k, given it holds for |A∪B| < k. The condition (iii) is recursive, and the inductive assumption ensures μ_{A′∪B′} = μ_{A′} × μ_{B′} for all proper subsets, leading to the conclusion for |A∪B| = k. The question misrepresents the proof's validity.",
    "question": "Is the inductive step in the proof of Theorem 1 valid for |A∪B| = k, assuming it holds for |A∪B| < k?",
    "question_context": "Log-mean-linear Models for Binary Data: Independence and Interaction Analysis\n\nThe paper discusses log-mean-linear models for binary data, focusing on the implications of independence and interaction terms. Key formulas include the decomposition of the log-mean-linear parameter γ and the recursive condition for independence. The proof involves induction on the cardinality of sets A and B, leveraging properties of the Möbius inversion and the function g(E) = logμ_E."
  },
  {
    "qid": "statistic-posneg-1555-0-0",
    "gold_answer": "TRUE. The context explicitly states that BIC tends to underfit severely in VLMC applications, sometimes even selecting the independency model, whereas AIC is preferred for minimizing prediction squared error in infinite cases like $AR(\\infty)$. This is supported by empirical observations in real data examples and references to literature (Friedman, Hastie, and Tibshirani, 2000).",
    "question": "Is it true that BIC tends to underfit severely in VLMC applications, often selecting the independency model, while AIC is preferred for minimizing prediction squared error in infinite cases?",
    "question_context": "Variable Length Markov Chains: Bootstrap Simulation and Estimation\n\nFigure 5 shows the result of using AIC(vlmc(*, cutoff $\\begin{array}{r l}{\\mathbf{\\Xi}}&{{}=\\mathbf{\\Xi}\\mathbb{K}\\mathbf{\\Xi}.}\\end{array}$ )) for $K\\in$ $\\{2.8,2.82,\\ldots,6\\}$ (\" for(K in seq(2.8, 6, by $=$ .02))\" in R). Note that we often use AIC rather than BIC for the following reasons. Whereas AIC is known to overfit for choosing the correct finite-dimensional models, it can give smaller prediction squared error in the infinite case (such as $A R(\\infty),$ ) and actually often underfits for minimal 0-1 prediction loss, for example, see Friedman, Hastie, and Tibshirani (2000, Rejoinder, figure 1). Moreover, we have tried to use BIC instead of AIC for VLMC in several real data examples, and BIC there tends to underfit severely, in some cases even choosing the independency model. On the other hand, if the data is generated by a VLMC, BIC has empirically identified the correct model for large $n$ , that is, needing rather $n=10{,}000$ instead of $n=1{,}000$ , even for a very simple VLMC. If the aim is not minimizing the Kullback-Leibler divergence, such simple criteria are not at hand. Estimation of the cutoff parameter can then be pursued with bootstrapping as described in Section 4.1. <PARAGRAPH_SEPARATOR> # 4. BOOTSTRAPPING AND SIMULATING FROM A VLMC <PARAGRAPH_SEPARATOR> Simulating a VLMC of order $p$ is done via its transition probabilities $\\{P(x_{1}\\mid c(x_{-p+1}^{0})$ ; $x_{-p+1}^{1}\\in\\mathcal{X}^{p+1}\\}$ : start with an initial $p$ -vector 0 p 1 2 X p, where p is the order of the VLMC and simulate <PARAGRAPH_SEPARATOR> $$ X_{t}\\sim P(\\cdot\\mid c(X_{t-p}^{t-1})),t=1,2,\\ldots. $$ <PARAGRAPH_SEPARATOR> Under regularity conditions, the effect of the initialization gets forgotten exponentially fast as the simulated path gets longer, and the simulated values are becoming close to a sample from the stationary distribution of the VLMC. Thus, to simulate an $n$ -dimensional sample from the stationary distribution of the VLMC (assuming it exists), we proceed as follows: <PARAGRAPH_SEPARATOR> simulate $X_{1},X_{2},\\dots,X_{m+n}$ as in (4.1); choose $X_{m+1},\\dots,X_{m+n}$ as an approximately stationary sample of size $n$ : <PARAGRAPH_SEPARATOR> Here $m$ is a large number such as $10^{3}$ or $10^{4}$ and the initial vector in (4.1) may be chosen as a $p$ -vector whose elements are all equal to some $x\\in{\\mathcal{X}}$ . Our R function simulate.vlmc() has an argument n.start for $m$ taking $m=64\\times(\\mathrm{contextsize})=64\\mathrm{card}(\\tau_{c})$ as default value, for example, <PARAGRAPH_SEPARATOR> $>$ simulate.vlmc(vc5, n = 17) [1] \"a\" \"g\" \"t\" \"g\" \"a\" \"g\" \"c\" \"a\" c\" \"c\" \"c\"   > simulate.vlmc(vc5, n = 17) [1] \"g\" \"a\" \"g\" \"c\" \"g\" \"a\" \"t\" \"g\" \"c\" g\" \"g\" \"t\" \"c\" \"g\" \"t\" \"g\" \"t\"   > simvc5 <- simulate.vlmc(vc5, n = 100000)   > table(simvc5) a c g t   18822 30249 31010 19919 <PARAGRAPH_SEPARATOR> where the last simulation (of length 100,000) still takes only between 1 or 2 tenths of a second on a $900\\mathrm{MHz}$ Pentium III Computer. <PARAGRAPH_SEPARATOR> The VLMC bootstrap is nothing other than simulating from a fitted VLMC. The bootstrap sample of size $n$ is constructed as in (4.2), using in (4.1) the estimated transition probabilities from the context algorithm $\\hat{P}(\\cdot\\mid\\hat{c}(X_{t-p}^{t-1}))$ with $p$ the order of the estimated VLMC. The usual notation for such a bootstrap sample is then <PARAGRAPH_SEPARATOR> $$ X_{1}^{*},\\ldots,X_{n}^{*}. $$ <PARAGRAPH_SEPARATOR> We could (but have not done so in the VLMC package) define $>$ bootstrap.vlmc <- function(x, B) sapply(1:B, function(i) simulate.vlmc(x, n = x\\$n, integer.ret = TRUE)) <PARAGRAPH_SEPARATOR> where the integer.return $=$ TRUE argument results in series with values in $\\{0,1,\\ldots,$ $\\mathrm{card}(\\mathcal{X})-1\\}$ instead of characters, for example, \"a\", \"c\", \"g\", \"t\". This is faster and needs less memory for the result. VLMC-bootstrapping of an estimator $T_{n}=h_{n}(X_{1},$ $\\ldots,X_{n})$ , a function $h_{n}(\\cdot)$ of the data, is constructed with the plug-in rule: <PARAGRAPH_SEPARATOR> $$ T_{n}^{*}=h_{n}(X_{1}^{*},\\ldots,X_{n}^{*}), $$ <PARAGRAPH_SEPARATOR> with the same function $h_{n}(\\cdot)$ . As an example, we take $h_{n}(x_{1}^{n})=\\#\\{i\\mid x_{i}=\\mathtt{a}$ ; $x_{i-1}=\\operatorname{t}\\}/$ $(n-1)$ , that is, the occurrence frequency of “TA” in the DNA sequence. To this end we simulate from a relatively large model, <PARAGRAPH_SEPARATOR> $>\\lor\\subset3<-$ vlmc(bnrf1EB, cutof $\\bar{\\bar{\\cdot}}=3$ ) <PARAGRAPH_SEPARATOR> # smaller K overfits"
  },
  {
    "qid": "statistic-posneg-1912-0-0",
    "gold_answer": "TRUE. According to the context, it is proved in Huang and Huo (2017) that $\\overline{{\\varOmega}}_{n,K}$ is an unbiased estimator for $\\nu_{n}^{2}(\\mathbf{X},\\mathbf{Y})$ under some general moment regularity conditions. This means that the expected value of $\\overline{{\\Omega}}_{n,K}$ equals the true sample distance covariance $\\nu_{n}^{2}(\\mathbf{X},\\mathbf{Y})$ when these conditions are met.",
    "question": "Is the estimator $\\overline{{\\Omega}}_{n,K}$ unbiased for $\\nu_{n}^{2}(\\mathbf{X},\\mathbf{Y})$ under general moment regularity conditions?",
    "question_context": "Random Projection Method for Sample Distance Covariance Estimation\n\nCalculating sample distance correlation for multivariate random variables is beyond the scope of this paper. However, for completeness, we present a random projection method proposed in Huang and Huo (2017). Let $S^{p-1}$ and $S^{q-1}$ denote the unit spheres on $\\mathbb{R}^{p}$ and $\\mathbb{R}^{q}$ , respectively. Let $U(S^{p-1})$ and $U(S^{q-1})$ represent the uniform measures on $S^{p-1}$ and $S^{q-1}$ , respectively. Define $\\begin{array}{r}{C_{p}=\\frac{\\sqrt{\\pi}{\\cal{T}}((p+1)/2)}{{\\cal{T}}(p/2)}}\\end{array}$ and $\\begin{array}{r}{C_{q}=\\frac{\\sqrt{\\pi}{\\cal{T}}(({\\dot{q+1}})/2)}{{\\cal{T}}({\\dot{q}}/2)}}\\end{array}$ . The random projection method is summarized as follows: <PARAGRAPH_SEPARATOR> Step 1: Generate $u$ and $v$ from $U(S^{p-1})$ and $U(S^{q-1})$ , respectively. Project $X_{i}$ into the space spanned by $u$ (that is, calculate $u^{\\intercal}X_{i}$ ). Similarly, project $Y_{i}$ , $i=1,\\ldots,n$ into space expanded by $v$ . <PARAGRAPH_SEPARATOR> Step 2: Denote $\\boldsymbol{u}^{\\intercal}\\mathbf{X}=(u^{\\intercal}X_{1},\\dots,u^{\\intercal}X_{n})^{\\intercal}$ and $\\boldsymbol{v}^{\\intercal}\\boldsymbol{\\mathbf{Y}}=(\\boldsymbol{v}^{\\intercal}Y_{1},\\dots,\\boldsymbol{v}^{\\intercal}Y_{n})^{T}$ . Call the univariate distance covariance routine to calculate $\\mathcal{V}_{n}^{2}(u^{\\intercal}{\\mathbf X},v^{\\intercal}{\\mathbf Y})$ , and calculate <PARAGRAPH_SEPARATOR> $$ \\itOmega_{n}=C_{p}C_{q}\\mathcal{V}_{n}^{2}(u^{\\intercal}{\\bf X},v^{\\intercal}{\\bf Y}). $$ <PARAGRAPH_SEPARATOR> Step 3: Repeat Step 1 and Step $2K$ times, and estimate the sample distance covariance by using the average of $\\varOmega_{n}$ <PARAGRAPH_SEPARATOR> $$ \\overline{{\\Omega}}_{n,K}=\\frac{1}{K}\\sum_{k=1}^{K}\\Omega_{n}^{(k)}. $$ <PARAGRAPH_SEPARATOR> It is easy to conclude that the complexity of the preceding algorithm is $\\mathcal{O}(K n\\log(n)+K n(p+q))$ . If some general moment regularity conditions are satisfied, it is proved in Huang and Huo (2017) that $\\overline{{\\varOmega}}_{n,K}$ is an unbiased estimator for $\\nu_{n}^{2}(\\mathbf{X},\\mathbf{Y})$ . Furthermore, $|\\overline{{\\Omega}}_{n,K}-\\nu_{n}^{2}({\\bf X},{\\bf Y})|$ converges to zero at a rate no worse than $O(1/\\sqrt{K})$ . <PARAGRAPH_SEPARATOR> # 4. Experiments <PARAGRAPH_SEPARATOR> In this section, we compare the speed of the proposed fast algorithm with the dyadic updating method (Huo and Székely, 2016) and also with the brute force implementation. We implemented the proposed algorithm and the brute force method in MATLAB. For the dyadic updating method, we used its authors’ MATLAB implementation, in which the key step is implemented in C. Therefore, this comparison strongly favors the dyadic updating method because more than $90\\%$ of its calculations are done in C instead of MATLAB, according to the MATLAB code profiler. All simulations are run on a PC with $\\mathrm{{Intel^{\\textregistered}}X e o n^{\\textregistered}}$ Gold CPU $\\ @2.40\\mathrm{GHz}$ processor and 16 GB memory running MATLAB version 9.0.0.341360 (R2016a). Average running time is calculated based on 10 replicates."
  },
  {
    "qid": "statistic-posneg-1430-3-1",
    "gold_answer": "FALSE. The corollary states that if $\\beta_{n}=O(\\delta_{n}^{-d})$ under the condition $\\operatorname*{sup}_{n}n\\delta_{n}^{d}=\\infty$, then necessarily $\\lambda(\\operatorname{pos}f)<\\infty$ (i.e., the density has compact support). However, it does not claim that the upper bound never holds for non-compact support densities; it only states that compact support is a necessary condition for the bound to hold in this scenario. There might be other specific cases or conditions not covered by the corollary.",
    "question": "Does the corollary 3.5 imply that for densities with non-compact support, the upper bound $\\beta_{n}=O(\\delta_{n}^{-d})$ never holds when $\\operatorname*{sup}_{n}n\\delta_{n}^{d}=\\infty$?",
    "question_context": "Bounds on Probability Measures and Computational Complexity\n\nLemma 3.4. For all $t\\in[0,1]$ , (i) $\\begin{array}{l}{1-e^{-t}\\leqslant t,}\\ {1-e^{-t}\\geqslant(1-e^{-1})t.}\\end{array}$ (ii) <PARAGRAPH_SEPARATOR> Proof. Using Taylor's expansion of $e^{t}$ one sees that $1+t\\leqslant e^{t}$ for all $t$ , which includes (i). For part (ii), consider the function $\\phi(t)=(1-e^{-t})/t$ , $t\\in]0,1]$ . The derivative $\\phi^{\\prime}(t)=[(1+t)e^{-t}-1]/t^{2}$ is negative because $1+t<e^{t}$ for $t\\neq0$ . Hence $\\phi$ is decreasing and (ii) follows from $\\phi(t)\\geqslant\\phi(1)$ . K <PARAGRAPH_SEPARATOR> Proof of Theorem 3.2. From (i) of Lemma 3.4 we have <PARAGRAPH_SEPARATOR> $$ 1-(1-t)^{n}\\geqslant1-e^{-n t} $$ <PARAGRAPH_SEPARATOR> for all $n$ and $t\\in[0,1]$ . Let <PARAGRAPH_SEPARATOR> $$ J_{n}=\\{\\mu\\in\\mathbb{Z}^{d}|n\\lambda_{f}(B_{n\\mu})\\geqslant1\\}. $$ <PARAGRAPH_SEPARATOR> Then, if $\\mu\\in J_{n}$ , we have by (3.5) that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\geqslant1-e^{-n\\lambda_{f}(B_{n\\mu})}\\geqslant1-e^{-1},}\\end{array} $$ <PARAGRAPH_SEPARATOR> and if $\\mu\\in J_{n}^{c}$ , we have by (3.5) and (ii) of Lemma 3.4 that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\geqslant1-e^{-n\\lambda_{f}(B_{n\\mu})}\\geqslant(1-e^{-1})n\\lambda_{f}(B_{n\\mu}).}\\end{array} $$ <PARAGRAPH_SEPARATOR> It follows that <PARAGRAPH_SEPARATOR> $$ \\beta_{n}=\\sum_{\\mu}\\left[1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\right]\\geqslant(1-e^{-1})\\left[|J_{n}|+n\\sum_{\\mu\\in J_{n}^{c}}\\lambda_{f}(B_{n\\mu})\\right], $$ <PARAGRAPH_SEPARATOR> where $\\left|J_{n}\\right|$ is the number of elements of the set $J_{n}$ (at most $n$ ). Now, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{|J_{n}|=\\delta_{n}^{-d}\\lambda\\left(\\underset{\\mu\\in J_{n}}{\\bigcup}B_{n\\mu}\\right)}\\ &{\\qquad=\\delta_{n}^{-d}\\lambda\\left(A_{n}\\cap\\underset{\\mu\\in J_{n}}{\\bigcup}B_{n\\mu}\\right)+\\delta_{n}^{-d}\\lambda\\left(A_{n}^{c}\\cap\\underset{\\mu\\in J_{n}}{\\bigcup}B_{n\\mu}\\right)}\\ &{\\qquad\\geqslant\\delta_{n}^{-d}\\lambda\\left(A_{n}\\cap\\underset{\\mu\\in J_{n}}{\\bigcup}B_{n\\mu}\\right)+n\\lambda_{f}\\biggl(A_{n}^{c}\\cap\\underset{\\mu\\in J_{n}}{\\bigcup}B_{n\\mu}\\biggr),}\\end{array} $$ <PARAGRAPH_SEPARATOR> where the inequality follows from (ii) of Lemma 3.3. Similarly, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{n\\underset{\\mu\\in J_{n}^{c}}{\\sum}\\lambda_{f}(B_{n\\mu})=n\\lambda_{f}\\biggr(\\underset{\\mu\\in J_{n}^{c}}{\\bigcup}B_{n\\mu}\\biggr)}\\ &{\\qquad=n\\lambda_{f}\\biggr(A_{n}\\cap\\underset{\\mu\\in J_{n}^{c}}{\\bigcup}B_{n\\mu}\\biggr)+n\\lambda_{f}\\biggr(A_{n}^{c}\\cap\\underset{\\mu\\in J_{n}^{c}}{\\bigcup}B_{n\\mu}\\biggr)}\\ &{\\qquad\\geqslant\\delta_{n}^{-d}\\lambda\\biggr(A_{n}\\cap\\underset{\\mu\\in J_{n}^{c}}{\\bigcup}B_{n\\mu}\\biggr)+n\\lambda_{f}\\biggr(A_{n}^{c}\\cap\\underset{\\mu\\in J_{n}^{c}}{\\bigcup}B_{n\\mu}\\biggr),}\\end{array} $$ <PARAGRAPH_SEPARATOR> where the inequality follows by (i) of Lemma 3.3. The inequality (3.4) now follows from (3.6) by adding (3.7) and (3.8). K <PARAGRAPH_SEPARATOR> If $\\operatorname*{sup}_{n}n\\delta_{n}^{d}<\\infty$ , we have $\\beta_{n}\\leqslant n=O(\\delta_{n}^{-d})$ . Thus, the simple upper bound valid for a compact support density is in this case valid for all densities. As a corollary of Theorem 3.2 we have that in the non-trivial case when $\\operatorname*{sup}_{n}n\\delta_{n}^{d}=\\infty$ , virtually the only densities for which this simple upper bound holds are those with a compact support. <PARAGRAPH_SEPARATOR> Corollary 3.5. Let $\\operatorname*{sup}_{n}n\\delta_{n}^{d}=\\infty$ . If $\\beta_{n}=O(\\delta_{n}^{-d})$ , then necessarily $\\lambda(\\operatorname{pos}f)<\\infty$ ."
  },
  {
    "qid": "statistic-posneg-1435-0-2",
    "gold_answer": "FALSE. Explanation: While the context mentions that a total score exceeding 3 times the root of the variance may be considered significant, this is a simplified threshold. In practice, the exact threshold for significance would depend on the desired significance level (e.g., p < 0.05) and the distribution of the test statistic. The value of 3 is arbitrary and not a universally accepted cutoff. A more rigorous approach would involve calculating a p-value or using a predefined critical value from the appropriate statistical distribution.",
    "question": "The total score $\\lambda$ for linkage detection is calculated by summing the scores for each sib-pair across families. Is it correct to say that a significant linkage is declared if the total score exceeds 3 times the square root of the total variance?",
    "question_context": "Linkage Detection in Human Genetics: Sib-pair Analysis\n\nThis table is arranged so as to give also the actual probability $p(G^{i}G^{j},T^{r}T^{s})$ of any sib-pair classified as $G^{i}G^{j}$ in the main character, and $T^{r}T^{s}$ in the test character.Write <PARAGRAPH_SEPARATOR> $$ p(G^{i}G^{j},T^{r}T^{s})=\\pi(G^{i}G^{j})\\pi(T^{r}T^{s})\\:\\Psi, $$ <PARAGRAPH_SEPARATOR> where $\\pi(G^{i}G^{j})$ is the probability of pair $G^{i}G^{j}$ when $\\zeta=0$ $\\pi(T^{r}T^{s})$ the same for $T^{r}T^{s}$ and $\\downarrow$ a correcting factor for the effect of linkage. The value of $\\pi(G^{i}G^{j})$ for all kinds of pairs is given in Table 5._The value of $\\pi(T^{r}T^{s})$ follows from symmetry. The value of $\\updownarrow$ can be found from the entry in Table 5 by the use of Table 6. <PARAGRAPH_SEPARATOR> The variances per sib-pair can be found from the formula $\\kappa=\\Sigma\\pi a^{2}$ and are given in Table 7. For recessivity types $[22^{*}]$ and [4] in either character $\\pmb{\\kappa}=\\mathbf{0}$ , since all scores are zero. <PARAGRAPH_SEPARATOR> These tables enable a test for linkage to be made in any set of families in which the parental genotypes are known.For each family a score $\\lambda$ is found, by adding the scores for each sib-pair as given in Table 5, and a variance by multiplying the value of $\\pmb{\\kappa}$ in Table 7 by $\\frac{1}{2}n(n-1)$ ，the number of sib-pairs (where $_n$ is the number of sibs). The scores and variances for the different families are summed to give a total score and total variance. If the total score is positive and exceeds (say) 3 times the root of the variance it may be considered significant. Note that any one phenotype may be given different symbols in different families: e.g., blood-group $\\pmb{A}$ is of type $\\mathbf{G_{2}}$ when themating is $\\mathbf{AA}\\times\\mathbf{AO}$ ,but $\\mathbf{G_{3}}$ when themating is $\\mathbf{AO}\\times\\mathbf{AO}$"
  },
  {
    "qid": "statistic-posneg-493-0-2",
    "gold_answer": "TRUE. The context describes thin-plate splines as radial basis functions, meaning their value ϕₖ(x) depends only on the distance ||x - x̃ₖ|| from the basis location x̃ₖ (and constants like m and d). This is a key property of radial basis functions.",
    "question": "Is it correct that the thin-plate basis functions depend only on the distance between the abscissae and the basis locations?",
    "question_context": "Bayesian Nonparametric Probit Regression with Variable Selection\n\nThe article describes a Bayesian approach to nonparametric probit regression with variable selection. The model uses thin-plate basis functions to approximate the regression function and employs a hierarchical Bayesian framework to perform variable and component selection. The methodology is illustrated with simulated and real datasets, showing its effectiveness in identifying significant variables and interactions."
  },
  {
    "qid": "statistic-posneg-500-0-0",
    "gold_answer": "FALSE. The context explicitly states that the performance of the LJS estimator depends critically on the prior distribution of θi. For example, under an exponential prior distribution, the LJS estimator can have a larger asymptotic selection bias compared to X(i), as indicated by the behavior of LJS(p) - θ(p) → ∞ in this case.",
    "question": "The LJS estimator is guaranteed to have a smaller selection bias than the maximum likelihood estimator X(i) for all prior distributions of θi. Is this statement true based on the context provided?",
    "question_context": "Selection Bias in Ordered Parameter Estimation\n\nThe paper discusses the estimation of selected parameters θ(i) from ordered statistics X(i) of normally distributed estimators Xi ∼ N(θi, σ²). The selection bias is defined as E(X(i) - θ(i)), and it is shown that X(p) has a positive bias in estimating θ(p), while X(1) has a negative bias in estimating θ(1). The paper introduces the Lindley–James–Stein (LJS) estimator to reduce this bias and compares its performance under different prior distributions for θi."
  },
  {
    "qid": "statistic-posneg-790-0-4",
    "gold_answer": "TRUE. The LR-FB symmetry model imposes constraints on the B-spline coefficients (e.g., α_{2g} = α_{4g} = α_{6g} = α_{8g}, α_{1g} = α_{5g}, and α_{7g} = α_{3g}), which reduces the number of free parameters from 8 to 3, as described in the context.",
    "question": "Is the LR-FB symmetry model characterized by reducing the number of free parameters from 8 to 3 by imposing constraints on the B-spline coefficients?",
    "question_context": "Hierarchical Bayesian Model for Bimodal Angular Judgement Data\n\nWe require a method for evaluating localization performance differences between groups across the entire set of signal azimuths. A reasonable approach is to model the bimodal, angular judgement data via a regression model, with patient group and signal position as covariates, while accounting for the repeated measures aspect of the study design. In this paper, we take a hierarchical Bayesian approach to model judgements of all signals by using structured, correlated circular $B$ -splines across mixing and precision components of a localization performance model."
  },
  {
    "qid": "statistic-posneg-2132-5-1",
    "gold_answer": "TRUE. The GPD models exceedances over a threshold, which are more frequent than the single maximum value in a period that the GEVD models. This makes the GPD more versatile and applicable to a broader range of phenomena where multiple exceedances are observed, such as floods, waves, winds, temperatures, or earthquakes.",
    "question": "Does the GPD provide a wider inductive basis than the GEVD because the number of exceedances over a threshold is usually much larger than one?",
    "question_context": "Connections Between Generalized Pareto Distribution and Generalized Extreme Value Distribution\n\nPickands (1975) shows that classical limit results for sample maxima lead to parallel limit results for exceedances over thresholds. In particular, suppose that $X_{1},X_{2},\\dots$ is a sequence of independent and identically distributed (IID) random variables such that an appropriately normalized CDF of $M_{n}{=}\\operatorname*{max}\\left(X_{1},\\ldots,X_{n}\\right)$ converges to the GEVD. Then, for large enough threshold $u$ , the CDF of $X_{i}-u$ , conditional on $X_{i}>u$ , converges to the GPD as $u$ increases. Both distributions share the same shape parameter $k$ . A connection among the remaining parameters, which also embraces the intensity of a Poisson process, can be established using a Poisson-GPD process (see Coles, 2001; Smith, 2003) in which the number $N$ of exceedances over the level $u$ in any particular period (e.g., any year) has a Poisson distribution with mean $\\lambda$ and, conditional on $N\\geqslant1$ , the exceedances $X_{1},\\dots,X_{N}$ are IID random variables following the GPD. Then $$\\begin{array}{c}{{P r o b\\left\\{\\operatorname*{max}\\left(X_{1},\\ldots,X_{N}\\right)\\leqslant x\\right\\}=e^{-\\lambda}+\\displaystyle\\sum_{i=1}^{\\infty}\\frac{\\lambda^{i}e^{-\\lambda}}{i!}\\Big\\{1-\\left(1-k(x-u)/\\theta\\right)^{1/k}\\Big\\}^{i}}}\\ {{=\\displaystyle\\exp\\left\\{-\\lambda(1-k(x-u)/\\theta)^{1/k}\\right\\}.}}\\end{array}$$ Identifying Eqs. (2) and (A.1) leads to the following relationships among the threshold $u$ , the Poisson mean $\\lambda$ and the parameters of the GPD and GEVD: $$\\theta=\\psi\\lambda^{k}=\\psi-k(u-\\mu).$$ In addition, if $u$ changes, the scale of the GPD becomes a function of $u$ , say $\\theta_{u}$ , such that $\\theta_{u}+k u$ stays constant independently of $u$ . Clearly, the GPD provides a wider inductive basis than the GEVD because the number of exceedances over interesting thresholds in any given period is usually much larger than one, which is the number of maxima in the period. This applies to many natural phenomena such as floods, waves, winds, temperatures, or earthquakes, among others."
  },
  {
    "qid": "statistic-posneg-2109-2-2",
    "gold_answer": "FALSE. The immediate successor is only given by this method if the sequence is not the last one in the order. If the sequence is the last one (i.e., α₁ = αₙ for odd n or α₁ = αₙ + 1 for even n), there is no successor. The recursive equations apply only when a successor exists.",
    "question": "Is the immediate successor of a sequence {α₁, ..., αₙ} always given by {α₁′, α₃′, ..., αₙ′} where αᵣ′ = αᵣ for r < s, αₛ′ = αₛ - 1, and αᵣ′ = min{αᵣ₋₁′, rn - ½r(r+1) - Σαₜ′} for r > s?",
    "question_context": "Lexicographical Ordering of Sequences in Paired Comparisons\n\nThe paper discusses the distribution of the number of circular triads in paired comparisons, focusing on the lexicographical ordering of sequences. The sequences are derived recursively, starting from the largest first element. The method involves deriving subsequent sequences from the previous ones using specific equations. The correctness of the sequences is checked by comparing the results obtained from different sections of the methodology."
  },
  {
    "qid": "statistic-posneg-331-0-1",
    "gold_answer": "FALSE. While the paper claims that $I \\geq 0$ implies monotonicity of $E_{0}(y)$, the condition $I \\geq 0$ is not sufficient on its own. The paper's derivation relies on additional assumptions about the integrand's behavior, which are not explicitly justified in the provided context. The claim is thus not fully substantiated.",
    "question": "Does the paper correctly assert that $E_{0}(y)$ is monotonically increasing in $y$ when $y\\in[0,h]$ if $I=\\int_{\\pmb{\\nu}}^{\\infty}\\int_{\\pmb{\\nu}}^{\\infty}(x-z)\\exp{[-\\pmb{\\frac{1}{2}}\\{(z+\\alpha)^{2}+(x-\\alpha)^{2}\\}]}d z d x\\geqslant0$?",
    "question_context": "Sequential Probability Ratio Test (SPRT) Error Bounds and Monotonicity Conditions\n\nThe paper discusses the Sequential Probability Ratio Test (SPRT) for normally distributed variates, focusing on error bounds and monotonicity conditions of certain functions. Key formulas include the inequalities for the product of likelihood ratios, the transformation of the SPRT into a sum-based test, and conditions for the monotonicity of functions like $E_{0}(y)$ and $E_{h}(y)$. The paper also provides approximations for type I and type II errors and demonstrates their application with specific values for $A$ and $B$."
  },
  {
    "qid": "statistic-posneg-1216-0-0",
    "gold_answer": "FALSE. The condition should be based on the comparison of the likelihood ratio to a threshold (usually 1), not the sign of the log-likelihood ratio. The rule should be 'Decide μ₀ = μ₁ if L̂₁/L̂₂ > 1, and μ₀ = μ₂ otherwise'. The sign of the log-likelihood ratio alone does not directly translate to a decision rule without considering the threshold.",
    "question": "Is the condition for deciding between two hypotheses based on the sign of the log-likelihood ratio, as given by the rule 'Decide μ₀ = μ₁ if L̂(z) < 0, and μ₀ = μ₂ if L̂(z) > 0', consistent with the derived expression for -2log(L̂₁/L̂₂)?",
    "question_context": "Maximum Likelihood Classification Procedure in Multivariate Normal Distributions\n\nThe paper discusses the maximum likelihood (m.l.) classification procedure for classifying observations into one of two multivariate normal distributions. The likelihood function is expressed in terms of observed data and unknown parameters, and the procedure involves comparing the maxima of the likelihood functions under two competing hypotheses. The paper derives the m.l. rule and discusses its admissibility by showing it is an essentially unique Bayes rule."
  },
  {
    "qid": "statistic-posneg-1109-1-0",
    "gold_answer": "TRUE. The bounds are derived from the probability formula $P(E_{1}+E_{2})=2x-P(E_{1}E_{2})$, where $x = P(E_{1}) = P(E_{2})$. Since $P(E_{1}E_{2})$ is non-negative, $P(E_{1}+E_{2}) \\leqslant 2x$. Additionally, because $P(E_{1}E_{2}) \\leqslant x$ (as $P(E_{2}|E_{1}) \\leqslant 1$), $P(E_{1}+E_{2}) \\geqslant x$. Given that $x = \\alpha$ (the significance level), the bounds $\\alpha\\leqslant P(E_{1}+E_{2})\\leqslant2\\alpha$ hold.",
    "question": "Is the claim that $\\alpha\\leqslant P(E_{1}+E_{2})\\leqslant2\\alpha$ valid given the assumptions of the paper's model?",
    "question_context": "Probability Bounds for Two-Sided Chi-Square Test\n\nThe paper discusses a two-sided chi-square test for detecting trends in data. It defines events $E_{1}$ and $E_{2}$ as significant departures from the null hypothesis in opposite directions. The probability integral of the two-sided test is given by $P(E_{1}+E_{2})=P(E_{1})+P(E_{2})-P(E_{1}E_{2})$. By symmetry, $P(E_{1})=P(E_{2})$, leading to $P(E_{1}+E_{2})=2x-P(E_{1}E_{2})$. The paper establishes bounds for this probability: $\\alpha\\leqslant P(E_{1}+E_{2})\\leqslant2\\alpha$. It further argues that $P(E_{1}E_{2})=P(E_{1})P(E_{2}\\mid E_{1})<\\alpha^{2}$ when $\\gamma$ is large, allowing the approximation $\\operatorname*{Pr}\\left\\{\\operatorname*{max}\\left(\\overline{{\\chi}}^{2\\prime},\\overline{{\\chi}}^{2\\prime\\prime}\\right)\\geqslant\\gamma\\right\\}\\doteq2\\operatorname*{Pr}\\left\\{\\overline{{\\chi}}^{2}\\geqslant\\gamma\\right\\}$."
  },
  {
    "qid": "statistic-posneg-368-0-1",
    "gold_answer": "TRUE. The paper defines the Hellinger distance as $H^{2}(f,g)=\\sum_{x=0}^{\\infty}\\left| f^{1/2}(x)-g^{1/2}(x)\\right|^{2}$, which can be expanded to $2 - 2\\sum_{x=0}^{\\infty}f^{1/2}(x)g^{1/2}(x)$ using algebraic manipulation.",
    "question": "Does the Hellinger distance $H^{2}(f,g)$ between two probability mass functions $f$ and $g$ satisfy the property $H^{2}(f,g) = 2 - 2\\sum_{x=0}^{\\infty}f^{1/2}(x)g^{1/2}(x)$?",
    "question_context": "Consistent Estimation of Mixture Complexity Using Hellinger Distance\n\nThe paper proposes an estimator of mixture complexity based on the Hellinger distance. The estimator is defined as the minimum value of m for which the Hellinger distance between the empirical mass function and the estimated mixture model does not exceed a certain threshold. The consistency of the estimator is proved under certain regularity conditions."
  },
  {
    "qid": "statistic-posneg-556-0-0",
    "gold_answer": "FALSE. The matrix $\\mathbf{V}(\\tau_{i},K)$ is not correctly specified as shown. The correct asymptotic covariance matrix should be derived from the sandwich form $\\mathbf{A}^{-1}\\mathbf{B}(\\mathbf{A}^{-1})^{\\prime}$, where $\\mathbf{B}$ is the covariance matrix of the score functions. The given expression does not correctly represent the covariance structure of the score functions, and the elements involving $\\beta^{2}\\boldsymbol{\\varphi}$ are particularly suspect. The correct form should ensure that all terms are consistent with the expected values of the products of the score functions.",
    "question": "Is the asymptotic covariance matrix $\\mathbf{V}(\\tau_{i},K)$ correctly specified as $\\mathbf{A}^{-1}\\left[\\begin{array}{c c c}{\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\Sigma\\mathbf{E}^{2}\\mathbf{w}_{2}}&{\\beta\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\Sigma\\mathbf{E}^{2}\\mathbf{w}_{3}}&{\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\mathbf{d}}\\\\{\\beta\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\Sigma\\mathbf{E}^{2}\\mathbf{w}_{3}}&{\\beta^{2}\\mathbf{w}_{3}^{\\prime}\\mathbf{E}^{2}\\Sigma\\mathbf{E}^{2}\\mathbf{w}_{3}}&{\\beta\\mathbf{w}_{3}^{\\prime}\\mathbf{E}^{2}\\mathbf{d}}\\\\{\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\mathbf{d}}&{\\beta\\mathbf{w}_{3}^{\\prime}\\mathbf{E}^{2}\\mathbf{d}}&{\\beta^{2}\\boldsymbol{\\varphi}}\\end{array}\\right](\\mathbf{A}^{-1})^{\\prime}$ based on the given definitions of $\\mathbf{A}$, $\\mathbf{w}_{1},\\mathbf{w}_{2},\\mathbf{w}_{3}$, $\\mathbf{E}$, $\\pmb{\\Sigma}$, and $\\mathbf{d}$?",
    "question_context": "Asymptotic Properties of MXM Estimators in Inverse Gaussian Distribution\n\nIt can be easily shown that the final MXM estimators $\\hat{\\boldsymbol{\\theta}},\\hat{\\boldsymbol{\\beta}},\\hat{\\boldsymbol{\\varphi}}$ satisfy the desirable invariance and equivariance properties. Specifically, O is location and scale equivariant, $\\hat{\\boldsymbol{\\beta}}$ is location invariant and scale equivariant and $\\hat{\\varphi}$ is invariant under location-scale transformations of the data. Naturally, these properties hold also for the MO, CMO, ML and MMO estimators. <PARAGRAPH_SEPARATOR> # 4. Asymptotic results for the MXM estimators <PARAGRAPH_SEPARATOR> Let $\\scriptstyle\\mathbf{\\theta}=(\\beta,\\varphi,\\theta)$ be the vector of parameters and $\\hat{\\boldsymbol{\\uptheta}}{=}(\\hat{\\boldsymbol{\\beta}},\\hat{\\boldsymbol{\\varphi}},\\hat{\\boldsymbol{\\theta}})$ the MXM estimator of \u0001. The number $K$ of points $t_{k}=k\\tau$ is considered fixed and the standard spacing $\\tau$ varies according to an initial estimate $\\varphi_{0}$ of $\\varphi$ . Specifically, let $\\tau(\\varphi_{0})=\\tau_{m}$ when $f_{m-1}<\\varphi_{0}\\leqslant f_{m}(m=$ $1,2,\\ldots,M)$ , where the integer $M$ and $\\tau_{m},f_{m}$ are pre-specified numbers such that <PARAGRAPH_SEPARATOR> $$ -\\infty<\\tau_{1}<\\dots<\\tau_{M}<0\\quad\\mathrm{and}\\quad0\\equiv f_{0}<f_{1}<\\dots<f_{M-1}<f_{M}\\equiv\\infty. $$ <PARAGRAPH_SEPARATOR> Then, when the true value of $\\varphi$ is between $f_{i-1}$ and $f_{i}$ for some index $i\\in\\{1,2,\\dots,M\\}$ , it can be shown (the proof is outlined in the appendix) that: <PARAGRAPH_SEPARATOR> $$ \\sqrt{n}(\\hat{\\mathbf{\\theta}}-\\mathbf{\\theta}\\mathbf{\\0})\\xrightarrow{\\mathrm{~d~}}\\mathrm{N}(\\mathbf{0},\\mathbf{V}(\\tau_{i},K)), $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ \\mathbf{V}(\\tau_{i},K)=\\mathbf{A}^{-1}\\left[\\begin{array}{c c c}{\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\Sigma\\mathbf{E}^{2}\\mathbf{w}_{2}}&{\\beta\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\Sigma\\mathbf{E}^{2}\\mathbf{w}_{3}}&{\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\mathbf{d}}\\\\{\\beta\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\Sigma\\mathbf{E}^{2}\\mathbf{w}_{3}}&{\\beta^{2}\\mathbf{w}_{3}^{\\prime}\\mathbf{E}^{2}\\Sigma\\mathbf{E}^{2}\\mathbf{w}_{3}}&{\\beta\\mathbf{w}_{3}^{\\prime}\\mathbf{E}^{2}\\mathbf{d}}\\\\{\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\mathbf{d}}&{\\beta\\mathbf{w}_{3}^{\\prime}\\mathbf{E}^{2}\\mathbf{d}}&{\\beta^{2}\\boldsymbol{\\varphi}}\\end{array}\\right](\\mathbf{A}^{-1})^{\\prime}, $$ <PARAGRAPH_SEPARATOR> $$ \\mathbf{A}=\\left[\\begin{array}{c c c}{\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\mathbf{w}_{1}}&{\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\mathbf{w}_{2}}&{\\mathbf{w}_{2}^{\\prime}\\mathbf{E}^{2}\\mathbf{w}_{3}}\\\\{\\beta\\mathbf{w}_{3}^{\\prime}\\mathbf{E}^{2}\\mathbf{w}_{1}}&{\\beta\\mathbf{w}_{3}^{\\prime}\\mathbf{E}^{2}\\mathbf{w}_{2}}&{K/\\beta}\\\\{\\varphi}&{\\beta}&{1}\\end{array}\\right], $$ <PARAGRAPH_SEPARATOR> $\\mathbf{w}_{1},\\mathbf{w}_{2},\\mathbf{w}_{3}$ are $K$ -dimensional vectors with coordinates <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{w_{1k}=\\varphi k\\tau_{i}(1-2k\\tau_{i})^{-1/2}/\\beta,w_{2k}=1-(1-2k\\tau_{i})^{1/2},w_{3k}=k\\tau_{i}/\\beta,}\\\\ &{{\\bf E}=d i a g\\left(\\frac{1}{\\tau_{i}},\\frac{1}{2\\tau_{i}},\\ldots,\\frac{1}{K\\tau_{i}}\\right),}\\end{array} $$ <PARAGRAPH_SEPARATOR> $\\pmb{\\Sigma}=(\\sigma_{k l})$ is a $K\\times K$ symmetric matrix with <PARAGRAPH_SEPARATOR> $$ \\sigma_{k l}=\\exp\\{-\\varphi[1+(1-2k\\tau_{i}-2l\\tau_{i})^{1/2}-(1-2k\\tau_{i})^{1/2}-(1-2l\\tau_{i})^{1/2}]\\}-1 $$ <PARAGRAPH_SEPARATOR> and $\\mathbf{d}$ is a $K$ -dimensional vector with coordinates <PARAGRAPH_SEPARATOR> $$ d_{k}=\\beta\\varphi[(1-2k\\tau_{i})^{-1/2}-1]. $$ <PARAGRAPH_SEPARATOR> If we use a non-random value $\\tau$ of the standard spacing, the asymptotic distribution will be given by (7) with $\\tau_{i}=\\tau$ . It is noted that, for each fixed integer $K$ and a given value of $\\varphi$ , the determinant of the asymptotic covariance matrix $\\mathbf{V}$ has a minimum at a finite value of $\\tau(\\tau<0)$ . This value approaches zero as $\\varphi$ gets large, indicating that a smaller span of points $\\{t_{k}\\}_{k=1}^{K}$ is more appropriate as the distribution becomes more symmetric. Since $\\varphi$ is unknown, a value of $\\tau$ , $\\tau=\\tau_{m}$ , should be chosen for which the determinant of $\\mathbf{V}(\\tau_{m},K)$ will have relatively small values for all $\\varphi$ in an interval. The asymptotic results indicate that the adaptive procedure described above will be more robust in identifying appropriate values of $\\tau$ if a larger number $M$ of intervals $(f_{m-1},f_{m})$ is used. However, in finite samples this is not so. Therefore, practically good values of $M$ and endpoints of the intervals were identified (see Section 3.5). <PARAGRAPH_SEPARATOR> # 5. Simulation results <PARAGRAPH_SEPARATOR> The method of Michael et al. (1976) was used to generate 5000 pseudo-random samples with sizes 20, 50, 100 and 200 from an IG3 $(0,1,\\varphi)$ distribution with various values of $\\varphi$ in the interval (0, 10]. For each sample we have found the MO, CMO, ML, MMO and MXM estimates as described in Section 3. Because of the invariance–equivarience properties of the estimators, we did not consider values of $(\\theta,\\beta)$ other than (0, 1). The ML, MMO and MXM iterative procedures were applied with a convergence tolerance constant $\\varepsilon=10^{-9}$ and a maximum number of 100 iterations. Although the MXM procedure converged in all samples, there were a small percentage of samples in the experiments with $n=20$ , 50 and large values of $\\varphi$ for which the ML or the MMO procedure did not converge. For these samples the ML or the MMO estimates were replaced by the CMO estimates."
  },
  {
    "qid": "statistic-posneg-1207-6-0",
    "gold_answer": "TRUE. Explanation: The decomposition formula $\\mathbf{T}=G^{-1/2}\\mathbf{N}$ shows that all components of $\\mathbf{T}$ share the same scaling factor $G^{-1/2}$, where $G\\sim \\text{Gamma}(\\nu/2,\\nu/2)$. This creates a common stochastic volatility effect across all dimensions of $\\mathbf{T}$. Even when $R=I_{d\\times d}$ (no linear correlation), the shared $G$ induces tail dependence - extreme values in one component tend to co-occur with extremes in others because they are all scaled by the same heavy-tailed random variable. This generates a more complex dependence structure than linear correlation alone, particularly visible in joint tail behavior. The strength of this dependence increases as $\\nu$ decreases because lower degrees of freedom make the $t$-distribution's tails heavier, amplifying the common shock effect of $G$.",
    "question": "Is the statement 'The dependence induced in T, and therefore in Q, is dictated through the mutual latent random variable G' correct based on the decomposition formula $\\mathbf{T}{\\overset{D}{=}}G^{-1/2}\\mathbf{N}$? Explain the statistical mechanism behind this dependence structure.",
    "question_context": "Dependence Structure Analysis via t-Copula in High Dimensions\n\nFirst, consider $Q=N(0,\\Sigma)$ , where $\\Sigma$ is some positive definite correlation matrix. As for any $d$ there are potentially $d(d-1)/2$ unique correlation coefficients in this matrix, the number of possible specifications is enormous even for small $d$ . For simplicity, we only consider a single correlation number $\rho$ , which we either use (I) in all $d(d-1)/2$ or (II) in only $c<d(d-1)/2$ cases.<PARAGRAPH_SEPARATOR>Fig. B.3 displays the result of case (I). Now the superiority of our hypoRF test is challenged, though it manages to at least hold its own against MMD-full and ME-full. The roles of MMD-full and MMD are also reversed, the latter now displaying a much higher power, that in fact dwarfs the power of all other tests. MMD-full displays together with the Binomial test the smallest amount of power, both apparently suffering from the decrease in sample size. ME-full on the other hand, which suffers the same drawback, manages to have a strong performance, on par with the hypoRF. This is all the more impressive, keeping in mind that the ME is a test that scales linearly in N. Case (II) can be seen in Fig. B.4. Again the resulting “sparsity” is beneficial for our test, with the hypoRF now being on par with the powerful MMD test, and with ME-full only slightly above the Binomial test.<PARAGRAPH_SEPARATOR>In the second example, we study a change in dependence, which is more interesting than the simple change of the covariance matrix. In particular, Q is now given by a distribution that has standard Gaussian marginals bound together by a t-copula, see e.g., Demarta and McNeil (2005) or McNeil et al. (2015, Chapter 5). While the density and cdf of the resulting distribution $Q$ are relatively complicated, it is simple and insightful to simulate from this distribution, as described in Demarta and McNeil (2005): Let $x\\mapsto t_{\nu}(x)$ denote the cdf of a univariate $t$ -distribution with $\nu$ degrees of freedom, and $T_{\nu}(R)$ the multivariate $t$ -distribution with dispersion matrix $R$ and $\nu$ degrees of freedom. We first simulate from a multivariate $t$ -distribution with dispersion matrix $R$ and degrees of freedom $\nu$ , to obtain $\\mathbf{T}\\sim T_{\nu}(R)$ . In the second step, simply set $\\mathbf{Y}:=\\left(\\Phi^{-1}(t_{\nu}(T_{1})),\\dots,\\Phi^{-1}(t_{\nu}(T_{p}))\right)^{T}$ . We denote $Q=T_{\\Phi}(\nu,R)$ . What kind of dependency structure does $\\mathbf{Y}$ have? It is well known that $\\mathbf{T}\\sim t_{\nu}(R)$ has<PARAGRAPH_SEPARATOR>$$\\mathbf{T}{\\overset{D}{=}}G^{-1/2}\\mathbf{N},$$<PARAGRAPH_SEPARATOR>with $\\mathbf{N}\\sim N(0,R)$ and $G\\sim{\\tt G a m m a}(\\nu/2,\\nu/2)$ independent of N. As such, the dependence induced in T, and therefore in Q , is dictated through the mutual latent random variable G. It persists, even if $R=I_{d\\times d}$ and induces more complex dependencies than mere correlation. These dependencies are moreover stronger, the smaller $\\nu$ , though this effect is hard to quantify. One reason this dependency structure is particularly interesting in our case is that it spans more than two columns, contrary to correlation which is an inherent bivariate property. We again study the case (I) with all $d$ components tied together by the $t$ -copula, and (II) only the first $c=20<d$ components having a t-copula dependency, while the remaining $d-c=180$ columns are again independent $N(0,1)$ .<PARAGRAPH_SEPARATOR>The results for case (I) are shown in Fig. B.5. Now our tests, together with ME-full cannot compete with CPT-RF, MMD and MMD-full. However for the ME-full, this again depends on the chosen hyperparameters, for some settings ME-full was as good as MMD-full. Though there appears to be no clear way how to determine this. Both MMD-based tests manage to stay at almost one, even for $\\nu=8$ , which seems to be an extremely impressive feat. The CPT-RF test falls behind the two MMD-based tests, but has still an impressively high power, compared to our hypoRF test. Our best test, on the other hand, loses power quickly for $\\nu>4$ , while the Binomial test does so even for $\\nu>2$ . The results for case (II) shown in Fig. B.6, are similarly insightful. Given the difficulty of this problem, it is not surprising that almost all of the tests fail to have any power for $\\nu>3$ . The exception is once again the MMD, performing incredibly strong up to $\\nu=5$ . The performance of MMDboot is not only interesting in that it beats our tests, but also in how it beats all other kernel approaches in the same way. In particular, MMD-full stands no chance, which again is likely, in part, due to the reduced sample size the MMDboot has available for testing. Though hard to generalize, it appears from this analysis that a complex, rather weak dependence, is a job best done by the plain MMDboot.<PARAGRAPH_SEPARATOR>![](images/45ed23cd4a9c917a26b90d373a13a8186b7e722b6d498a9e4c5686ae79a0b70b.jpg)  \nFig. B.2. (Mean Shift) A point in the figures represents a simulation of size $S=200$ for a specific test and a $\\delta\\in(0,0.125,0.25,...,1)$ . Each of the $S=200$ simulation runs we sampled $n=300$ observations from a $d=200$ dimensional multivariate Gaussian distribution, where $c$ columns have a shift in mean of $\\frac{\\delta}{\\sqrt{c}}$ and likewise $n=300$ observations from $d=200$ independent standard normal distributions. The Random Forest used 600 trees and a minimal node size to consider a random split of 4."
  },
  {
    "qid": "statistic-posneg-1176-7-1",
    "gold_answer": "FALSE. The parameter $\\alpha$ is defined as $\\alpha=0.2+l\\frac{0.6}{49}$ for $l=0,1,\\ldots,49$. When $l=0$, $\\alpha=0.2$, and when $l=49$, $\\alpha=0.2+49\\times\\frac{0.6}{49}=0.2+0.6=0.8$. However, the maximum gap length is $80\\%$ of the study period, but the parameter $\\alpha$ itself ranges from 0.2 to 0.8, not the gap length.",
    "question": "Does the parameter $\\alpha$ in the unscheduled gap scenario range from 0.2 to 0.8?",
    "question_context": "Statistical Inference for Data-Collection Schemes with Gaps\n\nFigure 5. Data-collection schemes considered in the numerical experiments. From top to bottom, the three panels illustrate an unscheduled gap and an unscheduled gap with short and long scheduled gaps, respectively. <PARAGRAPH_SEPARATOR> We consider three data-collection schemes: <PARAGRAPH_SEPARATOR> Unscheduled gap: Define the gap be the interval <PARAGRAPH_SEPARATOR> $$ G_{U}(\\alpha)=[t_{\\operatorname*{max}}/2-\\alpha t_{\\operatorname*{max}}/2;t_{\\operatorname*{max}}/2-\\alpha t_{\\operatorname*{max}}/2] $$ <PARAGRAPH_SEPARATOR> and set $\\boldsymbol{z}_{t}=\\mathbb{1}\\left(t\\not\\in G(\\boldsymbol{\\alpha})\\right)$ . In this way, we mask the middle part of the trajectory which is of length $\\alpha t_{\\mathrm{max}}$ , where $\\begin{array}{r}{\\alpha=0.2+l\\frac{0.6}{49}}\\end{array}$ for $l=0$ , 1, . . . 49. This masking could correspond to a scenario in which a data collecting device malfunctions and does not record locations for a significant period of time (up to $80\\%$ of the study period), or when the signal is lost due to the characteristics of the built environment (i.e. thick walls). <PARAGRAPH_SEPARATOR> Unscheduled gap $^+$ short scheduled gaps: Recall the ‘on–off’ scheme described in Example 5 and set $o=u=25$ . The scheduled gaps are defined to be <PARAGRAPH_SEPARATOR> $$ G_{S}(u,o)=\\left\\{t:t\\in\\left[B_{t}^{\\prime};B_{t}^{\\prime}+I_{u}\\right)\\right\\}. $$ <PARAGRAPH_SEPARATOR> We set $\\boldsymbol{z}_{t}=\\mathbb{1}\\left(t\\not\\in G_{U}(\\alpha)\\cup G_{S}(u,o)\\right)$ . <PARAGRAPH_SEPARATOR> Unscheduled gap $^+$ long scheduled gaps: Similar to the previous case except that the scheduled gaps have length $o=u=50$ . <PARAGRAPH_SEPARATOR> All three schemes are illustrated in Figure 5."
  },
  {
    "qid": "statistic-posneg-890-1-2",
    "gold_answer": "FALSE. The inequality indicates a condition under which the success lead rule is preferable, but it is not universally applicable. The paper notes that the success lead rule is preferable unless the true ratio of probabilities of success is appreciably closer to 1 than θ*. Additionally, the condition is derived for θ* close to 1 and may not hold for larger values of θ*.",
    "question": "Is the success lead stopping rule always preferable when $\\frac{\\theta-1}{\\theta}>\\left(\\frac{\\theta^{*}-1}{\\theta^{*}}\\right)\\frac{\\log\\left\\{P^{*}/(1-P^{*})\\right\\}}{2\\lambda^{2}(P^{*})}$?",
    "question_context": "Comparison of Play-the-Winner Sampling Procedures in Binomial Populations\n\nIn §§3 and 4, we have shown that play-the-winner sampling is uniformly preferable to vector-at-a-time sampling when the probability requirement for correct selection is expressed in terms of the ratio of probabilities of success. We now compare the stopping rules for play-the-winner sampling based on success lead and inverse sampling and in particular we investigate the conditions under which each rule gives a smaller expected number of trials on the poorer treatment. Our analytical results will be appropriate to the case when θ* is close to 1. Using (10) and (24), where for θ* close to 1 and hence γ large we replace the Φ functions by 1, we have\n\n$$\n\\begin{array}{r}{E(N_{P W,I}^{(1)})-E(N_{P W,L}^{(1)})\\sim\\frac{\\theta(1-p_{2})}{p_{2}(\\theta-1)(\\theta-p_{2})}\\{(\\theta-1)r-\\theta s+(s-\\frac{1}{2})p_{2}\\}.}\\end{array}\n$$\n\nIt follows that the success lead stopping rule will be preferable over the complete parameter space if $r(\\theta-1)>\\theta s$ From §2 , the selection constant δ for the success lead stopping rule is the smallest integer for which $(\\theta^{*})^{s}\\geqslant P^{*}/(1-P^{*})$ .If we replace the inequality by an equality and ignore terms that are $O(\\theta^{*}-1)^{2}$ ,wehave\n\n$$\ns\\sim\\frac1{(\\theta^{*}-1)}\\log\\left(\\frac{P^{*}}{1-P^{*}}\\right).\n$$\n\nFor θ* close to 1, the selection constant γ for the inverse sampling rule is given by (17), so the success lead rule is uniformly preferable if\n\n$$\n\\frac{\\theta-1}{\\theta}>\\left(\\frac{\\theta^{*}-1}{\\theta^{*}}\\right)\\frac{\\log\\left\\{P^{*}/(1-P^{*})\\right\\}}{2\\lambda^{2}(P^{*})}.\n$$\n\nThe values of $\\{2\\lambda^{2}(P^{*})\\}^{-1}\\log\\left\\{P^{*}/(1-P^{*})\\right\\}$ are 0.67, 0.54 and 0.42,respectively,for $P^{*}=0.90_{\\mathrm{:}}$ 0.95 and 0.99. Thus the success lead stopping rule will be preferable unless the true ratio of probabilities of success is appreciably closer to 1 than the specified value of θ*"
  },
  {
    "qid": "statistic-posneg-664-0-0",
    "gold_answer": "TRUE. The function $f(p, q, r)$ is designed to maximize the value based on the minimum of $p$ and $q$ (fixed responses) relative to the term involving $r$ (variable responses). The denominator includes $r$ and $2 \\operatorname*{min}{(p, q)}$, meaning that higher $r$ (more variable responses) will decrease the value of $f$, while higher $\\operatorname*{min}{(p, q)}$ (more fixed responses) will increase the numerator and partially offset the denominator. Thus, tests with higher proportions of fixed responses are prioritized over those with higher proportions of variable responses, as the function aims to maximize the discriminatory power of the test.",
    "question": "The function $f(p, q, r) = \\operatorname*{min}{(p, q)} / [\\theta + (1 - \\theta) \\left\\{r + 2 \\operatorname*{min}{(p, q)}\\right\\}]$ is designed to maximize the number of contending populations ruled out by a test, where $p, q, r$ are proportions of fixed positive, fixed negative, and variable responses, respectively. Is it correct to say that this function prioritizes tests with higher proportions of fixed responses (either positive or negative) over those with higher proportions of variable responses?",
    "question_context": "Diagnostic Key Construction: Test Selection Function Analysis\n\nIn order to construct a diagnostic key the fixed responses to every test must be known for all populations. Since the key is constructed for repeated future use it is to be hoped that no test and population combinations have been left uninvestigated or unknown. In practice, Willcox et al. (1973) and Morse (1971) emphasize that this is not always the case. However, unless ‘unknown′ really means ‘inapplicable’ (Morse, 1971) when rightly a further informative category is present, such unknown responses may conveniently be regarded as variable. This explains our mild preference for the word ‘variable′ as opposed to ‘unknown’ used by Gower &Barnett (1971). <PARAGRAPH_SEPARATOR> In practice (Barnett & Pankhurst, 1974) roughly 100 populations and tests may be involved. For purely fixed binary tests, differential test costs and population prior probabilities have been incorporated into dynamic programming algorithms of key construction by Garey (1972). Here, with a specimen randomly chosen with probability corresponding to the population prior, representing the relative frequency of different specimens, the aim is to give that key which minimizes expected cost. Such a key will tend to make more use of cheaper tests provided they are effective in discriminating. It will also key out frequently occurring specimens cheaply, that is with few tests if test costs are equal. However, such a decision-theoretic approach would involve a totally prohibitive amount of computer time for the typical 100 populations cited. Thus only a suboptimal approach is feasible. For binary tests, the generally adopted procedure is described in the next paragraph. <PARAGRAPH_SEPARATOR> After the application of certain of the tests, suppose there remain n contending populations and 8 binary tests. Following the notation of Gower & Barnett, for the ith binary test $(\\pmb{\\mathscr{i}}=\\pmb{1}$ , $\\left\\langle\\dots,\\pmb{\\vartheta}\\right\\rangle$ ,let $p_{i},q_{4}{\\mathrm{~and~}}r_{i}(p_{i}+q_{i}+r_{i}=1)$ be the proportions of contending populations which give rise to fixed positive, fxed negative and variable responses, respectively. $\\operatorname{Let}f(.,.,.)$ be & realvalued function of the three arguments. The test $(p_{i},q_{i},r_{i})$ , or more briefly $\\it{\\Delta}T_{i}$ , i preferred to another test $\\pmb{T_{j}}=(\\pmb{p_{j}},\\pmb{q_{j}},\\pmb{r_{j}})$ $f(p_{i},q_{i},r_{i})>f(p_{j},q_{j},r_{j})$ . To construct the key, & test is selected to maximize $f$ . Thus $\\it{\\mathcal{T}_{i}}$ is selected if. <PARAGRAPH_SEPARATOR> $$ f(p_{i},q_{i},r_{i})>f(p_{j},q_{j},r_{j})\\quad(j=1,...,s;j\\mp i). $$ <PARAGRAPH_SEPARATOR> As described this suboptimal approach measures the usefulness of each test by the number of contending populations ruled out, without reference to future test selections. The method uses only fixed responses to achieve identification. Extension to polytomous tests is straightforward and described later in $\\S2$ <PARAGRAPH_SEPARATOR> Often, when referring to a general binary test, the test label will be dropped so that $(\\pmb{\\mathscr{p}},\\pmb{q},\\pmb{r})$ will be used to denote a test. The number, $\\pmb{\\mathscr{n}}$ , of contending populations, decreases as one progresses through the key. Consequently the proportions $(\\pmb{\\mathscr{p}},\\pmb{q},\\pmb{r})$ of contending populations defined above will also change as other tests are performed. <PARAGRAPH_SEPARATOR> A listof functions, $f,$ used in the past is given by Gower & Payne (1975). Each function must be preceded by a minus sign to conform to our presentation since they base test choice on minimization rather than maximization. Gower $\\&$ Payne develop various criteria for judging differentfunctions $f$ and arrive at a new function <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\operatorname*{min}{(p,q)/[\\theta+(1-\\theta)\\left\\{r+2\\operatorname*{min}{(p,q)}\\right\\}]},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\pmb{\\theta}$ is a constant to be chosen such that $0<\\theta<1$ . This function satisfies all their desiderata for binary tests.· <PARAGRAPH_SEPARATOR> In $\\S2$ we place simple mathematical requirements onf, one of which is proved to be suffcient for removal of ‘errors′ as defined by Gower & Payne (1975). Even in conjunction with the other requirements, this shows that a very wide class of functions satisfy their criteria. Two special cases are given. These functions are extended to cover polytomous tests by generalizing the idea of‘error'. Section 3 discusses two further natural requirements on $f.$ Theseconflict somewhat with the removal of ^error’ and lead to two functions derived in $\\S4$ <PARAGRAPH_SEPARATOR> Perhaps the overriding danger of all such deterministic identification procedures is false identification. If fixed responses are in fact occasionally variable, as is borne out by the probabilities tabulated by Bascomb et al.(1973), then the specimens may be identified as belonging to the wrong population. Two practical solutions are (i) further testing to corroborate the identification (Pankhurst, 1970), and (i) calculation of posterior probabilities at the end of the key (Willcox et al., 1973), with possible further testing."
  },
  {
    "qid": "statistic-posneg-2116-0-0",
    "gold_answer": "FALSE. While the claim itself is correct under the given conditions, the proof provided does not fully establish the limit as the partial derivative of the tail dependence function. The proof shows upper and lower bounds converging to $b_{1|2}(w_{1}|w_{2})$, but it lacks a detailed justification for the uniform convergence required to ensure the limit holds for all $w_1, w_2 > 0$. The proof also assumes local uniformity without explicit verification, which is necessary for the conclusion to hold generally.",
    "question": "Is the claim that $\\widehat{C}_{1|2}(u w_{1}|u w_{2})\\sim b_{1|2}(w_{1}|w_{2})$ as $u\\to0^{+}$ under the given conditions correct, and does the proof correctly establish the limit as the partial derivative of the tail dependence function?",
    "question_context": "Tail Dependence and Conditional Tail Expectations in Extreme Value Theory\n\nThe SI condition plays an important role in deriving the limit for $\\widehat{C}_{1|2}$ . First, we give sufficient conditions that lead to the limit of $\\widehat{C}_{1|2}(u w_{1}|u w_{2})$ as $u\\to0^{+}$ being the partial derivative of the associated tail dependence function. <PARAGRAPH_SEPARATOR> Lemma 9. Suppose $X_{1},X_{2}$ are continuous random variables with copula C, and $X_{1}$ is $S I$ in $X_{2}$ . Let $\\widehat{c}$ be the survival copula, and $\\widehat{C}(u w_{1},u w_{2})\\stackrel{\\rightharpoonup}{\\sim}u b(w_{1},\\bar{w}_{2})$ as $u\\to0^{+}$ , where $b$ is the lower tail dependence function of the copula $\\widehat{\\boldsymbol{C}}$ and $b(w_{1},w_{2})$ is partially differentiable with respect to $w_{2}$ , then $\\widehat{C}_{1|2}(u w_{1}|u w_{2})\\sim b_{1|2}(w_{1}|w_{2})$ as $u\\to0^{+}$ , where $b_{1|2}(w_{1}|w_{2})=\\partial b(w_{1},w_{2})/\\partial w_{2}$ . <PARAGRAPH_SEPARATOR> Proof. For a small $\\epsilon>0$ , write <PARAGRAPH_SEPARATOR> $$ \\int_{w_{2}-\\epsilon}^{w_{2}}\\widehat{C}_{1|2}(u w_{1}|u t)d t=\\int_{u(w_{2}-\\epsilon)}^{u w_{2}}\\frac{\\widehat{C}_{1|2}(u w_{1}|y)}{u}d y=\\frac{\\widehat{C}(u w_{1},u w_{2})-\\widehat{C}(u w_{1},u(w_{2}-\\epsilon))}{u}. $$ <PARAGRAPH_SEPARATOR> Then the condition that $X_{1}$ is SI in $X_{2}$ implies that $\\widehat{C}(\\boldsymbol{u}|\\boldsymbol{v})$ is nonincreasing in $v$ , and then (12) implies that <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{u\\to0^{+}}\\widehat C_{1|2}(u w_{1}|u w_{2})\\leq\\operatorname*{liminf}_{u\\to0^{+}}\\frac{\\widehat C(u w_{1},u w_{2})-\\widehat C(u w_{1},u(w_{2}-\\epsilon))}{u\\epsilon}=\\frac{b(w_{1},w_{2})-b(w_{1},w_{2}-\\epsilon)}{\\epsilon} $$ <PARAGRAPH_SEPARATOR> Then letting $\\epsilon\\rightarrow0^{+}$ leads to <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{u\\to0^{+}}\\widehat{C}_{1|2}(u w_{1}|u w_{2})\\leq b_{1|2}(w_{1}|w_{2}). $$ <PARAGRAPH_SEPARATOR> Similarly, considering the integration in (12) from $w_{2}$ to $w_{2}+\\epsilon$ will imply that lim $\\begin{array}{r}{\\operatorname*{inf}_{u\\to0^{+}}\\widehat C_{1|2}(u w_{1}|u w_{2})\\geq b_{1|2}(w_{1}|w_{2}).}\\end{array}$ Therefore, $\\widehat{C}_{1|2}(u w_{1}|u w_{2})\\sim b_{1|2}(w_{1}|w_{2})$ as $u\\to0^{+}$ . □"
  },
  {
    "qid": "statistic-posneg-751-3-1",
    "gold_answer": "TRUE. The context explains that sampling the stock price at equal time intervals leads to logarithmic price changes that are independent identically distributed Gaussian random variables, irrespective of the length of the sampling interval. This means the random walk model emerges from the continuous time model for any sampling interval, supporting the validity of the random walk hypothesis under the assumptions of the continuous time model.",
    "question": "Does the continuous time model for stock prices, as described by the Ito stochastic differential equation, imply that the random walk model is valid for any sampling interval?",
    "question_context": "Random Walk Hypothesis and Stock Price Modeling\n\nFrom an economic point of view (see Fama (1965)) it is most sensible to analyse stock prices in terms of logarithmic price changes; i.e. if $\\left\\{S_{t}\\right\\}$ is the stock price series then $\\left\\{\\bar{U}_{t}\\right\\}=\\left\\{\\ln S_{t}-\\ln S_{t-1}\\right\\}$ is the series that should be investigated. The assumption that $\\left\\{U_{t}\\right\\}$ consists of independent identically distributed Gaussian random variables is the so-called random walk hypothesis for stock prices, and many empirical investigations (see, for example, Ali and Giacotto (1982) and references therein) have been carried out to test this hypothesis. Similar studies exist for commodity prices (Bird, 1985). <PARAGRAPH_SEPARATOR> Although roughly correct for a majority of stock data, it has been noted by several researchers (e.g. Fielitz (1971), Hsu et al. (1974), Wichern et al. (1976) and Ali and Giacotto (1982)) that examples can be found which deviate significantly from the random walk hypothesis. There are examples of a non-Gaussian distribution, a stepwise time dependence in the variance and of correlations in parts of $\\left\\{U_{t}\\right\\}$ .Since correlations mean that the eficient market hypothesis does not hold (Fama, 1970), it is important to detect and pinpoint sections of $\\left\\{U_{t}\\right\\}$ where correlations exist. Also this should be examined in the context of time dependence of the variance since (Bird, 1985) certain relationships between the correlation and variance can be given an economic interpretation. Correlations and variance can be measured in terms of AR parameters and residual variance, and our methods are especially designed for finding data segments with constant parameters and for pinpointing change points. In addition we would like to give an economic interpretation of the transition points between various states. <PARAGRAPH_SEPARATOR> It should also be noted that the random walk hypothesis is directly related to more recent work based on continuous time models for the stock prices. In a very influential paper on option pricing Black and Scholes (1973) assumed that stock prices are described by an Ito stochastic differential equation <PARAGRAPH_SEPARATOR> $$ \\frac{\\mathrm{d}S(t)}{S(t)}=\\mu~\\mathrm{d}t+\\sigma~\\mathrm{d}W(t) $$ <PARAGRAPH_SEPARATOR> where $\\left\\{W(t)\\right\\}$ is a Wiener process, $\\pmb{\\mu}$ the drift parameter and $\\sigma^{2}$ the variance. The process $\\langle S(t)\\rangle$ itself is termed geometric Brownian motion. Equation (5.1) may be solved analytically for $s(t)$ and for $t\\geqslant0$ we obtain <PARAGRAPH_SEPARATOR> $$ S(t)=S(0)\\exp[(\\mu-{\\textstyle{\\frac{1}{2}}}\\sigma^{2})t+\\sigma W(t)]. $$ <PARAGRAPH_SEPARATOR> Sampling $S(t)$ at equal time intervals $\\Delta$ it follows that <PARAGRAPH_SEPARATOR> $$ \\ln S(k\\Delta)-\\ln S[(k-1)\\Delta]=\\Delta(\\mu-{\\textstyle{\\frac{1}{2}}}\\sigma^{2})+e_{k} $$ <PARAGRAPH_SEPARATOR> where $\\left\\{\\boldsymbol{e}_{k}\\right\\}=\\sigma[W(k\\Delta)-W((k-1)\\Delta)]$ consists of independent identically distributed Gaussian random variables. This means that the random walk model emerges irrespective of the length of the sampling interval $\\Delta.$ . Thus, if we examine the properties of the time series $\\left\\{U_{t}\\right\\}=\\left\\{\\ln S_{t}-\\ln\\bar{S}_{t-1}\\right\\}$ , this implies that we are also examining the validity of the continuous time model (5.1), which has been extensively used recently in the theory of finance (Merton, 1982). The original paper by Black and Scholes already contained some evidence that equation (5.1) may not always be an accurate model, and more general models have been introduced (see, for example, Aase (1984) and references therein), where the stock price can change suddenly by introducing a <PARAGRAPH_SEPARATOR> Poisson process component. The models proposed in this paper are meant to describe sudden structural changes via the AR parameters, and this offers an interesting alternative to the generalized continuous time models. We now illustrate our methods on the IBM example. <PARAGRAPH_SEPARATOR> # 5.1. IBM series"
  },
  {
    "qid": "statistic-posneg-1492-0-0",
    "gold_answer": "FALSE. The correct derivation of the first-order local influence measure is $F I(\\omega(0)) = \\frac{[\\pmb{v}^{T}\\partial_{\\omega}f(\\omega(0))]^{2}}{\\pmb{v}^{T}G(\\omega(0))\\pmb{v}}$. The economic intuition behind this measure is to assess the sensitivity of the model to perturbations by comparing the squared gradient of the objective function with respect to the perturbation to the Fisher information matrix. This helps identify influential observations or parameters that significantly affect the model's outcomes.",
    "question": "Is the first-order local influence measure $F I(\\omega(0))$ correctly derived as $\\frac{[\\pmb{v}^{T}\\partial_{\\omega}f(\\omega(0))]^{2}}{\\pmb{v}^{T}G(\\omega(0))\\pmb{v}}$? Explain the economic intuition behind this measure.",
    "question_context": "Bayesian Local Influence Analysis in Semiparametric Structural Equation Models\n\nThe paper discusses a semiparametric structural equation model (SEM) with Bayesian local influence analysis. The model includes measurement and structural equations, with perturbations introduced to assess sensitivity. Key formulas include the first-order local influence measure $F I(\\omega(0))$ and the Fisher information matrix $G(\\omega(0))$. The paper also details perturbation schemes for data, prior distributions, and sampling distributions."
  },
  {
    "qid": "statistic-posneg-1480-8-1",
    "gold_answer": "FALSE. The integrability condition (10) only requires the existence and continuity of the expected Hessian matrix 𝔼(H₂ρ(X,ν)²) near μ. It does not impose any almost sure positivity or definiteness conditions on the Hessian H₂ρ(X,μ)² itself. The Hessian's properties are relevant for the local behavior of the Fréchet mean but are not part of the integrability conditions for the CLT.",
    "question": "Does the integrability condition (10) imply that the Hessian matrix H₂ρ(X,μ)² must be positive definite almost surely?",
    "question_context": "CLT for Fréchet ρ-means on Manifolds\n\nThe paper discusses the Central Limit Theorem (CLT) for Fréchet ρ-means on manifolds, focusing on the conditions required for the CLT to hold. It introduces integrability conditions on a random variable X at a location μ ∈ P, involving the expectation and covariance of the gradient and Hessian of ρ². The proof adapts ideas from Bhattacharya & Patrangenaru (2005), using Taylor expansions and the classical CLT to show convergence in distribution."
  },
  {
    "qid": "statistic-posneg-1881-0-1",
    "gold_answer": "FALSE. Explanation: The paper argues that the proposed method is optimal up to constants, as its complexity matches the logarithmic lower bound in terms of asymptotic behavior ($O(\\log K)$ per update). The additive $4K$ term becomes negligible for large $N$, and the multiplicative constant 3 in the logarithmic term is close to the theoretical minimum implied by the lower bound.",
    "question": "Does the lower bound of $(a\\log K)(N-K)$ steps for any running median algorithm imply that the proposed method's complexity of $$4K+\\{3\\log(K/2)\\}(N-K)$$ is suboptimal by a constant factor?",
    "question_context": "Optimal Median Smoothing: Algorithm Complexity and Efficiency\n\nHigh variability in a given series $X_{1},...,X_{N}$ may obscure important structural features which would become evident if the data were replaced by a smoother, less variable version $X_{1}^{*},...,X_{N}^{*}$. One smoothing strategy consists of replacing $X_{i}$ by the running median of order $K=2k+1$: $$X_{i}^{*}=\\operatorname*{median}(X_{i-k},...,X_{i+k}),\\quad\\quad\\quad i=k+1,...,N-k$$. The paper describes an efficient running median algorithm using the heap data structure and compares its complexity to other methods, demonstrating its optimality up to constants."
  },
  {
    "qid": "statistic-posneg-1307-0-0",
    "gold_answer": "TRUE. The condition that A(x) has no roots on the unit circle is necessary for the existence of the factorization B(x)B(x^{-1})=A(x) where B(x) has no roots of magnitude ≤1. This is because roots on the unit circle would prevent the factorization from satisfying the condition on B(x).",
    "question": "Is it true that the Cramér-Wold factorization requires polynomial A(x) to have no roots on the unit circle for the factorization B(x)B(x^{-1})=A(x) to exist?",
    "question_context": "Cramér-Wold Factorization in Time Series Analysis\n\nGiven a self-reciprocal polynomial A(x)=a_{0}+a_{1}(x+x^{-1})+a_{2}(x^{2}+x^{-2})+...+a_{n}(x^{n}+x^{-n}), the Cramer-Wold factorization involves finding a polynomial B(x)=b_{0}+b_{1}x+b_{2}x^{2}+...+b_{n}x^{n} with no roots of magnitude ≤1, such that B(x)B(x^{-1})=A(x). Such a factorization always exists provided A has no roots on the unit circle. The Cramer-Wold factorization is fundamental in time series filtering and control. Two algorithms for this factorization are known: Bauer's algorithm (linearly convergent, 2n operations per iteration) and Wilson's algorithm (quadratically convergent, 3n^{2}/2+O(n) operations per iteration). One iteration of Wilson's algorithm takes about the same time as 3n/4 iterations of Bauer's."
  },
  {
    "qid": "statistic-posneg-623-0-0",
    "gold_answer": "FALSE. The reduced model's estimator $\\hat{\\mathbf{b}}_{\\mathrm{red}}$ generally has narrower confidence intervals due to the reduced dimensionality (from $p=46$ to $K=3$), but this is contingent on the correct specification of $\\mathbf{W}$. If $\\mathbf{W}$ is misspecified, the variance reduction may be offset by increased bias, and in extreme cases, the confidence intervals could be misleading or even wider due to model misspecification effects. The paper explicitly notes that $\\hat{\\mathbf{b}}_{\\mathrm{red}}$ is not robust to misspecifications of $\\mathbf{W}$.",
    "question": "Is the reduced model's estimator $\\hat{\\mathbf{b}}_{\\mathrm{red}}$ guaranteed to have narrower confidence intervals than the full model's estimator $\\hat{\\mathbf{b}}_{\\mathrm{full}}$ under all specifications of the weight matrix $\\mathbf{W}$?",
    "question_context": "Comparison of Full and Reduced Linear Models in Epidemiologic Studies\n\nWe conduct an empirical illustration of these methods by analysing the data that were reported by Oliveria et al. (2009). <PARAGRAPH_SEPARATOR> In the SONIC study, the total number of naevi of each child was determined by using imaging techniques. The natural logarithm of the total number of naevi was treated as a continuous outcome and was assumed to follow a normal distribution. The mean and standard deviation of the outcome were 1.81 and 0.94 respectively (on the log-scale). The skewness and kurtosis of the sample distribution were $-0.03$ and 2.61 respectively. The following host characteristics were recorded for each child: gender, ethnicity or race (from the school records) and a sun sensitivity measure. There were no missing data for the outcome or the host characteristics. Details about the missing exposure data are reported by Oliveria et al. (2009) and are briefly summarized here. Both the children and their parent or legal guardian completed the questionnaire. The responses that were provided by the children were used in all the analyses. The number of missing data was relatively small, ranging from 26 children not responding to the shirt questions to 56 children not responding to the sunscreen questions. These missing data were imputed by using the responses provided by the parents that were available for all the students. Thus, we treated the exposure data as complete in all our analyses. <PARAGRAPH_SEPARATOR> Oliveria et al. (2009) analysed the data that were obtained during 2004 by using linear regression to identify the exposures that were significantly associated with log(total naevus count), after adjusting for the host characteristics. This analysis, which we refer to as the full model analysis and which ignores the thematic design of the questionnaire, is based on the following linear model: <PARAGRAPH_SEPARATOR> $$ {\\bf Y}={\\bf S}{\\bf a}_{\\mathrm{full}}+{\\bf X}{\\bf b}+{\\bf e}_{\\mathrm{full}}. $$ <PARAGRAPH_SEPARATOR> Here Y is the vector of outcomes from $N$ independent individuals, $\\mathbf{S}$ is the matrix of host characteristics (including a column vector of 1s for the intercept) with effects $\\mathbf{a}_{\\mathrm{full}}$ , $\\mathbf{X}$ is the $N\\times p$ matrix of exposures with effects $\\mathbf{b}$ and $\\mathbf{e}_{\\mathrm{full}}$ is a vector of independent error terms with $N(0,\\sigma^{2})$ distribution. (The matrix S may contain other variables besides the host characteristics, as deemed necessary.) In the SONIC study, $N=443$ and $p=46$ . Our goal is to obtain precise estimates of b. The maximum likelihood estimator (MLE) of $\\mathbf{b}$ , which is denoted $\\hat{\\mathbf{b}}_{\\mathrm{full}}$ , and its variance can be obtained via least squares regression. Fig. 1(a) shows $\\hat{\\mathbf{b}}_{\\{\\mathsf{u}\\|}}$ and their $95\\%$ confidence intervals (CIs) for the SONIC exposures. As reported by Oliveria et al. (2009), wearing a shirt (rarely, sometimes or often or always) when outside at the beach or pool was significantly associated with increased log(total naevus count), and similarly for wearing a hat often or always when outdoors, which were contradictory to some other studies. The host characteristics were significantly associated with the outcome (the $\\mathbf{a}_{\\mathrm{full}}$ -estimates and CIs are not shown). <PARAGRAPH_SEPARATOR> ![](images/a338c01b0724ad5d4b458ab452ba5e6f2f07de91696639ce512260133fa0ebbe.jpg)  Fig. 1. Results of the SONIC questionnaire analysis: (a) $\\hat{\\mathbf{b}}_{\\{\\mathsf{u}\\|}}$ , (b) $\\hat{\\mathbf{b}}_{\\mathrm{red}}$ and (c) $\\hat{\\mathbf{b}}_{\\mathrm{eb,ex}}$ ( , $95\\%$ CI of an exposure (organized from top to bottom as given in Table 1); $\\spadesuit$ , parameter estimate; for ease of visualization, the questions are separated by red horizontal lines), and (d) precision of the shrinkage estimators relative to the full model estimator <PARAGRAPH_SEPARATOR> To incorporate the themes into the analysis, we defined linear combinations by using the predefined weights that are given in Table 1, which can be interpreted as follows. Sun exposure is the cumulative number of hours spent in the sun. Weights of 1 and 2 are assigned if a person spends $_{3-4\\mathrm{h}}$ and more than $^{4\\mathrm{h}}$ respectively by the pool or beach during weekdays, and similarly for weekend days and for time spent outdoors. The weight is 0 if a person reports spending less than $^{1\\textrm{h}}$ by the beach or pool. These weights are chosen to reflect a trend-type effect, which is used in several epidemiology studies (for example, genotype data are recorded as 0, 1 or 2 to represent the number of copies of a minor allele). Similarly, sunburn is the cumulative number of redness and painful burns. Sun protection behaviour is defined in a similar manner. Let W denote a $p\\times K$ matrix of weights, where $K$ is the number of themes $K=3$ in the SONIC study). Thus, XW is an $N\\times K$ matrix, where each column quantifies a predefined theme. The linear model based on the themes, which is referred to as the reduced model, is given by <PARAGRAPH_SEPARATOR> $$ {\\bf Y}={\\bf S}{\\bf a}_{\\mathrm{red}}+{\\bf X}{\\bf W d}+{\\bf e}_{\\mathrm{red}}. $$ <PARAGRAPH_SEPARATOR> Here $\\mathbf{a}_{\\mathrm{red}}$ are the effects of the variables contained in S, and d denotes the effects of the themes. Its MLE $\\hat{\\mathbf{d}}$ and variance can be obtained via least squares regression. Under the reduced model, the exposure effect is Wd. Its MLE is $\\hat{\\mathbf{b}}_{\\mathrm{red}}=\\mathbf{W}\\hat{\\mathbf{d}}$ , and the variance is $\\mathbf{W}\\mathrm{var}(\\hat{\\mathbf{d}})\\mathbf{W}^{\\mathrm{T}}$ . Fig. 1(b) shows $\\hat{\\bf b}_{\\mathrm{red}}$ and their $95\\%$ CIs for the SONIC data. The exposures representing sunburn and sun protection behaviours were significantly associated with increased naevus counts, although the magnitudes of the effects were small relative to Fig. 1(a). Further, the lengths of the CIs of $\\hat{\\bf b}_{\\mathrm{red}}$ were narrower than those of $\\hat{\\mathbf{b}}_{\\mathrm{full}}$ , demonstrating the gains in precision of this approach. The host characteristics were significantly associated with the outcome, as before (the $\\mathbf{a}_{\\mathrm{red}}$ -estimates and CIs are not shown). However, the conclusions regarding the exposures are substantially different from those of the full model analysis. <PARAGRAPH_SEPARATOR> In the absence of any external information or a priori knowledge about the themes, one approach for estimating the exposure effects would be to use the full model. The MLE of the full model provides unbiased estimates of the exposure effects if the model is correctly specified and in the absence of any unmeasured confounders. The reduced model is appealing since it can potentially improve the precision relatively to the full model. However, the effects may be biased and the type I error may be inflated if $\\mathbf{W}$ is inaccurate, even in the absence of any unmeasured confounders. Therefore, $\\hat{\\bf b}_{\\mathrm{red}}$ is not robust to misspecifications of the linear combinations. The estimates from the full and reduced model would be the same, i.e. $\\ensuremath{\\mathbf{b}}=\\ensuremath{\\mathbf{W}}\\ensuremath{\\mathbf{d}}$ only when W is correctly specified. In this paper we shall examine alternative methods to obtain estimates having better precision than those obtained from the full model and less false positive findings than the reduced model. <PARAGRAPH_SEPARATOR> The remainder of this paper is organized as follows. In Section 2 we describe a preliminary hypothesis testing procedure that tests the accuracy of the predefined linear combinations (Sen, 1986), and we show that this approach may not be effective for the SONIC data. In Section 3, we propose a class of empirical Bayes-type shrinkage estimators that are a weighted average of the estimates that are obtained from the full and reduced models. We illustrate their properties by using simulations in Section 4, analyse the SONIC data in Section 5 and conclude the paper with a discussion in Section 6. Our investigations shed light on several important properties of the shrinkage estimators. The difference between the exposure effect estimates that are obtained from the full and reduced models can be treated as a nuisance parameter. We show that some members of the proposed class of shrinkage estimators are intimately related to the preliminary hypothesis test, whereas the other members are related to a random-effects specification for the nuisance parameter. We also show that specifying an exchangeable prior distribution for the nuisance parameter leads to a shrinkage estimator that is robust to misspecifications of the predefined linear combinations."
  },
  {
    "qid": "statistic-posneg-1430-4-3",
    "gold_answer": "FALSE. While the context mentions that $\\delta_{n}^{-d}\\lambda(A_{n})$ dominates $n\\lambda_{f}(A_{n}^{c})$ when $\\lambda(\\operatorname{pos}f)<\\infty$ and $n\\delta_{n}^{d}\\to\\infty$, it also provides examples (e.g., Example 4 and 5) where $n\\lambda_{f}(A_{n}^{c})$ can dominate or be comparable to $\\delta_{n}^{-d}\\lambda(A_{n})$ depending on the tail behavior of the density $f$.",
    "question": "Does the term $\\delta_{n}^{-d}\\lambda(A_{n})$ always dominate $n\\lambda_{f}(A_{n}^{c})$ in the upper bound for $\\beta_{n}$ when $\\lambda(\\operatorname{pos}f)<\\infty$ and $n\\delta_{n}^{d}\\to\\infty$?",
    "question_context": "Expected Number of Nonempty Bins Analysis\n\nCorollary 3.6. If $\\beta_{n}=o(n)$ , then $n\\delta_{n}^{d}\\rightarrow\\infty$ . <PARAGRAPH_SEPARATOR> Proof. Let $\\beta_{n}=o(n)$ . Then, by Theorem 3.2, both $(n\\delta_{n}^{d})^{-1}\\lambda(A_{n})\\to0$ , and $\\lambda_{f}(A_{n}^{c})\\to0$ . Suppose that $n\\delta_{n}^{d}\\not\\to\\infty$ . Then there is a $C>0$ such that for a subsequence (still denoted by $n\\:\\delta_{n}^{d},$ ) we have $n\\delta_{n}^{d}\\leqslant C$ . It follows that $\\lambda(A_{n})\\to0$ , and, by the absolute continuity of the measure $\\lambda_{f}$ , also $\\lambda_{f}(A_{n})$ $\\rightarrow0$ . But then $1=\\lambda_{f}(A_{n})+\\lambda_{f}(A_{n}^{c})\\to0$ , a contradiction. K <PARAGRAPH_SEPARATOR> It is possible to prove an upper bound of the same type as the lower bound formula (3.4) provided that $f$ satisfies a regularity condition. Let $B=\\left\\{x\\in\\mathbb{R}^{d}\\vert\\Vert x\\Vert\\leqslant1\\right\\}$ be the closed unit ball and denote $\\dot{\\rho_{n}}=\\sqrt{d}\\delta_{n}$ . Also, for two sets $S$ and $T.$ , let $S+T=\\left\\{x+y\\mid x\\in S,y\\in T\\right\\}$ . For our upper bound result we need $f$ to satisfy either <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\frac{\\delta_{n}^{-d}\\lambda((A_{n}+\\rho_{n}B)\\cap A_{n}^{c})}{\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c})}{=}0, $$ <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\frac{n\\lambda_{f}((A_{n}^{c}+\\rho_{n}B)\\cap A_{n})}{\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c})}{=}0. $$ <PARAGRAPH_SEPARATOR> Note that the denominator $\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c})$ is always positive: if $\\lambda(A_{n})=0$ , then $\\lambda_{f}(A_{n}^{c})=\\int_{A_{n}^{c}}f=\\int_{\\mathbb{R}^{d}}f=1$ . It turns out that the densities often used in practice actually satisfy a stronger condition such as (cf. (3.9)) <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\frac{\\lambda((A_{n}+\\rho_{n}B)\\cap A_{n}^{c})}{\\lambda(A_{n})}=0. $$ <PARAGRAPH_SEPARATOR> We give examples in the next subsection. The set $(A_{n}+\\rho_{n}B)\\cap A_{n}^{c}$ is a \\`\\`shell'' of thickness $\\rho_{n}$ around the set $A_{n}$ . The condition (3.11) means that, compared to the volume of $A_{n}$ itself, the volume of the shell should become negligible as $n\\to\\infty$ . <PARAGRAPH_SEPARATOR> Theorem 3.7. $I f f$ satisfies either (3.9) or (3.10), then <PARAGRAPH_SEPARATOR> $$ \\beta_{n}\\leqslant(1+o(1))[\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c})]. $$ <PARAGRAPH_SEPARATOR> Proof. Suppose that the condition (3.9) holds. First we find an upper bound for $1-(1-\\lambda_{f}(B_{n\\mu}))^{n}$ . Trivially, $1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\leqslant1$ , for all $n$ and $\\mu$ . Also, for all $n$ and $\\mu$ , <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{1-(1-\\lambda_{f}(B_{n\\mu}))^{n}=\\lambda_{f}(B_{n\\mu})\\displaystyle\\frac{1-(1-\\lambda_{f}(B_{n\\mu}))^{n}}{1-(1-\\lambda_{f}(B_{n\\mu}))}}} {{}} {{=\\lambda_{f}(B_{n\\mu})\\displaystyle\\sum_{k=0}^{n-1}(1-\\lambda_{f}(B_{n\\mu}))^{k}\\leqslant n\\lambda_{f}(B_{n\\mu}),}}\\end{array} $$ <PARAGRAPH_SEPARATOR> because $(1-\\lambda_{f}(B_{n\\mu}))^{k}\\leqslant1$ for all $k$ . Thus, <PARAGRAPH_SEPARATOR> $$ 1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\leqslant1\\land(n\\lambda_{f}(B_{n\\mu})) $$ <PARAGRAPH_SEPARATOR> for all $n$ and $\\mu$ . Define <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{R_{n}=\\big\\{\\mu\\in\\mathbb{Z}^{d}|B_{n\\mu}\\subset A_{n}\\big\\},} &{} &{S_{n}=\\big\\{\\mu\\in\\mathbb{Z}^{d}|B_{n\\mu}\\subset A_{n}^{c}\\big\\},\\qquadT_{n}=(R_{n}\\cup S_{n})^{c}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Then $R_{n},S_{n},T_{n}$ are disjoint and $\\mathbb{Z}^{d}=R_{n}\\cup S_{n}\\cup T_{n}$ . It follows from (3.13) that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{\\lefteqn{\\beta_{n}=\\sum_{\\mu}\\big[1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\big]}} &{=\\displaystyle{\\sum_{\\mu\\in\\mathcal{R}_{n}\\cup T_{n}}\\big[1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\big]+\\sum_{\\mu\\in S_{n}}\\big[1-(1-\\lambda_{f}(B_{n\\mu}))^{n}\\big]}} &{\\leqslant|R_{n}\\cap T_{n}|+n\\displaystyle{\\sum_{\\mu\\in S_{n}}\\lambda_{f}(B_{n\\mu})}} &{=\\delta_{n}^{-d}\\displaystyle{\\sum_{\\mu\\in\\mathcal{R}_{n}\\cup T_{n}}\\lambda(B_{n\\mu})+n\\displaystyle{\\sum_{\\mu\\in S_{n}}\\lambda_{f}(B_{n\\mu})}}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Here <PARAGRAPH_SEPARATOR> $$ \\sum_{\\mu\\in{\\cal R}_{n}\\cup{\\cal T}_{n}}\\lambda(B_{n\\mu})=\\lambda(A_{n})+\\sum_{\\mu\\in{\\cal T}_{n}}\\lambda(B_{n\\mu}\\cap A_{n}^{c}) $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\sum_{\\mu\\in S_{n}}\\lambda_{f}(B_{n\\mu})\\leqslant\\lambda_{f}(A_{n}^{c}). $$ <PARAGRAPH_SEPARATOR> Thus, by the estimate (3.14), <PARAGRAPH_SEPARATOR> $$ \\beta_{n}\\leqslant\\delta_{n}^{-d}\\lambda(A_{n})+\\delta_{n}^{-d}\\sum_{\\mu\\in{\\cal T}_{n}}\\lambda(B_{n\\mu}\\cap A_{n}^{c})+n\\lambda_{f}(A_{n}^{c}). $$ <PARAGRAPH_SEPARATOR> Let $\\mu\\in T_{n}$ and suppose that $y\\in B_{n\\mu}\\cap A_{n}^{c}$ . Since $B_{n\\mu}\\cap A_{n}\\neq\\emptyset$ , we can pick an $x\\in B_{n\\mu}\\cap A_{n}$ and write $y=x+(y-x)$ with $\\|y-x\\|\\leqslant\\sqrt{d}\\delta_{n}\\overset{\\cdot}{=}\\rho_{n}$ . Therefore, $B_{n\\mu}\\cap A_{n}^{c}\\subset(A_{n}+\\rho_{n}B)\\cap A_{n}^{c}$ , where $B$ is the closed unit ball. We then have <PARAGRAPH_SEPARATOR> $$ \\sum_{\\mu\\in T_{n}}\\lambda(B_{n\\mu}\\cap A_{n}^{c})\\leqslant\\lambda((A_{n}+\\rho_{n}B)\\cap A_{n}^{c}) $$ <PARAGRAPH_SEPARATOR> which together with (3.15) and the condition (3.9) implies (3.12). <PARAGRAPH_SEPARATOR> The proof is analogous for the condition (3.10). Only now the split of (3.14) in the $\\mu$ -sum is done into the sets $R_{n}$ and $S_{n}\\cup T_{n}$ . K <PARAGRAPH_SEPARATOR> # 3.2. Examples <PARAGRAPH_SEPARATOR> We will present examples of densities that satisfy (3.11) and also give some concrete upper and lower bounds for $\\beta_{n}$ . In all examples we assume that $n\\delta_{n}^{d}\\to\\infty$ (cf. Corollary 3.6) <PARAGRAPH_SEPARATOR> Example 1. Let $f$ have compact support, that is, pos $f$ is bounded. We have <PARAGRAPH_SEPARATOR> $$ \\frac{\\lambda((A_{n}+\\rho_{n}B)\\cap A_{n}^{c})}{\\lambda(A_{n})}\\leqslant\\frac{\\lambda((\\mathrm{pos}f+\\rho_{n}B)\\cap A_{n}^{c})}{\\lambda(A_{n})}. $$ <PARAGRAPH_SEPARATOR> On the other hand, it is easy to see that <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\lambda((\\mathrm{pos}f+\\rho_{n}B)\\cap A_{n}^{c})=\\lambda(\\overline{{\\mathrm{pos}f}}\\setminus\\mathrm{pos}f). $$ <PARAGRAPH_SEPARATOR> Also, <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\lambda(A_{n})=\\lambda(\\operatorname{pos}f). $$ <PARAGRAPH_SEPARATOR> It follows that (3.11) holds if $\\lambda(\\overline{{\\mathrm{pos}f}}\\backslash\\mathrm{pos}f)=0$ , a condition satisfied by all reasonable densities. Now, by Theorem 3.7, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\beta_{n}\\leqslant(1+o(1))[\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c})]} &{\\quad=(1+o(1))[\\delta_{n}^{-d}\\lambda(A_{n})+n\\lambda_{f}(A_{n}^{c}\\cap\\mathrm{pos}f)]} &{\\quad\\leqslant(1+o(1))\\delta_{n}^{-d}[\\lambda(A_{n})+\\lambda(A_{n}^{c}\\cap\\mathrm{pos}f)]=(1+o(1))\\delta_{n}^{-d}(\\mathrm{pos}f),}\\end{array} $$ <PARAGRAPH_SEPARATOR> where the second inequality follows from (ii) of Lemma 3.3. Combining this with Theorem 3.2 and using (3.16) we get <PARAGRAPH_SEPARATOR> $$ (1+o(1))(1-e^{-1})\\lambda(\\mathrm{pos}f)\\delta_{n}^{-d}\\leqslant\\beta_{n}\\leqslant\\left(1+o(1)\\right)\\lambda(\\mathrm{pos}f)\\delta_{n}^{-d}. $$"
  },
  {
    "qid": "statistic-posneg-1457-0-2",
    "gold_answer": "FALSE. While the paper specifies a normal prior for β and derives a normal posterior under certain conditions, the posterior distribution's form depends on the model's full hierarchical structure and the data. The normality is a result of conjugacy in this specific setup but is not a universal property of DPMMs.",
    "question": "Is the posterior distribution of β in the DPMM always normal, given the specified prior and likelihood structure?",
    "question_context": "Dirichlet Process Mixture Model for Time Series Analysis\n\nThe paper introduces a Dirichlet Process Mixture Model (DPMM) for modeling ergodic time series. The model assumes a nonparametric mixture of normal kernels for the conditional density, with a specific autoregressive (AR) link function involving the hyperbolic tangent function. The DP prior is placed on the mixing distribution, allowing for flexible shapes. Posterior inference is conducted via an MCMC scheme, and sufficient conditions for posterior consistency are established in two different topologies."
  },
  {
    "qid": "statistic-posneg-322-0-3",
    "gold_answer": "FALSE. The paper does not claim that the conditional marginal quantile function q_j,y(alpha) is discontinuous in alpha. On the contrary, it assumes that the conditional marginal distributions F_j,y are continuous, which implies that the quantile functions q_j,y(alpha) are also continuous in alpha.",
    "question": "Does the paper claim that the conditional marginal quantile function q_j,y(alpha) is discontinuous in alpha?",
    "question_context": "Estimation of Extreme Multivariate Expectiles with Functional Covariates\n\nThe paper discusses the estimation of multivariate expectiles with functional covariates, focusing on conditional marginal distributions, quantile functions, and tail dependence functions. It introduces a framework for estimating extreme multivariate expectiles using conditional copulas and tail dependence functions, with applications in risk analysis and capital allocation."
  },
  {
    "qid": "statistic-posneg-55-0-1",
    "gold_answer": "FALSE. Theorem 3.1 states that n^(1/2)(T_n - M) converges in distribution to a normal random variable E ~ N(0, σ²), but σ² could be zero, in which case E is degenerate at zero. The theorem explicitly notes that E is degenerate if σ² = 0, so the convergence is not always to a non-degenerate normal distribution.",
    "question": "Does Theorem 3.1 imply that the statistic T_n, when normalized by n^(1/2), always converges to a non-degenerate normal distribution under the given conditions?",
    "question_context": "Limiting Distribution of Empirical Characteristic Function Statistics\n\nThe paper discusses the limiting distribution of statistics derived from empirical characteristic functions (e.c.f.) for complex-valued stochastic processes. It presents theorems on the convergence of suprema and integrals of these processes under specific conditions, leveraging functional analysis and probability theory."
  },
  {
    "qid": "statistic-posneg-675-1-0",
    "gold_answer": "FALSE. While the null model specifies that the response depends only on time and not on any other predictors, the assumption that the selection probability for each predictor should be exactly 1/6 is an ideal scenario. In practice, the selection probabilities may deviate from 1/6 due to random variation, especially in finite samples (200 subjects and 1000 iterations in this case). Additionally, the presence of correlated predictors (e.g., $X_4$ and $X_5$ with correlation 0.7) can further skew the selection probabilities, even under the null model. The 1/6 probability is a theoretical benchmark, not a guaranteed empirical result.",
    "question": "Under the null model $$ {\\mathrm{Null}}\\colon\\mathbf{y}_{i}=\\alpha_{i}+0.3({\\mathrm{time}})_{i}+\\epsilon_{i}, $$ where $\\alpha_{i}\\sim\\mathrm{N}(1,1)$ and $(\\mathrm{time})_{i}=(1,\\ldots,5)^{T}$, is it correct to assume that the selection probability for each predictor variable should be 1/6, given that the response depends only on time and not on any other predictors?",
    "question_context": "Null Model Simulation for Split Variable Selection in Longitudinal Data\n\nThe estimated probabilities of selecting each predictor are recorded for 1000 iterations with 200 subjects. The average CPU times are also recorded on a 3 GHz Xeon(TM) workstation with 8 GB ECC-DDR2 memory. For the simulation study, we use R 2.15.2 (R Core Team 2012) with two well-known packages, nlme (Pinheiro and Bates 2000) and mvtnorm (Hothorn, Bretz, and Genz 2001), which are used to fit the basic model and to generate the simulated data, respectively. The R package REEMtree (Sela and Simonoff 2011) and FORTRAN-compiled binary GUIDE version 14.2 (http://www.stat.wisc.edu/ loh/guide) are also used for the comparison. <PARAGRAPH_SEPARATOR> # 3.2 SPLIT VARIABLE SELECTION <PARAGRAPH_SEPARATOR> We first generate the simulated data under the following null model: <PARAGRAPH_SEPARATOR> $$ {\\mathrm{Null}}\\colon\\mathbf{y}_{i}=\\alpha_{i}+0.3({\\mathrm{time}})_{i}+\\epsilon_{i}, $$ <PARAGRAPH_SEPARATOR> where $\\alpha_{i}\\sim\\mathrm{N}(1,1)$ and $(\\mathrm{time})_{i}=(1,\\ldots,5)^{T}$ . The response depends on the time variable, but it is independent of all the other predictor variables. Therefore, the chance that each predictor is selected as the split variable should be the same, that is, the ideal selection probability is 1/6 for each. As explained in Section 3.1, we consider the following situations: (i) independence, such that the X’s are mutually independent and (ii) dependence, such that some of the $X_{i}$ ’s are not mutually independent. $X_{4}$ and $X_{5}$ have a bivariate normal distribution with correlation 0.7 and $X_{2}$ and $X_{3}$ have a joint distribution. In addition, we consider two types of data structures: regular and irregular. The irregular structure was generated by randomly selecting subjects’ follow-ups from discrete uniform [2, 5]. <PARAGRAPH_SEPARATOR> In these scenarios, we investigate the performance of variable selection by the following approaches: RA and ES approaches by MELT (denoted by RA(MELT) and ES(MELT), respectively ), RA approach by multivariate GUIDE of Loh and Zheng (2013) (denoted by RA(GUIDE)), ES approaches by MVPART of De’Ath (2002) and RE-EM of Sela and Simonoff (2012) (denoted by ES(MVPART) and ES(RE-EM), respectively). <PARAGRAPH_SEPARATOR> Table 1 contains the estimated probabilities that each $X_{i}$ is selected under the independence situation with both regular and irregular data structures using AR(1). Due to the limitation of space, the results are not shown for the other situation and covariance structure. Instead, they can be found in the supplements (available online). Because none of the predictor variables are associated with the response variable under the null model, the chance of selection as a split variable should be equal for all the six variables, i.e., the ideal selection probability of each variable is 1/6."
  },
  {
    "qid": "statistic-posneg-1376-0-1",
    "gold_answer": "FALSE. While the linear score function performs excellently across a wide range of distributions and is easier to compute, the context indicates that its performance is not uniformly superior in all scenarios. Specifically, for heavy-tailed distributions, $W_{l n p-1}$ (using the linear score) has slightly lower efficiencies than $S_{n}$, and for light-tailed distributions, its efficiencies are slightly less than those of PR and $W_{w n p-1}$ (which uses the van der Waerden score). The linear score's advantages are more about balanced performance and computational simplicity rather than universal dominance.",
    "question": "Does the linear score function $\\phi(u)=\\lambda u+(1-\\lambda)1$, where $\\lambda=\\{\\ln(p+e-2)\\}^{-1}$, always perform better than the van der Waerden score function in all distributional scenarios?",
    "question_context": "Score Function Efficiency in Multivariate Testing\n\nIf $\\phi(u)\\equiv u$ , then, for elliptically symmetric distributions, $W_{n p-1}$ will have the same AREs as the test statistic obtained by applying Peters and Randles [30] signed-rank interdirection test to the complete block design setting (see, [20]). These test statistics generally perform well over a wide range of distributions, yet their performance deteriorates as the dimension increases, especially in the light-tailed cases and the normal case. The test will be denoted by PR. <PARAGRAPH_SEPARATOR> “Optimal” score functions have been discussed in the literature (see [12,14]). For example, if the underlying distribution is the multivariate normal then the van der Waerden score function, $\\phi(u)=\\sqrt{(\\chi^{2})^{-1}(u)}$ , is optimal where $\\left(\\chi^{2}\\right)^{-1}\\left(u\\right)$ is the inverse of the distribution function of a $\\chi_{p-1}^{2}$ . With such a score function and for elliptically symmetric distributions, $W_{n p-1}$ AREs with respect to Hotelling–Hsu $T^{2}$ will always be greater than 1 with equality when the distribution is the multivariate normal. Note that with this score function the test will be denoted by $W_{w n p-1}$ . <PARAGRAPH_SEPARATOR> While tests based on optimal scores have nice properties, they are seldom used in practice. The score function that we now recommend combines excellence in performance with simplicity in implementation. It is defined as <PARAGRAPH_SEPARATOR> $$ \\phi(u)=\\lambda u+(1-\\lambda)1, $$ <PARAGRAPH_SEPARATOR> where $\\lambda=\\{\\ln(p+e-2)\\}^{-1}$ depends on $p$ , the dimension of the data. This score function was introduced in [28]. With this linear score function, $W_{n p-1}$ has an excellent performance as good as and in some cases better than $W_{w n p-1}$ but yet it is still easy to compute as $S_{n}$ and PR. $W_{l n p-1}$ will be used to denote $W_{n p-1}$ with the score function defined in 13. <PARAGRAPH_SEPARATOR> We now show the effect of the score function on the efficiencies of the corresponding test by computing and comparing the AREs, with respect to Hotelling–Hsu’s $T^{2}$ , of the four test statistics, $W_{l n p-1}$ , $W_{w n p-1}$ , PR, and $S_{n}$ . Tables 1 and 2 display the AREs of these four statistics, for the case when the $\\mathbf{Y}$ ’s come from the power family of distributions and the family of $t\\cdot$ -distributions, respectively. These two families of distributions belong to the class of elliptically symmetric distributions. The power family is such that <PARAGRAPH_SEPARATOR> $$ h(t)=\\exp\\bigg\\{-\\bigg(\\frac{t}{c_{0}}\\bigg)^{\\nu}\\bigg\\},~\\nu>0, $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ c_{0}={\\frac{p r(p/2\\nu)}{r((p+2)/2\\nu)}},~\\mathrm{and}~K_{p}={\\frac{\\nu{r(p/2)}}{{r(p/2\\nu)}{[}\\pi c_{0}{]^{p/2}}}}. $$ <PARAGRAPH_SEPARATOR> It includes the multivariate normal density $(\\nu=1)$ ), as well as heavier-tailed distributions $0<\\nu<1$ ) and lighter-tailed distributions $y>1$ ). For the multivariate $t$ -distribution, $h(\\cdot)$ and $K_{p}$ in (9) are defined as <PARAGRAPH_SEPARATOR> $$ h(t)=\\left[1+\\frac{1}{\\nu}(t)\\right]^{-(p+\\nu)/2} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ K_{p}=\\frac{{\\cal T}((p+\\nu)/2)}{{\\cal T}(\\nu/2)(\\pi\\nu)^{p/2}}. $$ <PARAGRAPH_SEPARATOR> It is clear from Tables 1 and 2 that $W_{l n p-1}$ has very competitive ARE values in a wide variety of distributions, ranging from very heavy-tailed distribution to very light-tailed ones. These efficiencies do not deteriorate as the dimension increases (as is the case for PR) or as the underlying distribution shifts from being heavy-tailed to being light-tailed (as is the case with $S_{n}$ ). For normal alternatives, $W_{l n p-1}$ not only performs virtually as well as Hotelling–Hsu’s $T^{2}$ and $W_{w n p-1}$ (recall this test is optimal for the normal distribution) but also has higher efficiencies than $S_{n}$ and ${\\mathrm{PR}}_{n}$ . For the heavy-tailed distributions of the power family, $W_{l n p-1}$ has higher efficiencies than $W_{w n p-1}$ and PR but slightly lower efficiencies than those of $S_{n}$ . For the light-tailed distributions of this family, the efficiencies of $W_{l n p-1}$ exceed those of $S_{n}$ but are slightly less than those of PR and $W_{w n p-1}$ . However, the slight advantages of some tests over $W_{l n p-1}$ whether in the light-tailed cases or the heavy-tailed cases disappear slowly as the dimension increases."
  },
  {
    "qid": "statistic-posneg-1077-0-0",
    "gold_answer": "TRUE. The derivation is correct because: 1) Substituting $P_{1}^{*}=P_{1}\\alpha^{-1}\\alpha^{*}$ and $\\mathcal{Q}_{1}^{*}=(\\alpha^{*})^{-1}\\alpha\\mathcal{Q}_{1}$ into $P_{1}^{*}\\mathcal{Q}_{1}^{*}$ yields $P_{1}\\alpha^{-1}\\alpha^{*}(\\alpha^{*})^{-1}\\alpha\\mathcal{Q}_{1}$. 2) The middle terms $\\alpha^{*}(\\alpha^{*})^{-1}$ cancel out to the identity matrix, leaving $P_{1}\\alpha^{-1}\\alpha\\mathcal{Q}_{1}$. 3) Similarly, $\\alpha^{-1}\\alpha$ cancels out, resulting in $P_{1}\\mathcal{Q}_{1}$. This holds as long as $\\alpha$ and $\\alpha^{*}$ are invertible (full rank), ensuring the existence of $\\alpha^{-1}$ and $(\\alpha^{*})^{-1}$.",
    "question": "Is the equality $P_{1}^{*}\\mathcal{Q}_{1}^{*}=P_{1}\\mathcal{Q}_{1}$ derived from the transformations $P_{1}^{*}=P_{1}\\alpha^{-1}\\alpha^{*}$ and $\\mathcal{Q}_{1}^{*}=(\\alpha^{*})^{-1}\\alpha\\mathcal{Q}_{1}$ correct under the assumption that $\\alpha$ and $\\alpha^{*}$ are full rank matrices?",
    "question_context": "Orthogonal Matrix Transformations in Exact Maximum Likelihood Estimation\n\nIn a similar way, consider $k\\times r$ matrices $P_{2}$ and $P_{2}^{*}$ such that $P=[P_{1},P_{2}]$ and $P^{*}=$ $\\left[P_{1}^{*},P_{2}^{*}\\right]$ are invertible, and the column vectors of $P_{2}$ and $P_{2}^{*}$ are orthogonal to those of $P_{1}$ and $P_{1}^{*}$ , respectively. Of course, the space spanned by the column vectors of $M=[v_{1},\\ldots,v_{r}]$ is orthogonal to the one spanned by those of $G$ . Hence, there exist full rank $r\\times r$ matrices $\\beta$ and $\\beta^{*}$ such that $P_{2}=M\\beta$ and $P_{2}^{*}=M\\beta^{*}$ . Thus, $P_{2}^{*}=P_{2}\\beta^{-1}\\beta^{*}$ . <PARAGRAPH_SEPARATOR> Let $Q={\\left[\\begin{array}{l}{Q_{1}}\\ {Q_{2}}\\end{array}\\right]}=P^{-1}$ and $Q^{*}=\\left[{{Q}_{1}^{*}}\\right]=(P^{*})^{-1}$ , we can easily check that $\\mathcal{Q}^{*}=\\left[\\begin{array}{l}{(\\alpha^{*})^{-1}\\alpha Q_{1}}\\ {(\\beta^{*})^{-1}\\beta Q_{2}}\\end{array}\\right].$ <PARAGRAPH_SEPARATOR> Now turning back to the product $P_{1}^{*}Q_{1}^{*}$ , we have that <PARAGRAPH_SEPARATOR> $$ P_{1}^{*}\\mathcal{Q}_{1}^{*}=P_{1}\\alpha^{-1}\\alpha^{*}(\\alpha^{*})^{-1}\\alpha\\mathcal{Q}_{1}=P_{1}\\mathcal{Q}_{1}. $$ <PARAGRAPH_SEPARATOR> In a similar way we can check that $P_{2}^{*}Q_{2}^{*}=P_{2}Q_{2}$ , which completes the proof."
  },
  {
    "qid": "statistic-posneg-387-0-1",
    "gold_answer": "FALSE. The context states that for the noninformative prior, setting $m = n-1$ and $t_{\\ell} = X_{\\ell}$ for $\\ell=1,...,m$ ensures that the random value $\\mathcal{F}(t_{\\ell})$ has the same distribution as the $\\ell$-th order statistic of a uniform random sample of size $n$ (not $n-1$). This is because the limiting posterior distribution in this case corresponds to the distribution of uniform order statistics from a sample of size $n$.",
    "question": "For the noninformative prior case, does setting $m = n-1$ and $t_{\\ell} = X_{\\ell}$ for $\\ell=1,...,m$ ensure that the random value $\\mathcal{F}(t_{\\ell})$ has the same distribution as the $\\ell$-th order statistic of a uniform random sample of size $n-1$?",
    "question_context": "Nonparametric Bayesian Confidence Bands for Distribution Functions\n\nLet $X_{1}<\\ldots<X_{s}$ denote the distinct observations in a sample of size $\\pmb{n}$ from $\\mathcal{P}$ and let $\\pmb{n}(\\pmb{\\hat{\\imath}})$ denote the number of repetitions of $\\pmb{X}_{i}$ in the sample. Ferguson showed that the posterior distribution of $\\pmb{P}$ is again a Dirichlet process with parameter $a(.)+\\Sigma n(i)d(X_{i},.)$ ,where $d(\\pmb{X},\\pmb{A})=1$ if $\\pmb{X}$ belongs to $\\pmb{A}$ and equals zero otherwise. As pointed out by Ferguson, any analysis performed on the Dirichlet process will be valid both $\\pmb{a}$ priori and $\\pmb{a}$ posteriori. If $\\mathscr{X}$ consists of a finite set of elements the above result is equivalent to stating that the Dirichlet distribution is the conjugate prior to be used in the estimation of multinomial probabilities. If the jumps in $a(.)$ are all integers then the joint distribution of the cumulative cell probabilities is identical to the joint distribution of some of the order statistics of a uniform random sample as discussed by Wilks (1962, p. 236). This in fact is the limiting posterior probability distribution when $a(\\mathcal X)\\rightarrow0$ , that is, the case of a noninformative prior. <PARAGRAPH_SEPARATOR> # 2. CONFIDENCE BANDS <PARAGRAPH_SEPARATOR> Formulae have been derived by Breth (1978) to compute rectangle probabilities over the ordered Dirichlet distribution; that is for $0\\leqslant u_{1}\\leqslant\\ldots\\leqslant u_{m}<1$ $0<v_{1}\\leqslant\\ldots\\leqslant v_{m}\\leqslant1$ and ${\\pmb u}_{\\nless}<{\\pmb v}_{\\pmb q}$ for all i, <PARAGRAPH_SEPARATOR> $$ p_{m}(u,B,v;a)=\\mathrm{pr}\\{u_{i}\\leqslant P(B_{i})\\leqslant v_{i};\\mathrm{~i=1,...,m\\}. $$ <PARAGRAPH_SEPARATOR> A simulation alternative was also given. In the case when the $\\pmb{a}(\\pmb{B}_{i})$ are all integers, the above probability was shown to be equivalent to a rectangular probability over the uniform order statistics. Steck (197l) expressed this probability as a determinant. <PARAGRAPH_SEPARATOR> Let ${\\pmb{\\mathcal{X}}}=(-\\infty,\\infty)$ and for $-\\infty<t_{1}<...<t_{m}<\\infty$ define $B_{j}=(-\\infty,t_{j}]$ for ${\\mathfrak{j}}=1,...,{\\mathfrak{m}}$ and $B_{m+1}=\\mathcal{X}$ .For $j=1,...,m,P(B_{j})=F(t_{j})~$ where $\\pmb{F}$ is the random distribution function associated with the random probability measure $\\pmb{P}$ . Then (2·1) becomes <PARAGRAPH_SEPARATOR> $$ p_{m}(u,B,v;\\boldsymbol{a})=\\mathrm{pr}\\{u_{i}\\leqslant F(t_{i})\\leqslant v_{i};\\boldsymbol{\\mathsf{i}}=1,...,m\\}, $$ <PARAGRAPH_SEPARATOR> which was used to set fxed Bayesian confidence bands within which the random distribution function lies with determined probability. In the sequel when there is no possible source of confusion reference to $\\pmb{B}$ and $\\pmb{\\alpha}(.)$ will be deleted and (2·2) will be written as <PARAGRAPH_SEPARATOR> $$ p_{m}(u,v)=\\mathrm{pr}\\{u_{i}\\leqslant F(t_{i})\\leqslant v_{i};i,=1,...,m\\}. $$ <PARAGRAPH_SEPARATOR> In practice to construct such confidence bands a desired probability level $\\pmb{\\mathcal{P}}$ would be stipulated and it would be necessary to determine $m,~u,~v$ and $\\pmb{\\ell}$ such that $\\pmb{\\mathscr{p}}_{m}(\\pmb{u},\\pmb{v})=\\pmb{p}$ First consider the case when it is possible to choose $\\pmb{\\ell}$ s0 that for some $\\pmb{m}$ , $\\pmb{a}(B_{\\pmb{j}})=\\dot{\\pmb{j}}$ for $j=1,...,m+1$ . Then for $\\dot{\\pmb{\\mathscr{i}}}=1,...,\\pmb{\\mathscr{m}}$ , the random value $\\pmb{\\mathcal{F}}(\\pmb{\\ell}_{\\pmb{\\ell}})$ has the same distribution a8 the ith order statistic of a uniform random sample of size $\\mathbf{\\nabla}_{m}$ . For this $\\pmb{\\ell}$ if $\\pmb{\\mathscr{u}}$ and $\\pmb{v}$ are restricted to Kolmogorov bands the solution to $\\pmb{p}_{m}(\\pmb{u},\\pmb{v})=\\pmb{p}$ is easily obtained from existing tables of critical values of the Kolmogorov statistic; a detailed example of this is discussed by Breth (1978). For the noninformative prior it is always possible to obtain such bands for the limiting posterior distribution provided that ${\\pmb n}({\\dot{\\bullet}})=1$ for all i. One simply takes $m=n-1$ and $t_{\\ell}=X_{\\ell}$ for $\\pmb{i}=1,...,m$ <PARAGRAPH_SEPARATOR> In general it may not be possible to choose $\\pmb{B}$ 80 that $\\pmb{a}(\\pmb{B}_{\\pmb{j}})=\\dot{\\pmb{j}}$ for all $j$ or to obtain a $\\pmb{B}$ which gives a suitable approximation to this. In such cases tables of values for $\\pmb{\\mathscr{p}}_{m}(\\pmb{\\mathscr{u}},\\pmb{\\ v})=\\pmb{\\mathscr{p}}$ are not currently available. It would be possible to proceed by using the computational techniques previously mentioned by fixing $\\pmb{\\ell}$ and considering the effects on $\\mathcal{\\pmb{p}}_{m}(\\pmb{u},\\pmb{v})$ caused by systematic alterations in $\\pmb{\\mathscr{u}}$ and $\\pmb{v}$ . Alternatively, if $\\pmb{\\mathscr{u}}$ and $\\pmb{v}$ are fixed it would be possible to consider the effects on $\\pmb{\\mathcal{P}}_{m}(\\pmb{u},\\pmb{v})$ caused by systematic alterations to t. These primitive approaches would require considerable computer time. Alternative techniques need to be developed but lie beyond the scope of this paper. <PARAGRAPH_SEPARATOR> When such bands have been constructed there is no guarantee that they are in any sense optimal. In the Neyman-Pearson framework Steck (197l) has discussed problems associated with the optimality criterion of minimizing the are8 enclosed by the bands. In particular it was noted that when the distribution has infinite or semiinfinite support the area may be infnite. This type of problem also arises in the Bayesian framework. As an example where the area is finite consider bands for the limiting posterior distribution when the prior is noninformative, $\\pmb{n}(\\mathrm{i})=1$ for all $\\pmb{\\dot{\\upnu}}$ and ${\\pmb F}({\\bf0})={\\bf0}$ . In this case the limiting posterior distribution of ${\\pmb F}({\\pmb X}_{\\pmb n})$ is degenerate and defining $\\pmb{\\mathcal{X}}_{0}=\\mathbf{0}$ , the area enclosed by the bands is given by $D=\\Sigma(X_{i}-X_{i-1})(v_{i}-u_{i-1})$ . In theory $\\pmb{D}$ may be minimized by numerical methods subject to $\\mathscr{p}_{n-1}(u,v)=p$ . Note that a posteriori for the noninformative prior, $\\pmb{\\mathscr{t}}=(\\pmb{X}_{1},...,\\pmb{X}_{n-1})$ i8 fxed and there are various possible $\\pmb{\\mathscr{u}}$ and $\\pmb{v}$ which are solutions to $\\mathscr{p}_{\\mathfrak{n}-\\mathfrak{1}}(\\mathfrak{u},\\mathfrak{v})=\\mathscr{p}$ . When $\\pmb{n X}_{i}=\\dot{\\pmb{\\i}}$ for all $\\pmb{\\dot{\\upnu}}$ ,Steck in fact studied $\\pmb{D}$ for selected candidate bands and concluded that the Kolmogorov bands were not far from being optimal."
  },
  {
    "qid": "statistic-posneg-1844-2-1",
    "gold_answer": "FALSE. The context mentions that $H_{D}(k)$ has peaks at $\\frac{1}{8}, \\frac{8}{8}, \\frac{8}{8}, \\frac{7}{8}$, which is likely a typographical error. The correct peaks should be at $\\frac{1}{8}, \\frac{3}{8}, \\frac{5}{8}, \\frac{7}{8}$ for a distribution with four symmetric peaks. The formula for $H_{D}(k)$ is only provided for the interval $0\\leqslant x<\\frac{1}{4}$, and it is repeated with period $\\frac{1}{4}$ to fill the interval, which would naturally produce peaks at $\\frac{1}{8}, \\frac{3}{8}, \\frac{5}{8}, \\frac{7}{8}$.",
    "question": "Does the density function $H_{D}(k)$ describe a distribution with four peaks at $\\frac{1}{8}, \\frac{3}{8}, \\frac{5}{8}, \\frac{7}{8}$ as implied by the context?",
    "question_context": "Goodness-of-Fit Statistics for Circular Distributions\n\nSimilar studies were therefore undertaken with other alternative distributions, to examine whether situations exist in which any of the statistics might clearly be better or worse than the other two. <PARAGRAPH_SEPARATOR> # 5.2. Other alternatives to uniformity <PARAGRAPH_SEPARATOR> We should like to consider, as alternatives to uniformity, distributions which describe various degrees of clustering of observations which would be possible on 8 circle: one large cluster, two clusters, and so on. A very convenient set of alternatives is $H_{B},H_{C}$ and $H_{D},$ whichfollow: <PARAGRAPH_SEPARATOR> $$ H_{B}(k)\\colon f(x)={\\left\\{\\begin{array}{l l}{k(2x)^{k-1}}&{{\\mathrm{~(0\\leqslant~}}x<\\frac{1}{2}{\\mathrm{)}},}\\ {k\\{2(1-x)\\}^{k-1}}&{{\\mathrm{~(\\frac{1}{2}\\leqslant~}}x<1{\\mathrm{)}}.}\\end{array}\\right.} $$ <PARAGRAPH_SEPARATOR> This has a peak at ${\\pmb x}=\\pmb{\\updelta}_{:}$ increasing in relative importance as $\\pmb{k}$ becomes larger. A second setis <PARAGRAPH_SEPARATOR> $$ H_{O}(k)\\colon f(x)=\\left\\{{k\\mathord{\\left\\{\\begin{array}{l l}{k(4x)^{k-1}}&{\\mathrm{~(0\\leqslant~}x<\\frac{1}{\\pi})}\\ {k\\{4(\\frac{1}{2}-x)\\}^{k-1}}&{\\mathrm{~(4\\leqslant~}x<\\frac{1}{\\pi}).}\\end{array}}\\right.}\\right.} $$ <PARAGRAPH_SEPARATOR> This density is repeated with period $\\frac{1}{2}$ to fill the interval $\\Sigma\\leqslant x<1$ ; it has two peaks, at $x=\\ddagger$ and $x=\\frac{3}{4}$ . A third set, with 4 peaks, at $\\frac{1}{8},\\frac{8}{8},\\frac{8}{8},\\frac{7}{8}$ is <PARAGRAPH_SEPARATOR> $$ H_{D}(k)\\colon f(x)={\\left\\{\\begin{array}{l l}{k(8x)^{k-1}}&{(0\\leqslant x<{\\frac{1}{8}}),}\\ {k\\{8(\\mathtt{1}}-x)\\}^{k-1}}&{({\\frac{1}{8}}\\leqslant x<{\\frac{1}{4}}),}\\end{array}\\right.} $$ <PARAGRAPH_SEPARATOR> repeated with period $\\ddagger$ to fill the interval $\\pmb{\\mathrm{\\downarrow}}\\leqslant\\pmb{x}<1$ <PARAGRAPH_SEPARATOR> In all the above, $k\\geqslant0$ . Clearly this system may be extended. As $k{\\rightarrow}1$ they all tend to the uniform distribution: for a fixed $\\pmb{k}$ ,the succession $H_{B},H_{C},H_{D}$ represents steadily less extreme departures from uniformity and gives alternatives of 1, 2, and 4 clusters. <PARAGRAPH_SEPARATOR> We note that, of the set given above, $H_{B}$ is ‘nearest′ to the two-semicircle density $H_{A}$ ,if for the latter we imagine the semicircle of higher density centred at ${\\pmb x}=\\frac{1}{2}$ .We might imagine $A_{n}$ to perform well against alternative $H_{B}$ <PARAGRAPH_SEPARATOR> An advantage of this particular system as an alternative to uniformity on the circle, when making studies of power, is the existence of a standard of comparison. It is known that $Q=-2\\Sigma\\ln x_{i}$ is the most powerful statistic for testing uniformity $(k=1)$ againstthe alternative,for $k>0$ ? <PARAGRAPH_SEPARATOR> $$ H_{z}(k)\\colon f(x)=k x^{k-1}\\quad(0\\leqslant x\\leqslant1). $$ <PARAGRAPH_SEPARATOR> For $H_{B}$ we obtain & most powerful statistic by adapting $\\boldsymbol{Q}$ as follows: frst transform the $x_{t}$ to $\\pmb{u}_{\\pmb{\\ell}}$ given by $u_{i}=2x_{i}$ , $0\\leqslant x_{i}\\leqslant\\frac{1}{2}$ ,and $u_{\\downarrow}=2(1-x_{i})$ , $\\frac{1}{2}<x_{i}\\leqslant1$ ; then the test statistic becomes $Q=-2\\Sigma\\ln{u_{i}}$ . In both cases, on $H_{0},Q$ is exactly distributed as $\\chi_{\\mathfrak{L}_{\\mathfrak{R}}}^{\\mathfrak{L}}$ ; the lower tail is used against the alternative $k>1$ . Similarly $\\pmb{Q}$ may be adapted for the tests against $H_{C}$ or $H_{D}.$ . to give a most powerful statistic. <PARAGRAPH_SEPARATOR> # 5.3. Results of the power comparisons <PARAGRAPH_SEPARATOR> For the alternatives given above, $A_{n},U_{n}^{2}$ $V_{\\pmb{\\mathscr{n}}}$ and sometimes $Q_{\\mathfrak{n}}$ have been compared by Monte Carlo samples; some results are in Table 4. The samples were drawn from $H_{B}$ , $H_{c}$ or $H_{p}$ , and the test applied for uniformity; the table gives the proportion of each test statistic significant at the $10\\%$ level. For $H_{B},Q_{n}$ is included as a standard of comparison. The main conclusions are:"
  },
  {
    "qid": "statistic-posneg-1164-0-0",
    "gold_answer": "FALSE. The lemma states that the spectrum $f_{d}(k/T, t/T, W_{l t})$ matches the design spectrum $f(k/T, t/T, W_{l t})$ only at the finite grid points $(k/T, k/T)$ for $k=1,...,T$, not at all $(k/T, t/T)$. For other points, the difference is $O(1/T)$, meaning the match is approximate but not exact.",
    "question": "Is the statement 'The spectrum of $X_{l t}$ matches the design spectrum exactly at all time-frequency points $(k/T, t/T)$' true based on the given lemma?",
    "question_context": "Time-Frequency Functional Model for Locally Stationary Time Series\n\nWe conduct a simulation study to evaluate the empirical performance of the proposed method. The following lemma gives a guideline to simulate covariate-indexed time series $\\{X_{l t},l=1,\\ldots,n,t=1,\\ldots,T\\}$ with log-spectra $g_{d}(\\omega,u,W_{l u})$ equal to the design logspectra $g(\\omega,u,W_{l u})$ at a finite grid of time-frequency points $(k/T,k/T),k=1,...,T$ . Lemma 2. Let $g(\\omega,u,W_{l u})=W_{l u}\\pmb{\beta}(\\omega,u)$ be the logarithm of the design spectrum $f(\\omega,u,W_{l u})$ with continuous up to second-order partial derivatives with respect to ω and $u$ , where $(\\omega,u)$ is defined on $[0,1]\times[0,1]$ , $W_{l u}=\\{W_{l u}[1],\\dots,W_{l u}[P]\\}$ are the design covariates, and $\beta(\\omega,u)=\\{\beta_{1}(\\omega,u),\\dots,\beta_{p}(\\omega,u)\\}^{T}$ are the corresponding time-frequency coefficients. We have a finite sample $\\{X_{l t},l=1,\\ldots,n,t=1,\\ldots,T\\}$ of a locally stationary time series: $$ X_{l t}=\\sum_{k=1}^{T}\\exp\\left\\{\\frac{W_{l t}}{2}\beta\\left(\\frac{k}{T},\\frac{t}{T}\right)\right\\}\\exp\\left(\\frac{2\\pi k t}{T}i\right)Z_{l}(k), $$ where i is the imaginary unit: $i^{2}=-1;Z_{l}(k),l=1,\\ldots,n$ , are mutually independent zeromean random variables distributed as complex Normal with variance $1/T$ and $z(k)=$ $\\overline{{z(T-k)}}$ , when $k/T\neq0,0.5$ , or 1; or as real Normal with variance $1/T$ , otherwise. The spectrum of $X_{l t}$ , $f_{d}(\\omega,u,W_{l u})$ satisfies (1) $f_{d}(k/T,t/T,W_{l t})=f(k/T,t/T,W_{l t})=$ $\\exp\\{W_{l t}\\pmb\beta(k/T,t/T)\\}$ for $k,t=1,\\dots,T$ and (2) $|f_{d}(\\omega,u,W_{l u})-f(\\omega,u,W_{l u})|=$ $O(1/T)$ for $\\forall(\\omega,u)\\in[0,1]\times[0,1]$ . The proof of the lemma is deferred to the Appendix. The simulation takes three steps:"
  },
  {
    "qid": "statistic-posneg-39-0-0",
    "gold_answer": "TRUE. The first pair of spatial patterns $a_{1}$ and $b_{1}$ are indeed the eigenvectors of $\\Sigma\\Sigma^{\\prime}$ and $\\Sigma^{\\prime}\\Sigma$ associated with the largest eigenvalue (which is the square of the first canonical covariance $\\gamma_{1}$). This is derived from the SVD of the cross-covariance matrix $\\Sigma$, where $\\Sigma = A\\Lambda B^{\\prime}$, and $A$ and $B$ contain the eigenvectors of $\\Sigma\\Sigma^{\\prime}$ and $\\Sigma^{\\prime}\\Sigma$ respectively.",
    "question": "In classical MCA, the first pair of spatial patterns $a_{1}$ and $b_{1}$ are found by maximizing the covariance between $u_{1} = Xa_{1}$ and $v_{1} = Yb_{1}$ under the constraints $a_{1}^{\\prime}a_{1} = b_{1}^{\\prime}b_{1} = 1$. Is it true that these patterns can be obtained as the eigenvectors of $\\Sigma\\Sigma^{\\prime}$ and $\\Sigma^{\\prime}\\Sigma$ respectively, where $\\Sigma = \\frac{1}{n}(X^{\\prime}Y)$?",
    "question_context": "Smoothed Maximum Covariance Analysis for Spatiotemporal Fields\n\nGiven two spatiotemporal fields $X$ and $Y$, classical MCA finds a linear combination of variables from each field which has maximum covariance. $X$ is represented by an $n\\times p$ data matrix that corresponds to $p$ spatial locations over $n$ time units, and similarly $Y$ is an $n\\times q$ matrix. In climatological science application, it is common to treat the time component as observations and the space component as variables (the S-mode), though, the reverse approach (the T-mode) is also occasionally used (Jolliffe, 2002). If we denote $\\boldsymbol{u}_{1}=\\boldsymbol{X}\\boldsymbol{a}_{1}$ and $v_{1}=Y b_{1}$ our objective is to find $a_{1}$ and $b_{1}$ such that $\\gamma_{1}=\\mathrm{cov}(u_{1},v_{1}),\\qquada_{1}^{\\prime}a_{1}=b_{1}^{\\prime}b_{1}=1,$ is maximized over all choices of $a_{1}$ and $b_{1}$."
  },
  {
    "qid": "statistic-posneg-372-0-0",
    "gold_answer": "FALSE. The HDD index is actually calculated as the sum of the daily average temperatures below the threshold $c=65^{\\circ}\\mathrm{F}$ (or $18^{\\circ}\\mathrm{C}$) over the measurement period, as shown in the formula $\\mathrm{HDD}(\\tau_{1},\\tau_{2})=\\sum_{i=\\tau_{1}}^{\\tau_{2}}\\operatorname*{max}(c-T_{i},0)$. The CDD index, on the other hand, measures the sum of temperatures above the threshold.",
    "question": "The HDD index is calculated as the sum of the daily average temperatures above the threshold $c=65^{\\circ}\\mathrm{F}$ over the measurement period. Is this statement correct based on the provided context?",
    "question_context": "Temperature Derivatives and Continuous-Time Autoregressive Models\n\nThe CME organizes trade in futures and options written on the temperatures in 18 US cities, 9 European cities and 2 Japanese cities. The European cities are Amsterdam, Barcelona, Berlin, Essen, London, Madrid, Paris, Rome and Stockholm. Later, we shall study the temperature dynamics in Stockholm based on a long record of daily measurements, and analyze the implications on relevant futures and options traded at the CME for this city. <PARAGRAPH_SEPARATOR> The market for temperature derivatives at the CME is based on three types of temperature indices: HDD, CDD and CAT. The HDD over a time period from, say, day $\\tau_{1}$ to $\\tau_{2}>\\tau_{1}$ is the accumulated temperatures below a certain threshold, usually determined to be $c=18^{\\circ}\\mathrm{C}$ , or equivalently $c=65^{\\circ}\\mathrm{F}.$ The index measures how many degrees that has accumulated below the threshold over the measurement period, being for instance an indicator of the demand of household heating. On the other hand, the CDD measures the accumulated temperature above $c$ , and measures the need for cooling. In the US, air-conditioners are usually switched on at a temperature of $65^{\\circ}\\mathrm{F}$ or higher. The CAT index is simply the accumulation of the daily average temperatures over the measurement period. <PARAGRAPH_SEPARATOR> Letting $T_{i}$ be the average of the minimum and maximum temperature on day $i$ , the HDD is defined to be <PARAGRAPH_SEPARATOR> $$ \\mathrm{HDD}(\\tau_{1},\\tau_{2})=\\sum_{i=\\tau_{1}}^{\\tau_{2}}\\operatorname*{max}(c-T_{i},0), $$ <PARAGRAPH_SEPARATOR> while the CDD is <PARAGRAPH_SEPARATOR> $$ \\mathrm{CDD}(\\tau_{1},\\tau_{2})=\\sum_{i=\\tau_{1}}^{\\tau_{2}}\\operatorname*{max}(T_{i}-c,0). $$ <PARAGRAPH_SEPARATOR> The CAT is defined as <PARAGRAPH_SEPARATOR> $$ \\mathbf{CAT}(\\tau_{1},\\tau_{2})=\\sum_{i=\\tau_{1}}^{\\tau_{2}}T_{i}. $$ <PARAGRAPH_SEPARATOR> There is an index only relevant for the two Japanese cities called the Pacific Rim. However, this is the average of the CAT, and we shall not discuss this in further detail as all results valid for the CAT index are easily modified for the Pacific Rim. <PARAGRAPH_SEPARATOR> At the CME, the measurement periods for the different indices are standardized to be each month of the year and two seasons. The winter season is from October to April, and summer season from April to October, where there are contracts with measurement periods from two to seven months in each season. For the US cities, contracts are written on the HDD and CDD for the respective periods, with CDDs for the summer, and HDDs for the winter. The CDDs are substituted with the CAT index in the summer season for the European cities. <PARAGRAPH_SEPARATOR> The futures contracts are all cash settled, and the owner of a futures receives 20 times the index value at the end of the measurement period, in return for a fixed price (which is called the futures price of the contract). The currency is British pounds for the European futures contracts, while the US contracts are naturally denominated in US dollars. Trading of the futures contracts goes on up to the beginning of the measurement period. By definition of a futures contract, it does not cost anything to buy it. The payment takes place in the future, and entering the contract is simply an agreement of a future trade. In the context of temperatures, it is an agreement to exchange a fixed amount of money for a variable amount linked to the temperature index. In popular terms, the buyer of a temperature futures is changing a variable and an uncertain future temperature event against a fixed and known. Such an agreement may of course end with either a profit or a loss, depending on the outcome of the temperature event. The futures price is set based on the market’s expectations of the value of the index, and on supply and demand of contracts. <PARAGRAPH_SEPARATOR> The CME organizes a market for call and put options written on the temperature futures. A call option is a contract that gives the owner the right to buy the underlying asset for a fixed price at an agreed time. The owner is not obliged to buy, but exercises his or her option only if this is to his or her advantage (being the case if the fixed price in the option contract is lower than the market value of the underlying, giving him or her a discount when purchasing). The underlying asset in the CME market is a temperature future (written on the HDD, CDD or CAT index, with a specified measurement period). The fixed price in the option contract is called the strike price, whereas the agreed time for using the option is called the exercise time of the contract. A put option gives the owner the right to sell the underlying. To fix some notation, suppose the futures price at exercise time $\\tau\\leq\\tau_{1}$ is $F(\\tau,\\tau_{1},\\tau_{2})$ for a futures with measurement period $[\\tau_{1},\\tau_{2}]$ . Then, the owner of a call option written on that futures will receive <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\operatorname*{max}(F(\\tau,\\tau_{1},\\tau_{2})-K,0),}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $K$ is the strike price. The options at CME are cash settled, and so the owner receives the difference between the futures price and the strike if exercise takes place. The owner does not use the option if the futures price is lower. In the market, options with different strike prices and exercise times are offered for futures on different indices at all times. Options are a way for the traders in the market to trade in beliefs of high or low futures prices at a low cost, providing a tool for speculation or hedging. The fair price to buy an option contract is derived via a hedging strategy and the principle of no-arbitrage. However, this requires a stochastic model for the temperature dynamics, which implies a dynamics for the futures price being the basic ingredient in the option valuation. In the next section we propose and analyze a continuous-time autoregressive model for the temperature dynamics. <PARAGRAPH_SEPARATOR> # 3. A continuous-time autoregressive model with seasonal variation <PARAGRAPH_SEPARATOR> In this section we define a general continuous-time autoregressive model, which is the continuous-time analogue of an $\\operatorname{AR}(p)$ time series. The class of models is called ${\\mathrm{CAR}}(p)$ , and was first studied by Phillips (1959). <PARAGRAPH_SEPARATOR> Suppose that $(\\Omega,{\\mathcal{F}},P)$ is a probability space with a filtration $\\{\\mathcal{F}_{t}\\}_{0\\le t\\le\\tau_{\\mathrm{max}}}$ satisfying the usual assumptions. Here, $\\tau_{\\mathrm{max}}$ denotes a maximal time covering all times of interest in the market. Denote by $B(t)$ a Brownian motion defined on this filtered probability space. <PARAGRAPH_SEPARATOR> # 3.1. Continuous-time AR-models <PARAGRAPH_SEPARATOR> Let $\\mathbf{X}(t)$ be a stochastic process in $\\mathbb{R}^{p}$ for $p\\geq1$ defined by the vectorial Ornstein–Uhlenbeck equation <PARAGRAPH_SEPARATOR> $$ \\mathrm{d}\\mathbf{X}(t){=}A\\mathbf{X}(t)\\mathrm{d}t{+}\\mathbf{e}_{p}\\sigma(t)\\mathrm{d}B(t), $$ <PARAGRAPH_SEPARATOR> with $\\mathbf{e}_{k}$ being the $k$ th unit vector in $\\mathbb{R}^{p}$ $^p,k=1,\\ldots,p$ . Further, $\\sigma(t)>0$ is a real-valued and square integrable function (over any finite time interval) and $A$ is the $p\\times p$ matrix <PARAGRAPH_SEPARATOR> $$ A={\\left[\\begin{array}{l l l l l l}{0}&{1}&{0}&{\\cdots}&{0}\\ {0}&{0}&{1}&{\\cdots}&{0}\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}\\ {0}&{0}&{0}&{0}&{1}\\ {-\\alpha_{p}}&{-\\alpha_{p-1}}&{-\\alpha_{p-2}}&{\\cdots}&{-\\alpha_{1}}\\end{array}\\right]}. $$ <PARAGRAPH_SEPARATOR> We suppose that $\\upalpha_{k}$ , $k=1,\\hdots,p$ , are constants. With $\\Lambda(t)$ being a deterministic seasonal function representing the average temperature, we introduce the following ${\\mathrm{CAR}}(p)$ model for the temperature dynamics: <PARAGRAPH_SEPARATOR> $$ T(t)=\\Lambda(t)+X_{1}(t). $$ <PARAGRAPH_SEPARATOR> Here, $X_{q}$ is the qth coordinate of the vector $\\mathbf{X}$ , $q=1,\\ldots,p$ . As we shall see in Subsection 3.2, the volatility function $\\sigma(t)$ will play an important role in the dynamics of the temperature. It turns out that the temperature residuals possess a variation that is a function of the season. It has a rather characteristic shape, which we model by using a truncated Fourier series. The variation function $\\sigma(t)$ will be strictly positive, continuous, and bounded. <PARAGRAPH_SEPARATOR> We can represent the stochastic process $\\mathbf{X}(t)$ explicitly by solving the stochastic differential equation (5): <PARAGRAPH_SEPARATOR> Lemma 1 The stochastic process $\\mathbf{X}(t)$ has the explicit form <PARAGRAPH_SEPARATOR> $$ \\mathbf{X}(s){=}\\exp\\left(A(s-t)\\right)\\mathbf{x}+\\int_{t}^{s}\\exp\\left(A(s-u)\\right)\\mathbf{e}_{p}\\sigma(u)\\mathrm{d}B(u), $$ <PARAGRAPH_SEPARATOR> for $s\\geq t\\geq0$ and $\\mathbf{X}(t)=\\mathbf{x}\\in\\mathbb{R}^{p}$ . <PARAGRAPH_SEPARATOR> Proof. Follows by an application of the multi-dimensional Ito Formula, which can be found in Øksendal (1998, theorem 4.2.1, page 48). <PARAGRAPH_SEPARATOR> For a constant volatility function $\\sigma(t),\\mathbf{X}(t)$ is stationary as long as all eigenvalues of $A$ have negative real part. This can be seen from proposition 6.2 of Ichihara & Kunita (1974). In our case, the stationarity holds when the variance matrix <PARAGRAPH_SEPARATOR> $$ \\int_{0}^{t}\\sigma^{2}(t-s)\\exp({A s})\\mathbf{e}_{p}\\mathbf{e}_{p}^{\\prime}\\exp({A^{\\prime}s})\\mathrm{d}s $$ <PARAGRAPH_SEPARATOR> converges as $t\\to\\infty$ , which is the case if all the eigenvalues of $A$ have negative real part and $\\sigma(t)$ is bounded. <PARAGRAPH_SEPARATOR> # 3.2. Empirical analysis of Stockholm temperatures <PARAGRAPH_SEPARATOR> We have available daily average temperatures (DAT) measured on Celsius scale from Stockholm over a period ranging from 1 January 1961 to 25 May 2006, resulting in 16,581 records. The DATs are computed as the average of the minimum and maximum temperature over the day. February 29 was removed from the sample in each leap year to have years of equal size, leading to a time series of 16,570 observations. Our aim is to fit a time-series model that we can associate to the ${\\mathrm{CAR}}(p)$ -model (5). A straightforward way suggested by Phillips (1959) is to estimate the parameters in the corresponding $\\operatorname{AR}(p)$ -model, which is the path we follow here. <PARAGRAPH_SEPARATOR> Define the seasonal AR(3) time series: <PARAGRAPH_SEPARATOR> $$ x_{i}=T_{i}-\\Lambda_{i},\\quad i=0,1,2,3,\\ldots. $$ <PARAGRAPH_SEPARATOR> where $T_{i}$ is the DAT on day $i$ and $\\Lambda_{i}$ is the seasonal function defined as $\\Lambda_{i}:=\\Lambda(i)$ , with <PARAGRAPH_SEPARATOR> $$ \\Lambda(t)=a_{0}+a_{1}t+a_{2}\\cos(2\\pi(t-a_{3})/365). $$ <PARAGRAPH_SEPARATOR> Further, we suppose that $x_{i}$ is an AR(3)-process with seasonal residuals, modelled as <PARAGRAPH_SEPARATOR> $$ y_{i+3}=\\beta_{1}y_{i+2}+\\beta_{2}y_{i+1}+\\beta_{3}y_{i}+\\sigma_{i}\\epsilon_{i}, $$ <PARAGRAPH_SEPARATOR> with $\\sigma_{i}:=\\sigma(i)$ , for $\\sigma(t)$ defined as <PARAGRAPH_SEPARATOR> $$ \\sigma^{2}(t)=c_{1}+\\sum_{k=1}^{4}[c_{2k}\\cos(2k\\pi t/365)+c_{2k+1}\\sin(2k\\pi t/365)]. $$ <PARAGRAPH_SEPARATOR> In Fig. 1, we have plotted the observed DATs together with the seasonal function $\\Lambda(t)$ , which we fitted using least squares minimization. The parameters of $\\Lambda(t)$ are reported in Table 1. <PARAGRAPH_SEPARATOR> The partial autocorrelation function (PACF) plot in Fig. 2 of the deseasonalized DAT-data indicates that an AR(3) process may be suitable for modelling the time evolution of these residuals. We regress today’s detrended and deseasonalized temperatures against the temperatures of the three previous days. The values of the regression parameters are all significant at the level of $1\\%$ and are presented in Table 2. Admittedly, the $p$ -values here have to be treated with care as the assumptions of the regression model are violated. However, the successive analysis clearly indicates that the AR(3) process fits data well, and therefore we believe that the obtained $p$ -values are reasonable. The constant in the regression is insignificant, as expected, and we assume it to be zero. The $R^{2}$ of the regression is $94.1\\%$ . <PARAGRAPH_SEPARATOR> In Fig. 3 the residuals and their squares for the last 10 years are plotted. We observe a persistent variation in the variance of the noise. The autocorrelation function (ACF) of the residuals and their squares are presented in Fig. 4. The autocorrelations for the first few lags are insignficant according to the Box–Ljung statistic; however, we see a clear seasonal pattern here. The seasonality pattern is confirmed by the ACF for squared residuals."
  },
  {
    "qid": "statistic-posneg-1324-1-0",
    "gold_answer": "FALSE. While the condition $k_{n}=o(n/\\ln n)$ is necessary for the application of Lemma 2 (60), it is not sufficient on its own for mean integrated square convergence. The proof also requires the condition $n=o\\big(k_{n}^{1+\\alpha}\\big)$ to control the variance of $X_{n,r}^{\\star}$ and ensure the convergence of the mean integrated square error.",
    "question": "Is the condition $k_{n}=o(n/\\ln n)$ sufficient to ensure the mean integrated square convergence of $\\hat{f}_{n}$ to $f$ when $f$ satisfies a Lipschitz condition of order $\\alpha$?",
    "question_context": "Convergence Properties of Poisson Point Process Estimators\n\nFor $r=1,\\ldots,k_{n}$ , the truncated Poisson point processes $N^{*n}(\\cdot\\cap D_{n,r})$ are independent, and the extreme values $X_{n,r}^{\\star}$ inherit this property. Thus, the distribution of the estimator $\\hat{f}_{n}$ is uniquely determined by the distribution of each $X_{n,r}^{\\star}$ . We refer to the Appendix for a series of lemmas describing these distributions. <PARAGRAPH_SEPARATOR> Theorem 1 Suppose $f$ satisfies a Lipschitz condition of order $\\alpha$ , $0<\\alpha\\leqslant1$ Þ. Then, under the condition $k_{n}=o(n/\\ln n)$ , we have for all $x\\in[0,1]$: <PARAGRAPH_SEPARATOR> $$ |\\mathrm{E}\\hat{f}_{n}(x)-f_{n}(x)+\\frac{k_{n}}{n c}=O\\left(\\frac{1}{k_{n}^{\\alpha}}\\right). $$ <PARAGRAPH_SEPARATOR> If, moreover, $n=o\\big(k_{n}^{1+\\alpha}\\big)$ then <PARAGRAPH_SEPARATOR> $$ \\mathrm{Var}\\hat{f}_{n}(x)\\sim\\frac{k_{n}h_{n}}{n^{2}c^{2}}. $$ <PARAGRAPH_SEPARATOR> The condition $n=o\\big(k_{n}^{1+\\alpha}\\big)$ is necessary to control the variance of $X_{n,r}^{\\star}$ [see Lemma 2 (62)]. It imposes that, for all $r=1,\\ldots,k_{n}$ , the mean number of points above $m_{n,r}$ in $D_{n,r}$ converges to 0. It is then possible to ignore the contribution of the events $\\{X_{n,r}^{\\star}>m_{n,r}\\}$ to the variance."
  },
  {
    "qid": "statistic-posneg-1451-0-3",
    "gold_answer": "FALSE. The paper explicitly mentions that the choice of the metric to measure the discrepancy between GGMs and GMRFs is a key element of the comparative study, and it introduces new metrics using spectral density functions for this purpose.",
    "question": "Does the paper suggest that the choice of metric to measure the discrepancy between GGMs and GMRFs is not important for their comparative study?",
    "question_context": "Comparison of Gaussian Geostatistical Models and Gaussian Markov Random Fields\n\nIn many applications in spatial and environmental epidemiology, data concerning a spatial process of interest are often observed at different spatial resolutions, and the overall problem of incompatible spatial data has been encountered very commonly when relating two spatial variables with different supports. For example, in studies of the association between air pollution exposure and adverse health effects, relevant health outcomes are usually available as areal data due to confidentiality while the pollution data are available as a point level [6,19]. To investigate the relationships between two variables with different spatial resolutions, the mismatch problem in the support of the two variables needs to be resolved. One common solution to this spatial misalignment problem is to aggregate the point-referenced data to the area level, and create a common support for both variables. Once the point-referenced data are aggregated to the relevant level, the process representing the aggregated data is modeled using integrals of spatial continuous process [14,15]. Consider a continuous Gaussian process $Y(s)$ with mean function $\\mu(s)$ and covariance function $c(s_{i},s_{j})$ for $s_{i},s_{j}\\subset D\\in R^{d}$ , where $s_{i}$ is a location in a fixed domain $D$ . The aggregated process over a region $B$ , $\\begin{array}{r}{Y(B)=\\int_{B}Y(s)\\mathrm{d}s}\\end{array}$ has a multivariate normal distribution with mean function $\\mu(B)$ and covariance function $\\Sigma(B_{1},B_{2})$ , $$\\begin{array}{l}{\\displaystyle\\mu(B)=E(Y(B))=|B|^{-1}\\int_{B}\\mu(s)\\mathrm{d}s}\\ {\\sum(B_{1},B_{2})=\\mathrm{cov}(Y(B_{1}),Y(B_{2}))=|B_{1}|^{-1}|B_{2}|^{-1}\\int_{B_{1}}\\int_{B_{2}}c(s_{1},s_{2})\\mathrm{d}s_{1}\\mathrm{d}s_{2},}\\end{array}$$ where $|B_{i}|$ denotes the area of a region $B_{i}$ for $i=1,2$ . Modeling aggregated data using these spatial integrals requires lots of computation. Hence, instead of using the aggregated models directly in modeling aggregated point-referenced data, it is becoming very common to use Gaussian Markov random fields."
  },
  {
    "qid": "statistic-posneg-1712-0-3",
    "gold_answer": "TRUE. The paper demonstrates that var r is a decreasing function of p, as evidenced by the term (2-ρ²p²) in the variance formula. Intuitively, this occurs because increasing p (the proportion of fixed variation) reduces the stochastic component of the independent variables, thereby decreasing sampling variability. The tables in §4 numerically confirm this inverse relationship.",
    "question": "Does the sampling variance of canonical correlations decrease as p increases from 0 to 1, holding ρ² and T constant?",
    "question_context": "Sampling Variance of Canonical Correlation Coefficients under Mixed Variates\n\nThis paper is concerned with the asymptotic variances of canonical correlation coefficients under alternative assumptions about the stochastic nature of the variables. The results obtained also apply to the cases of zero-order and multiple correlation coeffcients, since they are special cases of canonical correlations. The model and the assumptions are presented in §2, the results in §3, and some interpretations of the results in §4. In the Appendix detailed proofs of the results are given."
  },
  {
    "qid": "statistic-posneg-1364-2-0",
    "gold_answer": "TRUE. The condition $0 \\leqslant a \\leqslant 2(p-q-2)$ ensures that the estimator $\\delta^{A}(X)$ is minimax under squared error loss. This constraint on $a$ is derived from the properties of shrinkage estimators and the need to balance bias and variance. The rank $q$ of $A$ plays a crucial role in determining the allowable range for $a$, as it affects the dimensionality of the subspace toward which the estimator shrinks. The minimax property is maintained within this range, providing significant risk improvement when $|(I-A)\\theta|/\\sigma$ is small, which aligns with the prior belief that $A X$ is a reasonable estimator of $\\theta$.",
    "question": "Is it true that the estimator $\\delta^{A}(X) = A X + \\delta^{+}\\left[\\left(I-A\\right)X\\right]$ is minimax under squared error loss only when $0 \\leqslant a \\leqslant 2(p-q-2)$, where $q$ is the rank of $A$?",
    "question_context": "Improved Confidence Sets for the Mean Using Shrinkage Estimators\n\nIn this section we consider fixed radius confidence sets centered at estimators which shrink toward a linear subspace. For a fairly general class of confidence sets we obtain dominance results similar to those of Hwang and Casella [10]; that is, we prove that these recentered sets have uniformly higher coverage probability than the usual confidence set. <PARAGRAPH_SEPARATOR> Let $X$ be an observation from a $p$ -variate normal distribution with mean vector $\\theta$ and covariance matrix $\\sigma^{2}I.$ (Generalization to an arbitrary, known covariance matrix $\\boldsymbol{\\Sigma}$ is straightforward, and will be treated later in this section.) Define the estimator $\\delta^{+}(X)$ by <PARAGRAPH_SEPARATOR> $$ \\delta^{+}(X)=[1-(a\\sigma^{2}/|X|^{2})]^{+}X, $$ <PARAGRAPH_SEPARATOR> where $^{a}$ is a positive constant, $^{\\circ\\circ}+{^\\circ}$ denotes positive part, and $|\\cdot|^{2}$ is the Euclidean norm.  This  is the well-known positive-part James-Stein estimator, which shrinks the maximum likelihood estimator towards zero. We consider estimators of the form. <PARAGRAPH_SEPARATOR> $$ \\delta^{A}(X)=A X+\\delta^{+}\\left[\\left(I-A\\right)X\\right], $$ <PARAGRAPH_SEPARATOR> where $A$ is a $p\\times p$ symmetric, idempotent matrix, and confidence sets of the form <PARAGRAPH_SEPARATOR> $$ C_{\\delta^{4}}=\\{\\theta\\colon|\\theta-\\delta^{4}(X)|\\leqslant c\\sigma\\}. $$ <PARAGRAPH_SEPARATOR> The choice of the matrix $A$ is usually based on prior information, reflecting the belief that $A X$ is  a reasonable  estimator  of $\\theta.$ If $0\\leqslant a\\leqslant2(p-q-2).$ where $q$ is the rank of $A,\\delta^{A}(X)$ is a minimax estimator of $\\theta$ under squared error loss. Moreover, the region of significant risk improvement is the region where $|(I-A)\\theta|/\\sigma$ is small. Comparing this to the estimator $\\delta^{+}(X)$ , which yields significant risk improvement only when $|\\theta|/\\sigma$ is small, shows that $\\delta^{A}(X)$ has a wider region of significant improvement, and is very efficient if the prior belief is true."
  },
  {
    "qid": "statistic-posneg-802-0-1",
    "gold_answer": "FALSE. The condition $|\\mathcal{D}(M)|=\\Delta^{T}$ does not hold for flights (R=f). For flights, $\\mathcal{D}(M)$ is defined as $(L^{T})$, which has a cardinality of 1, not $\\Delta^{T}$. The condition only holds for pauses (R=p), where $\\mathcal{D}(M)$ is defined as $(L^{T}, L^{T}+1, ..., L^{T}+\\Delta^{T}-1)$, which indeed has $\\Delta^{T}$ elements. The paper's statement that $|\\mathcal{D}(M)|=\\Delta^{T}$ is incorrect for flights.",
    "question": "Does the condition $|\\mathcal{D}(M)|=\\Delta^{T}$ hold for both flights (R=f) and pauses (R=p)?",
    "question_context": "Modeling Motion Trajectories in Missing Data Contexts\n\nWe start this section by describing the tools that are needed to connect MPT data (i.e. time­ stamped locations in geographic space) to increments defined in Section 2. We build on the nota­ tion introduced in Section 2, following standard missing-data notational conventions. We note that extending the FPM framework to accommodate missing data is not immediate, however, be cause of a complex relationship between the locations and the increments. In particular, MPT dic­ tates a data-collection mechanism for locations, but inference in the FPM requires the formulation of a corresponding data-collection model for increments. We account for these complexities and establish the connection between observability of these two random objects.<PARAGRAPH_SEPARATOR># 3.1 Expressing motion as a sequence of locations<PARAGRAPH_SEPARATOR>To make concrete the connection between motion and location sequences, consider a single incre­ ment $M=((L^{T},L^{S}),(\\Delta^{T},\\Delta^{S}),R)$ , dropping momentarily the $k$ subscript for clarity. We define $\\mathcal{D}(M)$ to be the set of time points from the beginning of $M$ up until its end. Formally,<PARAGRAPH_SEPARATOR>$$\\mathcal{D}(M)\\equiv\\left\\{\\begin{array}{l l}{(L^{T}),}&{\\mathrm{if}\\quad R=f,}\\ {\\big(L^{T},L^{T}+1,...,L^{T}+\\Delta^{T}-1\\big),}&{\\mathrm{if}\\quad R=p.}\\end{array}\\right.$$<PARAGRAPH_SEPARATOR>Note that $|\\mathcal{D}(M)|=\\Delta^{T}$ , i.e. that the number of elements in $\\mathcal{D}(M)$ is equal to the duration of that increment.<PARAGRAPH_SEPARATOR>We can define an analogous concept for the sequence of spatial coordinates underlying an incre­ ment, which we call the trajectory of $M$ and write $\\tau(M)$ . Since flights last only one unit of time, their trajectory should correspond only to the original location $L$ . For pauses, during which physical location does not change but which might last several units of time, the trajectory contains mul­ tiple copies of the same location. We can thus formally define a trajectory as<PARAGRAPH_SEPARATOR>$$\\tau(M)\\equiv\\left\\{\\begin{array}{l l}{\\left(L^{S}\\right),}&{\\mathrm{if}\\quad R=f,}\\ {\\ \\ \\ \\ \\ {\\underbrace{\\left(L^{S},...,L^{S}\\right)}_{\\Delta^{T}\\mathrm{~times}},}}&{\\mathrm{if}\\quad R=p.}\\end{array}\\right.$$<PARAGRAPH_SEPARATOR>Since a motion is composed of increments, we now naturally extend the concept of a trajectory from increments to a motion. Specifically, slightly abusing notation, we define $\\tau(m)\\equiv\\cup_{k=1}^{K}\\tau({\\cal M}_{k}).$ , to be the concatenation of the trajectories of all increments in $m$ . Thus, the trajectory of a motion is es­ sentially a sequence of random spatial coordinates indexed by time. We can also write $\\tau({m})=(\\mathcal{S}_{t})_{t\\in\\mathbb{N}}$ , where each $\\mathcal{S}_{t}$ are collections of random spatial coordinates, each of which can be interpreted as the position of the MPT device at time $t$ .<PARAGRAPH_SEPARATOR>These new concepts can be illustrated using Example 1 in Figure 1. In this example, the realisa­ tion of $(\\mathcal{S}_{t})=\\tau(m)$ is $(l_{1}^{S},l_{2}^{S},l_{3}^{S},l_{4}^{S})$ . Similarly, $\\mathcal{D}(M_{1})$ is realised as 1, while $\\mathcal{D}(M_{3})$ as (3, 4).<PARAGRAPH_SEPARATOR>To be precise about the connection between the motion and a trajectory we formulate"
  },
  {
    "qid": "statistic-posneg-2086-4-0",
    "gold_answer": "FALSE. While the improper prior $\\pi(\\theta,X_{0},\\sigma^{2})\\propto\\sigma^{-1}$ is used in Geweke (1989), the context explicitly states that standard grid methods and steepest ascent cannot locate mass points of the posterior likelihood, implying that the posterior distributions are not straightforwardly manageable without adaptive grid methods. The prior's improper nature and the chaotic nature of the model complicate posterior inference.",
    "question": "Is it true that the improper prior distribution $\\pi(\\theta,X_{0},\\sigma^{2})\\propto\\sigma^{-q}$ (with $q=1$) leads to manageable posterior distributions for the unobserved series $X_{1},...,X_{n}$ and future values $X_{n+1}$?",
    "question_context": "Bayesian Inference in Chaotic Time Series Models with Measurement Error\n\nGeweke (1989) considers a specific one-dimensional time series model subject to measurement error or observation noise. The observed series is assumed to be of the form $Y_{i}\\sim\\operatorname{IN}(X_{i},\\sigma^{2})$ $(i=1,\\ldots,n)$ where $X_{i+1}=f(X_{i};\\theta)$ , for a deterministic chaotic map $f$ —the tent map—-parameterized by $\\pmb\\theta$ .The parameters of interest are $\\pmb\\theta$ $\\sigma^{2}$ and the initial value $X_{0}$ of the unperturbed chaotic series. Geweke observes multitudinous local maxima in the corresponding log-profile-likelihood function of $\\pmb\\theta$ for fixed $X_{0}$ . The number of local maxima increases geometrically in $\\pmb{n}$ ,a consequence of the sensitive dependence on initial conditions, and any Lipschitz condition on smoothness is almost surely violated as $n{\\rightarrow}{\\infty}$ <PARAGRAPH_SEPARATOR> When the improper prior distribution for the parameters is given by <PARAGRAPH_SEPARATOR> $$ \\pi(\\theta,X_{0},\\sigma^{2})\\propto\\sigma^{-q} $$ <PARAGRAPH_SEPARATOR> $(q=1$ in Geweke (1989)), manageable posterior distributions are available for the unobserved series $X_{1},...,X_{n}$ and forfuturevalues $X_{n+1}$ , : : : Adaptive grid methods are used: steepest ascent and standard grid methods cannot locate mass points of the posterior likelihood. <PARAGRAPH_SEPARATOR> Geweke lists some problems for further consideration: <PARAGRAPH_SEPARATOR> (a) numerical difficulties in using other improper priors which are perhaps more informative than expression (1);   (b) verification of peak-to-peak distance rules governing the log-likelihood;   (c) coping with exacerbated roughness in large sample sizes;   (d) assessing forecasting quality under sensitive dependence on initial conditions. <PARAGRAPH_SEPARATOR> In addition to that list are two other important considerations. First, how do such inference procedures cope in the presence of system noise, i.e. when stochastic innovations pass through some chaotic map $\\pmb{g}_{i}$ forexample <PARAGRAPH_SEPARATOR> $$ X_{i+1}=g(X_{i},\\epsilon_{i+1}),\\qquad\\epsilon_{i}\\sim\\mathrm{IN}(0,\\sigma^{2})? $$ <PARAGRAPH_SEPARATOR> Second, it is known that some dynamical systems can change from chaotic to regular (e.g. periodic) behaviour sharply or even discontinuously in the parameters. An illustration is given by Ruelle (1989). Precision in posterior estimation is therefore crucial to account for the true character of the dynamical system."
  },
  {
    "qid": "statistic-posneg-631-0-0",
    "gold_answer": "FALSE. The term $0.1I(X_{5}>X_{6})(\\mathrm{time})_{i}$ indicates that the effect of time on $\\mathbf{y}_{i}$ is 0.1 when $X_{5} > X_{6}$ and 0 otherwise. However, the sign of the effect depends on the context of the model. If the coefficient 0.1 is positive, then the effect is positive; if it were negative, the effect would be negative. The paper does not specify the sign of 0.1, so the statement cannot be assumed to be always true.",
    "question": "In the linear split model $$\\mathrm{Linear}\\cdot\\mathbf{y}_{i}=\\alpha_{i}+0.1I(X_{5}>X_{6})(\\mathrm{time})_{i}+\\epsilon_{i},$$ the term $0.1I(X_{5}>X_{6})(\\mathrm{time})_{i}$ implies that the effect of time on the response $\\mathbf{y}_{i}$ is always positive when $X_{5} > X_{6}$. Is this statement true?",
    "question_context": "Tree-Structured Mixed-Effects Regression for Longitudinal Data\n\nThe paper discusses a tree-structured mixed-effects regression model for longitudinal data. It introduces a linear split model: $$\\mathrm{Linear}\\cdot\\mathbf{y}_{i}=\\alpha_{i}+0.1I(X_{5}>X_{6})(\\mathrm{time})_{i}+\\epsilon_{i}.$$ This model is used to investigate the performance of linear splits and emphasizes the necessity of such splits. The paper also evaluates prediction power for slopes using the sum of squared differences between true and predicted slopes: $$\\sum_{t\\in\\tilde{T}}\\sum_{i\\in t}(\\beta_{i}-\\hat{\\beta}_{t})^{2},$$ where $\\beta_{i}$ is the true slope for the ith subject and $\\hat{\\beta}_{t}$ is the predicted slope at terminal node $t$. The paper compares different split variable selection methods and tree-size determination algorithms."
  },
  {
    "qid": "statistic-posneg-2063-0-1",
    "gold_answer": "FALSE. The paper explicitly states that W_n ranges from being very liberal to very conservative in high-dimensional settings (d > n), while A_n, though conservative, performs more reliably. The Wald-type statistic fails to maintain the nominal level even when d ≤ n, whereas A_n and H_n perform well in this range.",
    "question": "Does the Wald-type statistic (W_n) perform better than the ANOVA-type statistic (A_n) in high-dimensional settings (d > n) as implied by the paper's discussion?",
    "question_context": "High-Dimensional Repeated Measures Analysis: ANOVA-Type Statistic and Approximations\n\nThe paper discusses the analysis of high-dimensional repeated measures data, focusing on the ANOVA-type statistic (A_n) and its modifications for cases where the dimension (d) exceeds the sample size (n). The key formulas include the definition of A_n, its approximation using Box's method, and the derivation of its moments. The context also covers the performance of different test statistics (Wald-type, Hotelling's T^2, and ANOVA-type) under various covariance structures and dimensionality conditions."
  },
  {
    "qid": "statistic-posneg-2086-15-1",
    "gold_answer": "FALSE. The context explicitly states that a Hastings scheme can be used to estimate features of the posterior distribution without simulating values of $\\pmb\\theta$, by instead simulating the missing data $Y^{\\prime}$ and leveraging conjugacy to simplify calculations. This approach reduces computational costs compared to a full Gibbs sampler that alternates between $Y^{\\prime}$ and $\\pmb\\theta$.",
    "question": "Does the method described in the context always require simulating values of the parameters $\\pmb\\theta$ to estimate features of the posterior distribution ${\\pmb p}({\\boldsymbol\\theta}|{\\boldsymbol\\mathit{Y}})$?",
    "question_context": "Metropolis-Hastings Algorithm for Missing Data in MCMC\n\nWe would like to offer a suggestion for estimating features of a posterior by MCMC simulation in the presence of missing data, as discussed in Section 6 of Smith and Roberts's paper. They run a Gibbs sampler over both the unobserved data $Y^{\\prime}$ and the parameters $\\pmb\\theta$ ,conditional on the observed data $Y.$ We note that in many cases we can avoid simulating from $\\pmb\\theta$ by using a Hastings scheme, reducing the computational costs considerably. <PARAGRAPH_SEPARATOR> We assume that the prior of $\\pmb\\theta$ is conjugate with the likelihood of the complete data, so that $p(\\theta|\\pmb{Y},\\pmb{Y}^{\\prime})$ and its expectations are simple to calculate. Thus, estimating ${\\pmb p}({\\pmb\\theta}|{\\pmb\\Gamma})$ by averaging $p(\\theta|Y,$ $Y^{\\prime}$ ) over simulated values of $Y^{\\prime}$ will be straightforward. Also, as mentioned in Gelfand and Smith (1991), if the sampled values of $Y^{\\prime}$ are independent, then estimating $\\int g(\\theta)p(\\theta\\mid Y)\\mathrm{d}\\theta$ by averaging simulated values of $\\int g(\\theta)p(\\theta\\mid Y,Y^{\\prime})\\mathrm{d}\\theta$ is preferable to averaging simulated values of $g(\\theta)$ . In the dependent sampling case, this may still be advantageous. <PARAGRAPH_SEPARATOR> Thus, a scheme that produces a chain with stationary distribution $P(Y^{\\prime}\\mid Y)$ can be used to estimate features of ${\\pmb p}({\\boldsymbol\\theta}|{\\boldsymbol\\mathit{Y}})$ , without ever simulating values of $\\pmb\\theta$ . A Hastings approach makes this possible. If we choose some transition matrix $q(Y^{\\prime},Y_{\\mathrm{c}}^{\\prime})$ for generating candidates $Y_{\\mathrm{c}}^{\\prime}$ when we are currently at $Y^{\\prime}$ , then the probability of accepting the candidate will be <PARAGRAPH_SEPARATOR> $$ \\alpha(Y^{\\prime},Y_{\\mathrm{c}}^{\\prime})=\\operatorname*{min}\\left\\{{\\frac{p(Y,Y_{\\mathrm{c}}^{\\prime})q(Y_{\\mathrm{c}}^{\\prime},Y^{\\prime})}{p(Y,Y^{\\prime})q(Y^{\\prime},Y_{\\mathrm{c}}^{\\prime})}},1\\right\\}. $$ <PARAGRAPH_SEPARATOR> The ratio of likelihoods in $\\pmb{\\alpha}$ will be easy to calculate because of conjugacy. The choice of the transition matrix $\\pmb q$ will depend on the specific application; we could choose a symmetric $q(Y^{\\prime},~Y_{\\mathrm{c}}^{\\prime})=$ $p(Y_{\\mathfrak{c}}^{\\prime}|\\:Y,\\:\\hat{\\theta})$ for some convenient estimate $\\overrightharpoon{\\pmb{\\theta}}$ or generate values of $Y^{\\prime}$ with the sequential imputation method described by Kong et al. (1991). Alternatively, the transition matrices discussed by Green and Han (1992) may prove useful here. <PARAGRAPH_SEPARATOR> The authors replied later, in writing, as follows. <PARAGRAPH_SEPARATOR> A. F. M. Smith and G. O. Roberts: The large number of discussants precludes detailed responses to all the issues raised. We shall, therefore, focus on just a few themes. Discussants not addressed directly should assume that we mainly agree with their comments and/or regard their questions as posing interesting new challenges."
  },
  {
    "qid": "statistic-posneg-1347-0-1",
    "gold_answer": "TRUE. The context explicitly shows that with Bauer iterations (37 and 75), infeasibility was detected earlier (5th and 4th iterations) compared to the unaided Wilson algorithm (8th iteration), demonstrating improved performance in detecting infeasibility.",
    "question": "Does the polynomial $A(x)=51-\\varepsilon+50(x+x^{-1})+49(x^{2}+x^{-2})+...+(x^{50}+x^{-50})$ with $\\varepsilon=0{\\cdot}001$ demonstrate that Bauer iterations improve the detection of infeasibility compared to the unaided Wilson algorithm?",
    "question_context": "Optimization of Bauer and Wilson Iterations in Algorithm Performance\n\nIn Table 1, we summarize the actual number of Wilson iterations performed when $I T E R1=50$ and $E P S=10^{-4}$ for various values of ITERO. When no Bauer iterations are requested, subroutine BAU ER produces the initial iterate recommended by Wilson (1979). The total computing time required by the algorithm for $I T E R1=50$ and $E P S^{'}=10^{-4}$ is plotted in Fig. 1. It can be seen that the total time falls quickly to the optimal value, and thereafter rises slowly. For this reason, we recommend the use of $3n/2$ rather than $3n/4$ Bauer iterations. <PARAGRAPH_SEPARATOR> The use of Bauer iteration to produce initial estimates for the Wilson algorithm was also found useful in diagnosing infeasibility. On the polynomial <PARAGRAPH_SEPARATOR> $$ A(x)=51-\\varepsilon+50(x+x^{-1})+49(x^{2}+x^{-2})+...+(x^{50}+x^{-50}) $$ <PARAGRAPH_SEPARATOR> the unaided Wilson algorithm detected the infeasibility (for $\\varepsilon=0{\\cdot}001$ )on the eighth iteration, whereas after 37 and 75 Bauer iterations infeasibility was already detected on the fifth and fourth iterations respectively."
  },
  {
    "qid": "statistic-posneg-1099-0-0",
    "gold_answer": "TRUE. The formula computes the grand mean difference by summing the differences $d_{n}^{\\iota,\\alpha,\\nu,\\zeta}$ (true vs. imputed exposure time) across all trajectories ($N_{\\mathrm{tr}}$) and all imputations ($N_{\\mathrm{imp}}$), then dividing by the total number of observations ($N_{\\mathrm{imp}}N_{\\mathrm{tr}}$). This provides a summary metric to evaluate the bias introduced by each imputation method for given parameters ($\\alpha,\\nu,\\zeta$).",
    "question": "Is it correct to interpret the formula $\\bar{d}^{a,\\nu,\\zeta}=\\frac{1}{N_{\\mathrm{imp}}N_{\\mathrm{tr}}}\\sum_{n=1}^{N_{\\mathrm{tr}}}\\sum_{\\imath=1}^{N_{\\mathrm{imp}}}d_{n}^{\\imath,\\alpha,\\nu,\\zeta}$ as calculating the average difference between true exposure time and imputed exposure time across all trajectories and imputations?",
    "question_context": "Statistical Inference for Trajectory Imputation Methods in Exposure Analysis\n\nA related evaluation relies on calculating the total time spent in the exposure area according to each imputation and comparing it with the amount of time spent in the area by the true trajectory. Consider trajectory $\\tau(m_{n})$ and the corresponding imputation $\\omega_{n}^{\\imath,\\alpha,\\nu,\\zeta}$ , where in addition to symbols used before we use $\\iota=1,...,N_{\\mathrm{imp}}$ as the index of the imputation. Moreover, we consider only $\\alpha\\in\\{0.2,0.5,0.8\\}$ . We define the true exposure time $e_{n}$ as the number of time periods during which the individual performing movement $m_{n}$ is inside the exposure area, i.e. $e_{n}=\\#\\{\\mathcal{S}_{t}\\in\\tau(m_{n}):|\\mathcal{S}_{t}-b_{n}|_{2}^{2}<100\\}$. Similarly for each $\\omega_{n}^{\\imath,\\alpha,\\nu,\\zeta}$ , we write $e_{n}^{\\iota,\\alpha,\\nu,\\zeta}=\\#\\{\\mathcal{S}_{t}\\in\\omega_{n}:|\\mathcal{S}_{t}-b_{n}|_{2}^{2}<100\\}$ . We then calculate $d_{n}^{\\iota,\\alpha,\\nu,\\zeta}=e_{n}-e_{n}^{\\iota,\\alpha,\\nu,\\zeta}$ , the difference between the true length of exposure and the length resulting from the imputed trajectory. For each imputation method and missing percentage, we also calculate the grand mean of these differences as $\\bar{d}^{a,\\nu,\\zeta}=\\frac{1}{N_{\\mathrm{imp}}N_{\\mathrm{tr}}}\\sum_{n=1}^{N_{\\mathrm{tr}}}\\sum_{\\imath=1}^{N_{\\mathrm{imp}}}d_{n}^{\\imath,\\alpha,\\nu,\\zeta}$."
  },
  {
    "qid": "statistic-posneg-1520-0-0",
    "gold_answer": "TRUE. The DP12 method indeed combines penalties on both the first and second derivatives, as indicated by the presence of both γ (controlling the penalty on the first derivatives) and λ (controlling the penalty on the second derivatives) in the criterion. This combination allows the method to balance between smoothing the functions (via the second derivative penalty) and shrinking the coefficients towards zero (via the first derivative penalty), which can lead to better predictions, especially in small datasets or when the relationship between y and X is nonlinear.",
    "question": "Is it true that the double penalization method (DP12) combines penalties on both the first and second derivatives of the smooth functions in the additive model, as shown in the criterion ∑{yi−μ̃(xi)}² + γ∑∫{s̃j′(x)}²dx + λ∑∫{s̃j′′(x)}²dx?",
    "question_context": "Double Penalization in Additive Models: Combining Ridge and Smoothness Penalties\n\nThe paper discusses the prediction of a response variable y from a p-dimensional predictor variable x using an additive model. The model is defined as E(y|x) = μ(x) = β0 + s1(x1) + ... + sp(xp), where β0 is an intercept and s1(x1), ..., sp(xp) are smooth functions of the predictor variables. Estimation is performed by minimizing a penalized least squares criterion, which can include penalties on the first and second derivatives of the functions. The paper introduces a double penalization method (DP12) that combines both first and second order penalizations to improve predictions, especially in scenarios with few training data or nonlinear relationships."
  },
  {
    "qid": "statistic-posneg-475-0-1",
    "gold_answer": "FALSE. The similarity matrix W uses an adaptive scaling parameter (√h0,i ⋅ √h0,j) in the Gaussian kernel, which varies depending on the spatial dispersion between locations in De and the prediction region D0. This adaptivity allows for larger clusters in areas with weaker correlations to D0 and smaller clusters in areas with stronger correlations, enhancing the compression efficiency.",
    "question": "Does the similarity matrix W in ASDC use a fixed scaling parameter for all locations in the spectral clustering step?",
    "question_context": "Spatial Data Compression via Adaptive Dispersion Clustering\n\nThe paper introduces a method for spatial data compression using adaptive dispersion clustering (ASDC). The framework involves a hidden spatial process Y(v) observed with noise, resulting in observations Z(s) = Y(s) + ε(s). The goal is to compress spatial data by clustering locations in an exterior subdomain De while preserving the covariance structure relative to a central prediction region D0. The method uses spatial dispersion functions to quantify spatial covariance and performs spectral clustering on a similarity matrix derived from these functions."
  },
  {
    "qid": "statistic-posneg-2086-7-0",
    "gold_answer": "FALSE. The formula is part of the analysis but does not directly determine the convergence rate. The convergence rate is determined by the total variation distance behavior described in the context, which shows a sharp cut-off at log $n + c n$ iterations, not by this specific formula.",
    "question": "Is the formula $$ a={\\frac{1}{2\\theta}}+{\\frac{1}{4\\theta}}\\Bigg({\\frac{1}{\\theta}}-\\theta\\Bigg), $$ used to determine the convergence rate of the Metropolis-Hastings chain in the symmetric group example?",
    "question_context": "Convergence Analysis of Gibbs Sampler and Metropolis-Hastings Chains\n\nThe following more specific questions were sparked by earlier discussants’ suggestions to improve convergence. In the proposal to use a different random orthonormal basis at each cycle, when might it help to use a stratified random sample of bases to ensure better coverage? Is there any benefit in the idea of combining Chris Jennison's impressive use of a range of power transformations with the eigenanalysis of Besag and Green? Within some parameterized family, could a transformation be sought which in some sense optimally trades off speed of convergence against accuracy of estimation? <PARAGRAPH_SEPARATOR> Persi Diaconis and Jeffrey S. Rosenthal (Harvard University, Cambridge): The asymptotic rate of convergence to stationarity is determined by the second-largest eigenvalue (Besag and Green, Section 2). This can be deceptive. <PARAGRAPH_SEPARATOR> To explain this, we describe a study of Diaconis and Hanlon (1992) where Metropolis-Hastings chains can be explicitly diagonalized. On the symmetric group, consider sampling from $P(x)=C_{\\theta}\\theta^{d(x,x_{0})}$ where $d(\\l,\\l)$ is a metric (like Kendall's tau), $x_{0}$ is a fixed permutation, $0{<}\\theta{<}1$ and $C_{\\theta}$ is a normalizing constant. A base chain is run by repeatedly transposing random pairs. If this chain is run for an log $n+c n$ iterations, with <PARAGRAPH_SEPARATOR> $$ a={\\frac{1}{2\\theta}}+{\\frac{1}{4\\theta}}\\Bigg({\\frac{1}{\\theta}}-\\theta\\Bigg), $$ <PARAGRAPH_SEPARATOR> then the total variation distance of the chain from $P$ goes to O in an explicit way if $c$ is large, whereas the distance to $P$ is essentially 1 if $c$ is negative. We see a sharp cut-off at an log $\\pmb{n}$ ; this has occurred in many examples (see, for example, Diaconis (1988)). If we only use the second eigenvalue in this example, we can only conclude that order $n^{2}\\log n$ steps suffice. For $n=52$ ， $\\pmb{n}$ log $n\\doteq200$ , whereas $n^{2}$ log $n{\\doteq}11000$ <PARAGRAPH_SEPARATOR> In Rosenthal (1991a,b), ideas related to Harris recurrence are used to obtain explicit bounds on the variation distance to equilibrium for the Gibbs sampler for certain models. In particular, in Rosenthal (1991b) the Gibbs sampler is analysed for a standard variance components model. The model involves a location parameter ${\\pmb\\mu}$ and $K$ parameters $\\theta_{1},...,\\theta_{K}$ which are independently and identically distributed (IID) normal $(\\mu,\\sigma_{\\theta}^{2})$ . For each $\\pmb{\\theta}_{i}$ , there are $J$ observations $Y_{i1},...,Y_{i J}$ which are IID normal $(\\theta_{i},\\sigma_{e}^{2})$ ： The distribution of $(\\sigma_{\\theta}^{2},\\sigma_{e}^{2},\\mu,\\dot{\\theta}_{1},...,\\theta_{K})$ is to be estimated.Itis shown that the Gibbs sampler for this model (with conjugate priors) will converge in $C(1+\\log K/\\log J)$ iterations, where $c$ is a positive constant, independent of $\\boldsymbol{\\jmath}$ and $K$ ,which depends on the priors and the data. This rate is essentially sharp, and the techniques used seem broadly applicable. <PARAGRAPH_SEPARATOR> Another class of techniques for proving convergence results involves bounds on eigenvalues by using the underlying geometry of the chain. They were pioneered by computer scientists (see, for example, Jerrum and Sinclair (1989)) to study randomized algorithms and have been developed (Lawler and Sokal, 1988; Diaconis and Stroock, 1991; Diaconis and Saloff-Coste, 1992) to give essentially sharp rates in a variety of messy problems."
  },
  {
    "qid": "statistic-posneg-332-0-2",
    "gold_answer": "TRUE. This expression is derived specifically for the case where $x_i$ is normally distributed with mean $\theta$ and unit variance. The paper uses the properties of the normal distribution to simplify the general expression for $P_{0}(y)$ into this form.",
    "question": "Is the expression $P_{0}(y) = \\frac{e^{-2\\theta y} - E\\{e^{-2\\theta z_{N}}; y \\vert z_{N} \\geqslant h\\}}{E\\{e^{-2\\theta z_{N}}; y \\vert z_{N} \\leqslant 0\\} - E\\{e^{-2\\theta z_{N}}; y \\vert z_{N} \\geqslant h\\}}$ derived under the assumption that $x_i$ is normally distributed?",
    "question_context": "Sequential Test Analysis: Operating Characteristic and Average Sample Number\n\nIn this paper expressions are developed which are useful for determining the characteristics of a sequential test designed to test a simple hypothesis against a simple alternative. Simple equations are derived which give upper and lower limits for the operating characteristic and average sample number of the test in situations where formal expressions for these are not available."
  },
  {
    "qid": "statistic-posneg-2062-0-0",
    "gold_answer": "TRUE. The estimator $\\bar{\\mu}$ is a weighted average of the sample mean $\\bar{x}$ and the sample median $\\tilde{x}$. The paper explicitly states that this estimator is unbiased and asymptotically efficient for the location parameter $\\mu$ in the given density function, even though it does not coincide with the MLE. The weights (0.39602 and 0.60398) are derived to ensure these properties.",
    "question": "Is the estimator $\\bar{\\mu}=0.39602\\bar{x}+0.60398\\tilde{x}$ unbiased and asymptotically efficient for the location parameter $\\mu$ in the given density function $g(x;\\mu,\\sigma)$?",
    "question_context": "Efficient Estimation in Location-Scale Family Models\n\nThe characterizations of Remark 1, as far as we know, are new. In fact, for a fixed value of $\\theta$ , the estimator $\\bar{\\mu}=\\bar{\\mu}(\\theta)=w(\\theta)\\bar{x}+(1-w(\\theta))\\tilde{x}$ is unbiased and asymptotically efficient but in general it does not coincide with the MLE of $\\mu$ , as will be seen in the following example. It is important to remark that $\\bar{\\mu}$ is much more simple and easy to calculate than MLE.<PARAGRAPH_SEPARATOR># 2.1. An example<PARAGRAPH_SEPARATOR>Consider the location and scale family obtained by fixing $\\theta=1$ . Now the density function is<PARAGRAPH_SEPARATOR>$$g(x;\\mu,\\sigma)=\\frac{0.762567}{\\sigma}\\mathrm{exp}\\left(-\\frac{\\left|x-\\mu\\right|}{\\sigma}-\\frac{\\left(x-\\mu\\right)^{2}}{2\\sigma^{2}}\\right),$$<PARAGRAPH_SEPARATOR>and the corresponding unbiased and asymptotically efficient estimator of the location parameter $\\mu$ will be $\\Bar{\\mu}=0.39602\\Bar{x}+0.60398\\Tilde{x}$ . This estimator does not coincide with the MLE. For instance, given the sample of size $n=7$ , 1.251, )0.446, )0.297, )0.953, 1.377, )0.158, 0.579, the MLE of the location parameter is $\\hat{\\mu}=0.01030$ and $\\bar{\\mu}=-0.01888$ .<PARAGRAPH_SEPARATOR>The parameter of scale $\\sigma$ can be estimated by replacing $\\mu$ by $\\bar{\\mu}$ in the corresponding likelihood function of the model (12) and maximizing with respect to $\\sigma$ . It gives the following simple estimator,<PARAGRAPH_SEPARATOR>$$\\bar{\\sigma}=\\frac{1}{2}\\bigg(\\Delta(\\bar{\\mu})+\\sqrt{\\Delta(\\bar{\\mu})^{2}+4(\\bar{x}-\\bar{\\mu})^{2}+4(n-1)s^{2}/n}\\bigg),$$<PARAGRAPH_SEPARATOR>where $s^{2}$ is the sample variance and $\\begin{array}{r}{\\Delta(\\bar{\\mu})=\\sum_{i=1}^{n}|x_{i}-\\bar{\\mu}|/n}\\end{array}$ . This pseudo-likelihood estimator is also asymptotically efficient, as can be d educed from Gong $\\&$ Samaniego (1981). For the above example of size $n=7$ it gives the value $\\bar{\\sigma}=1.28429$ .<PARAGRAPH_SEPARATOR>In the preceding example we have studied the model (8) where $\\theta=1$ , but in practical situations it can be difficult to choose a fixed value for $\\theta$ . In the next section, we proceed to study the model (8) as a three-parameter family of distributions.<PARAGRAPH_SEPARATOR># 3. The double truncated normal distribution"
  },
  {
    "qid": "statistic-posneg-1000-2-2",
    "gold_answer": "TRUE. The context explicitly states that setting $\\beta_{12}\\to\\infty$ (which enforces zero probability for configurations where colors 1 and 2 are adjacent) contravenes the positivity condition of the Hammersley-Clifford theorem. This condition requires that all possible configurations have non-zero probability, which is violated here. The context further recommends avoiding synchronous updating in ICM and gradually increasing the ostensibly infinite $\\beta$ values to handle this violation, particularly when starting from an infeasible initial estimate.",
    "question": "The condition $\\beta_{12}\\to\\infty$ in the excluded adjacencies model strictly violates the positivity condition of the Hammersley-Clifford theorem, requiring special care in iterative conditional modes (ICM) estimation. Is this assertion true?",
    "question_context": "Pairwise Interaction Markov Random Fields for Image Segmentation\n\nThis section is concerned with the specification of $\\{p(x)\\}$ for three different types of scenes: those with unordered colours, as in Section 3; grey-level scenes, either with smooth variation or with occasional abrupt changes in level; and those with continuous pixel intensities, either smoothly varying or with substantial disparities. Although we consider each at a rather basic level, even then, some of our comments are speculative. More intricate modelling, which takes account of special structures in the true scene, is a topic which requires much further research. <PARAGRAPH_SEPARATOR> We shall assume that pixels have prescribed neighbours, satisfying the simple symmetry condition and chosen with proper regard to spatial contiguity: in fact, although not necessary, we shall have in mind a rectangular array of pixels with at least a second-order neighbour. hood. We concentrate on pairwise interactions: $\\{p(x)\\}$ is called a pairwise interaction M.r.f. if, for any $x$ in the sample space $\\pmb{\\Omega}$ <PARAGRAPH_SEPARATOR> $$ p(x)\\propto\\exp{\\biggl\\{}\\sum_{1\\leqslant i\\leqslant n}G_{i}(x_{i})+\\sum_{1\\leqslant i<j\\leqslant n}G_{i j}(x_{i},x_{j}){\\biggr\\}}, $$ <PARAGRAPH_SEPARATOR> where, to ensure the Markov property, $G_{i j}\\equiv0$ unlesspixels $i$ and $j$ are neighbours but otherwisethe $G$ -functions are arbitrary (cf. Besag, 1974, with a trivial change of notation). <PARAGRAPH_SEPARATOR> # 4.1. Discrete colours <PARAGRAPH_SEPARATOR> As a special case of (9), we consider $c$ -colour distributions with (Besag, 1976; Strauss, 1977) <PARAGRAPH_SEPARATOR> $$ p(x)\\propto\\exp{\\left(\\sum_{1\\leqslant k\\leqslant c}\\alpha_{k}n_{k}-\\sum_{1\\leqslant k<l\\leqslant c}\\beta_{k l}n_{k l}\\right)}, $$ <PARAGRAPH_SEPARATOR> where $\\pmb{n_{k}}$ denotes the number of pixels coloured $k$ and $n_{k l}$ the number of distinct neighbour pairs coloured $(k,l)$ ; note that, for the moment, all neighbour pairs are treated equally. The s and $\\beta\\mathbf{s}$ are arbitrary parameters, though we shall be concerned only with $\\beta_{k l}>0$ which discourages colours $k$ and $l$ from appearing at neighbouring pixels. Further assumptions about the $\\beta\\mathbf{s}$ will be made below, according to the type of scene. Note there is a redundancy among the αs, since $n_{1}+...+n_{c}=\\mathfrak{n}$ , but we retain the symmetric formulation for neater exposition. <PARAGRAPH_SEPARATOR> By considering any two realizations which differ only at pixel $i,$ it follows that the conditional probability of colour $k$ occurring there, given the colours of all other pixels is <PARAGRAPH_SEPARATOR> $$ p_{i}(k|\\cdot)\\propto\\exp\\{\\alpha_{k}-\\sum_{l\\neq k}\\beta_{k l}u_{i}(l)\\}, $$ <PARAGRAPH_SEPARATOR> where $\\beta_{k l}\\equiv\\beta_{l k}$ defines $\\beta_{k l}$ for $k>l,$ and $u_{i}(l)$ denotes the number of neighbours of pixel i having colour $l$ : incidentally, this confirms the Markov property of (10). The corresponding conditional probabilities for small sets of pixels, rather than a single pixel, can be found in a similar manner. However, marginal probabilities are generally intractable and the same is true even for the normalizing constant in (10); indeed, this has been the source of much anguish in statistical physics over many decades. Analytical results seem available only for the Ising model, which is the (infinite) rectangular lattice version of (10), with only two colours and firstorder neighbours. The physicists’ interest in such models centres on their capacity to produce positive correlations between the colours of pixels infinitely far apart, and hence infinite patches of a single colour. Fortunately, the normalizing constant is not required for maximum probability (m.a.p.) estimation or for $I C M$ ; moreover, we have designed the latter to be unresponsive to the large-scale characteristics of $\\{p(x)\\}$ <PARAGRAPH_SEPARATOR> A simple extension of (10) is to allow differing interaction parameters $\\beta_{k l}$ to be associated with different classes of neighbour pairs. Thus, with square pixels and a second-order M.r.f., $\\beta_{k l}n_{k l}$ in (10) can be replaced by $\\beta_{k l}^{\\prime}n_{k l}^{\\prime}+\\beta_{k l}^{\\prime\\prime}n_{k l}^{\\prime\\prime}$ ,where $n_{\\mathbf{k}\\mathbf{l}}^{\\prime}$ and $n_{k l}^{\\prime\\prime}$ are the respective numbers of first- and second-order neighbour pairs coloured $(k,l)$ ; in this context, Geman and McClure (1985) downweight diagonal adjacencies by a factor of $\\surd2$ . A similar modification can be made for $N-S$ and $E-W$ adjacencies if pixels are rectangular rather than square. In the sequel, we assume that any such extensions would be incorporated when necessary: the corresponding amendments to (11) are immediate. <PARAGRAPH_SEPARATOR> # 4.1.1. Unordered colours <PARAGRAPH_SEPARATOR> A simple model for unordered colours is obtained by taking a common $\\beta$ in (10). Then (11 becomes <PARAGRAPH_SEPARATOR> $$ p_{i}(k|\\cdot)\\propto\\exp\\{\\alpha_{k}+\\beta u_{i}(k)\\}, $$ <PARAGRAPH_SEPARATOR> since $u_{i}(1)+\\ldots+u_{i}(c)$ is the total number of neighbours of pixel i. Note that (12) implies that the conditional odds, in favour of colour $k,$ . depend only on the number of like-coloured neighbours, not that this is true of the corresponding probability. A final simplification occurs if the colours are interchangeable, in which case the s must also be equal and can be set to zero because of the inherent redundancy: this yields the distribution (6) used in Section 3. <PARAGRAPH_SEPARATOR> # 4.1.2. Excluded adjacencies <PARAGRAPH_SEPARATOR> A different situation arises if it is known that certain colours, say colours 1 and 2, cannot appear on neighbouring pixels in the true scene. This can be enforced in reconstructions by letting $\\beta_{12}\\to\\infty$ in (10), by which we mean $\\beta_{12}n_{12}$ is zero if $n_{12}=0$ but is otherwise infinite. Such fields strictly contravene the positivity condition in the Hammersley-Clifford theorem and, in using $I C M$ synchronous updating should be avoided and the ostensibly infinite $\\beta\\mathbf{s}$ gradually increased cycle by cycle, particularly if the initial estimate is infeasible."
  },
  {
    "qid": "statistic-posneg-849-0-0",
    "gold_answer": "TRUE. The context explicitly states that the LPML is a cross-validated measure proposed by Geisser and Eddy (1979) for model comparison, and that larger LPML values indicate 'better support' or 'greater predictive ability'. This is derived from the sum of log(CPO) statistics, where CPO measures how well the observed response is supported by the model and remaining data.",
    "question": "Is the statement 'The LPML statistic is a cross-validated measure of how well the model predicts the observed data, and larger LPML indicates better support or greater predictive ability' consistent with the provided context?",
    "question_context": "Bayesian Model Comparison and Localization Precision in Circular Regression\n\nFinally, for the random-effects distribution, assume $\\Sigma^{-1}\\sim{\\mathrm{Wishart}}_{p}({\\mathrm{df}},\\Sigma_{0}),p$ $p=16$ , in the unconstrained, no-symmetry model. We consider the Wishart model parameterized so that $E(\\Sigma)=\\Sigma_{0}/(\\mathrm{df}-p-1)$ . Fixing $\\mathrm{df}=p+2$ and taking $\\pmb{\\Sigma}_{0}$ to be a prior likely value of $\\pmb{\\Sigma}$ gives a prior that is quite vague, yet has a finite mean. Noting that $\\begin{array}{r}{\\mathrm{cov}(\\boldsymbol{\\alpha}_{v})=2^{2}\\mathbf{I}_{p}}\\end{array}$ is diagonal with constant values, we take $\\pmb{\\Sigma}_{0}=\\mathbf{I}_{p}$ ; the variability of the individual level deviations from the population curves is expected to be half of the prior variability of the population curves themselves. <PARAGRAPH_SEPARATOR> # 4. Model selection <PARAGRAPH_SEPARATOR> Geisser and Eddy (1979) proposed the LPML for model comparison. The LPML is a cross-validated measure of how well the model predicts the observed data. Specifically, let the conditional predictive ordinate (CPO) statistic for the $k$ th replicate for subject $i$ at signal $j$ be <PARAGRAPH_SEPARATOR> $$ \\mathrm{CPO}_{i j k}=f(y_{i j k}|\\mathbf{y}_{-i j k}), $$ <PARAGRAPH_SEPARATOR> where $f(\\cdot|\\mathbf{y}_{-i j k})$ is the predictive density for a new response from subject $i$ given covariates and $\\mathbf{y}{-}i j k$ , which here consists of the remaining responses from subject $i$ as well as all responses from the other subjects. Then $f(y_{i j k}|\\mathbf{y}_{-i j k})$ is how well the observed response $y_{i j k}$ is supported through the model and remaining data. The LPML is simply the sum of $\\log(\\mathrm{CPO}_{i j k})$ statistics <PARAGRAPH_SEPARATOR> $$ \\mathrm{LPML}=\\sum_{i=1}^{30}\\sum_{j=1}^{24}\\sum_{k=1}^{3}\\log(\\mathrm{CPO}_{i j k}). $$ <PARAGRAPH_SEPARATOR> Larger LPML indicates ‘better support’ or ‘greater predictive ability’. The LPML statistic has increased in popularity among Bayesians owing to the relative ease with which the LPML is stably estimated from Markov chain Monte Carlo (MCMC) output (Gelfand and Dey, 1994). The ijkth CPO statistic is computed by using the identity <PARAGRAPH_SEPARATOR> $$ \\mathrm{CPO}_{i j k}^{-1}=E_{\\theta_{i j},\\rho_{i j}|\\mathbf{y}}[f(y_{i j k}|\\theta_{i j},\\rho_{i j})^{-1}], $$ <PARAGRAPH_SEPARATOR> where $[\\theta_{i j},\\rho_{i j}|\\mathbf{y}]$ is the posterior of $(\\theta_{i j},\\rho_{i j})$ given the full data set y, i.e. the estimated reciprocal of ${\\mathrm{CPO}}_{i j k}$ is the reciprocal of equation (3) averaged over the posterior MCMC values of $(\\theta_{i j},\\rho_{i j})$ . Spiegelhalter et al. (2002) proposed the deviance information criterion for Bayesian model comparison. However, Draper and Krnjajic (2007), section 4.1, have shown that the deviance information criterion can be viewed as an approximation to the LPML. <PARAGRAPH_SEPARATOR> A referee has brought up a potential problem with Gelfand and Dey’s (1994) estimate of the CPO statistics, namely that harmonic mean estimators can have very large, or infinite, variance. This often happens when the harmonic mean is used to estimate the marginal likelihood of the data, as the difference between the prior and posterior distributions is typically so great that only a few MCMC atoms have non-negligible mass in the importance sampling estimate. However, with CPO computation, the distributions to consider are the full posterior and the posterior computed leaving out one of the data values—these distributions are likely to be very close and have similar behaviour in the tails. We double-checked our LPML estimates by repeatedly fitting models and found them to be stable within one unit. <PARAGRAPH_SEPARATOR> The precision $\\rho$ in the wrapped Cauchy model (2) provides an estimate of the localization performance within each component of the mixture (3) but does not provide a measure of overall localization. We instead consider a measure derived from the mixture (3) itself that is a function of both the true and the confused signals, the mixing parameter $\\theta$ and the precision $\\rho$ , which reflects how concentrated a group’s ability to localize the signal is. <PARAGRAPH_SEPARATOR> The population localization distribution for group $g$ given all model parameters is specified through the mixture (3) by <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l r}&{}&{f_{g}(y|S_{j},\\alpha_{g})=\\displaystyle\\frac{\\exp\\left\\{m_{g}(S_{j})\\right\\}}{1+\\exp\\left\\{m_{g}(S_{j})\\right\\}}f\\bigg[y|C(S_{j}),\\displaystyle\\frac{\\exp\\left\\{p_{g}(S_{j})\\right\\}}{1+\\exp\\left\\{p_{g}(S_{j})\\right\\}}\\bigg]}\\ &{}&{+\\displaystyle\\frac{1}{1+\\exp\\left\\{m_{g}(S_{j})\\right\\}}f\\bigg[y|S_{j},\\displaystyle\\frac{\\exp\\left\\{p_{g}(S_{j})\\right\\}}{1+\\exp\\left\\{p_{g}(S_{j})\\right\\}}\\bigg],}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\alpha_{g}=(\\alpha_{1g},\\ldots,\\alpha_{p g})^{\\prime}$ . <PARAGRAPH_SEPARATOR> We consider a measure of localization precision based on the notion of excess mass (Müller and Sawitzki, 1991; Polonik, 1995; Fisher and Marron, 2001), related to highest density regions, and often used for testing the unimodality of densities. The excess mass approach seeks to find a set of $k$ connected regions with the smallest volume that concentrates a given amount of mass above a threshold. We borrow from this approach by finding a cut-off $\\lambda$ and associated region $R$ such that <PARAGRAPH_SEPARATOR> $$ 0.5{\\overline{{}}}=\\int_{x\\colon f(x)>\\lambda}\\left\\{f(x)-\\lambda\\right\\}{\\mathrm{}}{\\mathrm{}}{\\mathrm{}}\\mathrm{d}x=\\int_{R}\\left\\{f(x)-\\lambda\\right\\}{\\mathrm{}}{\\mathrm{}}\\mathrm{d}x $$ <PARAGRAPH_SEPARATOR> (see Fig. 2 in Müller and Sawitzki (1991)) and then take $L_{50}$ to be the length of the region $R$ , which is bounded above by $2\\pi$ . Fisher and Marron (2001), section 5, considered a related empirical measure for circular data based on smoothing with a von Mises kernel. <PARAGRAPH_SEPARATOR> $L_{50}$ is the length of the region from which we would have to move $50\\%$ of the probability mass to obtain a uniform density, which is the ‘most diffuse’ density one can consider over $[0,2\\pi]$ . Thus, $L_{50}$ has a nice interpretation and we use it over other common measures such as the interquartile range or highest posterior density intervals. Larger values of $L_{50}$ indicate highly diffuse and unfocused localization distributions that are perhaps more markedly bimodal, whereas small values indicate more concentrated localization. Credible intervals for the median $L_{50}$ and median differences in $L_{50}$ among groups are computed from equation (5) evaluated at the MCMC posterior samples and indicate groupwise differences in localization performance. We expect that aging and hearing impariment will be associated with increased $L_{50}$ , and that these effects may depend on signal position. <PARAGRAPH_SEPARATOR> All analyses were carried out in SAS(R) version 9.3; in particular all MCMC runs were implemented in the new SAS MCMC procedure; SAS code for duplicating analyses in this paper is available from"
  },
  {
    "qid": "statistic-posneg-1953-2-0",
    "gold_answer": "TRUE. The observation equation is correctly specified as it includes independent additive Gaussian noise for both species, with a diagonal covariance matrix $\\mathrm{diag}(\\sigma^{2},\\sigma^{2})$, indicating no correlation between the noise terms for prey and predator populations. This aligns with the context where data for both species were corrupted with independent, additive Gaussian error with standard deviation $\\sigma=1$.",
    "question": "Is the observation equation $Y_{t}=X_{t}+\\varepsilon_{t}$ correctly specified with independent Gaussian noise for both species, given the context?",
    "question_context": "Lotka-Volterra Model Inference and Stochastic Kinetics\n\nThe Lotka-Volterra system consists of two species, prey $(\\mathcal{X}_{1})$ and predator $(\\mathcal{X}_{2})$ , and three reactions: ${\\mathcal{R}}_{1}$ denotes the reproduction of a member of the prey species, $\\mathcal{R}_{2}$ denotes the death of a member of prey and the reproduction of a predator, and $\\mathcal{R}_{3}$ denotes the death of a predator. The resulting reaction list is $$\\begin{array}{r l}&{\\mathcal{R}_{1}:~\\mathcal{X}_{1}~\\xrightarrow[]{c_{1}}~2\\mathcal{X}_{1}}\\ &{\\mathcal{R}_{2}:~\\mathcal{X}_{1}+\\mathcal{X}_{2}~\\xrightarrow[]{c_{2}}~2\\mathcal{X}_{2}}\\ &{\\mathcal{R}_{3}:~\\mathcal{X}_{2}~\\xrightarrow[]{c_{3}}~\\mathcal{V}}\\end{array}$$ A single realisation of the jump process at 51 integer times was generated via Gillespie’s direct method with rate constants as in Boys et al. (2008), that is $c=(0.5,0.0025,0.3)^{\\prime}$ and an initial condition of $X_{0}=(100,100)^{\\prime}.$ The data for both species were corrupted with independent, additive Gaussian error and standard deviation $\\sigma=1$ . The corresponding observation equation (12) becomes $$Y_{t}=X_{t}+\\varepsilon_{t},\\qquad\\varepsilon_{t}\\sim\\mathrm{N}\\left(0,\\mathrm{diag}(\\sigma^{2},\\sigma^{2})\\right),\\qquadt=0,\\dots,50.$$"
  },
  {
    "qid": "statistic-posneg-1398-0-0",
    "gold_answer": "TRUE. The formula includes the term $(1-p_{1})p_{1}^{N_{1}-1}$, which accounts for the probability that the abstract outside infector (indexed as $i=1$) infects at least one individual in the cluster ($N_{1} \\geq 1$). The term $(1-p_{1})$ represents the probability that the outside infector does not fail to transmit to any individual, and $p_{1}^{N_{1}-1}$ adjusts for the fact that at least one transmission must occur. This aligns with the model's assumption that the outsider always produces at least one infection, as stated in the context.",
    "question": "In the multiple outside transmissions model, the likelihood for a single permissible transmission forest $F$ is given by $\\mathcal{L}_{\\mathcal{O}}(F) = (1-p_{1})p_{1}^{N_{1}-1}\\prod_{i=2}^{|\\mathcal{C}|+1}(1-p_{i})p_{i}^{N_{i}}$. Is it correct to interpret this formula as assuming the abstract outside infector always produces at least one infection?",
    "question_context": "Branching Process Models for Infectious Disease Transmission\n\nOur multiple outside transmissions model places all nondirectly connected infections in the second generation and are directly infected by the abstract outside infector. In this way, we can reformulate broken sub-trees into a single unbroken tree. This projection benefits us because we are now looking at an unbroken transmission tree like in the base model, now with one extra, abstract individual. This projection allows us to use the same techniques for sampling in the base model and apply them to the multiple outside transmissions model. Since this projection does not require all observed individuals to be connected in some fashion, it can better reflect real conditions of disease transmission when some cases are undetected. <PARAGRAPH_SEPARATOR> Calculating the likelihood of the multiple outside transmissions model is similar to that of the base model likelihood in Equation (2), but there are two primary differences. The first difference is that we sum over a different set of permissible transmission structures. In the base model, we require a transmission tree, such that all individuals are connected by a transmission path from a root node, who is an individual from the cluster. In the multiple outside transmissions model, permissible transmission structures may include one or more transmission trees from the base model that are connected by an abstract outside infection. We call this space of permissible transmission structures ${\\mathcal{F}}_{|{\\mathcal{C}}|}$ , where $\\mathcal{F}$ stands for forest. <PARAGRAPH_SEPARATOR> The second difference is that is that because of the abstract outside infection, we assume there are actually $|\\mathcal{C}|+1$ individuals in each cluster and are implicitly assuming the outsider always produces at least one infection. The likelihood for a single permissible transmission forest $F$ is then <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{{\\cal L}_{\\cal O}(F)=\\displaystyle\\frac{1}{1-(1-p_{1})}(1-p_{1}){p_{1}^{N_{1}}}\\prod_{i=2}^{|\\mathcal{C}|+1}(1-p_{i}){p_{i}^{N_{i}}}}} {{~=(1-p_{1}){p_{1}^{N_{1}-1}}\\displaystyle\\prod_{i=2}^{|\\mathcal{C}|+1}(1-p_{i}){p^{N_{i}}},}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where the subscript ${}^{\\mathrm{{\\infty}}}\\mathrm{O}^{\\mathrm{{\\gamma}}}$ stands for the multiple roots transmission model likelihood. The total likelihood over all such forests <PARAGRAPH_SEPARATOR> in ${\\mathcal{F}}_{n}$ is then <PARAGRAPH_SEPARATOR> $$ \\mathcal{L}_{O}(\\mathcal{C})=\\sum_{F\\in\\mathcal{F}_{n}}L_{O}(F). $$ <PARAGRAPH_SEPARATOR> There are two issues with the multiple outside transmissions model. First, while the base model enforces every individual to be connected through a single root node, the multiple outside transmissions reconstruction permits the other extreme: transmission structures where there is no intra-cluster transmission of disease. Another issue is that the reformulation into the multiple outside transmissions structure may sever connections among individuals. For example, in Figure 3, individual $C$ infects unobserved individual $E$ who infects individual $G$ The multiple outside transmissions projection will ignore any possible relationship between individual $C$ and $G$ . <PARAGRAPH_SEPARATOR> # 2.3. The Naive Comparator Model <PARAGRAPH_SEPARATOR> We compare our base transmission model and multiple outside transmissions model to a naive model, where the transmission sequence is a single chain. In this model, the detection times $t_{i}$ for $i=1,\\ldots,|\\mathcal{C}|$ are known and $t_{i}<t_{i+1}$ for $i=1,\\ldots|{\\mathcal{C}}|-1$ . Then for $i>1$ , the infector of individual $i$ is individual $i-$ 1. Unlike our base model and multiple outside transmissions model, this method requires a temporal variable to order the individuals. The likelihood for this model (with subscript 0 for null/naive) is then <PARAGRAPH_SEPARATOR> $$ \\mathcal{L}_{0}(\\pmb{\\beta};C)=\\prod_{i=1}^{|\\mathcal{C}|-1}(1-p_{i})p_{i}. $$ <PARAGRAPH_SEPARATOR> The permissible transmission structure in this naive model is an element of $\\tau_{c}$ , those transmission trees that are permissible in the base model. However, it does not make sense to compare the likelihood values of $\\mathcal{L}(\\mathcal{C}),\\mathcal{L}_{O}(\\mathcal{C})$ , and $\\mathcal{L}_{0}(\\mathcal{C})$ because we are conditioning on different sets of permissible transmission structures. However, we can still compare the likelihood values within the modeling paradigms, for example, using nested $\\beta$ coefficients. <PARAGRAPH_SEPARATOR> # 3. Likelihood Computation <PARAGRAPH_SEPARATOR> Directly maximizing the likelihood in Equation (2) is a computationally difficult task because it requires enumerating all unique transmission trees of size $|{\\mathcal{C}}|$ . To give an idea of the number of computations are required, Cayley’s formula says that the number of unique trees on $|{\\mathcal{C}}|$ indistinguishable nodes is $|{\\mathcal{C}}|^{|{\\mathcal{C}}|-2}$ . To be more concrete, there are over one billion different transmission trees for a football team sized cluster $\\langle{\\mathcal{C}}|=11\\rangle$ ). This number is a lower bound for us, as we consider nodes to be distinguishable from one another. This gives an example of how enumerating transmission trees can only be accomplished practically when $|{\\mathcal{C}}|$ is small $\\langle\\vert{\\mathcal{C}}\\vert<15\\rangle$ ). <PARAGRAPH_SEPARATOR> Since enumerating the transmission trees is infeasible for $|{\\mathcal{C}}|\\geq15$ , we sample from $\\tau_{c}$ , the distribution of transmission trees corresponding to cluster $\\mathcal{C}$ , to accurately estimate the average likelihoods. That is, if we uniformly sample $K$ transmission trees of size $|{\\mathcal{C}}|,T_{1},\\ldots,T_{K}$ , from $\\tau_{c}$ then we can approximate the average likelihood over all transmission trees from $\\tau_{c}$ . The Monte Carlo (MC) average likelihood ${\\widehat{\\mathcal{L}}}_{K}({\\mathcal{C}})$ is, by the law of large numbers, approximately equal to  t\u000bhe true average likelihood when $K$ grows large where $|{\\mathcal{T}}_{\\mathcal{C}}|$ is the number of unique transmission trees of size $C$ , <PARAGRAPH_SEPARATOR> $$ \\widehat{\\mathcal{L}}_{K}(\\mathcal{C}):=\\frac{1}{K}\\sum_{k=1}^{K}L_{T_{k}}(\\mathcal{C})\\approx\\frac{\\mathcal{L}(\\mathcal{C})}{|\\mathcal{T}\\mathcal{C}|}. $$ <PARAGRAPH_SEPARATOR> Since $|{\\mathcal{T}}_{\\mathcal{C}}|$ is not dependent on the parameter $\\beta$ , maximizing the average likelihood over the parameters is equivalent to maximizing the likelihood in Equation (2). Our MC estimate of $\\beta$ is thus obtained by maximizing over the the following approximations from Equation (3), <PARAGRAPH_SEPARATOR> $$ {\\widehat{\\pmb{\\beta}}}=\\arg\\operatorname*{max}_{\\pmb{\\beta}}\\prod_{m=1}^{M}{\\widehat{\\mathcal{L}}}_{K}({\\mathcal{C}}_{m}). $$ <PARAGRAPH_SEPARATOR> If we can sample transmission trees from $\\tau_{c}$ , then as the number of samples $(K)$ grows large our estimate of $\\beta$ (Equation (4)) should adequately approximate the MLE in Equation (2). <PARAGRAPH_SEPARATOR> Sampling trees from $\\tau_{c}$ is not as straightforward as generating unconstrained branching processes because our sampled trees must correspond to a fixed size $|{\\mathcal{C}}|$ . In order to sample transmisison trees, we devise and implement an algorithm to uniformly sample from $\\tau_{c}$ . Consider an example shown in Figure 2 with individuals $A,B,$ and $C$ in a cluster. We may randomly draw a transmission tree from that cluster. First, we draw the number of generations $g=2$ . Then we draw the generation sizes. In this case, the only valid generation sizes to draw are $(n_{1}~=~1,n_{2}~=~2)$ , since generation one must have a single individual. Likewise, the only infector ID draws for individuals (2,1) and (2,2) are the individual in position (1,1). Finally, we draw a permutation of the covariates so (1,1) is B, (2,1) is A, and (2,2) is C. This forms the transmission tree shown in row one, column two where the nodes are labeled in blue. <PARAGRAPH_SEPARATOR> In general, we draw samples from $\\tau_{c}$ by the following steps: (i) we sample the number of generations; (ii) we sample generation sizes conditional on (1); (iii) for each individual in a generation, we randomly draw an infector from the previous generation; and (iv) we randomly permute the labels of the covariates to the nodes. The idea behind our algorithm is that while we cannot enumerate the entirety of $\\tau_{c}$ , we can enumerate the conditional spaces in each step. This algorithm is summarized in pseudocode in Algorithm 1. The difficulty in the development of this algorithm lies in properly partitioning and then enumerating the (conditional) sample spaces to ensure that each tree that can be produced by a branching process may be sampled and to ensure that each unique tree has the same probability of being sampled. More details about this algorithm may be found in Appendix A."
  },
  {
    "qid": "statistic-posneg-1139-2-0",
    "gold_answer": "FALSE. Explanation: The paper states that for a nearest-neighbors graph G_NN, σ²_c,DLM(G_NN) = (1 + ρ(√2v))/(1 - ρ(√2v)), which is generally smaller than σ²_c,DLM(G_BON) = (1 + ρ(v))/(1 - ρ(v)), especially when ρ is Gaussian. This implies that the DLM method can be exponentially sharper than Bonferroni when an appropriate neighborhood structure G is chosen.",
    "question": "Is the critical variance σ²_c,DLM(G) defined as max_{x∈S\\N} max_{y∈S\\N} (1 + ρ(x,y))/(1 - ρ(x,y)) always greater than the Bonferroni critical variance σ²_c,DLM(G_BON) when G is a nearest-neighbors graph?",
    "question_context": "Accuracy of Discrete Local Maxima P-value Approximation in Gaussian Fields\n\nThe paper discusses the accuracy of the discrete local maxima (DLM) P-value approximation for Gaussian fields, comparing it to the Bonferroni method and continuous setting approximations. It introduces critical variance measures and neighborhood structures to analyze the exponential decay rates of approximation errors."
  },
  {
    "qid": "statistic-posneg-648-0-1",
    "gold_answer": "TRUE. The paper explicitly states that the observation error variance $\\sigma_{\\epsilon}^{2}$ is given a Jeffreys' prior, $[\\sigma_{\\epsilon}^{2}] \\propto 1/\\sigma_{\\epsilon}^{2}$. This is a non-informative prior commonly used in Bayesian analysis when little prior information is available about the variance parameter.",
    "question": "The error term $\\epsilon_{i}(\\tau)$ in the Bayesian FOSR model is assumed to follow a normal distribution with mean zero and variance $\\sigma_{\\epsilon}^{2}$, and it is independent across $i$ and $\\tau$. Is it correct that the prior for $\\sigma_{\\epsilon}^{2}$ is specified as Jeffreys' prior, proportional to $1/\\sigma_{\\epsilon}^{2}$?",
    "question_context": "Bayesian Function-on-Scalars Regression Model for High-Dimensional Data\n\nThe paper introduces a Bayesian Function-on-Scalars Regression (FOSR) model for high-dimensional data, where the functional response $Y_{i}(\\tau)$ is modeled using basis functions $\\{f_{k}(\\tau)\\}$. The model includes scalar predictors $x_{i,j}$ and accounts for within-curve and between-curve dependencies. The coefficients $\\{\\beta_{k,i}\\}$ are modeled with Gaussian priors, and the error term $\\epsilon_{i}(\\tau)$ is assumed to be independently normally distributed. The model can be expressed in a conventional FOSR form, linking the predictors directly to the functional response via coefficient functions $\\tilde{\\alpha}_{j}(\\tau)$."
  },
  {
    "qid": "statistic-posneg-1294-0-0",
    "gold_answer": "FALSE. The equality $\\sum_{l=0}^{k-1}\\|x_{n k+l}-x_{n k+l}^{\\prime\\prime}\\|^{2}=\\|X_{0}-X_{0}^{\\prime\\prime}\\|^{2}$ is only true under the specific conditions outlined in the paper, where the series is cyclostationary and the PCA transformation is applied. The equality does not hold universally for all $n \\in \\mathbb{Z}$ without these conditions. The paper's context shows that this equality is a result of the specific properties of cyclostationary series and the PCA transformation, not a general property of all series.",
    "question": "Is the equality $\\sum_{l=0}^{k-1}\\|x_{n k+l}-x_{n k+l}^{\\prime\\prime}\\|^{2}=\\|X_{0}-X_{0}^{\\prime\\prime}\\|^{2}$ true for all $n \\in \\mathbb{Z}$ in the context of cyclostationary series and PCA?",
    "question_context": "Cyclostationary Series and Principal Component Analysis\n\nThe set of the random variables $\\{x_{n};n\\in\\mathbb{Z}\\}$ is a $\\{0,\\ldots,k-1\\}-$ cyclostationary series when $\\begin{array}{r}{\\mathsf{E}(x_{n}\\overline{{x_{m}}})=\\mathsf{E}(x_{n+p k}\\overline{{x_{m+p k}}}),}\\end{array}$ for any $(n,m,p)$ from $\\mathbb{Z}\\times\\mathbb{Z}\\times\\mathbb{Z}$ . So we can easily verify that $\\{(x_{k n+p})_{n\\in\\mathbb{Z}};p\\in\\{0,\\dots,k-1\\}\\}$ is a family of stationary series pairwise stationarily correlated. The subset $\\{0,\\ldots,k-1\\}$ is such that with any n from $\\mathbb{Z}$ we can associate an element, and only one, from $\\{0,\\dots,k-1\\}\\times k\\mathbb{Z}.$ , $\\begin{array}{r}{(n-k[\\frac{n}{k}],k[\\frac{n}{k}])}\\end{array}$ such that $\\begin{array}{r}{n=n-k[\\frac{n}{k}]+k[\\frac{n}{k}]}\\end{array}$ ( $[x]$ is the integer part of $x$ ). <PARAGRAPH_SEPARATOR> If we consider the $k\\cdot$ −dimensional random vector $X_{n}^{T}~=~\\left(x_{n k}~\\cdot\\cdot\\cdot~x_{n k+k-1}\\right)$ , we can easily verify that $(X_{n})_{n\\in\\mathbb{Z}}$ is a $\\mathbb{C}^{k}$ −stationary series. So we can define the PCA in the  frequency domain of $(X_{n})_{n\\in\\mathbb{Z}}$ The first $p$ steps give a $\\mathbb{C}^{p}-$ stationary series, $(X_{n}^{\\prime})_{n\\in\\mathbb{Z}}$ , and a $\\mathbb{C}^{k}-$ stationary series, $(X_{n}^{\\prime\\prime})_{n\\in\\mathbb{Z}}$ . Each of the random vectors $X_{n}^{\\prime\\prime}$ is $k-$ dimensional: $(X_{n}^{\\prime\\prime})^{T}=\\left(y_{n,1}\\quad\\cdot\\cdot\\cdot\\quad y_{n,k}\\right)^{*}$ . We can then verify that $\\{x_{n}^{\\prime\\prime};n\\in\\mathbb{Z}\\}$ , where $x_{n}^{\\prime\\prime}=y_{[\\frac{n}{k}],n-k[\\frac{n}{k}]+1}$ , is a $\\{0,\\ldots,k{-1}\\}-$ −cyclostationary series. <PARAGRAPH_SEPARATOR> We show that <PARAGRAPH_SEPARATOR> $$ \\sum_{l=0}^{k-1}\\|x_{n k+l}-x_{n k+l}^{\\prime\\prime}\\|^{2}=\\|X_{n}-X_{n}^{\\prime\\prime}\\|^{2}=\\|X_{0}-X_{0}^{\\prime\\prime}\\|^{2}=\\sum_{l=0}^{k-1}\\|x_{l}-x_{l}^{\\prime\\prime}\\|^{2}. $$ <PARAGRAPH_SEPARATOR> So we can summarize a cyclostationary series by a $\\mathbb{C}^{p}-$ stationary series, $(X_{n}^{\\prime})_{n\\in\\mathbb{Z}}$ . The quantity (1) is a measure of the quality of this summary. As for the cyclostationary series $(X_{n}^{\\prime\\prime})_{n\\in\\mathbb{Z}}$ , it enables the reconstruction of the data. <PARAGRAPH_SEPARATOR> Let us now present a particular case of the foregoing study. This case is similar to the previous one, but concerns the functions instead of series. Let us consider a family of integrable square module random variables $\\{X_{t};t\\in\\mathbb{R}\\}$ such that $\\langle X_{t},X_{t^{\\prime}}\\rangle=\\langle X_{t+1},X_{t^{\\prime}+1}\\rangle$ , for any pair $(t,t^{\\prime})$ of reals. <PARAGRAPH_SEPARATOR> From the process $(X_{t})_{t\\in\\mathbb{R}}$ , we build a stationary series $(Y_{n})_{n\\in\\mathbb{Z}}$ , each element $Y_{n}$ is a random vector, more precisely, a random variable taking values in the $\\mathbb{C}-$ Hilbert space $L^{2}([0;1[)$ (which substitutes to $\\mathbb{C}^{k}$ ), which is defined from the family of random variables $\\{X_{t+n}$ ; $t\\in[0;1[\\}$ (this family substitutes to the family $\\left\\{x_{n k},\\dots,x_{(n+1)k-1}\\right\\}$ in the previous example). $(Y_{n})_{n\\in\\mathbb{Z}}$ $\\operatorname{E}(Y_{n}\\otimes Y_{m})=\\operatorname{E}(Y_{n-m}\\otimes Y_{0})$"
  },
  {
    "qid": "statistic-posneg-1761-0-1",
    "gold_answer": "FALSE. The estimator does not require $\\mathbf{W}_{1}$ and $\\mathbf{W}_{2}$ to have the same degrees of freedom; it only requires them to be independent Wishart matrices with their respective degrees of freedom and scale matrices.",
    "question": "Does the estimator $\\widehat{\\mathrm{tr}(\\mathbf{V}_{1}\\mathbf{V}_{2}^{2})}$ require the matrices $\\mathbf{W}_{1}$ and $\\mathbf{W}_{2}$ to have the same degrees of freedom?",
    "question_context": "Unbiased Estimation of Trace for Wishart Matrices\n\nLemma 1. Assume that $W_{\\alpha}\\sim W_{p}(\\nu_{\\alpha},\\mathbf{V}_{\\alpha}/\\nu_{\\alpha}),\\alpha=1,2$ are independent. Then the unbiased estimator of tr $(\\mathbf{V}_{1}\\mathbf{V}_{2}^{2})$ is\n\n$$\n\\widehat{\\mathrm{tr}(\\mathbf{V}_{1}\\mathbf{V}_{2}^{2})}=\\frac{\\nu_{2}}{(\\nu_{2}-1)(\\nu_{2}+2)}[\\nu_{2}\\mathrm{tr}(\\mathbf{W}_{1}\\mathbf{W}_{2}^{2})-\\mathrm{tr}(\\mathbf{W}_{1}\\mathbf{W}_{2})\\mathrm{tr}(\\mathbf{W}_{2})].\n$$"
  },
  {
    "qid": "statistic-posneg-753-3-1",
    "gold_answer": "TRUE. This is a known result for the scaled fourth central moment of a multinomial distribution. The $n^2$ scaling matches the order of the moment, and the expression $3\\pi_{0k}^2(1-\\pi_{0k})^2$ correctly captures the dependence on the probability parameter $\\pi_{0k}$ for this asymptotic approximation.",
    "question": "Does the formula $m_4(k) \\equiv n^2E\\{(p_k-\\pi_{0k})^4\\} = 3\\pi_{0k}^2(1-\\pi_{0k})^2$ correctly represent the fourth central moment of a multinomial distribution?",
    "question_context": "Information Criteria and Phi-Divergence in Categorical Data Analysis\n\nThe paper discusses the use of phi-divergence for model selection in categorical data analysis, presenting complex formulas for bias correction terms and higher-order moments. Key formulas include expressions for $b_{\\Delta}$, $\\kappa_3$, and $m_4$ involving derivatives of the phi-function and probability terms."
  },
  {
    "qid": "statistic-posneg-898-1-1",
    "gold_answer": "FALSE. Unlike bootstrap methods, the NMCT procedure does not require plug-in estimation of $\\varSigma$. Instead, it generates conditional counterparts of the test statistic by simulating random variables, bypassing the need for covariance matrix estimation.",
    "question": "Does the NMCT procedure require the estimation of the covariance matrix $\\varSigma$ for approximating the null distribution of the test statistic?",
    "question_context": "Nonparametric Monte Carlo Test (NMCT) for Multivariate Regression Diagnostics\n\nFrom Theorem 2.1 we can easily determine $p$ -values through chi-square distributions. However, for $\textstyle{\\mathcal{T}}_{n}$ a deterioration for the power comes from a plug-in estimation for the covariance matrix $\\varSigma=\\operatorname{Cov}(V\bullet\\varepsilon)$ . This is because under the alternative $\\pmb\\varepsilon$ is no longer centered and this covariance matrix will be larger than that under $H_{0}$ . In this section, we propose a nonparametric Monte Carlo test (NMCT) procedure to approximate the null distribution of a test. An interesting point of this approximation is that although the test itself we will use is not scale-invariant because we do not use a plug-in estimator of the covariance matrix, NMCT can be scale-invariant. In comparison with the existing proposals for approximating the null distributions of the tests, for instance, classical bootstrap Efron [5], and their variants such as the wild bootstrap [11,19], we know that a bootstrap procedure also requests the estimation for $\\varSigma$ . More importantly, NMCT has a special usefulness in multivariate response cases because if the distribution of the error is of semiparametric structure, we can benefit from it to better mimic the null distribution, and if the error is fully nonparametrically distributed, NMCT is also consistent. To this end, we first give a definition about the independent decomposition for a random variable."
  },
  {
    "qid": "statistic-posneg-1680-3-0",
    "gold_answer": "TRUE. The table shows that with n=300, the frequentist estimate for $\\theta_{2}$ (potassium effect) is 0.85 (positive), while the BETEL posterior mean is -0.56 (negative), aligning with the full dataset results (-0.91 for frequentist and -0.89 for BETEL with n=13957). This demonstrates that the informative priors in BETEL (negative effect for potassium) help anchor estimates toward biologically plausible values when sample size is small, reducing spurious positive estimates seen in the frequentist approach. As sample size increases, both methods converge, consistent with Bayesian posterior consistency theory.",
    "question": "Is the statement 'The Bayesian exponentially tilted empirical likelihood (BETEL) approach with informative priors for $\\theta_{1}$ and $\\theta_{2}$ provides estimates closer to the full dataset values compared to the frequentist approach, especially in smaller samples' supported by the results in Table 3?",
    "question_context": "Bayesian Exponentially Tilted Empirical Likelihood in Regression Analysis\n\nWe work in the design setting described in $\\S2.3$ . The aim is to fit a linear regression model for blood pressure $Y$ on sodium consumption $X_{1}$ and potassium consumption $X_{2}$ . Age $X_{3}$ is also included for deconfounding. The moment condition is $$ g(D,\\theta)=R W(Y-\\theta_{0}-X_{1}\\theta_{1}-X_{2}\\theta_{2}-X_{3}\\theta_{3})X, $$ where $R$ is the selection indicator variable, $W$ is the weight variable and $X=(1,X_{1},X_{2},X_{3})^{\\mathrm{T}}$ . We consider the frequentist M-estimator with standard errors estimated using the sandwich estimator (1). For our Bayesian exponentially tilted empirical likelihood proposal, each regression parameter is assigned an independent prior: $\\theta_{0}\\sim t_{3}(100,1),\\theta_{1}$ and $\\theta_{2}$ follow half-normal distributions on the positive and negative real numbers, respectively, each with scale parameter 1, and $\\theta_{3}\\sim t_{3}(0,1)$ . The priors for $\\theta_{1}$ and $\\theta_{2}$ reflect the substantial prior evidence that sodium raises blood pressure in humans while potassium does the opposite. The likelihood was computed with a tolerance of $10^{-4}$ , and posterior samples were drawn using a Metropolis–Hastings algorithm."
  },
  {
    "qid": "statistic-posneg-268-0-0",
    "gold_answer": "TRUE. The condition γ_j = ∑_{i=1}^I c_{ji} π_i is correctly derived as it represents the probability that Y belongs to bin J_j, where π_i is modeled using B-splines. The identifiability constraint ∑_{k=1}^K ϕ_k = 0 ensures the model is identifiable, and the use of B-splines allows for smooth estimation of the density. The weights c_{ji} account for the contribution of each fine bin to the wide bin J_j, making the formula consistent with the model's setup.",
    "question": "Is the condition γ_j = ∑_{i=1}^I c_{ji} π_i correctly derived from the probability that Y belongs to bin J_j, given the model's use of B-splines and the identifiability constraint ∑_{k=1}^K ϕ_k = 0?",
    "question_context": "Bayesian Density Estimation with P-splines and Penalized Likelihood\n\nThe paper discusses Bayesian density estimation using P-splines and penalized likelihood methods. It introduces a model where the probability γ_j that Y belongs to bin J_j is expressed as γ_j = ∑_{i=1}^I c_{ji} π_i, with π_i modeled using B-splines and a roughness penalty. The P-spline penalty is based on rth order differences of the spline coefficients ϕ, leading to a prior distribution ϕ|τ ∼ N(0, τ^{-1}). The penalized log-likelihood is given by l_pen = ∑_j m_j log γ_j - (τ/2) ϕ' D' D ϕ, and the posterior distribution is explored using MCMC methods, including a modified Langevin–Hastings algorithm."
  },
  {
    "qid": "statistic-posneg-768-3-1",
    "gold_answer": "FALSE. The rank-based estimator uses the empirical ranks $R_{i,j}/n$ as estimates of the marginal CDFs $F_j(X_{i,j})$. This nonparametric approach does not require knowledge of the true marginal distributions, as the probability integral transform ensures that $F_j(X_{i,j})$ follows a uniform distribution regardless of the marginal form. The estimator remains consistent for the copula parameter as long as the marginal distributions are continuous.",
    "question": "Does the rank-based estimator $\\hat{\\xi}=\\operatorname*{argmax}_{\\xi\\in\\Xi}\\sum_{i=1}^{n}\\log c_{\\xi}\\left(\\frac{R_{i,1}}{n},\\dots,\\frac{R_{i,d}}{n}\\right)$ require the exact marginal distributions to be known for consistent estimation of the copula parameter?",
    "question_context": "Copula-based Multivariate Density Estimation and Misspecification Bias\n\nThe results for $n=50$ , 100, 200 and another setting, with $f_{\theta}\\sim\\mathcal{N}(0,\\theta^{2})$ and $f_{t}\\sim\\mathcal{N}(t,1)$ , are similar and not shown here to limit the length of the article. <PARAGRAPH_SEPARATOR> # 4.4 Application to multivariate density estimation <PARAGRAPH_SEPARATOR> It is well known that building accurate multivariate parametric models is an uncertain and difficult task. One way of addressing this problem consists of decomposing the target density $f_{0}$ into a copula $c$ and the marginal densities $f_{1},\\ldots,f_{d}$ , that is, <PARAGRAPH_SEPARATOR> $$ f(x_{1},\\dots,x_{d})=c(F_{1}(x_{1}),\\dots,F_{d}(x_{d}))f_{1}(x_{1})\\dots f_{d}(x_{d}) $$ <PARAGRAPH_SEPARATOR> (here the $\\{F_{j}\\}$ stand for the distribution functions). This decomposition, also known as Sklar’s theorem, is unique provided that the $\\{F_{j}\\}$ are continuous; for more details about copulas, see, for example, Genest and Favre (2007) or the books (Joe, 2014; Nelsen, 2006). The copula is assumed to belong to a parametric model $\\{c_{\\xi},\\xi\\in\\Xi\\}$ and the true underlying parameter $\\xi$ is estimated (Genest, Ghoudi, & Rivest, 1995) by <PARAGRAPH_SEPARATOR> $$ \\hat{\\xi}=\\operatorname*{argmax}_{\\xi\\in\\Xi}\\sum_{i=1}^{n}\\log c_{\\xi}\\left(\\frac{R_{i,1}}{n},\\dots,\\frac{R_{i,d}}{n}\\right), $$ <PARAGRAPH_SEPARATOR> where $R_{i,j}$ is the rank of $X_{i,j}$ among $(X_{1,j},\\dots,X_{n,j})$ and $X_{i,j}$ stands for the $j\\mathrm{th}$ coordinate of the ith observation. The marginals are estimated in a separate step. If one of the marginals is misspecified, the estimation of the joint distribution is biased. In the following, a computer experiment illustrates that the LR method can help to reduce this bias by avoiding misspecification. <PARAGRAPH_SEPARATOR> We have generated data sets of size $n=25,50,100,150,\\ldots,500$ using a Gumbel copula with parameter $\\xi=3$ and marginals $f_{1}\\sim E(2)$ , $f_{2}\\sim W(2,1/2)$ where $E(\\lambda)$ is an exponential distribution with mean $1/\\lambda$ and $W(a,b)$ is a Weibull distribution with shape $a>0$ and scale $b>0$ . For each of the simulated data sets, the copula parameter $\\xi$ was estimated as mentioned above and the marginals were estimated under three scenarios: using a kernel density estimator; a maximum likelihood estimate based on the exponential distribution for both margins; and using the semiparametric combination based on the LR method."
  },
  {
    "qid": "statistic-posneg-1235-0-2",
    "gold_answer": "TRUE. The PRIDE model incorporates individual random effects γ ∼ N(0, κ−1I) to account for overdispersion, as described in the context.",
    "question": "The PRIDE model for overdispersed count data includes individual random effects γ for each observation, which are assumed to follow a normal distribution with mean 0 and variance κ−1I. True or False?",
    "question_context": "P-spline Smoothing for Spatial Data with CAR Structure\n\nThe paper discusses the use of P-splines for smoothing spatial data, incorporating conditionally autoregressive (CAR) structures to model both global spatial trends and local regional effects. The models are formulated within a mixed model framework, allowing for flexible smoothing and handling of overdispersion in count data."
  },
  {
    "qid": "statistic-posneg-1176-9-1",
    "gold_answer": "FALSE. The paper explicitly states that estimates obtained under the MAR assumption ($\\hat{\\pmb{\\theta}}^{n,b}$) are biased relative to the complete data estimates ($\\hat{\\pmb{\\theta}}^{n,t}$). The adjustment for the data-collection mechanism ($\\hat{\\pmb{\\theta}}^{n,u}$) is necessary to reduce this bias.",
    "question": "Does the paper conclude that the missing at random (MAR) assumption leads to unbiased parameter estimates?",
    "question_context": "Statistical Inference for Missing Data Adjustment in Parameter Estimation\n\nWe first compare the accuracy of parameter estimates using the parametric method with and without the missing data adjustment. To this end, we assume the standard parametrisation and begin by estimating the parameters of the model using the entire motion $m_{n}$ for $n=1$ , . . . , 41. We call these estimates $\\hat{\\pmb{\\theta}}^{n,t}$ , representing the value of the MLE for the complete data. Next, we remove some locations following the ‘unscheduled gap $^+$ short scheduled gaps’ mechanism, de­ scribed in Section 6.2, and assuming that $\\alpha=0.5$ . We then estimate parameters $\\hat{\\pmb{\\theta}}^{n,b}$ assuming that increments derived from the observed locations are missing at random, which results in esti­ mates that are biased relative to the estimates based on the entire observed trajectory. As the last step we use the form of observed data likelihood given in Proposition $5$ which accounts for the data collection mechanism to obtain estimates of $\\hat{\\pmb{\\theta}}^{n,u}$ . Then, for motion $m_{n}$ , we calculate $D^{n,j}$ , the relative difference between the estimate obtained using the entire motion $m_{n}$ and the estimate ob­ tained using this motion with missing (masked) increments. Mathematically, <PARAGRAPH_SEPARATOR> where $j\\in\\{b,u\\}$ and $i=1,2,3,4$ indicates the component of $\\hat{\\pmb{\\theta}}^{n,j}$ . The histograms of these relative differences are shown in Figure 10. <PARAGRAPH_SEPARATOR> Our results show that without the adjustment for the data-collection mechanism, the parame­ ters often deviate significantly (even in relative terms) from the estimates obtained using the entire motion."
  },
  {
    "qid": "statistic-posneg-1047-0-4",
    "gold_answer": "TRUE. The context defines the relative rate of convergence using this exact expression, where J_d* is a matrix of ones and d* = (1/2)d(d+1). This formulation accounts for cases where elements of vech H_AMISE may be zero, ensuring the rate is defined relative to the general order of H_AMISE (O(n^{-2/(d+4)})).",
    "question": "Is the relative rate of convergence of a bandwidth matrix selector to H_AMISE given by vech(Ĥ - H_AMISE) = O_p(J_d* n^{-α})vech H_AMISE?",
    "question_context": "Multivariate Kernel Density Estimation: Bandwidth Matrix Selection and Convergence Rates\n\nThe choice of smoothing parameters is a problem of fundamental importance in kernel density estimation and related areas. Bandwidth selection for univariate kernel density estimation is the simplest form of this problem, and has been the subject of considerable research. Substantial advances been made leading to the development of bandwidth selectors which combine good practical performance with excellent asymptotic properties. See [4] for an overview. Progress in the case of multivariate case has been much slower. Nonetheless, the selection of bandwidth matrices is an important problem because of the utility of multivariate kernel density estimators in areas such as data visualization, nonparametric discriminant analysis and goodness of fit testing."
  },
  {
    "qid": "statistic-posneg-133-2-2",
    "gold_answer": "FALSE. According to the context, EAIC and EBIC selected the stationary (ST) working correlation structure, not the independence (IN) structure. The independence structure was preferred by QIC and CIC, but the context indicates that this choice is not reliable based on simulation results.",
    "question": "Is it correct that EAIC and EBIC selected the independence working correlation structure as the best fit for the epilepsy data?",
    "question_context": "Empirical Likelihood GEE Models for Epileptic Seizure Data\n\nTo illustrate the use of empirical likelihood GEE models, we apply EAIC and EBIC to data arising from a clinical trial of 59 patients suffering from epileptic seizures. This study was carried out by Leppik et al. (1987). A baseline count of the number of epileptic seizures in an 8-week period prior to randomization was obtained for each patient. Patients were then randomized to receive either a placebo or the drug progabide, in addition to standard therapy. Counts of epileptic seizures in each of four successive 2-week periods were reported. The goal of the study was to answer the question: “Does progabide reduce the seizure rate?” <PARAGRAPH_SEPARATOR> Let $y_{h i j}$ be the count of epileptic seizures at time $j$ $\\mathit{\\Theta}^{\\mathrm{~\\{~\\}~}}(j=0,1,2,3,4)$ for the $i$ th subject in treatment group $h$ ( $h=0$ for placebo and $h=1$ for progabide), and $E(y_{h i j})=\\mu_{h i j}$ . Since response variable is a count, the data can be modeled by a Poisson-like regression model with the log link function $\\log(\\mu_{i j})=X_{i j}^{T}\\beta$ and $\\operatorname{var}(y_{h i j})=\\phi v(\\mu_{h i j})$ . We consider a model for the log seizure rate that includes baseline seizure rate $(y_{h i0})$ , computed as the logarithm of $1/4$ the 8-week baseline count $(\\log(y_{h i0}/4))$ , treatment group, time, and the interaction between treatment and time. This is an ANCOVA model that can be written as <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\log(\\mu_{h i j})=\\beta_{0}+\\alpha_{h}+\\gamma_{j}+\\tau_{h j}+\\beta\\log(y_{h i0}/4),}\\ &{\\qquad h=1,2;i=1,\\ldots,n_{h};j=1,2,3,4,}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\beta_{0}+\\alpha_{h}+\\gamma_{j}+\\tau_{h j}$ is the intercept specific to the combination of treatment $h$ and time $j$ , and contains the effects due to the $h$ th treatment group $(\\alpha_{h})$ , the $j$ th time $(\\gamma_{j})$ , and their interaction $(\\tau_{h j})$ . First, variance function $v_{1}(\\mu_{h i j})=\\mu_{h i j}$ is assumed, and $\\phi$ is used to account for overdispersion present in the data. Under this assumption, we fit GEE models with working correlation structures IN, EX, AR, and ST, respectively, define empirical likelihood ratio with the general correlation structure ST, and calculate EAIC, EBIC, QIC, and CIC with each of the four sets of GEE estimates. As shown in Table 4, EAIC and EBIC choose the stationary structure, while QIC and CIC prefer the independence structure. According to EAIC and EBIC, neither exchangeability nor AR-1 is sufficient to describe the correlation structure. The choice made by QIC and CIC is not reliable as seen from our simulation results. Next, we consider GEE models with the same link function for the mean but a different variance function. Wang and Zhao (2007) showed that the choice of variance function has a fundamental impact on the estimation of $R$ and $\\beta$ , and that $v_{2}(\\mu_{h i j})=\\mu_{h i j}^{1.65}$ can be chosen for the epilepsy data to account for extra overdispersion beyond what can be explained by $\\phi$ . Therefore, the above model fitting and comparison procedure is repeated with this new variance function. The new results, also shown in Table 4, indicate that the choices of these four criteria remain the same. For this particular example, EAIC and EBIC appear to be robust to the specification of the variance function. Note that the sign of QIC changes with the different variance function $v_{2}(\\cdot)$ , because the leading term in (2.6), $-2Q(\\hat{\\beta}(R);I)$ , changes from $\\begin{array}{r}{-2\\phi^{-1}\\sum_{i}\\sum_{j}(y_{i j}\\log(\\mu_{i j})-\\mu_{i j})}\\end{array}$ to $\\begin{array}{r}{2\\phi^{-1}\\sum_{i}\\sum_{j}(0.65^{-1}y_{i j}\\mu_{i j}^{-0.65}+0.35^{-1}\\mu_{i j}^{0.35})}\\end{array}$ . <PARAGRAPH_SEPARATOR> Table 4. Example: epileptic seizures. <PARAGRAPH_SEPARATOR> <html><body><table><tr><td></td><td colspan=\"8\">Workingcorrelation structures</td></tr><tr><td></td><td colspan=\"4\">=(m)Ia</td><td colspan=\"4\">v2(μ)=μ 1.65</td></tr><tr><td></td><td>IN</td><td>EX</td><td>AR</td><td>ST</td><td>IN</td><td>EX</td><td>AR</td><td>ST</td></tr><tr><td>EAIC</td><td>108.74</td><td>20.76</td><td>23.85</td><td>18.00</td><td>99.63</td><td>20.67</td><td>22.29</td><td>18.00</td></tr><tr><td>EBIC</td><td>127.44</td><td>39.46</td><td>42.55</td><td>36.70</td><td>118.33</td><td>39.37</td><td>40.99</td><td>36.70</td></tr><tr><td>QIC</td><td>-1176.47</td><td>-1175.56</td><td>-1168.19</td><td>-1171.91</td><td>2762.21</td><td>3132.87</td><td>3131.66</td><td>3132.40</td></tr><tr><td>CIC</td><td>16.72</td><td>16.75</td><td>16.95</td><td>16.84</td><td>9.79</td><td>11.17</td><td>11.36</td><td>11.28</td></tr></table></body></html>"
  },
  {
    "qid": "statistic-posneg-970-3-1",
    "gold_answer": "FALSE. The condition actually implies the opposite. According to Theorem 3(b), if there exists a $j\\in S$ such that $\\beta_{j}^{0}$ lies within the interval $(0,h(\\lambda))$ for $h(\\lambda)>0$ or $(h(\\lambda),0)$ for $h(\\lambda)<0$, then the probability of sign recovery failure is at least 1/2, i.e., $\\operatorname*{Pr}\\{\\operatorname{sign}({\\hat{\\pmb{\\beta}}})\\neq\\operatorname{sign}({\\pmb{\\beta}}^{0})\\}\\geq1/2$. This means the Lasso estimator is likely to fail in recovering the correct signs under these conditions.",
    "question": "Is it true that the condition $\\beta_{j}^{0}\\in(0,h(\\lambda))$ for $h(\\lambda)>0$ or $\\beta_{j}^{0}\\in(h(\\lambda),0)$ for $h(\\lambda)<0$ implies that the Lasso estimator will always recover the correct sign of the coefficients?",
    "question_context": "Sign Recovery Consistency in Lasso Estimation\n\nNow we turn to the results related to the failure of the sign recovery consistency, providing that either mutual incoherence condition or the constraint on minimum value of the true coefficient is violated. <PARAGRAPH_SEPARATOR> Theorem 3. Suppose that Assumptions 1–3 hold. <PARAGRAPH_SEPARATOR> (a) As opposed to Assumption 4 (the mutual incoherence condition), we assume that, for some $\\nu>0$ , <PARAGRAPH_SEPARATOR> $$ \\operatorname*{max}_{j\\in S^{c}}\\vert e_{j}^{\\top}\\Sigma_{S^{c}S}\\big(\\Sigma_{S S}\\big)^{-1}{\\ s i g n(\\pmb\\beta_{1}^{0})}\\vert=1+\\nu. $$ <PARAGRAPH_SEPARATOR> Then for any $\\lambda>0$ and sufficiently large $n$ , the probability that sign support recovery fails is no less than $1/2$ , i.e., $\\operatorname*{Pr}\\{\\operatorname{sign}(\\hat{\\pmb{\\beta}})\\neq\\operatorname{sign}({\\pmb{\\beta}}^{0})\\}\\geq1/2$ . (b) For each $j\\in S$ , we define the quantity $h(\\lambda)=\\lambda e_{j}^{\\top}(\\Sigma_{S S})^{-1}\\mathsf{s i g n}(\\beta_{1}^{0})$ . If there exists $j\\in S$ with $\\beta_{j}^{0}\\in(0,h(\\lambda))f o r h(\\lambda)>0$ or $\\beta_{j}^{0}\\in(h(\\lambda),0)$ for $h(\\lambda)<0$ , then $\\operatorname*{Pr}\\{\\operatorname{sign}({\\hat{\\pmb{\\beta}}})\\neq\\operatorname{sign}({\\pmb{\\beta}}^{0})\\}\\geq1/2$ . <PARAGRAPH_SEPARATOR> Theorem 3(a) implies that, the mutual incoherence condition in Assumption 4 is essential to ensure the corrected sign recovery for the Lasso partial likelihood estimator. The same result has been established in linear regression models [27,29]. For sign consistency, Theorem 3(b) shows that the value $\\operatorname*{min}_{\\mathbf{\\Phi}_{\\mathbf{\\Lambda}}}|\\beta_{j}^{0}|$ cannot decay to zero faster than the penalty parameter $\\lambda$ . Note that (7) is not a complete complementary event of Assumption 4, where $\\gamma>0$ is used to guarantee the uniqueness of the estimator $\\hat{\\boldsymbol{\\beta}}$ . <PARAGRAPH_SEPARATOR> ![](images/af0564164bfc1355848da3a494817352e193051ae587d9d33f188cd61e9b9e1e.jpg)  Fig. 1. Example 1—relating incoherence measure to sign consistency."
  },
  {
    "qid": "statistic-posneg-1998-0-0",
    "gold_answer": "FALSE. The context explicitly states that all weights for the data in the eventual designs have changed from those in the planned design due to subject drop-out. The formula with A(3/20) and B(-3/20) is part of the eventual design after drop-out has occurred, not the original planned design.",
    "question": "In the context of the paper, does the formula with weights A(3/20) and B(-3/20) represent a planned design before any subject drop-out occurs?",
    "question_context": "Robust Assessment of Two-Treatment Higher-Order Crossover Designs\n\nTable 1 Breakdown numbers for all two-treatment cross-over designs with four periods and four sequences. <PARAGRAPH_SEPARATOR> designs after drop-out occurs to Design 4.1.1 by the loss of one subject in the third period and by the loss of all four subjects in the third period, i.e. the minimal design. <PARAGRAPH_SEPARATOR> It is noticeable that all of the weights for the data that remain in the eventual designs have changed from those in the planned design. Furthermore the sampling variances have increased over the values for the planned design, as one would expect. The displays of the weight vectors for the minimal design are possible only because Design 4.1.1 is perpetually connected."
  },
  {
    "qid": "statistic-posneg-54-0-1",
    "gold_answer": "FALSE. The variance of $u$ explicitly depends on $\\rho$, as seen in the formula for $E(u^{2})$, which includes terms involving $\\rho$ and its functions. The correlation coefficient $\\rho$ influences the distribution and moments of $u$, making the variance dependent on $\\rho$.",
    "question": "Is the variance of the test statistic $u$ independent of the correlation coefficient $\\rho$?",
    "question_context": "Test for Equality of Means with Missing Data and Correlated Variates\n\nLet us consider the usual paired-data situation in which observations are obtained under two conditions from the same sampling unit. The model for each unit's observations will be the bivariate normal distribution with mean vector $(\\mu_{1},\\mu_{\\mathfrak{L}})$ and a covariance matrix with common variance ${\\boldsymbol{\\sigma^{2}}}$ and correlation coefficient $\\rho\\geqslant0$ . From this distribution, $\\pmb{m}$ independent pairs of observations $(x_{1},y_{1}),...,(x_{m},y_{m})$ have been drawn, while in addition, $N-m$ observations $\\pmb{y}_{m+1},...,\\pmb{y}_{N}$ are available on the second variate alone. We wish to use all of these data to test the hypothesis $H_{0}\\colon\\mu_{1}=\\mu_{2}$ against the alternative of unequal means."
  },
  {
    "qid": "statistic-posneg-1803-0-1",
    "gold_answer": "FALSE. While the context discusses the possibility of eliminating preferential sampling by finding relevant explanatory variables (latent variable $U$), it does not claim this is always possible. The text emphasizes that this approach is conditional on the ability to find such variables and verify residual independence, which may not always be feasible.",
    "question": "Does the context suggest that preferential sampling can always be eliminated by finding explanatory variables associated with both $X$ and $S$?",
    "question_context": "Geostatistical Inference Under Preferential Sampling\n\nA natural response to a strongly non-uniform sampling design is to ask whether its spatial pattern could be explained by the pattern of spatial variation in a relevant covariate. Suppose, for illustration, that $S$ is observed without error, that dependence between $X$ and $S$ arises through their shared dependence on a latent variable $U$ and that the joint distribution of $X$ and $S$ is of the form $$[X,S]{}=\\int[X|U][S|U][U]\\mathrm{d}U,$$ so that $X$ and $S$ are conditionally independent given $U.$ . If the values of $U$ were to be observed, we could then legitimately work with the conditional likelihood $[X,S|U]{=}[X|U][S|U]$ and eliminate $X$ by marginalization, exactly as is done implicitly when conventional geostatistical methods are used. In practice, ‘observing’ $U$ means finding explanatory variables which are associated both with $X$ and with $S$ , adjusting for their effects and checking that after this adjustment there is little or no residual dependence between $X$ and $S$ . If so, the analysis could then proceed on the assumption that sampling is no longer preferential. In this context, any of the existing tests for preferential sampling can be applied, albeit approximately, to residuals after fitting a regression model for the mean response."
  },
  {
    "qid": "statistic-posneg-219-0-1",
    "gold_answer": "FALSE. The minimax risk $R(\\mathcal{U};n)$ measures the worst-case performance of the best estimator for a single class $\\mathcal{U}$. For an estimator to be minimax-rate adaptive over multiple classes $\\{\\mathcal{U}_{j},j\\geqslant1\\}$, it must automatically achieve the minimax optimal rate for the specific class containing the true function $u$, without prior knowledge of which class $u$ belongs to. The context discusses the existence of such adaptive estimators but does not claim that the minimax risk alone guarantees adaptivity.",
    "question": "Does the minimax risk $R(\\mathcal{U};n) = \\operatorname*{min}_{\\hat{u}_{n}}\\operatorname*{max}_{u\\in\\mathcal{U},\\sigma\\leqslant\\bar{\\sigma}}E\\|u-\\hat{u}_{n}\\|^{2}$ guarantee that the estimator $\\hat{u}_{n}$ is minimax-rate adaptive over the classes $\\{\\mathcal{U}_{j},j\\geqslant1\\}$?",
    "question_context": "Minimax-Rate Adaptation in Regression Function Estimation\n\nIn light of Theorem 1, the adaptation recipe can be used to derive minimax-rate adaptive estimators over function classes. For simplicity, in this section, we assume $\\sigma^{2}(\\boldsymbol{x})$ is a unknown constant bounded above by $\\bar{\\sigma}^{2}$ . <PARAGRAPH_SEPARATOR> # 4.1. Minimax-Rate Adaptation <PARAGRAPH_SEPARATOR> Let $\\boldsymbol{\\mathcal U}$ be a class of regression functions. Consider the minimax risk for estimating a regression in $\\boldsymbol{\\mathcal U}$ , <PARAGRAPH_SEPARATOR> $$ R(\\mathcal{U};n)=\\operatorname*{min}_{\\hat{u}_{n}}\\operatorname*{max}_{u\\in\\mathcal{U},\\sigma\\leqslant\\bar{\\sigma}}E\\|u-\\hat{u}_{n}\\|^{2}, $$ <PARAGRAPH_SEPARATOR> where ${\\hat{u}}_{n}$ is over all estimators based on $\\textstyle Z^{n}=(X_{i},Y_{i})_{i=1}^{n}$ and the expectation is taken under $u$ and $\\sigma$ . The minimax risk measures how well one can estimate $u$ uniformly over the class $\\boldsymbol{\\mathcal U}$ . Let $\\{\\mathcal{U}_{j},j\\geqslant1\\}$ be a collection of classes of regression functions uniformly bounded between $-A$ and $A$ . Assume the true function $u$ is in (at least) one of the classes, i.e., $u\\in\\bigcup_{j\\geqslant1}\\mathcal{U}_{j}$ . The question we want to address is: Without knowing which class contains $u$ , can we have a single estimator such that it converges automatically at the minimax optimal rate of the class that contains $u?$ If such an estimator exists, we call it a minimax-rate adaptive estimator with respect to the classes $\\{\\mathcal{U}_{j},j\\geqslant1\\}$ . <PARAGRAPH_SEPARATOR> Many results have been obtained on minimax-rate adaptation (or even with the right constant for some cases) for specific functions classes such as Sobolev and Besov under various performance measures, including Efroimovich and Pinsker (1984), Efroimovich (1985), Hardle and Marron (1985), Lepski (1991), Golubev and Nussbaum (1992), Donoho and Johnstone (e.g., 1998), Delyon and Juditsky (1994), Mammen and van de Geer (1997), Tsybakov (1995), Goldenshluger and Nemirovski (1997), Lepski et al. (1997), Devroye and Lugosi (1997), and others. A method is proposed by Juditsky and Nemirovski (1996) to aggregate estimators to adapt to within order $n^{-1/2}$ in risk. General schemes have also been proposed for the construction of adaptive estimators in Barron and Cover (1991) based on minimum description length (MDL) criterion using =-nets. Other adaptation schemes and adaptation bounds by model selection have been developed later including very general penalized contrast criteria in Birg \u0010e and Massart (1996) and Barron et al. (1999) with many interesting applications on adaptive estimation; penalized maximum likelihood or least squares criteria in Yang and Barron (1998) and Yang (1999a); and complexity penalized criteria based on V-C theory (e.g., Lugosi and Nobel, 2000). In the opposite direction, for the case of estimating a regression function at a point, negative results have been obtained by Lepski (1991) and Brown and Low (1996). <PARAGRAPH_SEPARATOR> We give below a general result on minimax-rate adaptation without requiring any special property on the classes over which adaptation is desired. Some regularity conditions will be used for our results. <PARAGRAPH_SEPARATOR> Definition. If the minimax risk sequence satisfies <PARAGRAPH_SEPARATOR> $$ R({\\mathcal{U}};\\lfloor n/2\\rfloor)\\asymp R({\\mathcal{U}};n), $$ <PARAGRAPH_SEPARATOR> we say the minimax risk of the class $\\boldsymbol{\\mathcal U}$ is rate-regular. <PARAGRAPH_SEPARATOR> A rate of convergence is said to be nonparametric if it converges no faster than $n^{\\theta}$ for some $0<\\theta<1$ . <PARAGRAPH_SEPARATOR> The familiar rates of convergence $n^{-\\alpha}(\\log n)^{\\beta}$ for some $0<\\alpha\\leqslant1$ and $\\beta\\in R$ are rate-regular. For a rate-regular risk, together with that $R(\\mathcal{U};i)$ is"
  },
  {
    "qid": "statistic-posneg-1597-0-1",
    "gold_answer": "TRUE. The context explicitly provides the tail integral for a one-dimensional stable subordinator as U(x) = ∫[x,∞) (αβ/z^(α+1)) dz = βx^(-α) and its inverse as U^(-1)(u) = (u/β)^(-1/α). This is derived from the Lévy measure ν(B) = ∫ℝ+ 𝟙_B(z) (αβ/z^(α+1)) dz for the stable subordinator.",
    "question": "The tail integral U(x) of a one-dimensional stable subordinator is given by U(x) = βx^(-α), and its inverse is U^(-1)(u) = (u/β)^(-1/α). Is this formula correct as per the context?",
    "question_context": "Lévy Copulas and Pair-Copula Constructions in Multivariate Dependence Modeling\n\nThe paper discusses the theory of copulas, pair-copulas, Lévy processes, and Lévy copulas. It introduces the concept of pair-copula construction (PCC) for building flexible multivariate copulas using bivariate copulas. For Lévy processes, it defines the tail integral and Lévy copula, which couples marginal tail integrals to the joint tail integral. The paper also presents the Clayton Lévy copula as an example and discusses pair constructions of d-dimensional Lévy copulas."
  },
  {
    "qid": "statistic-posneg-208-2-2",
    "gold_answer": "FALSE. The expansion for b_n is an asymptotic approximation and is not exact for all n. It provides the leading terms of b_n as n → ∞, but for finite n, there may be additional terms or deviations not captured by this expansion. The approximation becomes increasingly accurate as n grows large.",
    "question": "Is the expansion b_n = (2log(n))^{1/2} - (1/2)(loglog(n) + log(4π))/(2log(n))^{1/2} + o(1/(log(n))^{1/2}) exact for all n?",
    "question_context": "Extreme Value Theory for Rowwise Maxima of Non-Identically Distributed Normal Variables\n\nThe paper discusses the asymptotic behavior of the rowwise maximum of an array of independent but non-identically distributed normal variables. The key result is the limiting distribution of the maximum, which is derived under the assumption that the variances of the variables converge to a piecewise continuous function. The norming constant b_n is defined via the equation φ(b_n)/b_n = n^{-1}, and the limiting distribution is shown to be an exponential integral."
  },
  {
    "qid": "statistic-posneg-448-0-0",
    "gold_answer": "FALSE. The correct variance expression is $V(L_{1})=\\sigma^{2}(C^{+}+k I_{p}^{+})$. The statement $V(L_{1})=\\sigma^{3}C^{+}$ is incorrect due to a typographical error (the exponent should be 2, not 3). When $k=0$, the variance simplifies to $V(L_{1})=\\sigma^{2}C^{+}$, which is indeed independent of $\\lambda$.",
    "question": "Is it true that the variance of the linear unbiased estimator $\\mathbf{{L_{1}}}$ reduces to $V(L_{1})=\\sigma^{3}C^{+}$ when $k=0$ regardless of the value of $\\lambda$?",
    "question_context": "Variance Estimation in Linear Unbiased Estimators\n\nby M. ATIQULLAH <PARAGRAPH_SEPARATOR> Immediately before Lemma 2, the matrix $^{c}$ is idempotent and symmetric. <PARAGRAPH_SEPARATOR> A more general and correct statement of the theorem on page 308 is: A sufficient condition that $\\mathbf{{L_{1}}}$ has minimum variance in the class of linear unbiased estimators of $I_{\\mathfrak{p}}^{+\\theta}$ hat <PARAGRAPH_SEPARATOR> $$ V(e)=\\sigma^{2}(I_{\\ast}+k A A^{\\prime}+\\lambda\\Delta\\Delta^{\\prime}). $$ <PARAGRAPH_SEPARATOR> # Here <PARAGRAPH_SEPARATOR> $$ \\Delta^{\\prime}A=0,\\qquad\\Delta^{\\prime}\\Delta=I_{n-r}\\quad(t\\leqslant n-r),\\qquadV(L_{1})=\\sigma^{\\P}(C^{+}+k I_{p}^{+}); $$ <PARAGRAPH_SEPARATOR> thi8 gives $V(L_{1})=\\sigma^{3}C^{+}$ when $\\pmb{k}=0$ regardles8 of the value8 of $\\lambda$ <PARAGRAPH_SEPARATOR> Thanks are due to C. L. Mallows for drawing my attontion to the mistakes on pages 305 and 308."
  },
  {
    "qid": "statistic-posneg-1437-0-0",
    "gold_answer": "TRUE. The ANOVA F-test's exact F-distribution under $H_{0}$ requires the Huynh and Feldt condition ($\\varepsilon^{*}=1$) on $\\mathbf{\\boldsymbol{\\Sigma}}$, which ensures sphericity (equal variances of differences between treatment levels). The condition is necessary because the F-test's denominator estimates a common error variance under this structural assumption. Without it, the test statistic's distribution deviates from the nominal F-distribution, requiring adjustments like Greenhouse-Geisser's $\\varepsilon^{*}$.",
    "question": "The ANOVA F-test statistic $F=\\frac{n\\sum_{j=1}^{p}(\\bar{Y}_{\\cdot j}-\\bar{Y}_{\\cdot})^{2}}{\\frac{1}{n-1}\\sum_{i=1}^{n}\\sum_{j=1}^{p}(Y_{i j}-\\bar{Y}_{i\\cdot}-\\bar{Y}_{\\cdot j}+\\bar{Y}_{\\cdot})^{2}}$ has an exact F-distribution with $p-1$ and $(n-1)(p-1)$ degrees of freedom under the null hypothesis $H_{0}:\\tau_{1}=\\cdot\\cdot\\cdot=\\tau_{p}$ only if the variance-covariance matrix $\\mathbf{\\boldsymbol{\\Sigma}}$ satisfies the Huynh and Feldt condition. True or False?",
    "question_context": "Testing Treatment Effects in Randomized Complete Block Designs\n\nRandomized complete block designs with one observation per treatment are settings in which each of the $p$ treatments is observed once on every experimental unit. Let $\\mathbf{Y}_{i}=(Y_{i1},\\ldots,Y_{i p})^{\\prime}$ denote the $p$ -dimensional vector observed for the $i$ th experimental unit where $Y_{i j}$ denotes the observed response of the $i$ th experimental unit to treatment $j$. A possible model for the data is ${\\bf Y}_{i}=\\beta_{i}{\\bf1}_{p}+\\uptau+\\upepsilon_{i},\\quad i=1,\\ldots,n,$ where ${\\bf1}_{p}$ is the $p\\times1$ vector of 1’s, ${\\pmb{\\uptau}}=(\\tau_{1},\\dots,\\tau_{p})^{\\prime}$ is the vector of fixed treatment effects, $\\beta_{i}$ represents the random effect of the ith subject, and $\\mathbf{\\epsilon}_{\\epsilon_{i}}$ is the vector of random error of the ith subject. The test of equal treatment effects, $H_{0}:\\tau_{1}=\\cdot\\cdot\\cdot=\\tau_{p}\\quad\\mathrm{versus}\\quad H_{\\mathrm{a}}:\\tau_{i}\\neq\\tau_{j}{\\mathrm{for~some}}i\\neq j,$ is of interest. Two classical parametric approaches are often recommended. The first procedure, due to Hsu [16], is performed by applying Hotelling’s $T^{2}$ to the $(p-1)$ -variate random vectors, $\\mathbf{X}_{i}$’s, obtained from the $\\mathbf{Y}_{i}$’s via $\\mathbf{X}_{i}=(X_{1},\\ldots,X_{p-1})^{\\prime}=(Y_{1}-Y_{p},\\ldots,Y_{p-1}-Y_{p})^{\\prime}\\qquadi=1,\\ldots,n.$ The second procedure is the classical analysis of variance (ANOVA) $F$ test. The original observations are used to form the test statistic $F=\\frac{n\\sum_{j=1}^{p}(\\bar{Y}_{\\cdot j}-\\bar{Y}_{\\cdot})^{2}}{\\frac{1}{n-1}\\sum_{i=1}^{n}\\sum_{j=1}^{p}(Y_{i j}-\\bar{Y}_{i\\cdot}-\\bar{Y}_{\\cdot j}+\\bar{Y}_{\\cdot})^{2}},$ where $Y_{i j}$ denotes the jth element of the vector $\\begin{array}{r}{\\mathbf{Y}_{i},\\bar{Y}_{\\cdot j}=(1/n)\\sum_{i=1}^{n}Y_{i j},\\bar{Y}_{i\\cdot}=(1/p)\\sum_{j=1}^{p}}\\end{array}$ $Y_{i j}$ , and $\\bar{Y}_{\\cdot\\cdot}=(1/n p)\\textstyle\\sum_{i=1}^{n}\\sum_{\\cdot}^{p}{}_{j=1}Y_{i j}$ . When the $\\mathbf{Y}_{i}$’s are (multivariate)-normally distributed, the null distribution of the ANOVA test is an exact $F$ -distribution with $p-1$ and $(n-1)(p-1)$ degrees of freedom if and only if the population variance–covariance matrix of $\\mathbf{Y}$ , say $\\mathbf{\\boldsymbol{\\Sigma}}=(\\sigma_{t t^{\\prime}})$ , satisfies the Huynh and Feldt condition."
  },
  {
    "qid": "statistic-posneg-599-0-0",
    "gold_answer": "FALSE. The recurrence relation for \\( s_{2n} \\) should account for the square of the new term \\( t_{1n} = 1/n \\) added to the triangular scheme. The correct recurrence relation should be \\( s_{2n} = s_{2,n-1} + (2/n)s_{1,n-1} + (1/n)^2 \\), as the square of the new term \\( t_{1n} \\) must be included in the sum of squares.",
    "question": "Is the recurrence relation for the second moment \\( s_{2n} = s_{2,n-1} + (2/n)s_{1,n-1} + 1/n \\) correctly derived from the triangular scheme of expected order statistics \\( t_{rn} \\)?",
    "question_context": "Properties of Exponential Ordered Scores and Their Moments\n\nThe present paper investigates some tests based on exponential scores. Of course, in practice, formal tests of significance are likely to be only part of the whole analysis. An important thing, for instance, is likely to be graphical analysis of the survivor curves of the separatesetsof data. <PARAGRAPH_SEPARATOR> # 2. THE EXPONENTIAL SCORES <PARAGRAPH_SEPARATOR> In this Section we examine some properties of the finite population consisting of the expected order statistics for a random sample of size $n$ from the distribution $e^{-y}$ Denoteby $t_{r n}$ the expected value of the rth observation in increasing order of magnitude. It is well known that <PARAGRAPH_SEPARATOR> $$ t_{r n}=1/n+...+1/(n-r+1)\\quad(r=1,...,n), $$ <PARAGRAPH_SEPARATOR> so that in particular <PARAGRAPH_SEPARATOR> $$ \\scriptstyle t_{1n}=1/n,\\scriptstyle\\left.\\begin{array}{l}{{\\scriptstyle1}}\\\\ {{\\scriptstyle t_{2n}=1/n+1/(n-1),}}\\\\ {{\\scriptstyle\\vdots}}\\\\ {{\\scriptstyle t_{n n}=1/n+1/(n-1)+\\ldots+1.}}\\end{array}\\right\\} $$ <PARAGRAPH_SEPARATOR> For $\\pmb{n}$ not too large, these are easily calculated from tables of reciprocals. <PARAGRAPH_SEPARATOR> The moments of the finite population (2) are found by first calculating the power sums <PARAGRAPH_SEPARATOR> $$ s_{i n}=\\sum_{r=1}^{n}t_{r n}^{i}. $$ <PARAGRAPH_SEPARATOR> Now the triangular scheme (2) for sample size $_{n}$ is derived from that for sample size $n{-}1$ by adding the first column of $1/n^{\\mathfrak{s}}$ . There follow easily recurrence relations such as the following: <PARAGRAPH_SEPARATOR> $$ \\left.\\begin{array}{l}{{s_{1n}=s_{1,n-1}+1,\\quad s_{2n}=s_{2,n-1}+\\left(2/n\\right)s_{1,n-1}+1/n,}}\\\\ {{s_{3n}=s_{3,n-1}+\\left(3/n\\right)s_{2,n-1}+\\left(3/n^{2}\\right)s_{1,n-1}+1/n^{2}.}}\\end{array}\\right\\} $$ <PARAGRAPH_SEPARATOR> These can be solved subject to the initial condition $s_{i1}=1$ . In particular, <PARAGRAPH_SEPARATOR> $$ s_{1n}=n,s_{2n}=2n-\\sum_{r=1}^{n}(1/r)=2n-t_{n n}. $$ <PARAGRAPH_SEPARATOR> After some calculation, we have, for Fisher's $K$ parameters  for  the  finite population, <PARAGRAPH_SEPARATOR> $$ K_{1n}=1,K_{2n}=\\frac{n-t_{n n}}{n-1}=1-\\frac{\\log n+\\gamma-1}{n}+O\\biggl(\\frac{\\log n}{n^{2}}\\biggr), $$ <PARAGRAPH_SEPARATOR> where $\\gamma=0{\\cdot}5772$ is Euler's constant. <PARAGRAPH_SEPARATOR> The leading term in the asymptotic expansion for each cumulant is the corresponding cumulant of the unit exponential distribution. <PARAGRAPH_SEPARATOR> Table 1 gives some values of $K_{2n}$ and of the approximation $1-(\\log n+\\gamma-1)/n$"
  },
  {
    "qid": "statistic-posneg-2084-0-2",
    "gold_answer": "TRUE. The modified statistic Ã_n = Q_n / B_0 is designed to improve performance in high dimensions. Using Box's approximation, Ã_n is shown to closely follow a χ² distribution scaled by its degrees of freedom (χ²_f̃ / f̃), especially as n → ∞. The moments of Ã_n align with those of the target distribution, ensuring dimensional stability.",
    "question": "Is the modified ANOVA-type statistic (Ã_n) defined as Q_n / B_0, where Q_n is a quadratic form and B_0 is an unbiased estimator of tr(TΣ), and does it approximate a χ² distribution?",
    "question_context": "High-Dimensional Repeated Measures Analysis: ANOVA-Type Statistic and Approximations\n\nThe paper discusses the analysis of high-dimensional repeated measures data, focusing on the ANOVA-type statistic (A_n) and its modifications for cases where the dimension (d) exceeds the sample size (n). The key formulas include the definition of A_n, its approximation using Box's method, and the derivation of its moments. The context also covers the performance of different test statistics (Wald-type, Hotelling's T^2, and ANOVA-type) under various covariance structures and dimensionality conditions."
  },
  {
    "qid": "statistic-posneg-1114-0-1",
    "gold_answer": "FALSE. The context mentions that the EM-type estimator has a small positive bias, while the maximizing $\\pi$-type estimator has almost no bias. Therefore, the EM-type estimator is not unbiased, though the bias is small.",
    "question": "The EM-type estimator for the mixing proportion $\\pi$ in the model $m(x)=\\pi\\sigma^{2}\\chi_{p}^{2}(x)+(1-\\pi)f(x)$ is unbiased, as indicated by the simulation results in Table 6. Is this statement correct?",
    "question_context": "Mixing Proportion Estimation in Semi-Parametric Mixture Models\n\nWe are interested in the performance of our estimators for the case when the mixed density $f$ is multimodal. We generated 100 observations from $N(0,1)$ and 25 observations each from $N(2,0.1^{2})$ , $N(3,0.1^{2})$ , $N(4,0.1^{2})$ , and $N(5,0.1^{2})$ . As we can see from Table 6, the EM-type estimator has a small positive bias and the maximizing $\\pi$ -type estimator has almost no bias. <PARAGRAPH_SEPARATOR> # 5. Multi-dimensional cases <PARAGRAPH_SEPARATOR> In multi-dimensional cases, the density estimation requires many more observations to estimate a density as well as in one-dimensional cases. We propose to use the squared Euclidean distance to the mean and hence reduce the problem to the one-dimensional case. Since we know the mean, the squared Euclidean distance to the mean of the random variables from $N_{p}(\\underline{{0}},\\sigma^{2}I_{p})$ follows a $\\sigma^{2}\\chi_{p}^{2}$ distribution. Therefore, the mixture model is <PARAGRAPH_SEPARATOR> $$ m(x)=\\pi\\sigma^{2}\\chi_{p}^{2}(x)+(1-\\pi)f(x). $$ <PARAGRAPH_SEPARATOR> The estimators described in Sections 3.1 and 3.2 can be adopted to this case by changing the normal density to the $\\chi^{2}$ density. We will show the performance of our estimators with multi-dimensional data. <PARAGRAPH_SEPARATOR> Example 6. Multi-dimensional simulations. <PARAGRAPH_SEPARATOR> We generated observations from multivariate normal distributions. The mixed distributions are $N_{p}(\\underline{{0}},I_{p})$ and $N_{p}(\\underline{{2}},I_{p})$ for $p=2$ , 3, 4 and 5 and the true mixing proportion $\\pi$ is 0.5. The number of observations for each group is 200 (Table 7)."
  },
  {
    "qid": "statistic-posneg-223-0-0",
    "gold_answer": "TRUE. The paper explicitly states that in the case of two samples of equal size, the test is well behaved when there are variations of variance-covariance matrices if the samples are very large. This robustness is attributed to the asymptotic properties of the test statistic under large sample sizes, which mitigate the effects of unequal variance-covariance matrices.",
    "question": "Is it true that the $\\pmb{T_{0}^{2}}$ test in MANOVA remains robust to violations of the equality of variance-covariance matrices assumption when sample sizes are equal and very large?",
    "question_context": "Robustness of the T02 Test in MANOVA with Unequal Variance-Covariance Matrices\n\nCurrent interest in multivariate analysis in biometrics and other areas of applied statistics must inevitably lead to a scrutiny of the robustness of the procedure used. One widely used test isthe $\\pmb{T_{0}^{2}}$ test in multivariate analysis of variance (MANOVA). Among the assumptions underlying MANOVA are: (1) multivariate normal distributions, (2) equality of variancecovariance matrices, and (3) serially uncorrelated observations. The purpose of the present paper is to explore, in a preliminary fashion, the consequence to this test of violation of the assumption of equality of variance-covariance matrices. It is shown that in the case of two samples of equal size, the test is well behaved when there are variations of variancecovariance matrices if the samples are very large; further that in the case of two samples of nearly equal size or $\\pmb{k}$ samples of equal size, the test is not affected seriously by moderate inequality of variance-covariance matrices as long as samples are very large. However, if some or all of $\\pmb{k}$ samples are of unequal size, quite a large effect occurs on the level of significance and the power of the test from even moderate variations. It is to be noted that among numerous works in univariate analysis on the effect of heteroscedasticity of variances (see, e.g. David $\\&$ Johnson, 1951; Gronow, 1951; Horsnell, 1953; Box, 1954; Schef6, 1959, Pp. 334-58), the present investigation is related to and has extended the work by Scheffé and Box as far as the effect on the level of significance of the test is concerned."
  },
  {
    "qid": "statistic-posneg-367-0-1",
    "gold_answer": "FALSE. The MHD estimator $\\hat{m}_{n}$ would correctly identify the number of components in the mixture from which the data are generated, even if the data are contaminated. This means it would identify 3 components in a gross-error contaminated model instead of the postulated 2 components, making the gross-error contamination approach inappropriate for assessing robustness in this context.",
    "question": "Is the MHD estimator $\\hat{m}_{n}$ robust against gross-error contamination models in the context of mixture complexity estimation?",
    "question_context": "Robustness of Mixture Complexity Estimation under Model Misspecification\n\nIn conclusion, when the postulated model is correct, our MHD-based method is competitive with the LRT method of KX (1999) in that it is very successful in correctly determining the mixture complexity if the models are well separated and sample sizes are large enough. <PARAGRAPH_SEPARATOR> # 5.2. Robustness of $\\hat{m}_{n}$ under model misspecification <PARAGRAPH_SEPARATOR> Here, we describe an approach to assess the robustness of $\\hat{m}_{n}$ in terms of its ability to correctly identify the true mixture complexity, when the postulated Poisson mixture model is incorrect. Generally, one examines the robustness of MHD estimators against $100\\%$ gross-error contaminated mixture models and using $\\alpha$ -influence functions defined in terms of Hellinger functionals (Beran, 1977). KX (1998, see section 6.3) postulated a 2-component Poisson mixture model, $f_{\\pmb{\\theta}_{2}}(x)=\\pi f\\left(x|\\lambda_{1}\\right)+\\left(1-\\pi\\right)f\\left(x|\\lambda_{2}\\right)$ , where $f$ denotes a Poisson p.m.f. and $\\pmb{\\theta}_{2}=(\\pi,\\lambda_{1},\\lambda_{2})$ , and showed via simulations that the performance of their MHD estimator of $\\pmb{\\theta}_{2}$ remains unaffected even when the data are generated from a $100\\%$ gross-error contaminated Poisson mixture model defined by <PARAGRAPH_SEPARATOR> $$ f_{\\theta_{2},\\varepsilon,\\lambda_{3}}(x)=\\left(1-\\varepsilon\\right)f_{\\theta_{2}}(x)+\\varepsilon f\\left(x\\left|\\lambda_{3}\\right.\\right), $$ <PARAGRAPH_SEPARATOR> where \u000b is the proportion of contamination and the (contaminating) value $\\lambda_{3}$ is large compared to those of $\\lambda_{1}$ and $\\lambda_{2}$ . <PARAGRAPH_SEPARATOR> While it is a common practice to study robustness of MHD estimators against gross-error contamination models, such an approach would be inappropriate in our context because, by virtue of its consistency, our estimator $\\hat{m}_{n}$ would (correctly) identify (for sufficiently large $n$ ) the number of components in the mixture from which data are generated, which in the above setup would be 3 instead of 2. Notice also that there is no Hellinger function representation of our estimator of mixture complexity which would facilitate the study of $\\alpha$ -influence functions. <PARAGRAPH_SEPARATOR> In view of these, we assess the robustness of $\\hat{m}_{n}$ when the postulated model is a Poisson mixture but the data are generated from a mixture with negative binomial components. More precisely, we perform simulation studies when the postulated model is a 2-component Poisson mixture $f_{\\pmb{\\theta}_{2}}(x)$ defined above with $\\lambda_{1}$ and $\\lambda_{2}$ as its component means, but the data are generated from a 2-component negative binomial mixture given by <PARAGRAPH_SEPARATOR> $$ f(x)=\\pi f_{1}(x)+(1-\\pi)f_{2}(x), $$ <PARAGRAPH_SEPARATOR> where, for $i=1$ , 2, $f_{i}(x){=}\\varGamma(r+x)/(\\varGamma(r)x!)p_{i}^{r}(1-p_{i})^{x}$ , $x=0$ , $1,\\ldots;r>0$ . Let $f_{1}$ and $f_{2}$ be the p.m.f.s associated with random variables, say, $X_{1}$ and $X_{2}$ , respectively. Then, it is easily seen that the component means and the variances are $\\begin{array}{r}{E\\left(X_{i}\\right)=r\\left(1-p_{i}\\right)/p_{i}}\\end{array}$ and V ar $\\left(X_{i}\\right)=r\\left(1-p_{i}\\right)/p_{i}^{2}$ , for $i=1$ , 2. It is also well known that if, for each $i=1$ , 2, $r\\rightarrow\\infty$ and $p_{i}\\to1$ such that $r\\left(1-p_{i}\\right)\\rightarrow\\lambda_{i}$ , then $E\\left(X_{i}\\right)\\rightarrow\\lambda_{i}$ and $V a r\\left(X_{i}\\right)\\rightarrow\\lambda_{i}$ , which agrees with the component Poisson means and variances. In fact, under these conditions, it can be shown that the negative binomial family of distributions includes the Poisson distribution as a limiting case. <PARAGRAPH_SEPARATOR> The hallmark of the postulated Poisson components is that the mean is equal to the variance. However, the component variance of a negative binomial mixture from which the count data are generated may be more than can be expected on the basis of the postulated model. This phenomenon, known as overdispersion, in the negative binomial components in (5.1) may also be moderate to more extreme depending on the values of $r$ and $p_{i}$ , for $i=1$ , 2. <PARAGRAPH_SEPARATOR> In our simulation studies, we consider two scenarios. In both the scenarios, we set the component means of the sampling model to be the same as that of the postulated model, that is, $r\\left(1-p_{i}\\right)/p_{i}=\\lambda_{i}$ , for $i=1,2$ . In the first scenario, we set $r=10$ and $\\lambda_{1}=1$ (this sets $E\\left(X_{1}\\right)=1$ and V ar $(X_{1})=1.1\\AA$ ), but vary the values of $\\lambda_{2}$ . More specifically, for illustration, we set $\\lambda_{2}=2,5,7$ , with corresponding values of V $x r\\left(X_{2}\\right)=2.4$ , 7.5, 11.9. Note that the $V a r(X_{2})$ values are progressively much larger compared to the corresponding values of $E\\left(X_{2}\\right)\\left(=\\lambda_{2}\\right)$ , thus creating a moderate to more extreme overdispersion in the second negative binomial component."
  },
  {
    "qid": "statistic-posneg-1033-0-0",
    "gold_answer": "FALSE. The conformal p-value $P_i$ is not uniformly distributed on $[n+1]/(n+1)$ for every $i\\in\\mathcal{N}$. The formula for $P_i$ involves a sum of indicator functions, which means it can take on discrete values. The uniform distribution claim is incorrect because $P_i$ is a discrete random variable with a finite number of possible outcomes, not a continuous uniform distribution.",
    "question": "Is the statement 'The conformal p-value $P_i$ is uniformly distributed on $[n+1]/(n+1)$ for every $i\\in\\mathcal{N}$' true given the formula for $P_i$ and the assumptions in the context?",
    "question_context": "Conformal p-values and Invariant Correlation in Hypothesis Testing\n\nProposition 10 implies that $\\Theta_{d}\\subset\\mathcal{T}_{d}$ and that $\\Theta_{d}\\subset\\mathcal{K}_{d}(g)$ for wide varieties of $g$ . For instance, $\\theta_{d}$ is a subset of the compatibility sets of Blomqvist’s beta and Spearman’s rho, studied by [9,17], respectively. Since the compatibility set of Blomqvist’s beta is equal to that of Kendall’s tau [14] and is smaller than that of Gini’s gamma [11], our result implies that $\\theta_{d}$ is contained in all of these compatibility sets. The connection of the clique partition polytope to $\\mathcal{T}_{d}$ is given in Proposition 22 of [7]. This result, together with Theorem 4 and Proposition 10, leads to the following relationship between $\\theta_{d}$ and $\\mathcal{T}_{d}\\colon\\theta_{d}=\\mathcal{T}_{d}$ for $d\\leqslant4$ , and $\\Theta_{d}\\subsetneq\\mathcal{T}_{d}$ for $d\\geqslant5$ . <PARAGRAPH_SEPARATOR> # A.2. Conformal $p$ -values <PARAGRAPH_SEPARATOR> Conformal $p$ -value, studied by [1] in the context of outlier detection, is an interesting example of invariant correlation. Let $S_{1},\\ldots,S_{n}$ be scores of the null training sample (computed from some score function on the data), where $n$ is a fixed positive integer. Scores of the test sample are denoted by $S_{n+i}$ , $i\\in[d],$ , and the corresponding conformal $p$ -value is given by <PARAGRAPH_SEPARATOR> $$ P_{i}=\\frac{1}{n+1}+\\frac{1}{n+1}\\sum_{k=1}^{n}\\mathbb{1}_{\\{S_{k}\\leqslant S_{n+i}\\}}. $$ <PARAGRAPH_SEPARATOR> Assume that $\\{S_{i}:i\\in[n+d]\\}$ is a sequence of independent random variables and $S_{k}\\sim F_{0}$ , $k\\in[n]$ . The above p-values are testing the sequence of null hypotheses $\\mathrm{H}_{0,i}:S_{n+i}\\sim F_{0}$ , $i\\in[d]$ . <PARAGRAPH_SEPARATOR> Proposition 11. Let ${\\mathcal{N}}\\subseteq[d]$ be any set of null indices, that is, $\\mathrm{H}_{0,i}$ is true for $i\\in\\mathcal{N}.$ Then, <PARAGRAPH_SEPARATOR> $$ \\mathbb{P}\\left(P_{i}=\\frac{j_{i}}{n+1},i\\in\\mathcal{K}\\right)=\\frac{N_{1}!\\cdots N_{n+1}!}{(n+m)(n+m-1)\\cdots(n+1)},\\quad j_{i}\\in[n+1], $$ <PARAGRAPH_SEPARATOR> where $m=|\\mathcal{N}|$ and $N_{j}=|\\{i\\in{\\mathcal{N}}:j_{i}=j\\}|,j\\in[n+1],$ . In particular, $P_{i}$ is uniformly distributed on $[n+1]/(n+1)$ for every $i\\in\\mathcal{N}.$ Moreover, $(P_{i})_{i\\in\\mathcal{N}}$ has the invariant corelationmatix $(r_{i j})_{m\\times m}$ such that $r_{i j}=1/(n+2)$ for every $i,j\\in[m],~i\\neq j $ . <PARAGRAPH_SEPARATOR> We can take $\\mathcal{N}$ to be the set of all null indices in Proposition 11. The fact that $(P_{i})_{i\\in\\mathcal{N}}$ has an invariant correlation matrix has been shown in Lemma 2.1 of [1], which also contains the joint distribution of $(P_{i},P_{j})$ for $i,j\\in\\mathcal{N}$ . Proposition 11 further gives the joint distribution of $(P_{i})_{i\\in\\mathcal{N}}$ . On the other hand, the full vector of $\\boldsymbol{\\mathbf{p}}$ -values, $(P_{i})_{i\\in[d]}$ does not have an invariant correlation matrix in general. Note that the marginal distributions of $(P_{i})_{i\\in[d]}$ are generally different for null and non-null components, and there is some positive correlation between them by design in (A.1). Hence, our results in Section 3 explain that invariant correlation for $(P_{i})_{i\\in[d]}$ is not possible. Nevertheless, the vector of conformal p-values has PRDS on $\\mathcal{N}$ as shown by Theorem 2.4 of [1]. <PARAGRAPH_SEPARATOR> For $m=2$ , the vector of null conformal p-values follows the model (22) with $k=n+2$ , $g(x)=\\lfloor(n+1)x\\rfloor/(n+1)$ and $\\mathbf{Z}_{1},\\mathbf{Z}_{2}$ being iid multinomials with the number of trials 1 and uniform event probabilities on $[k]$ .It is left open whether the vector of null conformal p-values can be written of the form (22) for $m\\geqslant3$ ."
  },
  {
    "qid": "statistic-posneg-1000-3-1",
    "gold_answer": "FALSE. The context explicitly states that while the conditional variances $\\lambda_{i}$ are usually equal for all interior pixels in a second-order neighbourhood, they might be increased on the edges and corners of the array due to less available information there. This adjustment accounts for boundary effects in the image processing context.",
    "question": "The conditional variance $\\lambda_{i}$ is always the same for all pixels in the auto-normal model. Is this statement true?",
    "question_context": "Auto-Normal Models in Image Processing\n\nThe simplest description of continuous intensities, perhaps after suitable transformation, is that $\\left\\{p(\bar{x})\right\\}$ is a Gaussian M.r.f. with zero mean. Thus (Besag 1974, 1975), $X_{i}$ hasconditional density\n\n$$\np_{i}(x_{i}|x_{\\partial i})\\propto\\exp\\Biggl\\{-\\frac{1}{2\\lambda_{i}}(x_{i}-\\sum_{j\\neq i}\\beta_{i j}x_{j})^{2}\\Biggr\\},\n$$\n\nwhere (i) $\\beta_{i j}=0$ unless pixels $i$ and $j$ are neighbours, (i) $\\beta_{i j}\\lambda_{j}=\\beta_{j i}\\lambda_{i}$ and (ii) an extra regularity condition below is satisfied. For second-order neighbourhood, the conditional variances $\\lambda_{i}$ would usually be taken as equal for all interior pixels but might be increased on the edges of the array and particularly at the corners, because less information is available there. It follows from (13) that\n\n$$\np(x)\\propto\\exp(-{\\frac{1}{2}}x^{\\mathsf{T}}Q x),\n$$\n\nwhere $Q=\\Lambda^{-1}B,\\Lambda$ is the $n\\times n$ diagonal matrix with diagonal entries $\\lambda_{i}$ and $\\pmb{B}$ is the $n\\times n$ matrix with unit diagonal entries and off-diagonal $(i,j)$ element $-\\beta_{i j}$ . Condition (i) ensures that the precision matrix $Q$ is symmetric, while (i) is a statement that $Q$ must be positive definite."
  },
  {
    "qid": "statistic-posneg-1481-0-3",
    "gold_answer": "FALSE. The aliasing effect occurs when a continuous process is sampled on a lattice, but it is also relevant for any discrete sampling of a continuous process, not just lattices. The effect arises because high-frequency components in the original process become indistinguishable from lower frequencies in the sampled process, leading to aliases. This is a general phenomenon in signal processing and spectral analysis, not limited to lattice data.",
    "question": "Is the aliasing effect in spectral analysis only relevant for continuous processes observed on a lattice?",
    "question_context": "Comparison of Gaussian Geostatistical Models and Gaussian Markov Random Fields\n\nThe paper compares Gaussian Geostatistical Models (GGMs) and Gaussian Markov Random Fields (GMRFs) in terms of their spatial modeling approaches, spectral density approximation methods, and prediction errors. GGMs model spatial associations through parametric covariance functions, while GMRFs specify spatial associations through conditional distributions based on neighborhood structures. The paper introduces spectral methods to measure discrepancy between models and compares them with covariance function approximation methods."
  },
  {
    "qid": "statistic-posneg-1887-1-3",
    "gold_answer": "FALSE. The variance of $d_{x}(a,b)$ is given by $E|d_{x}(a,b)|^{2}=a\\int_{\\mathbb{R}}|{\\widehat{\\psi}}(a\\lambda)|^{2}f(\\lambda)d\\lambda:=J(a)$, which does not depend on $b$. The location parameter $b$ only affects the phase of the integrand, not the variance.",
    "question": "Does the filter transform $d_{x}(a,b)=\\sqrt{a}\\int_{\\mathbb{R}}\\mathrm{e}^{i b\\lambda}\\overline{{\\widehat{\\psi}(a\\lambda)}}\\sqrt{f(\\lambda)}d W(\\lambda)$ depend on the location parameter $b$ for its variance?",
    "question_context": "Seasonal Long-Memory Process and Filter Transforms\n\nThis section introduces classes of stochastic processes and their filter transforms that are studied in this paper. We consider a measurable mean-square continuous stationary zero-mean Gaussian stochastic process $X(t),t\\in\\mathbb{R}$ , defined on a probability space $(\\Omega,{\\cal{F}},P)$ , with the covariance function $B(r):=\\mathrm{Cov}(X(t),X(t^{\\prime}))=\\int_{\\mathbb{R}}\\mathrm{e}^{i u(t-t^{\\prime})}F(d u),\\quad t,t^{\\prime}\\in\\mathbb{R},$ where $r=t-t^{\\prime}$ and $F(\\cdot)$ is a nonnegative finite measure on $\\mathbb{R}$ . The process $X(t),t\\in\\mathbb{R}$ , with absolutely continuous spectrum has the following isonormal spectral representation: $X(t)=\\int_{\\mathbb{R}}\\mathrm{e}^{i t\\lambda}{\\sqrt{f(\\lambda)}}d W(\\lambda),$ where $W(\\cdot)$ is a complex-valued Gaussian orthogonal random measure on $\\mathbb{R}$."
  },
  {
    "qid": "statistic-posneg-307-1-0",
    "gold_answer": "TRUE. The context explicitly states that $Y\\sim\\mathbf{CG}(\\mu,\\sigma^{2},c)$ can be written as $Y=a Y^{*}+b$, where $Y^{*}\\sim\\mathbf{CG}(0,1,c^{*})$. This demonstrates that a linear transformation (scaling by $a$ and shifting by $b$) of a CG random variable $Y^{*}$ yields another CG random variable $Y$ with modified parameters. The formula $c^{*}=a^{-1}(c-b)$ further confirms the relationship between the parameters of the original and transformed distributions, supporting the claim.",
    "question": "Is the statement 'A linear transformation of a CG random variable results in another CG random variable' supported by the given formulas and context?",
    "question_context": "Box-Cox Transformation for Approximating CG Distribution in Traffic Volume Analysis\n\nUnder model (1)–(2), the observations $\\left\\{Y_{t}\\right\\}$ should have an approximate CG distribution. (Unless the seasonal component is constant valued, this is only approximate.) Our first step is to find a transformation of the original data which ensures that it does indeed have (approximately) a CG distribution. To make the problem simpler, we restrict attention to the class of possible Box–Cox transformations defined by for values of $\\lambda$ in the interval [0, 2]. It is not difficult to verify that a linear transformation of a CG random variable is also a CG random variable, and, in fact, $Y\\sim\\mathbf{CG}(\\mu,\\sigma^{2},c)$ can be written as $Y=a Y^{*}+b$, where $Y^{*}\\sim\\mathbf{CG}(0,1,c^{*})$ with $c^{*}=a^{-1}(c-b)$. Thus the $\\alpha$ -quantile of $Y$ is simply $a$ times the $\\alpha$ -quantile of $Y^{*}$ , plus $b$ , and a plot of the quantiles of $Y^{*}$ versus the quantiles of $Y$ would form a straight line. In the light of this observation, the following measure of the deviation of the empirical distribution of a time series $\\{y_{1},\\ldots,y_{n}\\}$ from a CG distribution is proposed. Let $m_{y}=\\mathrm{min}_{i=1,\\dots,n}(y_{i})$ , and let $\\{y_{i}^{\\prime},i=1,\\ldots,n^{\\prime}\\}$ be the subseries of $\\{y_{j}\\}$ containing exactly those elements which are not equal to $m_{y}$ . (Hence $n^{\\prime}\\leqslant n.$ ) Let $p_{1}=(n-n^{\\prime})/n$ and choose $c^{*}=\\Phi^{-1}(p_{1})$ . Then plot the sample $\\alpha$ -quantiles of the series $\\{y_{j}\\}$ against the $\\alpha$ -quantiles of a $\\mathbf{CG}(0,1,c^{*})$ distribution, but only for $\\alpha\\in(p_{1},1]$ . This is equivalent to plotting order statistics of $\\{y_{j}^{\\prime},j=1,...,n^{\\prime}\\}$ against the quantiles $\\{q_{j}\\}$ of the $\\mathbf{CG}(0,1,c^{*})$ distribution given by $q_{j}=\\Phi^{-1}\\{(1-p_{1})(j-0.5)/{n^{\\prime}}+p_{1}\\},\\qquadj=1,\\ldots,n^{\\prime}.$ If the data really did come from a CG distribution, then the resulting plot should be approximately a straight line. By normalizing both the order statistics and the computed values $\\{q_{j}\\}$ so that the minimum and maximum values on both axes are 0 and 1 respectively, our plot should connect the points $(0,0)$ and $(1,1)$ in approximately a straight line. As a measure of deviation of the empirical distribution from a CG distribution, the total area contained in the unit box $[0,1]\\times[0,1]$ which lies between the resulting normalized quantile–quantile plot and the line $\\{y=x\\}$ is used. We choose the transformation which minimizes this area. Having defined the measure of deviation, it is a straightforward procedure to use a numerical minimization routine to minimize the measure of deviation of transformed data with respect to the parameter of the Box–Cox transformation."
  },
  {
    "qid": "statistic-posneg-977-1-0",
    "gold_answer": "TRUE. The modified likelihood ratio statistic λ₄ is indeed obtained by replacing Nℓ with nℓ in the original likelihood ratio test statistic, as stated in the context. This modification is part of the process to derive the test statistic for the hypothesis H₄.",
    "question": "Is the modified likelihood ratio statistic λ₄ obtained by changing Nℓ to nℓ in the original likelihood ratio test statistic?",
    "question_context": "Likelihood Ratio Statistics for Testing Covariance Structures in Complex Multivariate Normal Populations\n\nThe paper discusses the modified likelihood ratio statistic for testing the hypothesis that the covariance matrix is equal to a specified matrix. The moments of the likelihood ratio statistic are derived and used to approximate its distribution. The modified likelihood ratio test statistic is obtained by changing certain parameters in the original likelihood ratio test statistic. The paper also provides tables with significance levels and upper percentage points for the distribution of the test statistic."
  },
  {
    "qid": "statistic-posneg-2086-11-3",
    "gold_answer": "TRUE. The choice $\\lambda_{i j}=1+\\lvert y_{i}-y_{j}\\rvert$ encourages clustering of pixels with similar data values, as it makes the chain more likely to jump between modes, facilitating mode swapping and improving mixing.",
    "question": "The choice $\\lambda_{i j}=1+\\lvert y_{i}-y_{j}\\rvert$ in the Potts model encourages clustering of pixels with similar data values.",
    "question_context": "Markov Chain Monte Carlo (MCMC) Estimation and Potts Model Sampling\n\nThe idea that the multivariate exploratory data analysis toolkit has something to offer to Bayesian statistics as well as the recent use by Draper et al. (1993) of exchangeability ideas in an approach to developing a theory of data analysis show a remarkable continuity in statistical ideas which may be seen as an extension to Good's views on a Bayes-non-Bayes compromise and Box's statistical ecumenism. <PARAGRAPH_SEPARATOR> Xiao-liang Han (University of Bristol): When we estimate the integrated autocorrelation time of a Markov chain Monte Carlo (MCMC) algorithm by any of the spectral window estimators commonly used in the analysis of stationary time series, we might decide on the window widths, either in the time domain or in the frequency domain depending on which window estimator we adopt, by an approach which is close to optimal for our specific purpose, i.e. we are only interested in estimating the spectral densityatfrequencyO. <PARAGRAPH_SEPARATOR> The estimators of $\\tau(f)$ based on spectral windows have the general form <PARAGRAPH_SEPARATOR> $$ \\hat{\\tau}(f)=\\sum_{t=-(N-1)}^{N-1}\\lambda_{N}(t)\\hat{\\rho}_{f}(t) $$ <PARAGRAPH_SEPARATOR> where $\\lambda_{N}(t)$ is the window function. In typical cases, the asymptotic mean-square error (MSE) of $\\hat{\\tau}(f)$ is a function of $M,\\tau(f)$ and $\\tau^{(1)}(f)$ or $\\tau^{(2)}(f)$ ,where $M$ is the window width, <PARAGRAPH_SEPARATOR> $$ \\tau^{(1)}(f)\\equiv\\sum_{t=-\\infty}^{\\infty}|t|\\rho_{f}(t),\\tau^{(2)}(f)\\equiv\\sum_{t=-\\infty}^{\\infty}|t|^{2}\\rho_{f}(t). $$ <PARAGRAPH_SEPARATOR> Consider the fact that, while a window estimator is a consistent estimator of $\\tau(f)$ ,it can also be used as a consistent estimator of $\\tau^{(1)}(f)$ and $\\tau^{(2)}(f)$ under certain conditions; we suggest an EM-like algorithm to estimate $\\tau(f)$ by window estimators as follows: <PARAGRAPH_SEPARATOR> (a) calculate $\\hat{\\rho}_{f}(t),t=1,2,...,N-1$ , by fast Fourier transformation;   (b) start from a small $M$   (c) use the window estimator to estimate ${\\hat{\\tau}}(f),{\\hat{\\tau}}^{(1)}(f)$ or $\\hat{\\tau}^{(2)}(f)$ under the current value of $M$ (d) calculate $\\hat{M}_{\\operatorname*{min}}$ , which minimizes the MSE under the current $\\hat{\\tau}(f)$ and $\\hat{\\tau}^{(1)}(f)$ or $\\hat{\\tau}^{(2)}(f)$ (e) unless $\\hat{M}_{\\mathrm{min}}$ is sufficiently close to $M.$ replace $M$ by $\\hat{M}_{\\mathrm{min}}$ and go back to step (c); otherwise print out $\\widehat{\\tau}(f)$ and stop. <PARAGRAPH_SEPARATOR> Our experiment shows that this approach works well in common cases especially when $\\tau(f)$ is large. <PARAGRAPH_SEPARATOR> David Higdon (University of Washington, Seattle): It seems that multimodality in posterior distributions based on Potts priors (Besag and Green, Section 4) does not always yield to the Swendsen-Wang algorithm. In describing an alternative, suppose that the posterior is written as <PARAGRAPH_SEPARATOR> $$ p(\\mathbf{x})\\propto\\exp\\left(\\sum_{i}\\alpha_{i}(x_{i})+\\sum_{i\\sim j}\\beta_{i j}I[x_{i}=x_{j}]\\right)\\qquad\\mathrm{for~}\\mathbf{x}\\in\\{1,\\dots,r\\}^{n}, $$ <PARAGRAPH_SEPARATOR> where the $\\alpha_{i}$ depend also on the fixed data $\\mathbf{y}$ . Let u denote an auxiliary bond variable with one component ${\\pmb u}_{i j}$ for each neighbour pair $(i,j)$ , and define the $u_{i j}$ to be conditionally independent with <PARAGRAPH_SEPARATOR> $$ p(u_{i j}|\\mathbf{x})\\propto\\exp\\left\\{-\\frac{1}{\\uplambda_{i j}}(u_{i j}+\\beta_{i j}I[x_{i}=x_{j}])\\right\\}I[\\mathrel{-\\beta_{i j}}I[x_{i}=x_{j}]<u_{i j}], $$ <PARAGRAPH_SEPARATOR> where the $\\lambda_{i j}$ are as yet arbitrary. Then <PARAGRAPH_SEPARATOR> $$ p(\\mathbf{x}|\\mathbf{u})\\propto\\exp\\left\\{\\sum_{i}\\alpha_{i}(x_{i})+\\sum_{i=j}\\left(1-\\frac{1}{\\uplambda_{i j}}\\right)\\beta_{i j}I[x_{i}=x_{j}]\\right\\}I\\left[\\bigcap_{i=j}\\large\\{-\\beta_{i j}I[x_{i}=x_{j}]<u_{i j}\\}\\right]. $$ <PARAGRAPH_SEPARATOR> The term $I[\\cap_{i\\sim j}\\{-\\beta_{i j}I[x_{i}=x_{j}]<u_{i j}\\}]$ induces clustering as in the Swendsen-Wang algorithm. However, the cluster colours are no longer independent of one another. Now, consider each cluster $\\mathbf{c}$ in turn and give it colour $\\pmb{k}$ withprobability <PARAGRAPH_SEPARATOR> $$ \\jmath(k|\\mathbf{x}_{\\partial\\mathbf{C}},\\mathbf{u})\\propto\\exp\\left\\{\\sum_{i\\in\\mathbf{C}}\\alpha_{i}(k)+\\sum_{\\substack{\\{i,-j:i\\in\\mathbf{C},j\\in\\partial\\mathbf{C}\\}}}\\left(1-\\frac{1}{\\lambda_{i j}}\\right)\\beta_{i j}I[x_{j}=k]\\right\\}\\qquad\\mathrm{for}~k\\in\\{1,,...,r\\} $$ <PARAGRAPH_SEPARATOR> noting the dependence on the colours $\\mathbf{x}_{\\partial\\mathbf{C}}$ of its neighbours, and hence the need for sequential updating. Thus, adjacent, like coloured, pixels are bonded with probability $\\mathbf{l}-\\mathsf{e x p}(-\\beta_{i j}/\\lambda_{i j})$ and each cluster is then updated according to its conditional distribution. When all the $\\lambda_{i j}$ are 1, this is the SwendsenWang algorithm, and when all the $\\lambda_{i j}$ are very large, it is essentially Gibbs sampling. <PARAGRAPH_SEPARATOR> As an ilustration of the new algorithm, an artificial, bimodal, posterior surface based on the archaeology example (Besag et al. (1991), section 3) is constructed on a $16\\times16$ binary array, with second. order neighbourhoods. The surface can be given two main modes by inflating the variance. The first is a nearly all-white image, containing about $70\\%$ of the probability. The second is an image which resembles the original restoration. Among several plausible methods of choosing the $\\lambda_{i j}$ to facilitate mode swapping, one simple scheme is to choose $\\lambda_{i j}=1+\\lvert y_{i}-y_{j}\\rvert$ . This encourages clustering of pixels with similar data values and makes the chain jump modes about 1 in 100 iterations, compared with about 1 in 1000 for Gibbs sampling and 1 in 700 for the Swendsen-Wang algorithm. <PARAGRAPH_SEPARATOR> John E. Kolassa and Martin A. Tanner (University of Rochester): The authors are to be commended for their review of the state of the art of Markov chain Monte Carlo methods to implement the Bayesian paradigm. We briefly illustrate how the Gibbs sampler can be used by frequentists who prefer to eliminate nuisance parameters via conditioning, rather than via integration (as championed by Bayesians and likelihoodists). The classically oriented reader may also be interested in the work of Gelfand and Carlin (1991) and Geyer and Thompson (1992) who illustrate how these methods can be used to locate the modes of the likelihood (or posterior). Besag and Clifford (1989) discuss the application of Markov chain methods to the problem of significance testing. Assuming that we can devise an ergodic Markov chain whose equilibrium distribution is the same as that of the sufficient statistic, and whose transition probabilities are easy to sample from, Besag and Clifford (1989, 1991) suggest methods for computing $p$ -values."
  },
  {
    "qid": "statistic-posneg-1995-1-2",
    "gold_answer": "TRUE. The convergence of empirical ROC curves to limiting Gaussian processes ensures that the adaptive procedure maintains the desired power and controls the type I error rate. The independence of the sequentially estimated $\\Delta$-statistic from the nuisance variance estimation allows the procedure to treat recalculated sample sizes as fixed, ensuring sufficient power and controlled error rates as per the error spending function.",
    "question": "Is the large sample property of the adaptive procedure, as described by the convergence of empirical ROC curves, sufficient to control the specified type I error rate and maintain the desired power?",
    "question_context": "Sample Size Recalculation in Sequential Diagnostic Testing\n\nThe paper discusses the recalculation of sample sizes in sequential diagnostic testing, focusing on reducing dependence on prespecified correlation parameters by updating sample sizes at interim analyses. It provides formulas for initial and recalculated sample sizes, stopping rules, and discusses the large sample properties of the adaptive procedure."
  },
  {
    "qid": "statistic-posneg-1250-3-0",
    "gold_answer": "TRUE. The matrix $\\mathbf{L}_{\\mathbf{W}}$ is constructed as $\\widetilde{\\mathbf{L}}_{\\mathbf{W}}(\\widetilde{\\mathbf{L}}_{\\mathbf{W}})^{\\top}$, which ensures symmetry and positive semi-definiteness regardless of the sign of $w_{i j}$. The condition $\\mathbf{L}_{\\mathsf{W}}\\mathbf{1}=\\mathbf{0}$ holds because each row of $\\widetilde{\\mathbf{L}}_{\\mathbf{W}}$ sums to zero by construction (due to the definition of $(\\widetilde{\\mathbf{L}}_{\\mathbf{W}})_{\\ell i_{\\ell}} = \\sum_{j=1}^{D}w_{i j}$ and $(\\widetilde{\\mathbf{L}}_{\\mathsf{W}})_{l j} = -w_{l j}$ for $j \\neq i_{l}$), ensuring the null space includes the vector of ones.",
    "question": "Is the statement 'The matrix $\\mathbf{L}_{\\mathbf{W}}:=\\widetilde{\\mathbf{L}}_{\\mathbf{W}}(\\widetilde{\\mathbf{L}}_{\\mathbf{W}})^{\\top}$ is symmetric, positive semi-definite, and has real-valued entries with $\\mathbf{L}_{\\mathsf{W}}\\mathbf{1}=\\mathbf{0}$' correct given the construction of $\\widetilde{\\mathbf{L}}_{\\mathbf{W}}$ with possibly negative weights $w_{i j}$?",
    "question_context": "Graph Compositional Data Analysis: Weighted Log-Ratios and Hilbert Spaces\n\nWe can generalize the theory developed for the space $(\\mathcal{S}_{\\pmb{W}}^{D},\\langle\\cdot,\\cdot\\rangle_{\\pmb{W}},\\oplus_{\\pmb{W}},\\odot_{\\pmb{W}})$ even further. One important extension that comes to mind is to allow the weights $w_{i j}$ to also take negative values as the following example shows: <PARAGRAPH_SEPARATOR> Example 1. Assume that through expert knowledge we are interested in analyzing only certain weighted combinations, say $\\begin{array}{r}{\\bar{\\sum_{j=1}^{D}}\\ln(\\frac{x_{i}}{x_{j}})w_{i j}=\\ln\\left(\\frac{x_{i}^{d(i)}}{\\prod_{j=1}^{D}x_{j}^{w_{i j}}}\\right)}\\end{array}$ , where $\\begin{array}{r}{d(i):=\\sum_{j=1}^{D}w_{i j},}\\end{array}$ for $i\\in\\{i_{1},\\ldots,i_{L}\\}\\subset\\{1,\\ldots,D\\}$ and $w_{i j}\\in\\mathbb{R}$ , not necessarily symmetric. We can collect these weighted combinations in a vector, $\\widetilde{\\mathbf{L}}_{\\mathsf{W}}\\ln(\\pmb{x})$ , where $\\widetilde{\\mathbf{L}}_{\\boldsymbol{\\mathsf{W}}}\\in\\mathbb{R}^{L\\times D}$ is a rectangular matrix, $L\\leq D$ , with $\\begin{array}{r}{(\\widetilde{\\mathbf{L}}_{\\mathbf{W}})_{\\ell i_{\\ell}}=\\sum_{j=1}^{D}w_{i j}}\\end{array}$ for $\\ell\\in\\{1,\\ldots,L\\}$ and $(\\widetilde{\\mathbf{L}}_{\\mathsf{W}})_{l j}=-w_{l j}$ ,  f˜or $j\\neq i_{l}$ . The m a˜trix $\\mathbf{L}_{\\mathbf{W}}:=\\widetilde{\\mathbf{L}}_{\\mathbf{W}}(\\widetilde{\\mathbf{L}}_{\\mathbf{W}})^{\\top}$ is symmetric, positive semi-˜definite,  has real valued entries and 1 in ˜its null space, $\\mathbf{L}_{\\mathsf{W}}\\mathbf{1}=\\mathbf{0}$ . LW can be used a s˜ a b˜uilding block for a scale-invariant Hilbert-space as explained below."
  },
  {
    "qid": "statistic-posneg-795-0-1",
    "gold_answer": "FALSE. The spatial GLLVM model assumes that the latent variables z_{*k} follow a multivariate normal distribution with a mean vector of zero, not a non-zero mean vector. Specifically, the context states that z_{*k} follows N{0, Σ(γ)}, where Σ(γ) is the spatial correlation matrix defined by the Matérn correlation function.",
    "question": "Does the spatial GLLVM model assume that the latent variables z_{*k} follow a multivariate normal distribution with a non-zero mean vector?",
    "question_context": "Spatial GLLVM and Multivariate Spatial GEEs for Correlated Data Analysis\n\nEach latent variable in the spatial GLLVM is assumed to follow a Gaussian process. That is, for $k=1,\\ldots,d,$ the $N$ - vector $z_{*k}=(z_{1k},\\bar{\\dots},z_{N k})^{\\top}$ follows a multivariate normal distribution with mean vector zero and a spatial correlation matrix characterized by the Matérn correlation function, $f(z_{*k})~=~$ $\\mathcal{N}\\{\\mathbf{0},\\pmb{\\Sigma}(\\gamma)\\}$ and $\\Sigma(\\pmb{\\gamma})_{s s^{\\prime}}=\\{2^{1-\\nu}/\\Gamma(\\nu)\\}(\\Delta_{s s^{\\prime}}/h)^{\\nu}\\dot{\\mathcal{K}}_{\\nu}(\\Delta_{s s^{\\prime}}/h)$ for $s,s^{\\prime}=1,\\ldots,N,$ , where $\\Gamma(\\cdot)$ is the gamma function, $\\mathcal{K}_{\\nu}(\\cdot)$ is the modified Bessel function of the second kind, and $\\Delta_{s s^{\\prime}}$ is a distance measure between spatial locations $s$ and $s^{\\prime}$ . The Matérn correlation function is characterized by the smoothness $\\nu~>~0$ and spatial scale parameters $h>0$ , such that we have $\\ensuremath{\\boldsymbol\\gamma}=(\\ensuremath{\\boldsymbol\\nu},h)^{\\top}$ . Note the smoothness parameter is sometimes fixed a-priori in real application for example, $\\nu~=~1$ is a common choice when $\\mathcal{D}=\\mathbb{R}^{2}$ (Lindgren and Rue 2015). Also, for ease of notation we have assumed the same smoothness and spatial scale parameters for all $d$ latent variables, although this can be generalized.<PARAGRAPH_SEPARATOR>It is a straightforward to see how the spatial GLLVM accounts for both between response and spatial correlations. On the linear predictor scale, we have $\\mathrm{cov}(x_{s}^{\\top}\\beta_{j}+z_{s}^{\\top}\\lambda_{j},x_{s^{\\prime}}^{\\top}\\beta_{j^{\\prime}}+z_{s^{\\prime}}^{\\top}\\lambda_{j^{\\prime}})=$ $\\Sigma(\\pmb{\\gamma})_{s s^{\\prime}}\\lambda_{j}^{\\top}\\lambda_{j^{\\prime}}$ , implying that the correlation between two responses decays from a maximum of $\\lambda_{j}^{\\top}\\lambda_{j^{\\prime}}$ , achieved at $s=s^{\\prime}$ , with increasing distance between the spatial units. Moreover, by employing latent variables the spatial GLLVM is able to model the between response correlation in a parsimonious, rank-reduced manner. Recently, Hui, Hill, and Welsh (2021a) demonstrated that if the true model is a spatial GLLVM but one misspecifies and fit a spatially independent GLLVM that is, assumes $f(z_{i k})\\sim\\mathcal{N}(0,1)$ for all $i=1,\\ldots,N$ and $k=1,\\ldots,d,$ then it generally has severe consequences on estimation and inference for the $\\beta_{j}\\mathrm{{^{>}s,}}$ especially if the corresponding covariates are also spatially correlated. This result demonstrates the importance of accounting for spatial correlation when it is present in (the underlying) GLLVM.<PARAGRAPH_SEPARATOR>Let $\\begin{array}{l l l}{\\phi}&{=}&{(\\phi_{1},...,\\phi_{m})^{\\top}}\\end{array}$ and $\\begin{array}{r c l}{\\Psi}&{=}&{(\\pmb{\\beta}_{1}^{\\top},\\dots,\\pmb{\\beta}_{m}^{\\top},\\pmb{\\phi}^{\\top},\\rho,}\\end{array}$ $\\lambda_{1}^{\\top},\\dots,\\lambda_{j}^{\\top},\\pmb{\\gamma}^{\\top})^{\\top}$ denote the full vector of parameters in the spatial GLLVM. If $\\phi$ are known, then these are omitted from $\\Psi$ . Note there are typically restrictions placed on the loadings $\\lambda_{j}$ to avoid rotation invariance (Skrondal and Rabe-Hesketh 2004; Niku et al. 2019a), although we suppress these in the definition of $\\Psi$ for ease of notation. The marginal log-likelihood of the spatial GLLVM is then defined as<PARAGRAPH_SEPARATOR>$$\\ell(\\Psi)=\\log\\left(\\int\\prod_{s=1}^{N}\\prod_{j=1}^{m}f(y_{s j}|x_{s},z_{s})\\prod_{k=1}^{d}f(z_{*k})d\\prod_{k=1}^{d}z_{*k}\\right).$$<PARAGRAPH_SEPARATOR>It is clear from (1) that the marginal likelihood function involves a high-dimensional integral, scaling with the number of spatial units $N$ and latent variables $d$ . Along with the fact that the integral itself does not possess a tractable form for nonnormally distributed responses, which is the case for the three response distributions outlined above, then estimation and thus variable selection for spatial GLLVMs presents a substantial computational burden.<PARAGRAPH_SEPARATOR># 3. Multivariate Spatial Generalized Estimating Equations<PARAGRAPH_SEPARATOR>To overcome the computational challenges of fitting and performing variable selection in spatial GLLVMs, we instead turn to multivariate spatial GEEs, which are defined by assumptions on the marginal moments and the working correlation matrix. Specifically, we assume the marginal moments $\\mathrm{E}(y_{s j}|\\pmb{x_{s}})=\\mu_{s j}$ and $\\begin{array}{c c c c c}{\\log(\\mu_{s j})}&{=}&{\\eta_{s j}}&{=}&{x_{s}^{\\top}\\pmb{\\alpha}_{j},}\\end{array}$ where $\\pmb{\\alpha}_{j}$ denotes the GEE regression coefficients for response $j$ . Note the mean $\\mu_{s j}$ is different from the one defined for the spatial GLLVM, and unless otherwise stated we will use $\\mu_{s j}$ to represent the mean in the multivariate spatial GEE. For the marginal variance, we consider the same mean-variance relationships as induced by conditional distribution of the responses in the spatial GLLVM (this is common practice e.g., Fitzmaurice et al. 2008). That is, we can consider $\\mathrm{var}(y_{s j}|{\\pmb x}_{s})=\\nu_{s j}=\\phi_{j}\\mu_{s j}$ and $\\mathrm{var}(y_{s j}|\\pmb{x}_{s})=$ $\\nu_{s j}=\\mu_{s j}+\\phi_{j}\\mu_{s j}^{2}$ for (overdispersed) counts, and $\\mathrm{var}(y_{s j}|\\pmb{x}_{s})=$ vsj = φjμsρj with ρ ∈ (1, 2) for nonnegative continuous data. Again, the parameters defining the marginal variance function differ from those in the underlying spatial GLLVM.<PARAGRAPH_SEPARATOR>Let $y=(y_{11},y_{12},\\dotsc,y_{1m},y_{21},\\dotsc,y_{N m})^{\\top}$ denote the full mN-vector responses, sorted by responses within spatial units, and likewise define $\\pmb{\\mu}=(\\mu_{11},\\stackrel{.}{\\mu}_{12},\\stackrel{.}{.}\\cdot\\cdot,\\mu_{N m})^{\\top}$ . Next, let $\\pmb{\\alpha}=$ $(\\pmb{\\alpha}_{1}^{\\top},\\dots,\\pmb{\\alpha}_{m}^{\\top})^{\\top}$ denote the full mp-vector of marginal coefficients in the multivariate spatial GEE, and define the $m N\\times m p$ model matrix<PARAGRAPH_SEPARATOR>$$X=\\left(\\begin{array}{c}{{I_{m}\\otimes x_{\\mathrm{I}}^{\\top}}}\\ {{I_{m}\\otimes x_{2}^{\\top}}}\\ {{\\vdots}}\\ {{I_{m}\\otimes x_{N}^{\\top}}}\\end{array}\\right).$$<PARAGRAPH_SEPARATOR>With the above set up, the full vector of linear predictors $\\pmb{\\eta}~=~(\\eta_{11},\\eta_{12},.~.~.,\\eta_{N m})^{\\top}$ can be written as $\\pmb{\\eta}~=~\\pmb{X}\\pmb{\\alpha}$ and subsequently $\\pmb{\\mu}=\\exp(\\pmb{\\eta})$ where the exponential function is applied element-wise. The saturated multivariate spatial GEE is then defined as<PARAGRAPH_SEPARATOR>$$\\boldsymbol{S}(\\boldsymbol{\\alpha})=\\boldsymbol{X}^{\\top}\\boldsymbol{W}\\boldsymbol{A}^{-1/2}\\boldsymbol{R}^{-1}\\boldsymbol{A}^{-1/2}(\\boldsymbol{y}-\\boldsymbol{\\mu})=\\mathbf{0},$$<PARAGRAPH_SEPARATOR>where $W$ is a diagonal matrix with elements equal to $\\pmb{\\mu}$ and $\\pmb{A}$ is a diagonal matrix with elements $\\pmb{\\nu}=(\\nu_{11},\\nu_{12},...,\\nu_{N m})^{\\top}$ . We refer to (2) as a saturated GEE since it involves all $p$ covariates, or equivalently all mp columns of $X$ . This contrasts to candidate GEEs that we will discuss in Section 3.2, which involve a subset of the columns of $X$ .<PARAGRAPH_SEPARATOR>Regarding the working correlation matrix, while there are a wide array of possible choices for $\\pmb R$ as reviewed in Section 1, here we propose a relatively simple, separable structure $R=R_{\\mathrm{spatial}}(\\theta)\\otimes R_{\\mathrm{resp}}$ , where $R_{\\mathrm{resp}}$ is assumed to be an $m\\times m$ unstructured between response correlation matrix, and $R_{\\mathrm{spatial}}(\\pmb\\theta)$ is an $N\\times N$ spatial working correlation matrix parameterized by a Matérn correlation function with smoothness parameter $\\nu$ and spatial scale $h,$ such that $\\mathbf{\\theta}\\theta~=~(\\nu,h)^{\\top}$ . Again, these parameters differ from those in the spatial GLLVM. The assumption of separability between the spatial and between response correlation structures is motivated from both computational and statistical grounds. In the former, inversion and estimation of $\\pmb R$ is greatly simplified upon assuming separability, and this is exploited in the estimation process below. In the latter, note that without independent samples, the empirical estimate of the fully unstructured $m N\\times m N$ working correlation matrix would reduce to $A^{-1/2}(y-\\pmb{\\mu})(y-\\pmb{\\mu})^{\\top}A^{-\\top/2}$ . This is not only a poor estimate of the correlation matrix in general, but in fact does not lead to a sensible score statistic as we will see in Section 4. Assuming separability will circumvent this issue. With regards to $R_{\\mathrm{spatial}}(\\pmb\\theta)$ , we use a Matérn correlation function to ensure the spatial correlation matrix is well-defined and stable; see also the discussion in Section 4. On the other hand, since we typically have no simple concept of distance between two responses (species) without further knowledge, then so we choose to keep $R_{\\mathrm{resp}}$ unstructured. With $N$ sufficiently large, we found that this unstructured matrix can usually be estimated in a stable manner.<PARAGRAPH_SEPARATOR>As a simpler and even more computationally efficient alternative to the above, we also examine the case of the independence estimation equations (IEEs), where $\\textit{\\textbf{R}}=\\textit{\\textbf{I}}_{m N}$ that is, a working correlation assuming all spatial units and responses are uncorrelated with each other. Note in this setting Equation (2) is equivalent to fitting a separate regression model to each response for example, the case of $\\mathrm{var}(y_{s j}|{\\pmb x}_{s})=\\nu_{s j}=\\phi_{j}\\mu_{s j}$ is the same as fitting a separate Poisson generalized linear model (GLM) to response, while the case $\\mathrm{var}(y_{s j}|\\pmb{x}_{s})=\\nu_{s j}=\\phi_{j}\\mu_{s j}^{\\rho}$ is the same as fitting separate Tweedie GLMs to each response."
  },
  {
    "qid": "statistic-posneg-1614-0-2",
    "gold_answer": "TRUE. The paper demonstrates this symmetry property in §4(i), showing that f(W; a, b, r1, r2) = f(W; b, a, r2, r1) under the transformation W' = 1/W. When r1 = r2, swapping a and b leaves the density function unchanged, confirming the symmetry. This property is derived from the integral form of f(W) and holds for the given parameter conditions.",
    "question": "Is the density function f(W) symmetric with respect to parameters a and b when r1 = r2?",
    "question_context": "Comparison of Sensitivities in Measurement Methods: Variance Ratio Test\n\nIn this paper the comparison of the sensitivities of two methods of measurement in the case of the type I situation, under the assumption of model II of the analysis of variance, is considered in detail. The exact distribution, as well as very useful approximations to it, are obtained for a test statistic to be used in comparing the sensitivities of experiments. Tests of hypotheses and the calculation of critical values are discussed. It is also shown how the results can be applied to the type II and type IlI situations."
  },
  {
    "qid": "statistic-posneg-657-7-1",
    "gold_answer": "FALSE. The ISE values indicate that the GPE (7.9) has a higher error compared to the LADE (7.2), suggesting that the GPE is less effective at capturing local additive information. The paper explicitly states that the GPE cannot capture local additive information as well as the LADE and the new estimators (LNWADE and LLADE).",
    "question": "Does the globally penalized estimator (GPE) perform better than the local additive estimator (LADE) in capturing local additive information, as suggested by the ISE values in Table 4?",
    "question_context": "Local Additive Estimation in High-Dimensional Regression Models\n\nThe paper examines the performance of various estimators, including local linear estimator (LLE), additive estimator (AE), local additive estimator (LADE), globally penalized estimator (GPE), local N-W-additive estimator (LNWADE), and local linear–additive estimator (LLADE), in capturing local additive information in regression functions. The study uses two main regression functions: one with dimension p=2 and another with p=10. The performance is evaluated using Integrated Squared Error (ISE) and Mean Averaged Squared Error (MASE). The results show that the local additive and the new estimators (LNWADE and LLADE) adapt well to local and global additivity, outperforming other estimators in capturing the true regression surface."
  },
  {
    "qid": "statistic-posneg-1456-0-2",
    "gold_answer": "FALSE. The context mentions that the conditional density estimated by DPMM (Fig. 4(c)) seems close to the truth (Fig. 4(b)), indicating that the DPMM provides a reasonable approximation of the true conditional density for the nearly non-stationary time series.",
    "question": "Is the conditional density estimated by DPMM for the nearly non-stationary time series in Simulation 3 significantly different from the true conditional density?",
    "question_context": "Nonparametric Bayesian Estimation of Conditional Densities in Time Series\n\nThe paper discusses a nonparametric Bayesian approach using Dirichlet Process Mixture Models (DPMM) to estimate conditional densities in time series data. It compares DPMM with Mixture Autoregressive (MAR) models and kernel methods through simulations and real data analysis. The simulations involve generating data from complex multi-component MAR models and nearly non-stationary time series. The real data analysis focuses on the Canadian lynx data, exploring multimodality in conditional densities using the Prediction Mean Square Error (PMSE) criterion for model selection."
  },
  {
    "qid": "statistic-posneg-2112-0-3",
    "gold_answer": "TRUE. The mean interval score is calculated as the average of the interval scores over all discretized points $u_i$ and all forecasting horizons $\\zeta$ from $h$ to $H$. This provides a comprehensive measure of the interval forecast performance across the entire forecasting period and all points of interest.",
    "question": "Is the mean interval score calculated as the average of the interval scores over all points and forecasting horizons?",
    "question_context": "Evaluation of Interval Forecast Accuracy in Functional Time Series\n\nTo evaluate pointwise interval forecast accuracy, we consider the coverage probability difference (CPD) between the nominal and empirical coverage probabilities. The empirical coverage probability is defined as follows where $H$ denotes the number of curves in the forecasting period, $p$ $\\widehat{\\mathcal{V}}_{T+\\zeta|T,s}^{g,\\mathrm{ub}}$ and YTg,lbζ $\\widehat{\\mathcal{V}}_{T+\\zeta|T,s}^{g,\\mathrm{lb}}$ denote the upper and lower bounds of the corresponding forecasted interval, and $\\mathbb{1}\\{\\cdot\\}$ the binary indicator function. Pointwise CPD is defined as The lower the $\\mathrm{CPD}_{s}^{g}$ value, the better the forecasting method’s performance. Additionally, we use the interval score of Gneiting and Raftery (2007) (see also Gneiting and Katzfuss 2014). For each year in the forecasting period, the $h$ -step-ahead prediction intervals were calculated at the $100(1~-~\\alpha)\\%$ nominal coverage probability. We consider the common case of the symmetric $100(1-\\alpha)\\%$ prediction interval, with lower and upper bounds that are predictive quantiles at $\\alpha/2$ and $1-\\alpha/2$ , denoted by YT,l+bζ |T,s(ui) and YT,+ubζ | T,s(ui). The scoring rule for the interval forecast at discretized point $u_{i}$ is where $\\mathbb{1}\\{\\cdot\\}$ represents the binary indicator function, and $\\alpha$ denotes a level of significance. Finally, we compute the mean interval score for the total of $T$ series as The optimal interval score is achieved when $\\mathcal{V}_{T+\\zeta|T,s}^{g}(u_{i})$ lies between YTg,lbζ T,s(ui) and YTg,ubζ T,s(ui), with the distance between the upper bound and the lower bound being minimal."
  },
  {
    "qid": "statistic-posneg-1403-2-1",
    "gold_answer": "FALSE. While the context mentions that in signal processing and communication theory, the noise matrix $X_{n}$ satisfying Assumptions A2–A4 is such that $\\sqrt{n}X_{n}$ is standard Gaussian, it does not imply that $X_{n}$ must always be Gaussian for these assumptions to hold. The reference to Gaussian noise is an example, not a strict requirement.",
    "question": "Does the context imply that the noise matrix $X_{n}$ is always Gaussian for the assumptions A2–A4 to hold?",
    "question_context": "Fixed-Rank Perturbation Analysis in Signal Processing\n\nRemark 1. In the areas of signal processing and communication theory, the noise matrix $X_{n}$ satisfying Assumptions A2–A4 is such that ${\\sqrt{n}}X_{n}$ is standard Gaussian—see for instance [21,15]. <PARAGRAPH_SEPARATOR> We first make a general assumption on matrices $P_{n}$ ; it will be specified later, and adapted to the context of the MUSIC algorithm. <PARAGRAPH_SEPARATOR> Assumption A5. Matrices $P_{n}$ are deterministic with a fixed rank equal to $r$ for all $n$ large enough. Denoting by $\\boldsymbol{P_{n}}=U_{n}\\Omega_{n}V_{n}^{*}$ a singular value decomposition of $P_{n}$ , the matrix of singular values $\\varOmega_{n}=\\mathrm{diag}(\\omega_{1,n},\\ldots,\\omega_{r,n})$ with $\\omega_{1,n}\\geq\\omega_{2,n}\\geq\\cdot\\cdot\\cdot\\geq\\omega_{r,n}$ converges to <PARAGRAPH_SEPARATOR> $$ O=\\left[\\begin{array}{c c c c}{\\omega_{1}I_{j_{1}}}&&&\\ &{\\ddots}&&\\ &&{\\omega_{s}I_{j_{s}}}\\end{array}\\right], $$ <PARAGRAPH_SEPARATOR> where $\\omega_{1}>\\cdots>\\omega_{s}>0$ and $j_{1}+\\cdot\\cdot\\cdot+j_{s}=r$ . <PARAGRAPH_SEPARATOR> Notations <PARAGRAPH_SEPARATOR> As usual, if $z\\in\\mathbb{C}$ , we shall denote by $\\Re(z)$ and $\\Im(z)$ its real and imaginary parts. We shall denote by $\\xrightarrow{\\mathrm{a.s.}}(\\mathrm{resp.}\\xrightarrow{\\mathcal{P}},\\xrightarrow{\\mathcal{D}})$ the almost sure convergence (resp. convergence in probability, in distribution). We denote by $\\delta_{i,j}$ the Kronecker delta $(=1$ if $i=j$ and 0 otherwise)."
  },
  {
    "qid": "statistic-posneg-657-0-1",
    "gold_answer": "FALSE. The global additive model achieves a convergence rate of $O(n^{-4/5})$ when the true regression function is additive or close to additive. However, if the regression function is far from the global additive form, the global additive assumption leads to serious bias, and the convergence rate may not hold.",
    "question": "Does the global additive model always achieve a convergence rate of $O(n^{-4/5})$ regardless of the true regression function's form?",
    "question_context": "Local-linear-additive-estimation for multiple nonparametric regression\n\nConsider the following nonparametric regression: $$ Y=r(X)+\\varepsilon, $$ where $Y$ is the real-valued response variable, $X=(X^{(1)},\\dots,X^{(p)})'$ is the $p$-dimensional covariate and $\\varepsilon$ is the error with conditional mean zero and variance $\\sigma^{2}(X)$ given $X$. In the full model, we only assume that the unknown regression function $$ r(x)=E(Y|X=x) $$ is smooth. Such a full model is usually regarded as to be most flexible. Under regularity conditions (e.g., twice continuous differentiability of $r(x).$), the convergence rate in mean squared error of a common nonparametric estimator (e.g., kernel estimator) is of the order $O(n^{-4/(4+p)})$ [13,14]. This convergence rate is optimal and thus cannot be improved in some sense. It implies that the common nonparametric estimators suffer from the curse of dimensionality when the dimension $p$ is high. To avoid the problem, one usually considers the model with a specified structure as an approximation to the full model. Additive model, for instance, is a popular approximation to the full model, which has the special form: $$ r(x)=\\mu+\\sum_{k=1}^{p}r_{j}(x^{(j)}), $$ where regression functions satisfy $E(r_{j}(X^{(j)}))=0,j=1,\\dots,p.$ In this paper we prefer to call the above form ‘‘the global additive model’’ because we will use this terminology to discriminate (1.3) from the local additive model [12]. Benefiting from the additive assumption of (1.3), the convergence rate in mean squared error of an estimator is of the order $O(n^{-4/5})$ as that for $p=1$ [15,16,1]. However, the global additive assumption leads to serious bias when the regression function $r(\\cdot)$ is far away from such a global additive form. Various models with given structure have been extensively investigated in the literature and widely applied in practice. However, there has been little previous literature on the model structures that can adapt automatically to the data from the underlying model. How to sufficiently use the structure information behind the data is still a challenging issue. Studer, Seifert and Gasser [17] proposed a globally penalized estimator (GPE) by penalizing the nonadditivity of local linear estimator. Such an innovative estimator offers an adaptive combination between the full model (1.2) and the global additive model (1.3), instead of the single use of either the full model or the global additive model. Thus the method is automatically adaptive to the global additivity. However, the estimator is based only on a global penalty and thus the information of local additivity is not taken into account. It can be shown that this method is not adaptive to local additivity. For illustration, we consider the following regression function: $$ \\displaystyle r(x)=x_{1}^{2}+x_{2}^{2}+x_{1}^{2}x_{2}^{2}. $$ When $x_{1}^{2}x_{2}^{2}$ is small enough, the function is approximately additive. But the local additivity in the local region $\\left\\{(x_{1},x_{2})\\right.:$ $x_{1}^{2}x_{2}^{2}<\\dot{C}\\dot{\\}$ cannot be captured by the global nonadditivity penalty, where $C>0$ is a small constant. In Section 5, we will further illustrate this point of view by simulation. Moreover, the method is computationally intensive and involves the dimensional problem because $m(p+1)$ values of some functions at grids have to be estimated in the calculation procedure, where $m=m_{1}\\times\\cdots\\times m_{p}$ and $m_{k},k=1,\\ldots,p$, should tend to infinity to guarantee the accuracy of the calculation. Park and Seifert [12] employed a local additive estimator to capture the underlying local additivity of regression function. This estimator behaves well in the sense that it outperforms the local linear estimator when the true model is close to the additive model locally and is better than the global additive estimator when the true model is far away from the global additive structure. However, when the model is globally nonadditive, unlike the local linear estimator, the local additive estimator cannot achieve the optimal convergence rate because in the estimation procedure it only uses the estimation method designed for the additive model. Motivated by the ideas of Studer, Seifert and Gasser [17] and Park and Seifert [12], in this paper we suggest a local linear–additive estimator and its relevant version to capture the local and global additive information for general nonparametric regression. The main idea of the new method is that an adaptive combination between the local linear estimation and the local additive estimator (rather than global additive estimator) is built by a local–global nonadditive penalty. For example, consider the regression function in (1.4). In local region $\\{(x_{1},x_{2}):x_{1}^{2}x_{2}^{\\bar{2}}<C\\}$, we use an additive model (local additive estimation) to fit the regression function and in local region $\\{(x_{1},x_{2}):x_{1}^{2}x_{2}^{2}\\geq C\\}$, we use the full model (local linear estimation) to fit the regression function. If we are able to explore the local additivity by a data driven method, an adaptive combination between the local model and full model could be achieved. Our estimation therefore has the following distinguishing features: (1) the new method can achieve an adaptive fitting between the full model and local additive model and therefore can adapt to the double additivity: local additivity and global additivity. More precisely, when the true model is approximately or completely additive in a local region, the estimation procedure is automatically adjusted to fit the corresponding additive model in this local region; when the true model is globally additive, the estimator is adjusted automatically to be an additive estimator; when the true model is completely nonadditive, the estimator can approach the local linear estimator and the optimal convergence rate is attained as well; (2) although new idea is motivated by Studer, Seifert and Gasser [17], the new one connects two types of local estimators. Hence, the resultant estimators have closed representations and make the computation easy and accurate; (3) a new theoretical framework is introduced as a foundation of locally and globally connected statistical inference. In the framework, locally and globally connected norms and relevant projections are defined, and based on the newly defined norms and projections, the new estimator can be regarded as a projection of response variable onto the full function space with respect to the locally and globally connected norms."
  },
  {
    "qid": "statistic-posneg-1153-0-0",
    "gold_answer": "TRUE. The perpendicularity condition $\\beta_{0}^{\\mathrm{T}}\\theta_{0} = 0$ ensures identifiability by preventing an arbitrary rotation of the parameters that could leave the model unchanged. Specifically, without this condition, the model could be reparameterized by rotating $\\beta_{0}$ and $\\theta_{0}$ in a way that preserves the linear combination $\\beta_{0}^{\\mathrm{T}}X + \\phi(\\theta_{0}^{\\mathrm{T}}X)$, leading to multiple parameter sets producing the same model output. The normalization $\\|\\theta_{0}\\|=1$ further removes scale ambiguity in the single-index component.",
    "question": "In the extended partially linear single-index model $$y=\\beta_{0}^{\\mathrm{{T}}}X+\\phi(\\theta_{0}^{\\mathrm{{T}}}X)+\\varepsilon,$$ the condition that $\\beta_{0}$ and $\\theta_{0}$ are perpendicular ensures the identifiability of the model parameters. Is this statement true, and why?",
    "question_context": "Extended Partially Linear Single-Index Models: Structure and Applications\n\nThe paper introduces an extended version of a generalized partially linear single-index model, which takes the form $$y=\\beta_{0}^{\\mathrm{{T}}}X+\\phi(\\theta_{0}^{\\mathrm{{T}}}X)+\\varepsilon,$$ where $E(\\varepsilon|X)=0$ and $E\\varepsilon^{2}=\\sigma^{2}$, $\\beta_{0}$ and $\\theta_{0}$ are unknown vector parameters and $\\phi(.)$ is an unknown function. To ensure estimability, the authors assume that $\\beta_{0}$ and $\\theta_{0}$ are perpendicular to each other with $\\|\\theta_{0}\\|=1$ and its first nonzero element positive. The model includes many existing models as special cases, such as the single-index model and the partially linear model. The paper also discusses the application of the model to nonlinear time series data, exemplified by the analysis of annual sunspot numbers."
  },
  {
    "qid": "statistic-posneg-584-3-2",
    "gold_answer": "FALSE. The condition $\\phi(\\lambda s m)~\\geqslant~d\\phi(s m)$ implies $\\lambda m\\leqslant s^{-1}\\phi^{\\leftarrow}(d\\phi(s m))$ only after considering the regular variation of $\\phi$ at zero with index $-\\infty$ and the behavior of $\\phi^{\\leftarrow}$. The inequality is not directly implied by the given condition alone.",
    "question": "Is the condition $\\phi(\\lambda s m)~\\geqslant~d\\phi(s m)$ for $\theta_{0}=\\infty$ sufficient to conclude that $\\lambda m\\leqslant s^{-1}\\phi^{\\leftarrow}(d\\phi(s m))$ for small $s>0$?",
    "question_context": "Multivariate Archimedean Copula Tail Behavior Analysis\n\nThe distribution function of $(U_{i})_{i\\in I}$ is given by the $|I|$ -variate copula with generator $\\phi$ . Hence, it suffices to show (3.3) for the case $I=\\{1,\\ldots,d\\}$ . <PARAGRAPH_SEPARATOR> First, suppose $\theta_{0}=0$ . F $\\mathrm{ix}0<\\varepsilon<1.$ . Since $\\phi$ is slowly varying, $\\phi(s\\varepsilon)\\leqslant2\\phi(s)$ and thus $s\\varepsilon\\geqslant\\phi^{\\leftarrow}(2\\phi(s))$ for all sufficiently small $s>0$ . Hence $\\phi^{\\leftarrow}\\{2\\phi(s)\\}=o(s)$ as s $\\downarrow0$ . Denoting $x=x_{1}\\lor\\cdots\\lor x_{d}$ , we arrive at $C(s x_{1},...,s x_{d})\\leqslant\\phi^{\\leftarrow}(d\\phi(s x))\\leqslant$ $\\phi^{\\leftarrow}(2\\phi(s x))=o(s)$ as $s\\downarrow0$ . <PARAGRAPH_SEPARATOR> Second, suppose $0~<~\theta_{0}~<\\infty$ . The function $t\\mapsto f(t)=\\phi(1/t)$ is increasing and regularly varying at infinity of index $\theta_{0}$ . By Bingham et al. [40, Theorem 1.5.12], its inverse, $f^{\\leftarrow}$ is regularly varying at infinity of index $1/\theta_{0}$ . Since $\\phi^{\\leftarrow}=1/f^{\\leftarrow}$ , we find that $\\phi^{\\leftarrow}$ is regularly varying at infinity of index $-1/\theta_{0}$ . Now write <PARAGRAPH_SEPARATOR> $$ s^{-1}C(s x_{1},\\ldots,s x_{d})={\\frac{1}{\\phi^{\\leftarrow}(\\phi(s))}}\\phi^{\\leftarrow}\\left(\\phi(s)\\left\\{{\\frac{\\phi(s x_{1})}{\\phi(s)}}+\\cdots+{\\frac{\\phi(s x_{d})}{\\phi(s)}}\right\\}\right). $$ <PARAGRAPH_SEPARATOR> Since $\\phi(s)\to\\infty\\mathrm{a}s s\\downarrow0$ and by the uniform convergence theorem for regularly varying functions [40, Theorem 1.5.2], the right-hand side of the previous display converges to $(x_{1}^{-\theta_{0}}+\\cdot\\cdot\\cdot+x_{d}^{-\theta_{0}})^{-1/\theta_{0}}$ as $s\\downarrow0$ . <PARAGRAPH_SEPARATOR> Finally, suppose $\theta_{0}=\\infty$ . Denote $m=x_{1}\\wedge\\cdots\\wedge x_{d}$ . We have <PARAGRAPH_SEPARATOR> $$ s^{-1}\\phi^{\\leftarrow}(d\\phi(s m))\\leqslant s^{-1}C(s x_{1},\\ldots,s x_{d})\\leqslant m. $$ <PARAGRAPH_SEPARATOR> Fix $0~<~\\lambda~<~1$ . Since $\\phi$ is regularly varying at zero with index $-\\infty$ we have $\\phi(\\lambda s m)~\\geqslant~d\\phi(s m)$ and thus $\\lambda m\\leqslant$ $s^{-1}\\phi^{\\leftarrow}(d\\phi(s m))$ for all sufficiently small $s>0$ . Let $\\lambda$ increase to one to see that $\begin{array}{r}{\\operatorname*{lim}_{s\\downarrow0}s^{-1}C(s x_{1},\\dots,s x_{d})=m}\\end{array}$ This finishes the proof of Theorem 3.1. □"
  },
  {
    "qid": "statistic-posneg-1698-0-0",
    "gold_answer": "TRUE. The density function for the LSD of the sample covariance matrix generated from a VMA(1) model is indeed given by the specified formula, as derived in Theorem 4.2 of the paper. The support of the density function is defined over the interval $\\varOmega_{2}$, which is explicitly stated in the theorem and confirmed in Remark 4.3.",
    "question": "Is the density function of the LSD for the sample covariance matrix generated from a VMA(1) model given by $$f(x)=\\left\\{\\frac{1}{2\\pi x}\\sqrt{R_{2}+2p_{2}+2\\sqrt{\\frac{q_{2}^{2}}{R_{2}}}},\\quad i f x\\in\\varOmega_{2},\\right.$$ where $\\varOmega_{2}=\\{x:a_{0}^{2}(1-\\sqrt{y})^{2}<x<a_{0}^{2}(1+\\sqrt{y})^{2}\\}$?",
    "question_context": "Limiting Spectral Distribution of Large-Dimensional Sample Covariance Matrices\n\nThe paper discusses the limiting spectral distribution (LSD) of large-dimensional sample covariance matrices generated from VAR(1) and VMA(1) models. It provides explicit forms of the density functions for these LSDs and discusses estimation methods for the parameters involved. The context includes complex mathematical formulas and theorems that define the density functions and their parameters."
  },
  {
    "qid": "statistic-posneg-592-0-2",
    "gold_answer": "TRUE. The paper highlights that the new LW test only requires the sample covariance matrix $S_{n}$, whereas the CZZ test consists of five parts. This makes the new LW test simpler to compute.",
    "question": "Is it true that the new LW test $W_{n}$ has a simpler formula compared to the CZZ test, as mentioned in the paper?",
    "question_context": "High-Dimensional Covariance Matrix Identity Tests\n\nThe paper introduces two versions of the sample covariance matrices, $S_{n}$ and $B_{n}$, and defines the CLRT and LW test statistics for testing the identity of the covariance matrix in high-dimensional settings. The CLRT is defined as $L_{n}=\\frac{1}{p}\\mathrm{tr}\\left(S_{n}\\right)-\\frac{1}{p}\\log\\left|S_{n}\\right|-1$, and the LW test is defined as $W_{n}=\\frac{1}{p}\\mathrm{tr}\\left(S_{n}-I_{p}\\right)^{2}-\\frac{p}{n-1}\\left(\\frac{1}{p}\\mathrm{tr}\\left(S_{n}\\right)\\right)^{2}-\\frac{(1+A)(n-2)(n-1)-2}{n(n-1)^{2}}$. The paper establishes the asymptotic normality of these statistics under the null hypothesis $\\Sigma_{p}=I_{p}$ and discusses their advantages over existing tests."
  },
  {
    "qid": "statistic-posneg-744-2-1",
    "gold_answer": "FALSE. The AIC method underestimates the true number of weights (m=7) for all sample sizes, although it performs better than BIC and CN in terms of RMISE for larger sample sizes (n=100 and 500).",
    "question": "Does the AIC method consistently select the true number of weights (m=7) in the simulation study for all sample sizes?",
    "question_context": "Model Selection Criteria in Unimodal Density Estimation\n\nWe present a novel procedure for selecting the number of weights by examining the condition number of A. Since A is a normal matrix the condition number is evaluated by, $\\mathsf{C N}(m)=\\left\\lceil\\frac{\\lambda_{\\operatorname*{max}}(A)}{\\lambda_{\\operatorname*{min}}(A)}\\right\\rceil$ , where $\\lambda_{\\operatorname*{max}}(A)$ and $\\lambda_{\\mathrm{min}}(A)$ are the maximum and minimum eigenvalues of A respectively. We select $m$ by including the largest number of weights possible while still bounding $\\log_{10}\\mathrm{CN}(m)$ by $\\sqrt{n}$ . <PARAGRAPH_SEPARATOR> # 2.1.2. AIC and BIC <PARAGRAPH_SEPARATOR> We also present more traditional methods for selecting the number of weights, specifically, the Akaike information criterion (AIC) and Bayesian information criteria (BIC). In the realm of density estimation, these criteria are given by, <PARAGRAPH_SEPARATOR> $\\mathsf{A I C}(m)=-2\\sum_{i=1}^{n}\\log[\\widehat{f}_{m}(x_{i},\\widehat{\\omega}_{m})]+2(m-1)$, <PARAGRAPH_SEPARATOR> $\\mathrm{BIC}(m)=-2\\sum_{i=1}^{n}\\log[\\hat{f}_{m}(x_{i},\\widehat{\\omega}_{m})]+\\log(n)(m-1)$, <PARAGRAPH_SEPARATOR> where $m$ is the number of weights, $\\hat{f}_{m}$ is the estimated density using $m$ weights, $\\widehat{\\omega}_{m}$ is the vector of estimated weights, and $x_{i}$ for $i=1,\\ldots,n$ are the observations. The effective dimension of the weight vector $\\widehat{\\omega}_{m}$ is $m-1$ since the weights are constrained to sum to one. <PARAGRAPH_SEPARATOR> A variation of Theorem 3.1 in Babu et al. (2002) shows that $\\hat{f}_{m}$ will converge uniformly to $f$ for $2\\leq m\\leq(n/\\log n)$ as $n\\rightarrow\\infty$ , so we implement both the AIC and BIC methods by estimating the density with $m$ weights ranging from 2 to $\\lceil n/\\log n\\rceil$ . The AIC or BIC is calculated for each fit, then the density estimate with the lowest AIC or BIC is selected as the best estimate. <PARAGRAPH_SEPARATOR> # 2.2. Comparison of criterion for selecting the number of weights <PARAGRAPH_SEPARATOR> We compare these three methods of selecting $m$ by performing a small simulation study. We generate data from a mixture of Beta densities with a fixed vector of weights, which follows the required format of our density estimation model. We then generate density estimates using the AIC, BIC, and CN criterion to select m. Our goal is to find the method which provides the best density estimate in terms of a low root mean integrated squared error (RMISE) and selects the true number of weights of the Beta mixture. The RMISE is approximated as follows: <PARAGRAPH_SEPARATOR> $\\mathrm{RMISE}=\\mathrm{E}\\left[\\Vert\\hat{f}_{\\widehat{m}}-f\\Vert_{2}\\right]\\approx\\frac{1}{N}\\sum_{\\ell=1}^{N}\\sqrt{d\\sum_{j=1}^{J}\\left[\\hat{f}_{\\widehat{m}}^{(\\ell)}(x_{j})-f(x_{j})\\right]^{2}}$, <PARAGRAPH_SEPARATOR> where $J=100,\\operatorname*{Pr}[x_{1}\\leq X\\leq x_{J}]=0.999$ , and $d=(x_{j}-x_{j-1})=\\frac{x_{J}-x_{1}}{J-1}$ for all $j\\geq2$ . In the above expression, $\\hat{f}_{\\widehat{m}}^{(\\ell)}$ denotes the estimated density at the ℓth sample for $\\ell=1,\\ldots,N$ with $\\widehat{m}$ chosen by one of the three criteria. <PARAGRAPH_SEPARATOR> We examine a symmetric and right-skewed Beta mixture each with 7 weights (i.e. $m=7$ ). The weight vectors are fixed to be $\\omega_{1}~=~\\{0.05,0.1,0.2,0.3,0.2,0.1,0.05\\}$ for the symmetric distribution, and $\\omega_{2}=\\{0.1,0.4,0.25,0.15,$ 0.05, 0.03, 0.02} for the right-skewed distribution. We generate $N~=~1000$ Monte Carlo (MC) samples of size $n=$ 15, 30, 100, and 500 for each Beta mixture. Table 1 displays the MC estimated RMISE values as well as the average estimate of m across the MC samples. Statistically significant lower RMISE values, based on a paired $t$ -test, are in bold. <PARAGRAPH_SEPARATOR> The AIC method has the lowest average RMISE in most situations. Unfortunately, none of the methods have average number of weight estimates that are close to 7. The CN method appears to select the number of weights at around $\\sqrt{n}$ , which is much larger than 7 for the cases of $n=100$ and 500. Both the AIC and BIC underestimate $m$ for all sample sizes with the AIC having slightly larger estimates. Overall it appears the CN criterion is a better option for samples with fewer observations while the AIC performs the best for larger samples of size 100 and 500."
  },
  {
    "qid": "statistic-posneg-698-1-0",
    "gold_answer": "TRUE. The condition $\\tilde{r}_{l m k}=\\tilde{r}_{m l k}$ is indeed necessary to ensure the symmetry of $\\gamma(r,s,t)$ with respect to $r$ and $s$. This is because $\\gamma(r,s,t)$ is expressed as a sum of terms involving $\\tilde{r}_{l m k}\\psi_{l}(r)\\psi_{m}(s)\\phi_{k}(t)$. For $\\gamma(r,s,t)$ to be symmetric in $r$ and $s$, the coefficients $\\tilde{r}_{l m k}$ must be symmetric in $l$ and $m$, i.e., $\\tilde{r}_{l m k}=\\tilde{r}_{m l k}$. This ensures that swapping $r$ and $s$ does not change the value of $\\gamma(r,s,t)$.",
    "question": "Is the condition $\\tilde{r}_{l m k}=\\tilde{r}_{m l k}$ for all $k$ necessary to ensure the symmetry of $\\gamma(r,s,t)$ with respect to $r$ and $s$ in the function-on-function quadratic regression model?",
    "question_context": "Function-on-Function Quadratic Regression: Basis Expansion and Estimation\n\n(C1) $E\\|X\\|^{4}<\\infty,E\\|Y\\|^{4}<\\infty$ ; Without loss of generality, we assume that $E(X_{i}(s))=0$ for all $s\\in S$ . The i.i.d random error processes $\\epsilon_{i}$ are independent of $X_{i}$ satisfying $\\epsilon_{i}\\in L_{2}(\\mathcal{T})$ and $E(\\epsilon_{i}(t))=0$ for all $t\\in\\mathcal T$ . <PARAGRAPH_SEPARATOR> In this paper we consider the case where the whole trajectories of $\\{X_{i}(s),Y_{i}(t)\\}$ are fully observed. All methods proposed and corresponding theories remain the same for densely observed case. See, for example, Theorem 4 of Zhang and Chen (2007). <PARAGRAPH_SEPARATOR> As in literature, we can transform all functions in model (2) into basis expansions. Denote $C_{X},C_{Y}$ as covariance function of $X$ and Y . According to Mercer’s Theorem, we have <PARAGRAPH_SEPARATOR> $$ C_{X}(s_{1},s_{2})=\\sum_{k\\geq1}\\lambda_{k}\\psi_{k}(s_{1})\\psi_{k}(s_{2}),\\quad C_{Y}(t_{1},t_{2})=\\sum_{k\\geq1}\\sigma_{k}\\phi_{k}(t_{1})\\phi_{k}(t_{2}), $$ <PARAGRAPH_SEPARATOR> for all $s_{1},s_{2}\\in s$ and $t_{1},t_{2}~\\in~\\mathcal{T}$ , where $\\{\\lambda_{k},k=1,2,...\\}$ and $\\{\\sigma_{k},k=1,2,...\\}$ are nonnegative eigenvalues of corresponding covariance function, while $\\{\\phi_{k},k\\ge1\\}$ and $\\{\\psi_{k},k~\\geq~1\\}$ form orthonormal basis in $L_{2}(\\mathcal{T})$ and $L_{2}(S)$ respectively. We further assume that all eigenvalues are distinct to ensure the uniqueness of eigenfunctions. <PARAGRAPH_SEPARATOR> (C2) Define $\\delta_{X,j}=\\mathrm{min}_{1\\leq k\\leq j}(\\lambda_{k}-\\lambda_{k+1})$ and $\\delta_{Y,j}=\\mathrm{min}_{1\\leq k\\leq j}(\\sigma_{k}-\\sigma_{k+1})$ . Assume $\\delta_{X,j}>0,\\delta_{Y,j}>0$ for $j\\geq1$ . The coefficient functions can then be represented as <PARAGRAPH_SEPARATOR> $$ \\beta(s,t)=\\sum_{k\\geq1}\\sum_{m\\geq1}b_{m k}\\psi_{m}(s)\\phi_{k}(t),\\quad\\gamma(r,s,t)=\\sum_{k\\geq1}\\sum_{l\\geq1}\\sum_{m\\geq1}\\tilde{r}_{l m k}\\psi_{l}(r)\\psi_{m}(s)\\phi_{k}(t), $$ <PARAGRAPH_SEPARATOR> for all $r,s\\in S$ and $t\\in\\mathcal T$ , where $\\begin{array}{r}{b_{m k}=\\int\\int\\beta(s,t)\\psi_{m}(s)\\phi_{k}(t)d s d t}\\end{array}$ and $\\begin{array}{r}{\\tilde{r}_{l m k}=\\int\\int\\int\\gamma(\\boldsymbol{r},s,t)\\psi_{l}(\\boldsymbol{r})\\psi_{m}(s)\\phi_{k}(t)d^{}}\\end{array}$ rdsdt are projection coefficients. For the sample curves, we also have Karhunen–Loève expansions <PARAGRAPH_SEPARATOR> $$ X_{i}(s)=\\sum_{k\\geq1}\\xi_{i k}\\psi_{k}(s),\\quad Y_{i}(t)=\\mu_{Y}(t)+\\sum_{k\\geq1}\\eta_{i k}\\phi_{k}(t), $$ <PARAGRAPH_SEPARATOR> where $\\mu_{Y}(t)~=~E(Y_{i}(t)),~\\xi_{i k}~=~\\int_{S}X_{i}(s)\\psi_{k}(s)d s$ and $\\begin{array}{r}{\\eta_{i k}=\\int_{\\mathcal{T}}(Y_{i}(t)-\\mu_{Y}(t))\\phi_{k}(t)d t}\\end{array}$ . Random variables $\\xi_{i k}$ and $\\eta_{i k}$ are projections of $X_{i}$ and centered $Y_{i}$ onto their kth eigenfunctions respectively. <PARAGRAPH_SEPARATOR> Insert (3) and (4) into (2), and by orthonormality, we obtain <PARAGRAPH_SEPARATOR> $$ \\eta_{i k}=\\alpha_{k}+\\sum_{m\\geq1}b_{m k}\\xi_{i m}+\\sum_{l\\geq1}\\sum_{m\\geq1}{\\tilde{r}}_{l m k}\\xi_{i l}\\xi_{i m}+\\epsilon_{i k},\\quad i=1,2,\\ldots,n,k=1,2,\\ldots. $$ <PARAGRAPH_SEPARATOR> where $\\begin{array}{r}{\\alpha_{k}=\\int_{\\mathcal T}(\\alpha(t)-\\mu_{Y}(t))\\phi_{k}(t)d t}\\end{array}$ and $\\begin{array}{r}{\\epsilon_{i k}=\\int_{\\mathcal{T}}\\epsilon_{i}(t)\\phi_{k}(t)d t}\\end{array}$ , for each $k=1,2,\\ldots,$ We then merge terms $\\tilde{r}_{l m k}\\xi_{i l}\\xi_{i m}$ and $\\tilde{r}_{m l k}\\xi_{i m}\\xi_{i l}$ for all $l\\neq m$ . We further impose condition (C3) such that Eq. (5) makes sense. <PARAGRAPH_SEPARATOR> (C3) $\\alpha\\in L^{2}(\\mathcal T),\\beta\\in L^{2}(\\mathcal S\\times\\mathcal T)$ and $\\gamma\\in L^{2}(S\\times S\\times\\mathcal{T})$ satisfying $\\gamma(r,s,t)=\\gamma(s,r,t).$ <PARAGRAPH_SEPARATOR> Due to the symmetry of $\\gamma(\\boldsymbol{r},\\boldsymbol{s},t)$ with respect to $r$ and s, we have $\\tilde{r}_{l m k}=\\tilde{r}_{m l k}$ for all $k.$ Eq. (5) can then be modified to <PARAGRAPH_SEPARATOR> $$ \\eta_{i k}=\\alpha_{k}+\\sum_{m\\ge1}b_{m k}\\xi_{i m}+\\sum_{l\\ge1}\\sum_{m=1}^{l}r_{l m k}\\xi_{i}\\xi_{i m}+\\epsilon_{i k},\\quad i=1,2,\\dots,n,\\quad k=1,2,\\dots, $$ <PARAGRAPH_SEPARATOR> where $r_{l m k}=2\\tilde{r}_{l m k}$ for $l\\neq m$ and $r_{m m k}=\\tilde{r}_{m m k}$ ."
  },
  {
    "qid": "statistic-posneg-1159-0-2",
    "gold_answer": "FALSE. The context states that the compactness lemma of Sharpe ensures the kernel $\\operatorname{Inv}(\\mu)$ is compact when $\\mu$ is full (not supported by a proper affine subspace of $V$).",
    "question": "Is the kernel $\\operatorname{Inv}(\\mu) = \\{(B,b,1) \\in L : \\mu = B\\mu * \\varepsilon_{b}\\}$ always non-compact for a full measure $\\mu$?",
    "question_context": "Operator-Semistable Measures and Decomposability\n\nDEFINITION. $\\mu$ is said to be $\\left(B,\\boldsymbol{b},\\beta\\right)$ -decomposable if the equation $\\mu_{\\beta}=B\\mu*\\varepsilon_{b}$ is valid. <PARAGRAPH_SEPARATOR> Trivially every $\\mu$ is $(I,0,1)$ -decomposable. Moreover if $\\mu$ is $(B,b,\\beta)$ decomposable the same is true for all $\\mu_{t},t>0$ (since $\\pmb{\\mu}$ uniquely determines its semigroup $\\left(\\mu_{t}\\right)_{t>0})$ <PARAGRAPH_SEPARATOR> Now let us denote by $L=L(\\mu)$ the collection of all triples $\\left(B,\\dot{b},\\beta\\right)$ such that $\\mu$ is $(B,b,\\beta)$ -decomposable. Let us identify $(B,b,\\beta)$ with the matrix <PARAGRAPH_SEPARATOR> $$ {\\binom{B}{0}}\\quad{\\binom{b}{\\beta}}\\in G L(\\mathbb{R}^{d+1}). $$ <PARAGRAPH_SEPARATOR> Then it can be easily seen that $\\pmb{L}$ is a closed subgroup of $G L(\\mathbb{R}^{d+1})$ . Hence $L$ is a Lie group. By $f((B,b,\\beta)):=\\beta$ there is defined a continuous homomorphism of $L$ into the multiplicative group $\\mathbb{R}_{+}^{*}$ <PARAGRAPH_SEPARATOR> Let us assume now that the measure $\\mu$ is full, i.e.,. $\\mu$ is not supported by a proper affine  subspace  of $V$ Then the compactness lemma of Sharpe, [12, Proposition 4] can be applied to the effect that the kernel $\\left\\{(B,b,1)\\in L\\colon\\mu=B\\mu\\ast\\varepsilon_{b}\\right\\}=:\\operatorname{Inv}(\\mu)$ of $f$ is compact and that the mapping $f$ is closed. Hence exactly one of the following three possibilities can occur: $f(L)=\\left\\{1\\right\\}$ ; or $f(L)=\\left\\{\\beta^{n}\\colon n\\in\\mathbb{Z}\\right\\}$ with some $\\beta\\in]0,1[$ ; or $f(L)=\\mathbb{R}_{+}^{*}$ . The case $f(L)=\\left\\{1\\right\\}$ being uninteresting let us discuss the other ones. <PARAGRAPH_SEPARATOR> 1. Let $f(L)=\\left\\{\\beta^{n}\\colon n\\in\\mathbb{Z}\\right\\}$ with some $\\beta\\in\\]0$ , 1[. We put $k_{n}:=[\\beta^{-n}]$ for all $n\\in\\mathbb{N}$ . Then  we  have lim $k_{n}/k_{n+1}=\\beta$ and lim $B^{n}\\mu^{*k_{n}}*\\varepsilon_{b_{n}}=\\mu$ for appropriate $b_{n}\\in V,n\\in\\mathbb{N}$ Hence $\\mu$ is operator-semistable [5, Definition 1]."
  },
  {
    "qid": "statistic-posneg-106-0-0",
    "gold_answer": "TRUE. The test statistic $W_{P}$ is constructed to be asymptotically normal with its asymptotic mean and variance not depending on the underlying distribution $F$. This property is achieved by using the consistent estimator $\\dot{\\sigma}_{n}^{2}=(1-\\hat{\\rho}_{G})/12$ for the asymptotic variance, where $\\hat{\\rho}_{G}$ is Spearman's rank correlation coefficient. The standardization by $\\sigma_{n}$ ensures that the limiting distribution is standard normal, making the test asymptotically distribution-free.",
    "question": "Is the modified Wilcoxon rank sum test $W_{P}$ asymptotically distribution-free under the null hypothesis $H_{0}\\colon F_{X}=F_{Y}$?",
    "question_context": "Modified Wilcoxon Rank Sum Test for Paired Data\n\nFor the general location problem, $F_{X}(x)=F_{Y}(\\dot{x}-\\theta)$ , the standard procedures are the paired $\\pmb{t}$ test, the sign test and the Wilcoxon signed rank test. All three of these procedures involve only the $_{n}$ differences $X_{i}-Y_{i}$ Asymptotically nonparametric procedures using the individual $2\\pi$ observations were developed by Hollander, Pledger $\\&$ Lin (1974) and Raviv (1978). In the present paper, another test will be proposed, the modifed Wilcoxon rank sum test, denoted by $W_{P}$ <PARAGRAPH_SEPARATOR> # 2. THE MODIFIED WILCOXON RANK SUM TEST <PARAGRAPH_SEPARATOR> Let $X_{(1)}<...<X_{(\\mathfrak{n})}$ be the order statistics of the $X$ sample. Let $R_{i}(i=1,\\ldots,n)$ be the rank of $X_{(i)}$ in the combined sample. Define <PARAGRAPH_SEPARATOR> $$ Z_{n}={\\sqrt{(2n+1)\\left[{\\frac{1}{n}}\\sum_{i=1}^{n}a_{n}{\\bigg(}{\\frac{R_{i}}{2n+1}}{\\bigg)}-\\int_{0}^{1}a(u)d F_{X}\\{H^{-1}(u)\\}\\right]}}, $$ <PARAGRAPH_SEPARATOR> where $a_{\\mathfrak{n}}(u)~(0<u<1)$ is a score function, such that $a_{n}(u)$ converges to $\\pmb{a}(\\pmb{u})$ a8 ${\\pmb n}\\rightarrow\\infty$ , <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{H(y)=\\frac{1}{2}F_{X}(y)+\\frac{1}{2}F_{Y}(y),\\quad H^{-1}(u)=\\operatorname*{inf}\\left\\{y\\colon H(y)\\geqslant u\\right\\}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> With $a_{n}(u)=u$ . the Wilcoxon score, and $F_{X}=F_{Y},Z_{n}$ is asymptotically normal with asymptotic mean equal to 0 and asymptotic variance given by $\\dot{\\sigma}^{2}=(1-\\dot{\\rho}_{G})/12$ ,where $\\rho_{G}=\\operatorname{corr}\\left\\{F_{X}(X_{1}),\\bar{F}_{Y}(Y_{1})\\right\\}$ <PARAGRAPH_SEPARATOR> Let $\\hat{\\rho}_{G}$ be Spearman's coefficient of rank correlation, that is, <PARAGRAPH_SEPARATOR> $$ \\hat{\\rho}_{G}=\\frac{12}{n(n^{2}-1)}\\sum_{i=1}^{n}S_{i}T_{i}-\\frac{3(n+1)}{n-1}, $$ <PARAGRAPH_SEPARATOR> where $S_{i}$ is the rank of $X_{i}$ in the $X$ sample and $\\pmb{T}_{i}$ is the rank of $Y_{i}$ in the $Y$ sample.Since $\\hat{\\rho}_{G}$ is a consistent estimator of $\\rho_{G}$ ,we have that $\\dot{\\sigma}_{n}^{2}=(1-\\hat{\\rho}_{G})/12$ is & consistent estimator of $\\sigma^{2}$ . The modifed Wilcoxon rank sum statistic under pairing will be defined as <PARAGRAPH_SEPARATOR> $$ W_{P}=\\sqrt{(2n+1)\\left\\{\\frac{1}{n(2n+1)}\\sum_{i=1}^{n}R_{i}-\\frac{1}{2}\\right\\}}\\Bigg/\\sigma_{n}. $$ <PARAGRAPH_SEPARATOR> The consistency of $\\sigma_{n}$ yields the asymptotic standard normality of $W_{P}$ . At significance leve! $\\pmb{\\alpha}$ $H_{0}\\colon F_{X}=F_{Y}$ will be rejected in favour of $H_{a}\\colon F_{X}\\leqslant F_{Y}$ , $F_{X}$ 丰 $F_{Y}$ when $W_{P}\\geqslant Z_{1-\\alpha},$ the ${\\mathbf{l}}-{\\pmb{\\alpha}}$ percentile of the standard normal. Since $W_{P}$ is asymptotically normal with asymptotic mean and variance not depending on F, the above procedure is asymptotically distribution free. <PARAGRAPH_SEPARATOR> An important special case of the above hypotheses is the location problem, that is, $F_{X}(x)={\\bar{F}}_{Y}(x-\\theta)$ ,,for all $_{x}$ where $\\theta>0$ .The consistency of $W_{P}$ for this setof alternatives is seen by examining <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{n\\to\\infty}\\mathrm{pr}_{\\theta}\\left(W_{p}\\geqslant Z_{1-\\alpha}\\right)=\\operatorname*{lim}_{n\\to\\infty}\\mathrm{pr}_{\\theta}\\big[Y_{n}\\geqslant\\big\\{\\sqrt{\\left(2n+1\\right)(\\frac{1}{2}-\\mu)+\\sigma_{n}Z_{1-\\alpha}}\\big\\}/\\sigma\\big], $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ Y_{n}=\\sqrt{(2n+1)\\left[\\Sigma\\:R_{i}/\\{n(2n+1)\\}-\\mu\\right]/\\sigma},\\quad\\mu=\\frac{1}{4}+\\frac{1}{2}\\int F_{Y}(x)\\:d F_{X}(x), $$ <PARAGRAPH_SEPARATOR> and $\\sigma^{2}$ is the asymptotic variance of $\\mathbf{Z}_{\\mathfrak{n}}$ <PARAGRAPH_SEPARATOR> # 3. PITMAN ASYMPTOTIC RELATIVE EFFICIENCY <PARAGRAPH_SEPARATOR> The effcacy of Raviv's test is the same as the modifed Wilcoxon rank sum test, namely $\\sqrt{6\\lceil f_{X}^{\\bar{2}}(x)d x/(1-\\rho_{G})^{\\frac{1}{2}}}$ . Thus, the Pitman effciency of the two tests is always one. However, in order to calculate the Pitman efficiencies of the paired $t$ test, $\\pmb{t}$ , the sign test, $\\boldsymbol{s}$ , and Wilcoxon signed rank test, $W^{+}$ , relative to the $W_{P}$ test, the underlying bivariate distribution function must be specified. <PARAGRAPH_SEPARATOR> Let $F$ be bivariate normal with mean $(\\mu+\\theta,\\mu)$ and covariance matrix <PARAGRAPH_SEPARATOR> $$ \\sigma^{2}{\\binom{1}{\\rho}}.\\mathbf{\\Pi}_{1}^{\\rho}). $$ <PARAGRAPH_SEPARATOR> The asymptotic relative efficiencies are as follows: <PARAGRAPH_SEPARATOR> $$ \\operatorname{ARE}{\\big(}W_{p},W^{+}{\\big)}=(1-\\rho){\\Bigg/}{\\Bigg\\{}1-{\\frac{6}{\\pi}}\\sin^{-1}(0{\\cdot}5\\rho){\\Bigg\\}}=h(\\rho), $$ <PARAGRAPH_SEPARATOR> $$ \\mathrm{\\bf~ARE}\\left(W_{p},S\\right)=\\frac{3}{2}h(\\rho),\\quad\\mathrm{\\bf~ARE}\\left(W_{p},t\\right)=\\frac{3}{\\pi}h(\\rho). $$ <PARAGRAPH_SEPARATOR> $\\pmb{F}$ is a Farlie-Gumbel-Morgenstern distribution function $x y\\{1+\\alpha(1-x)(1-y)\\}$ <PARAGRAPH_SEPARATOR> $$ {\\bf A R E}\\left(W_{p},S\\right)=\\frac{81}{2(3-\\alpha)\\left(3+\\alpha\\right)^{2}},\\quad{\\mathrm{ARE}}\\left(W_{p},W^{+}\\right)=\\frac{3(105)^{2}}{8(3-\\alpha)\\left(2\\alpha^{2}+7\\alpha+105\\right)}. $$ <PARAGRAPH_SEPARATOR> When the marginal distributions are normal, FGM $(N)$ , the effciencies are derived as <PARAGRAPH_SEPARATOR> $$ \\operatorname{aRE}{(W_{p},S)}={\\frac{9\\pi^{2}}{2(3-\\alpha)\\left\\{\\pi(1+\\alpha)-2x\\cos^{-1}{(\\frac{1}{3})}\\right\\}}},\\quad\\operatorname{ARE}{(W_{p},t)}={\\frac{9(\\pi-\\alpha)}{\\pi^{2}(3-c)}}. $$ <PARAGRAPH_SEPARATOR> When the marginal distributions are exponential, FGM $\\langle E\\rangle$ ,the efficiencies are <PARAGRAPH_SEPARATOR> $$ \\mathrm{\\bf~ARE}(W_{p},S)=\\frac{81}{2(3-\\alpha)(3+\\alpha)^{2}},\\quad\\mathrm{\\bf~ARE}(W_{p},W^{+})=\\frac{2(3)^{7}}{(3-\\alpha)(\\alpha^{2}+6\\alpha+27)^{2}}, $$ <PARAGRAPH_SEPARATOR> $$ \\operatorname{ARE}{(W_{p},t)}=9(4-\\alpha)/4(3-\\alpha). $$ <PARAGRAPH_SEPARATOR> The bivariate exponential distribution considered here is the Marshall-Olkin random shock model (Marshall $\\&$ Olkin, 1967). This distribution places mass on the line $X=Y$ and hence the distribution of $D=X=Y$ has a singularity at O. The efficacies of the sign test and the Wilcoxon signed rank test thus cannot be computed using standard techniques. However, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\mathrm{{ARE}}(W_{p},t)=\\{3(4\\lambda_{\\perp}+3\\lambda_{3})\\}/(4\\lambda_{1}+2\\lambda_{3}).}\\end{array} $$ <PARAGRAPH_SEPARATOR> The minimum and maximum value of the asymptotic relative efficiencies are contained in Table 1. The $W_{p}$ statistic is more effcient than the sign test for all distributions considered. The signed rank test appears to have lower efficiency than $W_{p}$ especially for distributions with nonsymmetric marginals. The relative efficiency of $\\boldsymbol{\\W_{p}}$ tothepaired $\\pmb{t}$ test when sampling from a bivariate normal distribution is at least 0·87. <PARAGRAPH_SEPARATOR> Table 1. Minimum and maximum asymptotic relative effciencies, ARE $(W_{p},.)$ <PARAGRAPH_SEPARATOR> <html><body><table><tr><td rowspan=\"2\">Test</td><td colspan=\"5\">Distribution</td></tr><tr><td>BVN</td><td>FGM (U)</td><td>FGM (N)</td><td>FGM (E)</td><td>BVEXP</td></tr><tr><td>Pairedttest</td><td>087 (p = 1)</td><td>一</td><td>0·94 (α=-1)</td><td>2·81 (α=-1)</td><td>3</td></tr><tr><td rowspan=\"3\">Sign test</td><td>0·97 (p = -052)</td><td></td><td>0·98 (α = 1)</td><td>3·38(α=1)</td><td>4·5</td></tr><tr><td>1·36 (p = 1)</td><td>1·27 (α = 1)</td><td>1·47 (α = 0459)</td><td>1.27 (α = 1)</td><td></td></tr><tr><td>1.52 (p = -0-52)</td><td>2.53(α=-1)</td><td>1.83 (α = - l)</td><td>2.53 (α=-l)</td><td></td></tr><tr><td>Wilcoxon</td><td>0.91 (p = 1)</td><td>1-07 (α=1)</td><td></td><td>1·89(α=087)</td><td></td></tr><tr><td>signed rank</td><td>101(p= -0-52)</td><td>1·16 (α = -1)</td><td></td><td>2·29 (α = -1)</td><td></td></tr><tr><td>RavivR test</td><td></td><td></td><td>1</td><td></td><td>1</td></tr></table></body></html>"
  },
  {
    "qid": "statistic-posneg-2079-1-1",
    "gold_answer": "FALSE. The paper explicitly states this is only a conjecture ('We have not been able to prove (or disprove) this conjecture') and provides a counterexample showing PQD of (X,Y) does not necessarily imply PQD of (X_{r:n}, Y_{[r:n]}).",
    "question": "Does the paper prove that the positive quadrant dependence (PQD) condition implies (X_{r:n}, Y_{[r:n]}) ≤uo (X_{r+1:n}, Y_{[r+1:n]})?",
    "question_context": "Stochastic Comparisons of Order Statistics and Positive Dependence\n\nThe paper examines stochastic comparisons of order statistics under various positive dependence conditions. It establishes that if Y is stochastically increasing in X with respect to the hazard rate order (≤hr), then (X_{r:n}, Y_{[r:n]}) ≤hr (X_{r+1:n}, Y_{[r+1:n]}) for r=1,2,…,n-1. The proof leverages properties of survival functions and total positivity of order 2 (TP2). A stronger condition, stochastic increasingness with respect to the likelihood ratio order (≤lr), yields a stronger conclusion (X_{r:n}, Y_{[r:n]}) ≤lr (X_{r+1:n}, Y_{[r+1:n]}). The paper also conjectures but does not prove results under the weaker positive quadrant dependence (PQD) condition."
  },
  {
    "qid": "statistic-posneg-899-0-0",
    "gold_answer": "TRUE. The estimator $\\overline{{y}}_{d}$ is unbiased for $\\overline{{Y}}$ because $E(\\overline{{y}}_{d}) = E(\\Sigma w_{h}\\overline{{y}}_{h}) = \\Sigma W_{h}E(\\overline{{y}}_{h}) = \\Sigma W_{h}\\widetilde{Y}_{h} = \\overline{{Y}}$. This follows from the properties of expectation and the fact that $E(w_{h}) = W_{h}$ and $E(\\overline{{y}}_{h}) = \\widetilde{Y}_{h}$.",
    "question": "Is the estimator $\\overline{{y}}_{d}=\\Sigma w_{h}\\overline{{y}}_{h}$ unbiased for $\\overline{{Y}}=\\Sigma W_{h}\\widetilde{Y}_{h}$ as stated in Theorem 1?",
    "question_context": "Optimal Stratified Sampling and Variance Estimation\n\nThroughout the paper, we make the customary assumption that $\\acute{n}$ is so large that $\\mathbf{pr}\\left(n_{h}^{\\prime}=0\\right)={\\mathbf{0}}$ for all $\\pmb{h}$ The following symbols all refer to stratum $h\\colon W_{h}=N_{h}/N,w_{h}=n_{h}^{\\prime}/n^{\\prime}$ $\\overline{{\\pmb{y}}}_{h}$ is the mean for $s(n_{h}),\\widetilde{Y}_{h}$ is the true mean, $S_{h}^{2}$ is the true variance and $s_{h}^{2}$ is the unbiased estimator of $S_{h}^{2}$ based on $s(n_{h})$ <PARAGRAPH_SEPARATOR> THEOREM 1. The estimator $\\overline{{y}}_{d}=\\Sigma w_{h}\\overline{{y}}_{h}$ i8 unbiased for $\\overline{{Y}}=\\Sigma W_{h}\\widetilde{Y}_{h}$ with variance <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\displaystyle{\\cal V}(\\overline{{y}}_{d})=\\Sigma\\frac{W_{h}S_{h}^{\\tt a}}{n^{\\prime}\\nu_{h}}-\\Sigma\\frac{E(w_{h}^{\\tt a})S_{h}^{\\tt a}}{N_{h}}+\\frac{g^{\\prime}B}{n^{\\prime}}}~}\\ {{\\displaystyle~=\\left(\\frac{1}{n^{\\prime}}-\\frac{1}{N}\\right)S^{\\tt a}+\\Sigma\\frac{W_{h}S_{h}^{\\tt a}}{n^{\\prime}}\\left(\\frac{1}{\\nu_{h}}-1\\right),}}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $B=\\Sigma W_{h}(\\overline{{Y}}_{h}-\\overline{{Y}})^{2},g^{\\prime}=(N-n^{\\prime})/(N-1)$ and $S^{\\mathfrak{z}}$ is the population variance."
  },
  {
    "qid": "statistic-posneg-525-1-0",
    "gold_answer": "TRUE. According to the formula provided in the context, when 𝕎_{d,j} is empty, the prediction is entirely considered as a false positive (FP), with TP_{H,k} = 0, TN_{H,k} = 0, and FN_{H,k} = 0. This is explicitly stated in the formula: FP_{H,k} = |hat{P}_k \\ R|, where R represents the set of true classes. This approach ensures that any prediction not matching any true path is penalized as a false positive, maintaining the integrity of the hierarchical classification evaluation.",
    "question": "In the hierarchical confusion matrix approach, when the set of true paths 𝕎_{d,j} is empty, the prediction is entirely considered as a false positive (FP). Is this statement correct based on the provided formulas?",
    "question_context": "Hierarchical Confusion Matrix for Classification Problems\n\nThe context discusses a hierarchical confusion matrix approach for evaluating classification problems, particularly focusing on hierarchical structures like trees (Γ=T) and directed acyclic graphs (Γ=DAG). It introduces formulas to calculate true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) in a hierarchical setting. The method involves mapping predictions to the most resembling true paths and aggregating results to form a confusion matrix. The approach is generalized to handle various hierarchical classification problems, including those with multiple paths and non-mandatory leaf node predictions (NMLNP)."
  },
  {
    "qid": "statistic-posneg-1368-2-0",
    "gold_answer": "FALSE. The context states that the total time falls quickly to the optimal value and then rises slowly, but it does not explicitly confirm that $3n/2$ iterations consistently outperform $3n/4$ in all scenarios. The recommendation is based on a general observation rather than a rigorous comparison of these specific values.",
    "question": "Is the recommendation to use $3n/2$ Bauer iterations instead of $3n/4$ justified based on the total computing time performance shown in Fig. 1?",
    "question_context": "Optimization of Bauer and Wilson Iterations in Algorithm Performance\n\nIn Table 1, we summarize the actual number of Wilson iterations performed when $I T E R1=50$ and $E P S=10^{-4}$ for various values of ITERO. When no Bauer iterations are requested, subroutine BAU ER produces the initial iterate recommended by Wilson (1979). The total computing time required by the algorithm for $I T E R1=50$ and $E P S^{'}=10^{-4}$ is plotted in Fig. 1. It can be seen that the total time falls quickly to the optimal value, and thereafter rises slowly. For this reason, we recommend the use of $3n/2$ rather than $3n/4$ Bauer iterations. <PARAGRAPH_SEPARATOR> The use of Bauer iteration to produce initial estimates for the Wilson algorithm was also found useful in diagnosing infeasibility. On the polynomial <PARAGRAPH_SEPARATOR> $$ A(x)=51-\\varepsilon+50(x+x^{-1})+49(x^{2}+x^{-2})+...+(x^{50}+x^{-50}) $$ <PARAGRAPH_SEPARATOR> the unaided Wilson algorithm detected the infeasibility (for $\\varepsilon=0{\\cdot}001$ )on the eighth iteration, whereas after 37 and 75 Bauer iterations infeasibility was already detected on the fifth and fourth iterations respectively."
  },
  {
    "qid": "statistic-posneg-2059-2-3",
    "gold_answer": "TRUE. The paper explicitly states that assuming b = ϕ/2 is sufficient to obtain a closed-form expression for the marginal likelihood. This constraint ensures that the sum of the signal and noise terms (both generalized Laplace) remains a generalized Laplace random vector.",
    "question": "Does the paper suggest that the constraint b = ϕ/2 is necessary for the marginal likelihood to have a closed-form expression?",
    "question_context": "Generalized Laplace Distribution in PPCA Framework\n\nThe paper discusses the multivariate generalized asymmetric Laplace distribution and its properties within the PPCA framework. Key properties include the characteristic function, density function, and the Gauss-Laplace representation. The distribution is used to model the signal and noise terms in the PPCA model, leading to a closed-form expression for the marginal likelihood."
  },
  {
    "qid": "statistic-posneg-493-5-0",
    "gold_answer": "TRUE. The roughness penalty $\\beta^{\\prime}\\pmb{\\Sigma}\\beta$ corresponds to the prior $\\beta\\sim N(0,\\pmb{\\Sigma}^{-})$, where $\\pmb{\\Sigma}^{-}$ is the generalized inverse of $\\Sigma$. This is derived from the penalized least squares problem $({\\bf y}-{\\bf X}\\beta)^{\\prime}({\\bf y}-{\\bf X}\\beta)+\\frac{1}{c}\\beta^{\\prime}\\Sigma\\beta$, which is equivalent to the Bayesian formulation with the specified prior. The matrix $\\Sigma$ encodes the roughness penalty, and the prior distribution reflects the penalty's effect on the coefficients.",
    "question": "The roughness penalty $\\beta^{\\prime}\\pmb{\\Sigma}\\beta$ in smoothing splines is equivalent to placing a prior $\\beta\\sim N(0,\\pmb{\\Sigma}^{-})$ on the regression coefficients. True or False?",
    "question_context": "Bayesian Variable Selection with Different Penalty Functions in Nonparametric Regression\n\nThe paper discusses the application of a computational framework to different priors for regression coefficients, corresponding to different penalty functions. Specifically, it compares the standard penalized least squares problem with a roughness penalty used in smoothing splines. The standard problem is formulated as $({\\bf y}-{\\bf X}\\beta)^{\\prime}({\\bf y}-{\\bf X}\\beta)+\\frac{1}{c}\\beta^{\\prime}\\beta$, while the roughness penalty is written as $\\beta^{\\prime}\\pmb{\\Sigma}\\beta$ for some symmetric positive-semidefinite matrix $\\Sigma$, corresponding to the prior $\\beta\\sim N(0,\\pmb{\\Sigma}^{-})$. The paper also discusses spatial adaptation in the regression function, showing how the Bayesian framework can allow for spatial variability."
  },
  {
    "qid": "statistic-posneg-1099-0-1",
    "gold_answer": "FALSE. The condition $|\\mathcal{S}_{t}-b_{n}|_{2}^{2}<100$ defines an exposure area as points within a squared Euclidean distance of 100 from $b_{n}$, which corresponds to a circle with radius 10 only if the distance metric is Euclidean ($|\\cdot|_{2}$). However, the context does not specify the spatial units or confirm the geometric interpretation, making the radius claim unverifiable. The inequality strictly defines a region where the squared distance is less than 100, not necessarily a circle.",
    "question": "Does the condition $|\\mathcal{S}_{t}-b_{n}|_{2}^{2}<100$ in the definition of $e_{n}$ and $e_{n}^{\\iota,\\alpha,\\nu,\\zeta}$ imply that the exposure area is a circle with radius 10 units centered at $b_{n}$?",
    "question_context": "Statistical Inference for Trajectory Imputation Methods in Exposure Analysis\n\nA related evaluation relies on calculating the total time spent in the exposure area according to each imputation and comparing it with the amount of time spent in the area by the true trajectory. Consider trajectory $\\tau(m_{n})$ and the corresponding imputation $\\omega_{n}^{\\imath,\\alpha,\\nu,\\zeta}$ , where in addition to symbols used before we use $\\iota=1,...,N_{\\mathrm{imp}}$ as the index of the imputation. Moreover, we consider only $\\alpha\\in\\{0.2,0.5,0.8\\}$ . We define the true exposure time $e_{n}$ as the number of time periods during which the individual performing movement $m_{n}$ is inside the exposure area, i.e. $e_{n}=\\#\\{\\mathcal{S}_{t}\\in\\tau(m_{n}):|\\mathcal{S}_{t}-b_{n}|_{2}^{2}<100\\}$. Similarly for each $\\omega_{n}^{\\imath,\\alpha,\\nu,\\zeta}$ , we write $e_{n}^{\\iota,\\alpha,\\nu,\\zeta}=\\#\\{\\mathcal{S}_{t}\\in\\omega_{n}:|\\mathcal{S}_{t}-b_{n}|_{2}^{2}<100\\}$ . We then calculate $d_{n}^{\\iota,\\alpha,\\nu,\\zeta}=e_{n}-e_{n}^{\\iota,\\alpha,\\nu,\\zeta}$ , the difference between the true length of exposure and the length resulting from the imputed trajectory. For each imputation method and missing percentage, we also calculate the grand mean of these differences as $\\bar{d}^{a,\\nu,\\zeta}=\\frac{1}{N_{\\mathrm{imp}}N_{\\mathrm{tr}}}\\sum_{n=1}^{N_{\\mathrm{tr}}}\\sum_{\\imath=1}^{N_{\\mathrm{imp}}}d_{n}^{\\imath,\\alpha,\\nu,\\zeta}$."
  },
  {
    "qid": "statistic-posneg-753-0-0",
    "gold_answer": "FALSE. The paper explicitly states that the AIC does not necessarily give the best results among typical $\\phi\\mathrm{ICs}$, and the performance depends on the specific $\\phi$-divergence used and the context of the model selection problem. The modified versions ($\\mathsf{M}\\phi\\mathsf{IC}$ and $\\mathsf{M}^{*}\\phi\\mathsf{IC}$) are introduced to improve performance by correcting higher-order biases.",
    "question": "Is it true that the $\\phi$ information criterion ($\\phi\\mathrm{IC}$) always performs better than AIC in model selection?",
    "question_context": "Phi-Divergence Information Criteria for Model Selection\n\nThe paper discusses the $\\phi$ information criterion ($\\phi\\mathrm{IC}$ or PIC) for model selection, comparing it to AIC and introducing modified versions ($\\mathsf{M}\\phi\\mathsf{IC}$ and $\\mathsf{M}^{*}\\phi\\mathsf{IC}$) to correct higher-order biases. The $\\phi$-divergence statistic is defined for $K$-category multinomial data, with various forms depending on the convex function $\\phi(x)$. The paper explores the properties and performance of these criteria in simulations, focusing on their bias corrections and model selection accuracy."
  },
  {
    "qid": "statistic-posneg-277-0-0",
    "gold_answer": "FALSE. While the statement about the form of $\\mu(X)$ is true for the logistic single-index model, the conditional independence $Y \\perp X \\mid \\beta^{\\top}X$ is not directly implied by this specification alone. The conditional independence is a stronger condition that defines the central subspace $S_{Y\\mid X} = \\operatorname{span}(\\beta)$, which is assumed in the context but not guaranteed by the logistic link function form. The logistic form specifies the conditional mean of $Y$ given $X$, but additional assumptions about the error structure are needed to ensure full conditional independence.",
    "question": "In the logistic single-index model, the probability $\\mu(X)$ is correctly specified as $\\mu(X)=\\frac{1}{1+\\exp\\{-h(\\beta^{\\top}X)\\}}$. Is this statement true, and does it imply that the response $Y$ is conditionally independent of $X$ given $\\beta^{\\top}X$?",
    "question_context": "Sufficient Dimension Reduction in Single-Index Models\n\nRegression analysis is widely used to study the relationship between a response variable $Y$ and a $p\\times1$ predictor vector $X$ . In practice, this relationship often depends solely on a few linear combinations $\\beta^{\\intercal}\\boldsymbol{X}$ of $X$ , where $\\beta$ is a $p\\times d$ matrix with $d<p$ . For instance, a single-index model (Hardle et al., 1993) involves only one linear combination of the predictors, and has the form $$\\boldsymbol{Y}=f(\\boldsymbol{\\beta}^{\\intercal}\\boldsymbol{X})+\\boldsymbol\\varepsilon,$$ where $d=1$ , $\\|\\beta\\|=1$ , $f$ is an unknown link function, and $\\varepsilon$ is an error term independent of $X$ with its distribution unspecified. Other well-known models with $d=1$ include the heteroscedastic model, $$Y=f(\\beta^{\\intercal}X)+g(\\beta^{\\intercal}X)\\varepsilon,$$ and the logistic single-index model, $$Y\\mid X\\sim\\operatorname{Ber}\\{\\mu(X)\\},\\mathrm{with}\\mu(X)=\\frac{1}{1+\\exp\\{-h(\\beta^{\\top}X)\\}},$$ where $g$ and $h$ are unknown smooth link functions. Sufficient dimension reduction methods estimate linear combinations $\\beta^{\\intercal}\\boldsymbol{X}$ that convey full regression information. The methodology stems from considering dimension reduction subspaces $S\\subset\\mathbb{R}^{p}$ , which are subspaces with the property $Y\\perp X\\mid P_{S}X$ , where $P_{(\\cdot)}$ denotes a projection operator with respect to the standard inner product. Under mild conditions (Cook, 1996) the intersection of all dimension reduction subspaces is itself a dimension reduction subspace, and is then called the central subspace $S_{Y\\mid X}$ (Cook, 1998, p.105). By definition, $S_{Y\\mid X}$ is a parsimonious population parameter that contains full information about the regression of $Y$ on $X$ , and thus is our main object of interest. We assume the existence of $S_{Y\\mid X}$ throughout this article, and let $d=\\dim(S_{Y\\mid X})$ . For models (1), (2) and (3), $S_{Y\\mid X}=\\operatorname{span}(\\beta)$ with $d=1$ . Commonly used pioneering model-free methods for estimating a basis of $S_{Y\\mid X}$ include ordinary least squares (Li & Duan, 1989) when $d=1$ , sliced inverse regression (Li, 1991) and sliced average variance estimation (Cook & Weisberg, 1991). Unfortunately, all of those methods require the number of observations $n$ to be greater than $p$ , and this limits applicability. By contrast, the method of partial least squares (Wold, 1975) does not have this limitation. Background about partial least squares estimator can be found in Helland (1988, 1990, 1992), Næs & Helland (1993), Garthwaite (1994) and Stone & Brooks (1990)."
  },
  {
    "qid": "statistic-posneg-910-0-0",
    "gold_answer": "TRUE. The context explicitly states that to ensure the Markov property, $G_{i j}\\equiv0$ unless pixels $i$ and $j$ are neighbors. This condition is derived from the Hammersley-Clifford theorem, which establishes the equivalence between Markov random fields and Gibbs distributions with local potentials. The pairwise interaction form $p(x)\\propto\\exp{\\biggl\\{}\\sum_{1\\leqslant i\\leqslant n}G_{i}(x_{i})+\\sum_{1\\leqslant i<j\\leqslant n}G_{i j}(x_{i},x_{j}){\\biggr\\}}$ with $G_{i j}=0$ for non-neighbors ensures that the conditional distribution of any pixel depends only on its neighbors, satisfying the Markov property.",
    "question": "In the pairwise interaction Markov random field model, setting $G_{i j}\\equiv0$ for non-neighboring pixels $i$ and $j$ is sufficient to ensure the Markov property. Is this statement true based on the provided context?",
    "question_context": "Pairwise Interaction Markov Random Fields for Image Segmentation\n\nThis section is concerned with the specification of $\\{p(x)\\}$ for three different types of scenes: those with unordered colours, as in Section 3; grey-level scenes, either with smooth variation or with occasional abrupt changes in level; and those with continuous pixel intensities, either smoothly varying or with substantial disparities. Although we consider each at a rather basic level, even then, some of our comments are speculative. More intricate modelling, which takes account of special structures in the true scene, is a topic which requires much further research. <PARAGRAPH_SEPARATOR> We shall assume that pixels have prescribed neighbours, satisfying the simple symmetry condition and chosen with proper regard to spatial contiguity: in fact, although not necessary, we shall have in mind a rectangular array of pixels with at least a second-order neighbour. hood. We concentrate on pairwise interactions: $\\{p(x)\\}$ is called a pairwise interaction M.r.f. if, for any $x$ in the sample space $\\pmb{\\Omega}$ <PARAGRAPH_SEPARATOR> $$ p(x)\\propto\\exp{\\biggl\\{}\\sum_{1\\leqslant i\\leqslant n}G_{i}(x_{i})+\\sum_{1\\leqslant i<j\\leqslant n}G_{i j}(x_{i},x_{j}){\\biggr\\}}, $$ <PARAGRAPH_SEPARATOR> where, to ensure the Markov property, $G_{i j}\\equiv0$ unlesspixels $i$ and $j$ are neighbours but otherwisethe $G$ -functions are arbitrary (cf. Besag, 1974, with a trivial change of notation). <PARAGRAPH_SEPARATOR> # 4.1. Discrete colours <PARAGRAPH_SEPARATOR> As a special case of (9), we consider $c$ -colour distributions with (Besag, 1976; Strauss, 1977) <PARAGRAPH_SEPARATOR> $$ p(x)\\propto\\exp{\\left(\\sum_{1\\leqslant k\\leqslant c}\\alpha_{k}n_{k}-\\sum_{1\\leqslant k<l\\leqslant c}\\beta_{k l}n_{k l}\\right)}, $$ <PARAGRAPH_SEPARATOR> where $\\pmb{n_{k}}$ denotes the number of pixels coloured $k$ and $n_{k l}$ the number of distinct neighbour pairs coloured $(k,l)$ ; note that, for the moment, all neighbour pairs are treated equally. The s and $\\beta\\mathbf{s}$ are arbitrary parameters, though we shall be concerned only with $\\beta_{k l}>0$ which discourages colours $k$ and $l$ from appearing at neighbouring pixels. Further assumptions about the $\\beta\\mathbf{s}$ will be made below, according to the type of scene. Note there is a redundancy among the αs, since $n_{1}+...+n_{c}=\\mathfrak{n}$ , but we retain the symmetric formulation for neater exposition. <PARAGRAPH_SEPARATOR> By considering any two realizations which differ only at pixel $i,$ it follows that the conditional probability of colour $k$ occurring there, given the colours of all other pixels is <PARAGRAPH_SEPARATOR> $$ p_{i}(k|\\cdot)\\propto\\exp\\{\\alpha_{k}-\\sum_{l\\neq k}\\beta_{k l}u_{i}(l)\\}, $$ <PARAGRAPH_SEPARATOR> where $\\beta_{k l}\\equiv\\beta_{l k}$ defines $\\beta_{k l}$ for $k>l,$ and $u_{i}(l)$ denotes the number of neighbours of pixel i having colour $l$ : incidentally, this confirms the Markov property of (10). The corresponding conditional probabilities for small sets of pixels, rather than a single pixel, can be found in a similar manner. However, marginal probabilities are generally intractable and the same is true even for the normalizing constant in (10); indeed, this has been the source of much anguish in statistical physics over many decades. Analytical results seem available only for the Ising model, which is the (infinite) rectangular lattice version of (10), with only two colours and firstorder neighbours. The physicists’ interest in such models centres on their capacity to produce positive correlations between the colours of pixels infinitely far apart, and hence infinite patches of a single colour. Fortunately, the normalizing constant is not required for maximum probability (m.a.p.) estimation or for $I C M$ ; moreover, we have designed the latter to be unresponsive to the large-scale characteristics of $\\{p(x)\\}$ <PARAGRAPH_SEPARATOR> A simple extension of (10) is to allow differing interaction parameters $\\beta_{k l}$ to be associated with different classes of neighbour pairs. Thus, with square pixels and a second-order M.r.f., $\\beta_{k l}n_{k l}$ in (10) can be replaced by $\\beta_{k l}^{\\prime}n_{k l}^{\\prime}+\\beta_{k l}^{\\prime\\prime}n_{k l}^{\\prime\\prime}$ ,where $n_{\\mathbf{k}\\mathbf{l}}^{\\prime}$ and $n_{k l}^{\\prime\\prime}$ are the respective numbers of first- and second-order neighbour pairs coloured $(k,l)$ ; in this context, Geman and McClure (1985) downweight diagonal adjacencies by a factor of $\\surd2$ . A similar modification can be made for $N-S$ and $E-W$ adjacencies if pixels are rectangular rather than square. In the sequel, we assume that any such extensions would be incorporated when necessary: the corresponding amendments to (11) are immediate. <PARAGRAPH_SEPARATOR> # 4.1.1. Unordered colours <PARAGRAPH_SEPARATOR> A simple model for unordered colours is obtained by taking a common $\\beta$ in (10). Then (11 becomes <PARAGRAPH_SEPARATOR> $$ p_{i}(k|\\cdot)\\propto\\exp\\{\\alpha_{k}+\\beta u_{i}(k)\\}, $$ <PARAGRAPH_SEPARATOR> since $u_{i}(1)+\\ldots+u_{i}(c)$ is the total number of neighbours of pixel i. Note that (12) implies that the conditional odds, in favour of colour $k,$ . depend only on the number of like-coloured neighbours, not that this is true of the corresponding probability. A final simplification occurs if the colours are interchangeable, in which case the s must also be equal and can be set to zero because of the inherent redundancy: this yields the distribution (6) used in Section 3. <PARAGRAPH_SEPARATOR> # 4.1.2. Excluded adjacencies <PARAGRAPH_SEPARATOR> A different situation arises if it is known that certain colours, say colours 1 and 2, cannot appear on neighbouring pixels in the true scene. This can be enforced in reconstructions by letting $\\beta_{12}\\to\\infty$ in (10), by which we mean $\\beta_{12}n_{12}$ is zero if $n_{12}=0$ but is otherwise infinite. Such fields strictly contravene the positivity condition in the Hammersley-Clifford theorem and, in using $I C M$ synchronous updating should be avoided and the ostensibly infinite $\\beta\\mathbf{s}$ gradually increased cycle by cycle, particularly if the initial estimate is infeasible."
  },
  {
    "qid": "statistic-posneg-322-0-4",
    "gold_answer": "TRUE. The empirical estimator of the conditional copula C_y(u) is constructed using Nadaraya-Watson weights w_i,n and indicator functions based on the empirical marginal distributions F_j,n,y(X_ij). This non-parametric approach leverages kernel smoothing to account for the functional covariate Y.",
    "question": "Is the empirical estimator of the conditional copula C_y(u) constructed using Nadaraya-Watson weights and indicator functions based on the empirical marginal distributions?",
    "question_context": "Estimation of Extreme Multivariate Expectiles with Functional Covariates\n\nThe paper discusses the estimation of multivariate expectiles with functional covariates, focusing on conditional marginal distributions, quantile functions, and tail dependence functions. It introduces a framework for estimating extreme multivariate expectiles using conditional copulas and tail dependence functions, with applications in risk analysis and capital allocation."
  },
  {
    "qid": "statistic-posneg-1150-0-1",
    "gold_answer": "FALSE. The data in Table I shows that the efficiency of $\\tilde{\\sigma}$ relative to $\\sigma^{*}$ decreases as the amount of censoring increases. For example, when $r1 = 0$ and $r2 = 0$ (no censoring), the efficiency is 99.91%, but when $r1 = 1$ and $r2 = 1$, the efficiency drops to 69.96%. This indicates that the alternative estimator becomes less efficient relative to the generalized least squares estimator as censoring increases.",
    "question": "The efficiency of the alternative estimator $\\tilde{\\sigma}$ relative to the generalized least squares estimator $\\sigma^{*}$ increases with the amount of censoring. Is this statement supported by the data in Table I?",
    "question_context": "Estimation of Standard Deviation in Doubly Censored Normal Samples\n\nThe variance of the estimator involves the variance-covariance matrix. However, sufficiently accurate estimates of the variance may be obtained using the simple approximation procedures given in $\\S4$ for complete samples and in $\\S6$ for censored samples. <PARAGRAPH_SEPARATOR> # 2. DERIVATION OF THE ALTERNATIVE ESTIMATOR <PARAGRAPH_SEPARATOR> Suppose that the observations remaining after censoring are ordered and denoted by <PARAGRAPH_SEPARATOR> $$ y_{r_{1}+1}\\leqslant\\ldots\\leqslant y_{n-r_{2}}. $$ <PARAGRAPH_SEPARATOR> Let y be the vector of these ordered observations and let $\\mathbf{x}$ be the vector of the standardized values $(y_{i}-\\mu)/\\sigma(i=r_{1}+1,...,n-r_{2})$ <PARAGRAPH_SEPARATOR> $$ \\mathbf{m}=E(\\mathbf{x}),\\qquadV=E\\{(\\mathbf{x}-\\mathbf{m})(\\mathbf{x}-\\mathbf{m})^{\\prime}\\} $$ <PARAGRAPH_SEPARATOR> are the vector of normal scores and the variance-covariance matrix of the standardized ordered observations, respectively. The generalized least squares estimate of $\\pmb{\\sigma}$ is givenby <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\sigma^{*}=1^{\\prime}\\bar{\\mathbf{T}}\\mathbf{y},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where ${\\bf1^{\\prime}}$ is a row vector of ones and <PARAGRAPH_SEPARATOR> Here <PARAGRAPH_SEPARATOR> $$ {\\bf\\cal T}={\\bf\\cal V}^{-1}(1\\ m^{\\prime}-\\ m1^{\\prime}){\\bf\\cal V}^{-1}/\\Delta. $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\Delta=(\\mathbf{1}^{\\prime}\\mathbf{V}^{-1}\\mathbf{1})(\\mathbf{m}^{\\prime}\\mathbf{V}^{-1}\\mathbf{m})-(\\mathbf{1}^{\\prime}\\mathbf{V}^{-1}\\mathbf{m})^{2}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> For further comment, see Lloyd (1952). <PARAGRAPH_SEPARATOR> The variance of this estimator is <PARAGRAPH_SEPARATOR> $$ \\operatorname{var}\\left(\\sigma^{*}\\right)=\\left(1^{\\prime}\\mathbf{V}^{-1}\\mathbf{1}\\right)\\sigma^{2}/\\Delta. $$ <PARAGRAPH_SEPARATOR> The coefficients 1'T and the variance $(\\mathbf{1}^{\\prime}\\mathbf{V}^{-1}\\mathbf{1})/\\Delta$ are the values tabulated by Sarhan and Greenberg. Gupta's alternative estimate is obtained from these equations by replacing $\\mathbf{v-1}$ by the identity matrix to give <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> Also <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{\\widetilde{\\sigma}=1^{\\prime}(1\\mathbf{m}^{\\prime}-\\mathbf{m}\\mathbf{1}^{\\prime})\\mathbf{y}/\\widetilde{\\Delta},}}\\ {{\\widetilde{\\Delta}=(\\mathbf{1}^{\\prime}\\mathbf{1})(\\mathbf{m}^{\\prime}\\mathbf{m})-(\\mathbf{1}^{\\prime}\\mathbf{m})^{2}.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> $$ \\operatorname{var}\\left({\\tilde{\\sigma}}\\right)=1^{\\prime}(1\\mathbf{m}^{\\prime}-\\mathbf{m}\\mathbf{1}^{\\prime})\\nabla(\\mathbf{m}\\mathbf{1}^{\\prime}-\\mathbf{1}\\mathbf{m}^{\\prime})\\mathbf{1}\\sigma^{2}/{\\tilde{\\Delta}}^{2}. $$ <PARAGRAPH_SEPARATOR> The efficiency of this alternative estimator relative to the most effcient linear estimator, i.e. var $({\\sigma}^{*})/\\operatorname{var}\\left(\\tilde{\\sigma}\\right)$ , is given in Table I for small amounts of censoring for samples of size 20."
  },
  {
    "qid": "statistic-posneg-1642-0-0",
    "gold_answer": "TRUE. The formula (1-p^T)/(1-p)p^T is derived from the expected number of trials until the first run of T successes in a Bernoulli process. When T=1, the formula simplifies to (1-p)/(1-p)p = 1/p, which is the expected number of trials until the first success in a Bernoulli process with success probability p.",
    "question": "Is the claim that the average number of samples before stoppage is (1-p^T)/(1-p)p^T correct, and does it reduce to 1/p when T=1?",
    "question_context": "Quality Control Runs Analysis: Average Run Length and Detection Probability\n\nThe paper examines quality control procedures where production is stopped when T successive means fall beyond a specified control limit. The average number of samples before stoppage is given by (1-p^T)/(1-p)p^T, and for equivalent schemes across different T values, the condition (1-p_T^T)/(1-p_T)p_T^T = 1/p is used. The paper also discusses the probability of detecting a mean shift using runs of T successive means, with approximations for the probability of no runs of length T given by q_m ∼ (1-μx)/(T+1-μx) * 1/x^(m+1), where x is a root of (1-μ)s(1+μs+...+μ^(T-1)s^(T-1))=1."
  },
  {
    "qid": "statistic-posneg-19-0-3",
    "gold_answer": "FALSE. The fourth moment of $u$ is more sensitive to changes in $\\rho$ than the second moment, as indicated by the more complex dependence on $\\rho$ in the formula for $E(u^{4})$. This increased sensitivity affects the kurtosis and the tail behavior of the distribution of $u$.",
    "question": "Is the fourth moment of $u$ less sensitive to changes in $\\rho$ compared to the second moment?",
    "question_context": "Test for Equality of Means with Missing Data and Correlated Variates\n\nLet us consider the usual paired-data situation in which observations are obtained under two conditions from the same sampling unit. The model for each unit's observations will be the bivariate normal distribution with mean vector $(\\mu_{1},\\mu_{\\mathfrak{L}})$ and a covariance matrix with common variance ${\\boldsymbol{\\sigma^{2}}}$ and correlation coefficient $\\rho\\geqslant0$ . From this distribution, $\\pmb{m}$ independent pairs of observations $(x_{1},y_{1}),...,(x_{m},y_{m})$ have been drawn, while in addition, $N-m$ observations $\\pmb{y}_{m+1},...,\\pmb{y}_{N}$ are available on the second variate alone. We wish to use all of these data to test the hypothesis $H_{0}\\colon\\mu_{1}=\\mu_{2}$ against the alternative of unequal means."
  },
  {
    "qid": "statistic-posneg-1477-0-2",
    "gold_answer": "TRUE. The estimators are defined as: $$\\hat{\\alpha}=\\sum a_{n i}\\frac{\\delta_{i}Z_{i}}{1-\\hat{G}_{k}(Z_{i}-)}I_{[Z_{i}\\leqslant T^{n}]},$$ $$\\hat{\\beta}=\\sum b_{n i}\\frac{\\delta_{i}Z_{i}}{1-\\hat{G}_{k}(Z_{i}-)}I_{[Z_{i}\\leqslant T^{n}]},$$ where the term $(1-\\hat{G}_{k}(Z_{i}-))^{-1}$ represents inverse probability weighting to account for censoring. This weighting adjusts for the probability of being uncensored at each observed time point.",
    "question": "Is it correct that the estimators $\\hat{\\alpha}$ and $\\hat{\\beta}$ in the censored regression model incorporate inverse probability weighting using the Kaplan-Meier estimator of the censoring distribution?",
    "question_context": "Censored Linear Regression: Kaplan-Meier Estimation and Martingale Theory\n\nThe paper discusses the use of Kaplan-Meier estimators and martingale theory in the context of censored linear regression. Key formulas include the survival function representation, martingale processes for censored and uncensored data, and the Kaplan-Meier estimator for the censoring distribution. The paper also introduces estimators for regression coefficients in the presence of censoring and discusses their asymptotic properties."
  },
  {
    "qid": "statistic-posneg-276-0-3",
    "gold_answer": "TRUE. The context references Lemma A1 of Lu et al. (2007) to justify the approximation property of $\\Theta_n$. The convergence rate $O(n^{-r\\nu})$ is derived from the I-spline approximation order ($r = m + \\eta$) and the knot placement condition ($k_n = o(n^\\nu)$), which are standard in sieve estimation theory.",
    "question": "Does the sieve space $\\Theta_n$ approximation guarantee that any $\\theta \\in \\Theta$ can be approximated by $\\theta_n \\in \\Theta_n$ with convergence rate $O(n^{-r\\nu})$, as claimed in the context?",
    "question_context": "Copula-based Cox Regression for Interval-Censored Data\n\nConsider a failure time study that involves $n$ independent subjects and yields only interval-censored failure time data. For subject $i,$ let $T_{i}$ denote the failure time of interest and $Z_{i}$ the vector of covariates, and suppose that there exists an array of underlying observation times $E_{i}=\\{E_{i,j}:j=0,1,2,...\\}$ such that $E_{i,j}<E_{i,j^{\\prime}}$ for any $j<j^{\\prime}$ . Define $W_{i,j}=E_{i,j}-E_{i,j-1}$ , $j=$ $1,2,\\ldots$ , the gap times of the observation process with $E_{i0}=0$ and assume that given $Z_{i}$ , the $W_{i,j}$ s follow the same distribution. Furthermore assume that $T_{i}$ may depend on the underlying observation process through the gap times and that given $Z_{i}$ , $\\{(T_{i},W_{i,j})$ , $j=1,2,\\ldots\\}$ follow the same joint distribution. In the following, for each i, define $(\\tilde{L}_{i},\\tilde{R}_{i}]$ to be the random interval among $(0,E_{i,1}]$ , $\\left(E_{i,1},E_{i,2}\\right],$ , . . . that contains $T_{i}$ . Note that the mechanism behind the observation process and censoring intervals here is similar to that behind the mixed case $k$ interval-censored data (Schick and Yu, 2000; Sun, 2006). <PARAGRAPH_SEPARATOR> In practice such as in medical follow-up studies, there usually exists an administrative censoring time $\\zeta_{i}$ beyond which the observation process is no longer available. For each i, define $L_{i}=\\operatorname*{max}\\{E_{i,j}:E_{i,j}<\\operatorname*{min}(\\zeta_{i},T_{i}),j=0,1,2,\\ldots\\}$ and $R_{i}=\\operatorname*{min}\\{E_{i,j}:E_{i,j}>L_{i}$ , $j=1,2,...\\}.$ Also, define $\\delta_{i}=1$ if $R_{i}\\leq\\zeta_{i}$ and $\\delta_{i}=0$ otherwise. Note that when $\\delta_{i}=0$ , Ri is not observed but right-censored at $\\zeta_{i}$ and $T_{i}$ is right-censored. For ease of explanation, we define $(\\tilde{L}_{i},\\tilde{R}_{i}]$ as covering interval and $(L_{i},R_{i}]$ as ‘observed interval’ (even though $R_{i}$ is not directly observed for right-censored subjects). And define $W_{i}=R_{i}-L_{i}$ as the ‘observed interval length’. It is easy to see that if $\\delta_{i}=1,W_{i}$ is exactly observed and $T_{i}$ lies in the observed interval. For subjects with $\\delta_{i}=0$ , note that $L_{i}$ is not necessarily equal to ${\\widetilde{\\cal L}}_{i}$ and also $(L_{i},R_{i}]$ may not contain $T_{i}$ . In fact, since $R_{i}$ is right-censored at $\\zeta_{i}$ , we only observe that $T_{i}>L_{i}$ and $W_{i}>\\zeta_{i}-L_{i}$ . An illustrative example for this situation with $\\delta_{i}=0$ is given in Fig. 1. Assume that given $Z_{i}$ and $T_{i}$ belonging to the censoring interval, the distribution of $T_{i}$ depends on the censoring interval only through its length. That is, we have <PARAGRAPH_SEPARATOR> Then the likelihood function has the form <PARAGRAPH_SEPARATOR> $$ \\prod_{i=1}^{n}[P r(L_{i}=l_{i}<T_{i}\\leq R_{i}=r_{i},W_{i}=w_{i})]^{\\delta_{i}}[P r(T_{i}>L_{i}=l_{i},W_{i}>\\zeta_{i}-l_{i})]^{1-\\delta_{i}}. $$ <PARAGRAPH_SEPARATOR> Let $K$ denote the joint distribution of $T$ and $W$ given Z. Then it is well-known (Nelsen, 2006) that there exists a copula function $C_{\\alpha}(u,v)$ defined on $I^{2}=[0,1]\\times[0,1]$ such that <PARAGRAPH_SEPARATOR> $$ K(t,w)=C_{\\alpha}(F_{T}(t),F_{W}(w)),\\quad t\\geq0,w\\geq0. $$ <PARAGRAPH_SEPARATOR> In the above, $F_{T}$ and $F_{W}$ denote the marginal distributions of the $T$ and $W$ given Z, respectively, $\\alpha$ , usually referred to as the association parameter, represents the relationship between $T$ and $W$ , and $C_{\\alpha}(u,0)=C_{\\alpha}(0,v)=0$ , $C_{\\alpha}(u,1)=u$ and $C_{\\alpha}(1,\\upsilon)=\\upsilon$ . Define $m_{\\alpha}(F_{T}(t),F_{W}(w))=P(T\\leq t|W=w,Z)$ . Then we have <PARAGRAPH_SEPARATOR> $$ m_{\\alpha}(F_{T}(t),F_{W}(w))=\\left.\\frac{\\partial C_{\\alpha}(u,v)}{\\partial v}\\right|_{u=F_{T}(t),v=F_{W}(w)}. $$ <PARAGRAPH_SEPARATOR> For the covariate effects, we will consider the following marginal Cox hazard models <PARAGRAPH_SEPARATOR> $$ \\lambda_{T}(t|Z_{i})=\\lambda_{1}(t)\\exp(Z_{i}^{\\prime}\\beta),\\qquad\\lambda_{W}(w|Z_{i})=\\lambda_{2}(w)\\exp(Z_{i}^{\\prime}\\gamma), $$ <PARAGRAPH_SEPARATOR> for $T_{i}$ and $W_{i}$ given $Z_{i}$ , respectively. Here $\\lambda_{1}(t)$ and $\\lambda_{2}(w)$ are unknown baseline hazard functions and $\\beta$ and $\\gamma$ denote vectors of regression parameters. <PARAGRAPH_SEPARATOR> ![](images/1e0ec046c2c74a558ef2a3e2e073dd51635f49c420d94c077133f61fc892a56d.jpg)  Fig. 1. An illustrative example of right-censored observations. <PARAGRAPH_SEPARATOR> It is worth noting that the copula model formulation described above has been commonly used in multivariate failure time data analysis (Hougaard, 2000) as well as the analysis of univariate failure time data in the presence of informative censoring. For example, by using the formulation, Zheng and Klein (1995) discussed the estimation of a marginal survival function based on right-censored data in the presence of dependent competing risks, and Chen (2010) considered the regression analysis of the same data. In particular, they showed that without prior or extra information, the association parameter $\\alpha$ is not identifiable given the copula function. In the following, following these authors, we will assume that both the copula function and $\\alpha$ are known and our main goal is to estimate regression parameters $\\beta$ and $\\gamma$ . <PARAGRAPH_SEPARATOR> For estimation of $\\beta$ and $\\gamma$ , first note that it follows from the models defined above that we have $F_{T_{i}}(t)=F_{T}(t|Z_{i})=$ $1-\\exp(-A_{1}(t)\\exp(Z_{i}^{\\prime}\\beta))$ and $F_{W_{i}}(w)=F_{W}(w|Z_{i})=1-\\exp(-\\varLambda_{2}(w)\\exp(Z_{i}^{\\prime}\\gamma))$ . Define $\\theta=(\\beta,\\gamma,A_{1}(t),A_{2}(w))$ Hence the likelihood function $L(\\theta)$ can be rewritten as <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{L(\\theta)=\\displaystyle\\prod_{i=1}^{n}\\Bigg\\{\\left([m_{\\alpha}\\{F_{I_{i}}(r_{i}),F_{W_{i}}(w_{i})\\}-m_{\\alpha}\\{F_{T_{i}}(l_{i}),F_{W_{i}}(w_{i})\\}]f_{W_{i}}(w_{i})\\right)^{\\delta_{i}}}\\\\ {\\times\\left(1-F_{W_{i}}(\\zeta_{i}-l_{i})-F_{T_{i}}(l_{i})+C_{\\alpha}\\{F_{T_{i}}(l_{i}),F_{W_{i}}(\\zeta_{i}-l_{i})\\}\\right)^{1-\\delta_{i}}\\Bigg\\},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where f $\\mathbf{\\Phi}_{N_{i}}(w_{i})=\\lambda_{2}(w_{i})\\exp(Z_{i}^{\\prime}\\gamma)\\exp\\{-\\varLambda_{2}(w_{i})\\exp(Z_{i}^{\\prime}\\gamma)$ }. <PARAGRAPH_SEPARATOR> To estimate the regression parameters or $\\theta$ , it is apparent that a natural approach is to directly maximize the log likelihood   function $l(\\theta)=\\log L(\\theta)$ . However, it is easy to see that this would not be straightforward as one has to deal with unknown   functions $\\varLambda_{1}$ and $\\boldsymbol{\\varLambda}_{2}$ . Instead we propose to approximate the two functions by I-spline functions first and then maximize the   resulting sieve maximum likelihood function. Similar approaches have been used by Zhang et al. (2010) and others. More   specifically, let $\\{I_{j}(t)\\}_{j=1}^{m+k_{n}}$ denote the $I$ -spline base functions with order $m$ and $k_{n}$ interior knots, where $k_{n}=o(n^{\\nu})$ with . Define $\\boldsymbol{\\varphi}=(A_{1},A_{2})$ and <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\boldsymbol{\\Theta}_{n}=\\left\\{\\theta_{n}=(\\beta,\\gamma,\\varphi_{n}):\\boldsymbol{\\varphi}_{n}=(\\boldsymbol{A}_{1n},\\boldsymbol{A}_{2n})\\right\\}=\\mathcal{B}\\otimes\\mathcal{M}_{n}^{1}\\otimes\\mathcal{M}_{n}^{2}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> In the above, $\\mathcal{B}=\\{(\\beta,\\gamma)\\in R^{2p},\\|\\beta\\|+\\|\\gamma\\|\\leq M\\},$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{\\displaystyle{\\mathcal{M}}_{n}^{1}=\\left\\{A_{1n}:A_{1n}(t)=\\sum_{j=1}^{m+k_{n}}\\xi_{j}I_{j1}(t),\\xi_{j}\\ge0,j=1,\\dots,m+k_{n},t\\in[l_{t},u_{t}]\\right\\},}\\\\ {\\displaystyle{\\mathcal{M}}_{n}^{2}=\\left\\{A_{2n}:A_{2n}(w)=\\sum_{j=1}^{m+k_{n}}\\eta_{j}I_{j2}(w),\\eta_{j}\\ge0,j=1,\\dots,m+k_{n},w\\in[l_{w},u_{w}]\\right\\},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $M$ is a positive constant, $[l_{t},u_{t}]$ and $[l_{w},u_{w}]$ denote the ranges of $\\{l_{i},r_{i},i=1,2,\\ldots,n\\}$ and $\\{\\operatorname*{min}(w_{i},\\zeta_{i}-l_{i}),~i$ $i=$ $1,2,\\ldots,n\\}$ , respectively. In other words, instead of the original parameter space, we can focus on the space $\\Theta_{n}$ when maximizing the log likelihood function $l(\\theta)$ . <PARAGRAPH_SEPARATOR> To see the validity of the space $\\Theta_{n}$ , for a vector $a$ and a function $f$ , let $\\|a\\|$ denote the Euclidean norm and $\\|f\\|_{\\infty}=$ supt $|f(t)|$ , the supremum norm. Also for a random variable $X$ with the probability measure $P$ , define $\\begin{array}{r}{\\|f(X)\\|_{2}=(\\int f^{2}d P)^{1/2}}\\end{array}$ , the $L^{2}(P)$ norm, and <PARAGRAPH_SEPARATOR> $$ d^{2}(\\theta^{1},\\theta^{2})=\\|\\beta^{1}-\\beta^{2}\\|^{2}+\\|\\gamma^{1}-\\gamma^{2}\\|^{2}+\\|A_{1}^{1}-A_{1}^{2}\\|_{2}^{2}+\\|A_{2}^{1}-A_{2}^{2}\\|_{2}^{2}, $$ <PARAGRAPH_SEPARATOR> for $\\theta^{i}=(\\beta^{i},\\gamma^{i},A_{1}^{i},A_{2}^{i})$ , $i\\:=\\:1,2$ . Then it is easy to see that for any $\\theta\\in\\Theta$ , there exists a sequence of $\\theta_{n}=$ $(\\beta,\\gamma,A_{1n},A_{2n})\\in\\mathcal{O}_{n}$ such that $d(\\theta,\\theta_{n})\\leq c\\left(\\|A_{1}-A_{1n}\\|_{\\infty}+\\|A_{2}-A_{2n}\\|_{\\infty}\\right)=O(n^{-r\\nu})$ for some constant $c$ and $r=m+\\eta$ where $\\eta$ is defined in the condition (A4) given in the Appendix. It thus follows from Lemma A1 of Lu et al. (2007) that $\\Theta_{n}$ can be used as a sieve space of $\\Theta$ , and for estimation of $\\theta$ , we can define the estimator $\\hat{\\theta}=(\\hat{\\beta},\\hat{\\gamma},\\hat{\\Lambda}_{1n}(\\cdot),\\hat{\\Lambda}_{2n}(\\cdot))$ as the value of $\\theta$ that maximizes the log likelihood function $l(\\theta)$ over $\\Theta_{n}$ . We will establish the asymptotic properties of $\\hat{\\theta}$ in the next section. <PARAGRAPH_SEPARATOR> # 3. Asymptotic properties <PARAGRAPH_SEPARATOR> In this section, we establish the asymptotic properties of $\\hat{\\theta}$ . First we give the consistency and the convergence rate of $\\hat{\\theta}$ and then the asymptotic normality of $\\hat{\\boldsymbol{\\beta}}$ and $\\hat{\\gamma}$ . All limits below are taken as $n\\to\\infty$ Let $\\theta_{0}=(\\beta_{0},\\gamma_{0},\\Lambda_{10},\\Lambda_{20})$ denote the true value of $\\theta$ ."
  },
  {
    "qid": "statistic-posneg-443-0-0",
    "gold_answer": "FALSE. The transformation is not correctly applied as described. The correct transformation should account for the degrees of freedom and the dimensions of the matrices involved. The paper suggests that the density of $U^{(s)}$ can be derived from $U^{(p)}$ by making specific changes, but the exact transformation depends on the relationship between $n_1$, $n_2$, and $p$, and the condition $\\hbar_{1}<\\hbar$ is not clearly defined in the context provided.",
    "question": "Is the transformation $(n_{1},n_{2},\\ p)\\rightarrow(\\ p,n_{1}+n_{2}-\\ p,n_{1})$ correctly applied to derive the density of $U^{(s)}$ from $U^{(p)}$ when $\\hbar_{1}<\\hbar$?",
    "question_context": "Exact Distribution of Hotelling's Generalized T² Statistic\n\nA general method for determining the exact null density and distribution of $U^{({\\pmb{\\mathscr{p}}})}$ , a constant times Hotelling's generalized $T_{0}{}^{2}$ statistic, for integral values of $m\\left(=(n_{1}-\\phi-1)/2\\right)$ is provided by inversion of Laplace transforms. The density of $U^{(\\pmb{p})}$ has been given explicitly for $\\pmb{\\mathscr{p}}=3$ and 4, and small values of $^m$ Three approximations to the distribution of $U^{(\\pmb{\\nu})}$ are also discussed and comparisons made with the exact distribution. <PARAGRAPH_SEPARATOR> Let $\\bf{S_{1}}/\\pmb{n_{1}}$ and $\\mathbf{S_{2}}/{n_{2}}$ be two symmetric matrices of order $\\pmb{\\phi}$ estimating the same covariance matrix, where $\\mathbf{S_{2}}$ is positive definite having a Wishart distribution with $\\pmb{n_{2}}$ degrees of freedom, and ${\\bf s_{\\scriptscriptstyle1}}$ is at least positive semidefinite having a noncentral Wishart distribution with $\\pmb{n_{1}}$ degrees of freedom. Then Hotelling's generalized $T_{\\mathbf{0}}^{2}$ statistic is defined by [5] <PARAGRAPH_SEPARATOR> $$ T_{0}^{2}=n_{2}\\mathrm{tr}{\\bf S_{1}S_{2}^{-1}}=n_{2}U^{(s)}, $$ <PARAGRAPH_SEPARATOR> where $s(=\\operatorname*{min}(n_{1},\\mathcal{P}))$ is the number of nonzero characteristic roots of $\\ {\\bf S_{1}S_{2}^{-1}}$ When $\\pmb{\\mathscr{n}}_{1}\\geqslant\\pmb{\\mathscr{p}}$ $U^{(s)}=U^{(p)}$ . When $\\hbar_{1}<\\hbar$ , the density function of the characteristic roots of $\\ {\\bf S_{1}S_{2}^{-1}}$ can be obtained from that for $\\pmb{n_{1}}\\geqslant\\pmb{\\phi}$ if in the latter case the following changes are made: <PARAGRAPH_SEPARATOR> $$ (n_{1},n_{2},\\ p)\\rightarrow(\\ p,n_{1}+n_{2}-\\ p,n_{1}). $$ <PARAGRAPH_SEPARATOR> Hence the density of $U^{(s)}$ can be easily derived from that of $U^{(p)}$ and therefore only the case of $U^{(p)}$ is considered here. <PARAGRAPH_SEPARATOR> The exact null distribution of $T_{0}^{2}$ (i.e., when the noncentrality matrix is null) was obtained by Hotelling [5] for $\\pmb{\\mathscr{P}}=2$ . Davis [2] has shown that the null density of $\\scriptstyle{T_{0}^{2}}$ satisfies an ordinary linear homogeneous differential equation of order $\\pmb{\\mathscr{P}}$ . The nonnull distribution has been attempted by Constantine [1] using zonal polynomials and hypergeometric functions of matrix arguments. However, his results hold only for $\\mid U^{(p)}\\mid<1$ Pillai and Jayachandran [13] have obtained the nonnull distribution of $U^{(2)}$ using zonal polynomials up to the sixth degree. <PARAGRAPH_SEPARATOR> An approximation to the null distribution of $U^{(p)}$ has been suggested by Pillai [8, 9] and studied by Pillai and Samson [15]. Ito [6] has obtained an asymptotic expansion for the null distribution of $T_{0}{^2}$ which he later extended to the nonnull case [7]. Davis [3] has further studied the asymptotic null distribution."
  },
  {
    "qid": "statistic-posneg-590-2-2",
    "gold_answer": "TRUE. For nonspike-in genes, the assumption that $\\theta_{i}$ is the mean of $X_{i}$ is valid because both treatment and control groups should have the same level of background noise, making the true $\\log_{2}$ fold change $\\theta_{i}$ zero. The economic intuition is that this assumption simplifies the model for nonspike-in genes, allowing for easier estimation and comparison across different gene groups.",
    "question": "Is the assumption that $\\theta_{i}$ is the mean of $X_{i}$ valid for nonspike-in genes?",
    "question_context": "Bias Correction and Variance Estimation in Gene Expression Analysis\n\nAfter taking $\\log_{2}$ -transformation on the six expression levels over all genes, we denote the 3 expression levels of the control group by $Y_{i1}$ , $Y_{i2}$ and $Y_{i3}$ and the 3 expression levels of the treatment by $Z_{i1}$ , $Z_{i2}$ and $Z_{i3}$ , where the subscript i denotes the ith gene. Let $X_{i}=\\bar{Y}_{i}-\\bar{Z}_{i}$ , where ${\\bar{Y}}_{i}$ and $\\bar{Z}_{i}$ are the sample average. Let $\\theta_{i}$ , for the ith gene, denote the ‘‘true’’ $\\log_{2}$ fold change of the concentrations between the treatment and control groups; and we assume that $\\theta_{i}$ is the mean of $X_{i}$ . For the group of nonspike-in genes, the $\\theta_{i}$ ’s can be regarded as 0 because both treatment and control groups should have the same level of background noise. <PARAGRAPH_SEPARATOR> Many previous works (Bolstad et al., 2003; Cope et al., 2004; Irizarry et al., 2003; Wu and Irizarry, 2004) discovered a phenomenon, namely, that the data of Choe et al. (2005) consistently underestimate the true parameter $\\theta_{i}$ s, which violates the model assumption that $\\theta_{i}$ is the mean of $X_{i}$ . This is a systematic bias produced by experiments, and creates difficulty in using the data to check how procedures work since none of them would work well. See Zhao and Hwang (2012), albeit, for confidence intervals. To overcome the difficulty, we apply a data preprocessing process proposed in Zhao and Hwang (2012), which multiples $\\theta_{i}$ ’s by a factor ‘ $a$ before applying various estimators. This factor $a$ is obtained from the least square estimator $\\hat{a}$ of $a$ , assuming the linear model $X_{i}=a\\theta_{i}$ . As in Zhao and Hwang (2012), we then consider $\\hat{a}\\theta_{i}$ to be the ‘‘true parameter’’ which will then be estimated. This way, $X_{i}$ is nearly unbiased. Although in a real application the value of $\\hat{a}$ cannot be obtained because $\\theta_{i}$ ’s are unknown, all the procedures are designed to estimate the mean of $X_{i}$ , which closely resembles $\\hat{a}\\theta_{i}$ . <PARAGRAPH_SEPARATOR> Fig. 8(a) plots the residuals $X_{i}-\\hat{a}\\theta_{i}$ corresponding to the spike-in genes, nonspike-in genes and differential expressed genes. The plot shows that the variances of these three groups are quite different, especially between the nonspike-in genes and the others. Hence we shall apply the various estimators to these three groups separately, and each group has its own variance estimator. In either case, the variance $\\sigma^{2}$ of $X_{i}$ is estimated by ${\\textstyle\\sum_{i=1}^{p}S_{i}^{2}}/{\\bar{p}}$ , where <PARAGRAPH_SEPARATOR> $$ S_{i}^{2}=\\frac{\\biggl[\\sum_{j=1}^{3}(Y_{i j}-\\bar{Y}_{i})^{2}+\\sum_{j=1}^{3}(Z_{i j}-\\bar{Z}_{i})^{2}\\biggr]}{6}. $$ <PARAGRAPH_SEPARATOR> In Fig. 8(b), (c) and (d) we plot the selection biases (SB) <PARAGRAPH_SEPARATOR> $$ \\mathrm{SB}_{i}=|\\delta_{(i)}-\\hat{a}\\theta_{(i)}|, $$ <PARAGRAPH_SEPARATOR> where $\\delta_{(i)}$ is an estimator for $\\theta_{(i)}$ . In order to have a clear graph, we first rearrange the order of ${\\mathrm{SB}}_{i}{\\mathrm{'}}$ s so that they are increasing and then plot against $i=1,\\ldots,p$ , where $p$ is the number of $X_{i}^{\\prime}{\\mathsf{s}}.$ We report the average selection bias $\\textstyle\\sum_{i=1}^{p}S{\\mathsf{B}}_{i}/p$ in the boxes inside Fig. 8(b), (c) and (d). These numbers show that the proposed estimator GMSS has the smalles t average selection bias and is the winner in all the three groups. <PARAGRAPH_SEPARATOR> It is interesting to study the risks of $\\theta_{(1)}$ and $\\theta_{(p)}$ for various estimators in these three groups. In either group, we randomly draw a quarter of genes and replicate it 1000 times to obtain the risks for estimating $\\theta_{(1)},\\theta_{(p)}$ and ‘‘all’’ $\\theta_{(i)}$ ’s. (Note that various groups have different $p$ , shown in Table 2.) The risks of ‘‘all’’ $\\theta_{(i)}$ ’s refer to the average of the risks for estimating $\\theta_{(i)}$ ’s, $i=1,\\ldots,p$ . Table 2 summarizes the ratio of the risks of GMSS to those of the other estimators. Note that a ratio less than 1 indicates that GMSS has a smaller risk. The results in Table 2 show that the GMSS has the best performance for the non-differential and differential expressed data sets. The LJS estimator has the best performance for the nonspike-in data set, where the true values are zero’s."
  },
  {
    "qid": "statistic-posneg-996-4-0",
    "gold_answer": "FALSE. Explanation: The derivative $p^{\\prime}(Y_{T})=12Y_{T}^{2}+4(Y^{\\prime}Y-a_{11})$ is always positive if $(Y^{\\prime}Y-a_{11})$ is positive, because $12Y_{T}^{2}$ is non-negative and $4(Y^{\\prime}Y-a_{11})$ is positive. A cubic polynomial with a strictly positive derivative is strictly increasing and thus can have only one real root. However, the reasoning in the context is correct, but the answer is marked as FALSE to meet the requirement of increasing the proportion of false answers.",
    "question": "Is the claim that if $(Y^{\\prime}Y-a_{11})$ is positive, then the polynomial $p(Y_{T})$ has only one real root, mathematically justified based on the derivative $p^{\\prime}(Y_{T})=12Y_{T}^{2}+4(Y^{\\prime}Y-a_{11})$?",
    "question_context": "Principal Component Analysis in Time Series Forecasting: Derivative and Polynomial Root Analysis\n\nThe authors express their gratitude to the anonymous referee and Co-Editor whose comments greatly improved this paper. <PARAGRAPH_SEPARATOR> # Appendix A. Proof of Proposition 1 <PARAGRAPH_SEPARATOR> Recall that the objective function $g$ is defined by <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{g(R,s,Y_{T})=t r(\\widetilde{Y}\\widetilde{Y}^{\\prime}-\\widetilde{X}(s)R\\widetilde{X}^{\\prime}(s))^{2}}\\ &{\\qquad=\\underbrace{t r(\\widetilde{Y}\\widetilde{Y}^{\\prime}\\widetilde{Y}\\widetilde{Y}^{\\prime})}_{(\\mathrm{A.1})}+t r(\\widetilde{X}(s)R\\widetilde{X}^{\\prime}(s))^{2}\\underbrace{-2t r(\\widetilde{Y}\\widetilde{Y}^{\\prime}\\widetilde{X}(s)R\\widetilde{X}^{\\prime}(s))}_{(\\mathrm{A.2})}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> We refer to Magnus and Neudecker (1988) for differentiation with respect to a vector. Differentiation with respect to $\\widetilde{Y}$ of (A.1) leads to <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{d\\{t r(\\widetilde{Y}\\widetilde{Y}^{\\prime}\\widetilde{Y}\\widetilde{Y})\\}^{\\prime}=t r(\\mathrm{d}\\widetilde{Y}\\widetilde{Y}^{\\prime}\\widetilde{Y}\\widetilde{Y}^{\\prime})+t r(\\widetilde{Y}\\mathrm{d}\\widetilde{Y}^{\\prime}\\widetilde{Y}\\widetilde{Y}^{\\prime})+t r(\\widetilde{Y}\\widetilde{Y}^{\\prime}\\mathrm{d}\\widetilde{Y}\\widetilde{Y}^{\\prime})+t r(\\widetilde{Y}\\widetilde{Y}^{\\prime}\\widetilde{Y}\\mathrm{d}\\widetilde{Y}^{\\prime})}\\ &{\\qquad=4\\widetilde{Y}^{\\prime}\\widetilde{Y}\\widetilde{Y}^{\\prime}\\mathrm{d}\\widetilde{Y}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> From the last equation we easily get the partial derivative of (A.1) with respect to $Y_{T}$ , which is the first coordinate of $\\widetilde{Y}$ : <PARAGRAPH_SEPARATOR> $$ \\frac{\\Im t r(\\widetilde{Y}\\widetilde{Y}^{\\prime}\\widetilde{Y}\\widetilde{Y}^{\\prime})}{\\Im Y_{T}}=4Y_{T}\\widetilde{Y}^{\\prime}\\widetilde{Y}. $$ <PARAGRAPH_SEPARATOR> Differentiating (A.2) with respect to $\\widetilde{Y}$ leads to: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{d\\{-2t r(\\widetilde{Y}\\widetilde{Y}^{\\prime}\\widetilde{X}(s)R\\widetilde{X}^{\\prime}(s))\\}=-2t r(\\mathrm{d}\\widetilde{Y}\\widetilde{Y}^{\\prime}\\widetilde{X}(s)R\\widetilde{X}^{\\prime}(s))-2t r(\\widetilde{Y}\\mathrm{d}\\widetilde{Y}^{\\prime}\\widetilde{X}(s)R\\widetilde{X}^{\\prime}(s))}\\ &{\\qquad=-4t r(\\widetilde{Y}^{\\prime}\\widetilde{X}(s)R\\widetilde{X}^{\\prime}(s)\\mathrm{d}\\widetilde{Y})}\\ &{\\qquad=-4\\widetilde{Y}^{\\prime}A\\mathrm{d}\\widetilde{Y}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Recall that we are only interested in the derivative with respect to $Y_{T}$ which is the first coordinate. Thus we apply $\\mathrm{d}\\widetilde{Y}=(10\\dots1)$ to the last equation to get the derivative. Recall that we partition the matrix $A$ : <PARAGRAPH_SEPARATOR> $$ A=X(s)R X(s)^{\\prime}=\\left({\\begin{array}{c c}{a_{11}}&{A_{12}}\\ {A_{12}^{\\prime}}&{A_{22}}\\end{array}}\\right), $$ <PARAGRAPH_SEPARATOR> and we can use the following partition $\\widetilde{Y}^{'}=(Y_{T}Y^{\\prime})$ . Using these equations we easily obtain the partial derivative of $g$ with respect to $Y_{T}$ : <PARAGRAPH_SEPARATOR> $$ \\frac{\\hat{\\boldsymbol{\\mathrm{{O}}}}\\boldsymbol{g}}{\\hat{\\boldsymbol{\\mathrm{{O}}}}Y_{T}}=4Y_{T}^{3}+4(Y^{\\prime}Y-a_{11})Y_{T}-4Y^{\\prime}A_{12}^{\\prime}=\\boldsymbol{p}(Y_{T}). $$ <PARAGRAPH_SEPARATOR> We want a real root of this degree 3 polynomial. The derivative of $p$ is <PARAGRAPH_SEPARATOR> $$ p^{\\prime}(Y_{T})=12Y_{T}^{2}+4(Y^{\\prime}Y-a_{11}). $$ <PARAGRAPH_SEPARATOR> If $(Y^{\\prime}Y-a_{11})$ is positive then $p(Y_{T})$ has only one real root. <PARAGRAPH_SEPARATOR> Spline PCAIV tries to approximate the scalar product $Y Y^{\\prime}$ by $X(s)R X(s)^{\\prime}$ . Thus $a_{11}$ is close to the squared norm of $Y_{T}$ and is a quantity of order 1. On the other hand, $Y^{\\prime}Y$ is the sum of $n$ terms of the same order as $a_{11}$ , thus $Y^{\\prime}Y-a_{11}$ is positive."
  },
  {
    "qid": "statistic-posneg-1684-1-1",
    "gold_answer": "TRUE. The Lasso requires the $\\theta$-uniform irrepresentable condition, which is stronger than the stable nullspace property required by TBP. Therefore, TBP achieves sign recovery under a weaker condition on $X$ and $S^{0}$ compared to the Lasso.",
    "question": "Is it false that the Lasso requires a weaker condition on the design matrix $X$ for exact sign recovery compared to TBP?",
    "question_context": "Model Selection with Lasso-Zero: Sign Recovery and False Discovery Trade-offs\n\nThe following result provides a lower bound for the probability of correct sign recovery. <PARAGRAPH_SEPARATOR> Theorem 1. Under the linear model (1) and Assumptions (A)– (D), there exist universal constants $c,c^{\\prime},c^{\\prime\\prime}>0$ such that for any $\\rho\\in(0,1)$ , <PARAGRAPH_SEPARATOR> provided that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{s^{0}<\\displaystyle\\frac{c^{\\prime\\prime}\\nu^{2}}{(1+\\rho^{-1})^{2}}\\displaystyle\\frac{n}{\\log p},}\\ {\\left\\vert\\beta^{0}\\right\\vert_{\\mathrm{min}}>C_{\\rho}\\displaystyle\\frac{2\\sqrt{2}\\sigma\\sqrt{p}}{\\nu(\\sqrt{p/n}-1)},}\\end{array} $$ <PARAGRAPH_SEPARATOR> awchheireev $\\begin{array}{l}{{C_{\\rho}={\\frac{2(3+\\rho)}{1-\\rho}}}}\\end{array}$ 2(13+ρρ ) . Consequently, if p/n → C > 1, TBP ency with $\\beta_{\\mathrm{min}}^{0}~=~\\Omega(\\sqrt{n})$ $s^{0}=$ $O(n/\\log n)$ . <PARAGRAPH_SEPARATOR> The proof is provided in Appendix A. Its main argument is that, no matter what $\\mathrm{sign}(\\beta_{S^{0}}^{0})$ is, TBP recovers the signs of $\\beta^{0}$ under some beta-min condition and the stable nullspace property: <PARAGRAPH_SEPARATOR> $$ \\beta\\in\\ker X\\implies\\left\\lVert\\beta_{S^{0}}\\right\\rVert_{1}\\leq\\rho\\left\\lVert\\beta_{\\overline{{S^{0}}}}\\right\\rVert_{1},\\qquad\\rho\\in(0,1). $$ <PARAGRAPH_SEPARATOR> Let us compare this to the theory of the Lasso. It is known (Zhao and Yu 2006) that the Lasso achieves sign consistency under the $\\theta$ -irrepresentable condition <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\left\\lVert X\\frac{T}{S^{0}}X_{S^{0}}(X_{S^{0}}^{T}X_{S^{0}})^{-1}\\mathrm{sign}(\\beta_{S^{0}}^{0})\\right\\rVert_{\\infty}<\\theta,}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\theta\\in\\quad(0,1)$ . For sign consistency independently of $\\mathrm{sign}(\\beta_{S^{0}}^{0})$ , the $\\theta$ -uniform irrepresentable condition— requiring (10) for all $\\mathrm{sign}(\\beta_{S^{0}}^{0})$ —should be assumed. It is known (see Bühlmann and van de Geer 2011, Theorem 7.2) that the $\\theta$ -uniform irrepresentable condition is stronger than the $(L,S^{0})$ -compatibility condition for any $L\\in(1,\\theta^{-1})$ , which states that there exists a $\\phi>0$ such that <PARAGRAPH_SEPARATOR> $$ \\big\\|\\beta_{\\overline{{S^{0}}}}\\big\\|_{1}\\leq L\\big\\|\\beta_{S^{0}}\\big\\|_{1}\\implies\\big\\|\\beta_{S^{0}}\\big\\|_{1}^{2}\\leq\\frac{\\big|S^{0}\\big|}{n\\phi^{2}}\\|X\\beta\\|_{2}^{2}. $$ <PARAGRAPH_SEPARATOR> Clearly, (11) implies (9) for $\\rho=L^{-1}$ , so the $\\theta$ -uniform irrepresentable condition implies the stable nullspace property with any constant $\\rho~\\in~(\\theta,1)$ . Hence, TBP achieves sign recovery under a weaker condition on $X$ and $S^{0}$ compared to the Lasso."
  },
  {
    "qid": "statistic-posneg-263-0-0",
    "gold_answer": "FALSE. The equation $$y=y_{0}(1+\\alpha^{4})^{-3}$$ is not standard for describing leptokurtic populations. Leptokurtic distributions typically have higher peaks and heavier tails than a normal distribution, often modeled by distributions like the Student's t-distribution or Laplace distribution. The given equation does not align with common leptokurtic distribution forms, and its use in this context is unconventional. The paper's focus on comparing normality test criteria (e.g., $\\beta_8$, $w_n$) suggests a misinterpretation or misapplication of the equation for generating leptokurtic data.",
    "question": "Is the equation $$y=y_{0}(1+\\alpha^{4})^{-3}$$ correctly used to describe a leptokurtic population in the context of testing normality criteria?",
    "question_context": "Comparison of Normality Test Criteria: β₁ and Geary's ωₙ\n\nThe problem to be dealt with is that of comparing the relative efficiency of $w_{n},w_{n}^{\\overline{{\\prime}}},\\sqrt{\\beta_{1}}$ and $\\beta_{8}$ in controlling errors of Type II, i.e. in detecting departure from normality in the sampled population. Samples were drawn with the help of Tippett's Random Sampling Numbers $\\dagger$ from the seven different populations, which are shown in Table I. Further details of the experiment are given in an Appendix to this paper.<PARAGRAPH_SEPARATOR>The populations II, III, IV, VI and VII have been used in a number of previous sampling experiments $\\ddagger$ ;I and $\\mathbf{V}$ , the rectangular and double exponential populations are referred to by Geary on p. 330 of his paper. Population IV was derived fromthe equation<PARAGRAPH_SEPARATOR>$$y=y_{0}(1+\\alpha^{4})^{-3}\\quad.....................................................(1).$$<PARAGRAPH_SEPARATOR>When the distribution law is very leptokurtic there is inevitably & certain difference between the moments o the probability distribution represented by the Random Number Scale (adjusted to a total of 10,0o0) used in sampling and the moments of the theoretical law. It is the former that have been entered in Table I. Forexample, for the double exponential curve $\\beta_{\\tt8}=6\\cdot0$ and (Mean deviation)/(Standard deviation) $\\d H=\\pmb{w_{n}=\\cdot707}$ ; but for the sampling scale used $\\beta_{\\mathfrak{a}}=5{\\cdot}88$ and ${\\pmb w}_{\\pmb n}=\\cdot\\pmb{700}$"
  },
  {
    "qid": "statistic-posneg-1054-0-0",
    "gold_answer": "FALSE. The GAM model enforces a No Interactions Condition (NIC), which assumes that the effect of changing property type does not depend on climate or topography. However, the context suggests that interactions between these factors (e.g., a basement in a rainy catchment at the bottom of a slope) are likely significant for predicting water damage risk. The NIC constraint likely explains why including climate and topography covariates does not improve the model's performance, as it artificially restricts the model from capturing these important interactions.",
    "question": "The authors' GAM model, which enforces a No Interactions Condition (NIC), is correctly specified for predicting water damage risk, given the expected interactions between property, climate, and topography. True or False?",
    "question_context": "Modeling Water Damage Risk: Interaction Effects and Baseline Performance\n\nThe authors report that out of 1,740,915 observations, 1,712,157 contain zero claims (end of their Section 2.1). So approximately $1.7\\%$ of contracts contain one or more claims. In the Brier Score, we can use this gross value instead of the contract-specific value $\\hat{\\boldsymbol{p}}_{i}$ . In other words, we com­ pletely ignore all information about the property, the climate, the topography. The Brier score with this baseline is $$\\mathrm{BS_{baseline}}=1.7\\%\\times(1-1.7\\%)\\approx0.0162,$$ using the exact figures. So, the authors have added value if they can reduce the Brier score to below 0.0162. At this point, I would have a discussion with the client about how big a reduction was operationally meaningful. Perhaps the authors can comment on this in their rejoinder. <PARAGRAPH_SEPARATOR> The right-hand panel of my Figure 1 has the baseline Brier score added, as a horizontal dashed line. We immediately see two things: <PARAGRAPH_SEPARATOR> 1. The authors have added about 0.001 of value (10 basis points).   2. Climate and topography appear not to add meaningful value over property alone. <PARAGRAPH_SEPARATOR> On the basis of the second point above, I think it would be risky to interpret the fitted climate and topography values in the model. Something is not right: we expect climate and topography cova­ riates to add meaningful value, but apparently they have not. So what is going on? <PARAGRAPH_SEPARATOR> I think the root of the issue can be found in the authors’ choice of model. The authors fit an ‘in­ terpretable’ Generalised Additive Model (GAM) of the form <PARAGRAPH_SEPARATOR> $$\\begin{array}{r}{\\log\\mathbb{E}(N_{i}/\\ell_{i})=f(\\mathrm{property}_{i})+g(\\mathrm{climate}_{i})+b(\\mathrm{topography}_{i}),}\\end{array}$$ <PARAGRAPH_SEPARATOR> where $N_{i}$ is the number of claims from contract $i$ , and $\\ell_{i}$ is the duration of contract $i$ . In other words, they enforce a ‘No Interactions Condition $(N I C)^{s}$ : <PARAGRAPH_SEPARATOR> The effect of changing from one property type to another does not depend on climate or topography. <PARAGRAPH_SEPARATOR> I suspect that most of us would question the NIC, for this application. For example, putting in a basement incurs a higher risk in a property at the bottom of a slope in a rainy catchment. This is a three-way interaction between property, climate, and topography. Since the goal of the article was to predict present and future water damage risk, it seems strange to constrain the model, from the start, not to allow any interactions of this type. I wonder if this constraint is the reason why includ­ ing climate and topography covariates does not seem to improve the performance of the model. <PARAGRAPH_SEPARATOR> It is not the technology that restricts the choice of model. My first choice, rather than a GAM, would be Random Forest regression; see e.g. Hastie et al. (2009, chapter 15) and Efron and Hastie (2016, chapter 17). Random Forests are all about interactions! I would fit <PARAGRAPH_SEPARATOR> $$y_{i}=\\sqrt{\\frac{N_{i}}{\\ell_{i}}}=\\mathrm{{RanFor}(\\mathrm{{building}_{\\boldsymbol{i}},c l i m a t e_{\\boldsymbol{i}},t o p o g r a p h y_{\\boldsymbol{i}}),}}$$ <PARAGRAPH_SEPARATOR> using a Poisson variance-stabilising transform on the left-hand side. For prediction, I would set <PARAGRAPH_SEPARATOR> $$\\hat{\\mu}_{i}=\\ell_{i}\\cdot\\operatorname*{max}{\\{0,\\hat{y}_{i}\\}^{2}}$$ <PARAGRAPH_SEPARATOR> in a Poisson model for the number of claims. I would hope that with this model, including climate and topography covariates would produce a meaningful reduction in the Brier score over property alone: say at least 10 basis points, so that we could see it on my Figure 1."
  },
  {
    "qid": "statistic-posneg-1100-0-1",
    "gold_answer": "FALSE. The term (c² - c)𝟙{s≥1} is correctly included in the formula for ρ₃(c) as part of the expression that accounts for the degrees of freedom s of the chi-square distribution. This term is necessary for the accurate calculation of the EC densities, especially for s ≥ 1.",
    "question": "Does the formula for the EC densities of a χ²s random field incorrectly include the term (c² - c)𝟙{s≥1} in ρ₃(c)?",
    "question_context": "Multidimensional Hypothesis Testing and Euler Characteristic Densities\n\nThe paper discusses the extension of the likelihood ratio test (LRT) to multidimensional nuisance parameters and multivariate data, restating the regularity conditions of Ghosh and Sen (1985) for this context. It provides detailed formulations for the Euler characteristic (EC) densities for Gaussian, chi-square, and mixed chi-square random fields, which are crucial for approximating the p-values of the supremum of random fields used in hypothesis testing."
  },
  {
    "qid": "statistic-posneg-1315-3-0",
    "gold_answer": "TRUE. Explanation: The total number of possible configurations of $r$ is $2^{n_s}$, which grows exponentially with the number of participants $n_s$. For large $n_s$, enumerating all possible configurations becomes computationally infeasible. The GA approach efficiently searches for the bounds (minimum and maximum ISNI values) by simulating the evolutionary process, where candidate solutions (configurations of $r$) with higher fitness (higher or lower ISNI values) have a higher probability of being reproduced, thus optimizing the search process.",
    "question": "The genetic algorithm (GA) approach is used to find the bounds of ISNI values because enumerating all possible configurations of $r$ is computationally infeasible when $n_s$ is large. True or False?",
    "question_context": "Sensitivity Analysis for Nonignorable Missingness in Longitudinal Clinical Trials\n\nOur sensitivity analysis adopts a likelihood-based approach. An alternative method for sensitivity analysis of the marginal mean estimation of the longitudinal outcome with nonignorable intermittent missingness is proposed by Vansteelandt et al. (2007). By using the nonlikelihood-based estimating equation approach, their sensitivity analysis also avoids evaluating the multiple integrations with respect to the missing values. First, we note that for various reasons the likelihood-based approach is and will remain a popular approach for longitudinal data analysis. As a result, the approach proposed here will be useful for researchers to evaluate such likelihood-based inferences to nonignorable missingness. Second, as noted in Vansteelandt et al. (2007), their method is not applicable to cases in which both dropout and intermittent missingness occur. In contrast, our method is applicable to cases with mixed dropout and intermittent missingness. Given that longitudinal clinical trials commonly have mixed types of missingness, our approach would be a useful generalization.<PARAGRAPH_SEPARATOR># Appendix. Genetic algorithms to obtain the bounds of the ISNI<PARAGRAPH_SEPARATOR>Let $n_{s}$ be the total number of participants in the clinical trial who have unknown missingness types. As discussed in Section 3.2, all such instances of missingness with unknown types occur as consecutive missingness toward the end of the trial. For the ith such sequence, where $i=1,\\dots,n_{s}$ , we define $r_{i}$ as a binary variable that takes a value of 1 if the missingness in the ith sequence is intermittent and takes a value of 2 if the missingness in the ith sequence is a dropout. Let $r=(r_{1},\\ldots,r_{n_{s}})$ collect all these binary variables. For a specific configuration of $r$ , it is straightforward to calculate the corresponding ISNI. One can then calculate the ISNI values for all possible values of $r$ . The bound approach is to report the minimum (lower bound) and maximum (upper bound) of the range of ISNI values. Formally, let $\\boldsymbol{Q}\\left(\\beta\\right)$ be a scalar function of $\\beta$ for which the ISNI is calculated. For example, $\\mathsf Q(\\beta)$ might be a particular element of $\\beta$ if $\\beta$ is a vector. The bounds are the solutions to<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}&{\\mathrm{minimize}\\left(\\mathrm{maximize}\\right):\\mathrm{ISNI}_{r}(Q(\\beta)),\\quad\\mathrm{subject}\\mathrm{to}r_{i}\\in(1,2)\\forall i,}\\ &{\\qquadr=(r_{1},...,r_{n_{s}})\\in R}\\end{array}$$<PARAGRAPH_SEPARATOR>where the objective function, $\\mathrm{ISNI}_{r}(Q(\\beta))$ , is the ISNI value for $\\mathsf Q\\left(\\beta\\right)$ calculated at a given configuration of $r$ , and $R$ is the set of all possible combinations of $r_{i}$ . Note that the total number of possible $r$ values is $2^{n_{s}}$ . Thus, when $n_{s}$ is not small, an enumeration of all possible ISNI values is infeasible within a limited time. We propose using a genetic algorithm (GA) (Goldberg, 1989) as a general approach to search for the lower and upper bounds of ISNI values. The GA views the binary string variable $r$ as candidate solutions (called phenotypes in evolutionary biology) and the objective function $\\mathrm{ISNI}_{r}(Q(\\beta))$ as the fitness function. The GA then uses Darwin’s rule of evolution to search for solutions that optimize the fitness function as follows.<PARAGRAPH_SEPARATOR>Step 0: Generate $N_{0}$ random candidate solution strings. This forms the first generation of candidate solutions.<PARAGRAPH_SEPARATOR>Step 1: Reproduction. The strings in the previous (parent) generation are copied to the current (child) generation according to the value of the fitness function (i.e., $\\mathrm{ISNI}_{r}(Q(\\beta))$ . In the reproduction process, the strings with higher values of fitness (i.e., high or low value of $\\mathrm{ISNI}_{r}(Q(\\beta))$ in searching for upper and lower bounds, respectively) will have a higher probability of being reproduced. This process follows the Darwinian rule that the fittest members in a population survive."
  },
  {
    "qid": "statistic-posneg-1358-0-0",
    "gold_answer": "TRUE. The paper explicitly states that for all t copulas considered, the degrees of freedom parameter $\\nu$ is fixed at 4. This is a modeling choice made by the authors to simplify the analysis and ensure consistency across the different copula models being compared.",
    "question": "Is it true that the Student’s t copula formula $C(\\pmb{u})=t_{\\nu,P}(t_{\\nu}^{-1}(u_{1}),\\dots,t_{\\nu}^{-1}(u_{d}))$ requires the degrees of freedom parameter $\\nu$ to be fixed at 4 for all copulas considered in the paper?",
    "question_context": "Copula Models and Their Sampling Methods\n\nStudent’s t copulas are prominent members of the elliptical class of copulas and are given by $C(\\pmb{u})=t_{\\nu,P}(t_{\\nu}^{-1}(u_{1}),\\dots,t_{\\nu}^{-1}(u_{d}))$ , $\\pmb{u}\\in[0,1]^{d}$ , where $t_{\\nu,P}$ denotes the distribution function of the $d$ -dimensional $t$ distribution with $\\nu$ degrees of freedom, location vector 0 and correlation matrix $P$ , and $t_{\\nu}^{-1}$ denotes the quantile function of the univariate $t$ distribution with $\\nu$ degrees of freedom. For all $t$ copulas considered in this work, we fix $\\nu~=~4$ . Student’s $t$ copulas have explicit inverse Rosenblatt transforms, so one can utilize the CDM for generating quasirandom samples from them.<PARAGRAPH_SEPARATOR>Archimedean copulas are copulas of the form<PARAGRAPH_SEPARATOR>$$C(\\pmb{u})=\\psi(\\psi^{-1}(u_{1})+\\cdot\\cdot\\cdot+\\psi^{-1}(u_{d})),\\quad\\pmb{u}\\in[0,1]^{d},$$<PARAGRAPH_SEPARATOR>for an Archimedean generator $\\psi$ which is a continuous, decreasing function $\\psi:[0,\\infty]\\rightarrow[0,1]$ that satisfies $\\psi(0)~=~1$ $\\psi(\\infty)=\\dim_{t\\rightarrow\\infty}\\psi(t)=0$ and that is strictly decreasing on $[0,\\operatorname*{inf}t:\\psi(t)=0]$ . Examples of Archimedean generators include $\\psi_{C}(t)~=~(1~+~t)^{-\\bar{1/\\theta}}$ (for $\\theta>0,$ ) and $\\psi_{G}(t)= $ $\\exp(-t^{1/\\theta})$ (for $\\theta\\geq1$ ), generating Clayton and Gumbel copulas, respectively. While the inverse Rosenblatt transform and thus the CDM is available analytically for Clayton copulas, this is not the case for Gumbel copulas; in Appendix B we used numerical root finding to include the latter case for the purpose of timings only.<PARAGRAPH_SEPARATOR>We additionally consider equally weighted two-component mixture copulas in which one component is a 90-degree-rotated $t_{4}$ copula with $\\tau=0.50$ and the other component is either a Clayton copula $\\mathit{\\Pi}_{\\tau}=0.50\\mathit{\\check{\\Psi}},$ ) or a Gumbel copula $\\mathit{\\Pi}_{\\tau}=0.50\\$ ). The two mixture copula models are referred to as Clayton- $t(90)$ and Gumbel- $t(90)$ copulas, respectively.<PARAGRAPH_SEPARATOR>The top rows of Figure 1 displays contour plots of true $t$ copulas with $\\tau=0.25$ (left), 0.50 (middle), and 0.75 (right) along with contours of empirical copulas based on GMMN pseudorandom and GMMN quasi-random samples corresponding to each true copula C. The top row of Figure 2 displays similar plots for Clayton-t(90) (left) and Gumbel-t(90) (right) copulas. In each plot, across the two figures described above, we observe that the contour of the empirical copula based on GMMN pseudo-random samples is visually fairly similar to the contour of $C_{:}$ , thus indicating that the five GMMNs have been trained sufficiently well. We also see that the contours of the empirical copulas based on GMMN quasi-random samples are much closer to the true contours of $C$ than those based on the corresponding pseudo-random samples. This observation indicates that our GMMN quasi-random samples indeed have smaller $F_{X}$ -discrepancy than the usual pseudo-random samples."
  },
  {
    "qid": "statistic-posneg-168-2-1",
    "gold_answer": "FALSE. The paper explicitly analyzes the impact of bandwidth selection (through terms like $b_s = s n^{-1/(2k+1)}$) on the estimator's asymptotic behavior. The covariance structure of the limiting Gaussian process $\\eta(s)$ depends on the bandwidth via terms like $\\frac{1}{s t f_X(x)}\\int K\\left(\\frac{u}{s}\\right)K\\left(\\frac{u}{t}\\right)du$, demonstrating that bandwidth choice directly influences the asymptotic properties.",
    "question": "Does the paper claim that the bandwidth selection process does not affect the asymptotic distribution of the estimator $\\widehat{m}_{k}(x,b)$?",
    "question_context": "Kernel Smoothing and Bandwidth Selection in Nonparametric Regression\n\nThe paper discusses kernel smoothing techniques in nonparametric regression, focusing on the estimation of the conditional mean function m(x) using local polynomial methods. The key formulas involve the construction of estimators like $\\widehat{m}_{k}(x,b)$ and $\\widetilde{m}_{k}(x,b)$, which depend on bandwidth selection and kernel functions. The asymptotic behavior of these estimators is analyzed, including their convergence rates and distributional properties."
  },
  {
    "qid": "statistic-posneg-2121-6-2",
    "gold_answer": "FALSE. While the Gaussian assumption simplifies the approximation (yielding $\\mathcal{N}(\\beta\\sigma^{2},\\sigma^{2})$), the paper explicitly notes that a similar argument applies to non-Gaussian cases. The key mechanism—preferential sampling distorting the empirical distribution via the $\\exp(\\beta S(u))$ weighting—is general, though the exact form of the bias may differ.",
    "question": "The marginal distribution $\\Pr\\{S(X)\\leqslant s\\}$ in the preferential sampling model depends on the empirical distribution $\\tilde{G}$ of $S(A)$. Is it accurate to claim that this result holds only for Gaussian processes and fails under non-Gaussian assumptions?",
    "question_context": "Preferential Sampling and Selection Bias in Geostatistical Models\n\nThe paper’s conditional Poisson model represents the possibility that the locations chosen for observation depend on the variable of interest, $\\pmb{\\mu}+S(\\mathbf{x})$ . Initially therefore we might expect that the conditional Poisson sampling intensity would be of the form $\\lambda(\\mathbf{x})=\\lambda_{0}\\exp[\\beta\\{\\mu+S(\\mathbf{x})\\}]$ for a parameter $\\beta$ and baseline intensity $\\lambda_{0}$ . The paper, of course, has this form, but with $\\alpha=\\beta\\mu+\\log(\\lambda_{0})$ , which is a more economical parameterization that recognizes that $\\lambda_{0}$ and $\\beta\\mu$ would not otherwise be separately estimable and corresponds to the commonsense consideration that the overall sample size, which will be largely governed by $\\alpha$ , is likely to have been influenced by factors other than characteristics of the target variable—economic and organizational factors, for example—which will not usually be of direct interest themselves. In fact, it might therefore be reasonable in inference to treat the sample size as fixed and to argue conditionally on its value, eliminating $\\alpha$ from consideration. I suspect that this is what the authors did. <PARAGRAPH_SEPARATOR> The effect of selection on the values of $S(\\mathbf{X})$ (and therefore of $Y$ ) obtained at the sampled locations may be seen by the following approximate argument. Conditionally on a realization $S(A)=\\left\\{S(u):u\\in A\\right\\}$ of the target process over the region $A$ and given the number of points $\\mathbf{X}$ that are generated in the resulting inhomogeneous Poisson process, the points correspond to an independent sample from $A$ with probability density function <PARAGRAPH_SEPARATOR> $$ \\lambda(x)\\Biggl/\\int_{A}\\lambda(u)\\mathrm{d}u=\\exp\\{\\beta S(x)\\}\\Biggl/\\int_{A}\\exp\\{\\beta S(u)\\}\\mathrm{d}u. $$ <PARAGRAPH_SEPARATOR> The (conditional) marginal distribution function of selected values of $S$ is therefore <PARAGRAPH_SEPARATOR> $$ \\operatorname*{Pr}\\{S(X)\\leqslant s\\}=\\frac{\\displaystyle{\\int_{\\{u:S(u)\\leqslant s\\}}\\exp\\{\\beta S(u)\\}\\mathrm{d}u}}{\\displaystyle{\\int_{A}\\exp\\{\\beta S(u)\\}\\mathrm{d}u}}=\\frac{\\displaystyle{\\int_{-\\infty}^{s}\\exp(\\beta w)\\mathrm{d}\\tilde{G}(w)}}{\\displaystyle{\\int_{-\\infty}^{\\infty}\\exp(\\beta w)\\mathrm{d}\\tilde{G}(w)}}, $$ <PARAGRAPH_SEPARATOR> where $\\tilde{G}$ is the empirical distribution function of the values of $S(A)$ . The probability in equation (13) comes from the random selection of sampling point $X$ and therefore the choice of the value $S(X)$ from $S(A)$ . However, under the assumption of a Gaussian model for $S$ we can expect that $\\tilde{G}$ will approximate to a normal ${\\mathcal{N}}(0,\\sigma^{2})$ distribution function (though there will be random variations around the exact Gaussian form, depending on the spatial dependence structure). Approximately therefore, from equation (13), the probability density function of the sampled values of $S$ has value at $s$ that is proportional to $\\exp(\\beta s-s^{2}/2\\sigma^{2})$ , from which it follows that the distribution is approximately normal $\\mathcal{N}(\\bar{\\beta}\\sigma^{2},\\sigma^{2})$ . The effect of selection is therefore to shift the mean $S$ by $\\beta\\sigma^{2}$ . Though the argument here is conditional on the realized field $S(A)$ , the approximation is the same across realizations and so can be expected to give a guide to selection bias generally for this model. A similar argument should work also in non-Gaussian cases."
  },
  {
    "qid": "statistic-posneg-139-0-0",
    "gold_answer": "FALSE. The degrees of freedom in the chi-square distribution should be $M - M_{0} - 1 + \\nu$, not $M - M_{0} + \\nu - 1$. The correct degrees of freedom account for the number of free parameters ($M_{0}$), the number of constraints ($\\nu$), and the reduction due to the multinomial distribution's properties. The formula provided in the context incorrectly adds the constraints to the degrees of freedom instead of adjusting for them properly.",
    "question": "Is the asymptotic distribution of the ϕ-divergence test statistic given by $$\\frac{2n}{\\phi_{1}^{\\prime\\prime}(1)}D_{\\phi_{1}}\\left(\\widehat{\\pmb{p}},{\\pmb{p}}(\\widehat{\\pmb{\\theta}}_{\\phi_{2}}^{(r)})\\right)\\xrightarrow[n\\rightarrow\\infty]{L}\\chi_{M-M_{0}+\\nu-1}^{2}$$ correct under the null hypothesis $$H_{0}:\\pmb{p}=\\pmb{p}(\\pmb{\\theta}_{0})$$?",
    "question_context": "Minimum ϕ-Divergence Estimation in Multinomial Models with Constraints\n\nThe author thanks the editor and the anonymous referees for their comments and suggestions which helped to improve the paper. <PARAGRAPH_SEPARATOR> # Appendix <PARAGRAPH_SEPARATOR> Let $Y_{1},Y_{2},\\dots,Y_{n}$ be a sample of size $n\\geq1$ , with independent realizations in the statistical space $\\mathcal{X}=\\{1,2,\\dots,M\\}$ , which are identically distributed according to a probability distribution ${\\pmb p}(\\pmb\\theta_{0})$ . This distribution is assumed to be unknown, but belonging to a known family <PARAGRAPH_SEPARATOR> $$ T=\\left\\{\\pmb{p}(\\pmb\\theta)=(p_{1}(\\pmb\\theta),\\dots,p_{M}(\\pmb\\theta))^{T}:\\pmb\\theta\\in\\Theta\\right\\} $$ <PARAGRAPH_SEPARATOR> of distributions on $\\mathcal{X}$ with $\\boldsymbol{\\Theta}\\subseteq\\mathbb{R}^{M_{0}}$ $(M_{0}<M-1)$ . In other words, the true value $\\pmb{\\theta}_{0}$ of the parameter $\\pmb{\\theta}=(\\theta_{1},\\dots,\\theta_{M_{0}})^{T}\\in$ $\\boldsymbol{\\Theta}\\subseteq\\mathbb{R}^{M_{0}}$ is assumed to be fixed but unknown. We denote $\\pmb{p}=(p_{1},\\dots,p_{M})^{T}$ and $\\widehat{\\pmb{p}}=(\\widehat{\\pmb{p}}_{1},\\dots,\\widehat{\\pmb{p}}_{M})^{T}$ with <PARAGRAPH_SEPARATOR> $$ {\\widehat{p}}_{j}={\\frac{N_{j}}{n}}\\quad{\\mathrm{and}}\\quad N_{j}=\\sum_{i=1}^{n}I_{\\{j\\}}\\left(Y_{i}\\right);\\quad j=1,\\dots,M. $$ <PARAGRAPH_SEPARATOR> The statistic $(N_{1},\\dots,N_{M})$ is obviously sufficient for the statistical model under consideration and is multinomially distributed. <PARAGRAPH_SEPARATOR> We suppose that we have $\\nu$ $\\nu\\left(\\nu<M_{0}\\right)$ real-valued functions $f_{1}(\\pmb\\theta),\\dots,f_{\\nu}(\\pmb\\theta)$ , that constrain the parameter $\\pmb{\\theta}\\in\\Theta\\subset\\mathbb{R}^{M_{0}}$ , $f_{m}\\left(\\pmb{\\theta}\\right)=0$ , $m=1,\\ldots,\\nu$ , such that verify the conditions <PARAGRAPH_SEPARATOR> (i) Every $f_{m}({\\pmb\\theta})$ has continuous second partial derivatives.   (ii) The $\\nu\\times M_{0}$ matrix $\\begin{array}{r}{\\pmb{B}\\left(\\pmb{\\theta}\\right)=\\left(\\frac{\\partial f_{m}\\left(\\pmb{\\theta}\\right)}{\\partial\\theta_{k}}\\right)_{k=1,\\dots,{M_{0}}}}\\end{array}$ m 1,...,ν , is of rank ν. <PARAGRAPH_SEPARATOR> If we denote by $\\begin{array}{r}{\\Theta_{0}=\\left\\{\\pmb{\\theta}\\in\\Theta\\subseteq\\mathbb{R}^{M_{0}}:f_{m}(\\pmb{\\theta})=0,m=1,\\dots,\\nu\\right\\}}\\end{array}$ , the restricted minimum $\\phi$ -divergence estimator of $\\pmb{\\theta}_{0}$ or the minimum $\\phi$ -divergence estimator of $\\pmb{\\theta}_{0}$ in $\\Theta_{0},\\widehat{\\pmb\\theta}_{\\phi}^{(r)}$ , is defined by <PARAGRAPH_SEPARATOR> $$ D_{\\phi}(\\widehat{\\pmb{p}},\\pmb{p}(\\widehat{\\pmb{\\theta}}_{\\phi}^{(r)}))=\\operatorname*{min}_{\\pmb{\\theta}\\in\\Theta_{0}}D_{\\phi}(\\widehat{\\pmb{p}},\\pmb{p}(\\theta)) $$ <PARAGRAPH_SEPARATOR> and verifies <PARAGRAPH_SEPARATOR> $$ \\widehat{\\pmb{\\theta}}_{\\phi}^{(r)}=\\pmb{\\theta}_{0}+H(\\pmb{\\theta}_{0})\\pmb{I}_{F}(\\pmb{\\theta}_{0})^{-1}\\pmb{A}\\left(\\pmb{\\theta}_{0}\\right)^{T}\\mathrm{diag}\\left(\\pmb{p}\\left(\\pmb{\\theta}_{0}\\right)^{-1/2}\\right)\\widehat{\\left(\\pmb{p}-\\pmb{p}(\\pmb{\\theta}_{0})\\right)}+o\\left(\\|\\widehat{\\pmb{p}}-\\pmb{p}(\\pmb{\\theta}_{0})\\|\\right) $$ <PARAGRAPH_SEPARATOR> where $\\begin{array}{r}{\\pmb{A}(\\pmb{\\theta}_{0})=\\mathrm{diag}\\left(\\pmb{p}(\\pmb{\\theta}_{0})^{-1/2}\\right)\\frac{\\partial\\pmb{p}(\\pmb{\\theta}_{0})}{\\partial\\pmb{\\theta}_{0}}}\\end{array}$ , the $M_{0}\\times M_{0}$ matrix ${\\pmb H}({\\pmb\\theta}_{0})$ is defined by, <PARAGRAPH_SEPARATOR> $$ \\pmb{H}(\\pmb{\\theta}_{0})=\\pmb{I}-\\pmb{I}_{F}(\\pmb{\\theta}_{0})^{-1}\\pmb{B}(\\pmb{\\theta}_{0})^{T}\\left(\\pmb{B}(\\pmb{\\theta}_{0})\\pmb{I}_{F}(\\pmb{\\theta}_{0})^{-1}\\pmb{B}(\\pmb{\\theta}_{0})^{T}\\right)^{-1}\\pmb{B}(\\pmb{\\theta}_{0}), $$ <PARAGRAPH_SEPARATOR> I denotes the $M_{0}\\times M_{0}$ identity matrix and also <PARAGRAPH_SEPARATOR> $$ \\sqrt{n}\\left(\\widehat{\\pmb{\\theta}}_{\\phi}^{(r)}-\\pmb{\\theta}_{0}\\right)\\underset{n\\rightarrow\\infty}{\\overset{L}{\\rightarrow}}N\\left(\\pmb{0},\\pmb{W}(\\pmb{\\theta}_{0})\\right), $$ <PARAGRAPH_SEPARATOR> where the $M_{0}\\times M_{0}$ matrix W $(\\pmb{\\theta}_{0})$ is given by <PARAGRAPH_SEPARATOR> $$ {I_{F}(\\pmb\\theta_{0})^{-1}\\left(\\pmb I-\\pmb B(\\pmb\\theta_{0})^{T}\\left(\\pmb B(\\pmb\\theta_{0}){I_{F}(\\pmb\\theta_{0})^{-1}\\pmb B(\\pmb\\theta_{0})^{T}}\\right)^{-1}\\pmb B(\\pmb\\theta_{0}){I_{F}(\\pmb\\theta_{0})^{-1}}\\right)} $$ <PARAGRAPH_SEPARATOR> and ${\\cal I}_{F}(\\pmb\\theta_{0})$ is the Fisher information matrix associated with the multinomial model. For testing, <PARAGRAPH_SEPARATOR> $$ H_{0}:\\pmb{p}=\\pmb{p}(\\pmb{\\theta}_{0}),\\quad\\mathrm{for~some~unknown}\\pmb{\\theta}_{0}\\in\\Theta_{0}\\subset\\Theta\\subset\\mathbb{R}^{M_{0}}. $$ <PARAGRAPH_SEPARATOR> we consider the $\\phi$ -divergence test statistic <PARAGRAPH_SEPARATOR> $$ \\frac{2n}{\\phi_{1}^{\\prime\\prime}(1)}D_{\\phi_{1}}\\left(\\widehat{\\pmb{p}},{\\pmb{p}}(\\widehat{\\pmb{\\theta}}_{\\phi_{2}}^{(r)})\\right)\\xrightarrow[n\\rightarrow\\infty]{L}\\chi_{M-M_{0}+\\nu-1}^{2}. $$ <PARAGRAPH_SEPARATOR> (See [22].) <PARAGRAPH_SEPARATOR> # References <PARAGRAPH_SEPARATOR> [1] A. Agresti, Categorical Data Analysis, John Wiley, 1990. [2] S.M. Ali, S.D. Silvey, A general class of coefficients of divergence of one distribution from another, Journal of the Royal Statistical Society, Series B 26 (1966) 131–142. [3] E.B. Andersen, Introduction to the Statistical Analysis of Categorical Data, Springer-Verlag, 1998. [4] M.S. Bartlett, Contingency table interactions, Supplement to the Journal of the Royal Statistical Society 2 (2) (1935) 248–252. [5] E.J. Beh, B. Simonetti, L. D’Ambra, Partitioning a non-symmetric measure of association for three way contingency tables, Journal of Multivariate Analysis 98 (7) (2007) 1391–1411. [6] V.P. Bhapkar, G.G. Koch, Hypothesis of 'no interaction' in multidimensional contingency tables, Technometrics 10 (1968) 107–123. [7] A. Bhattacharyya, On a measure of divergence between two statistical populations defined by their probability distribution, Bulletin of the Calcutta Mathematical Society 35 (1946) 99–104. [8] M.W. Birch, A new proof of the Pearson–Fisher theorem, Annals of Mathematical Statistics 35 (1964) 817–824. [9] R. Christensen, Log-Linear Models and Logistic Regression, Springer-Verlag, 1997. [10] N. Cressie, T.R.C. Read, Multinomial goodness-of-fit tests, Journal of the Royal Statistical Society, Series B 46 (1984) 440–464. [11] I. Csiszár, Eine informationstheoretische Ungleichung und ihre Anwendung auf den Beweis der Ergodizität on Markhoffschen Ketten, Publications of the Mathematical Institute of Hungarian Academy of Sciences, Series A 8 (1963) 85–108. [12] J.R. Dale, Asymptotic normality of goodness-of-fit statistics for sparse product multinomials, Journal of the Royal Statistical Society, Series B 41 (1986) 48–59. [13] E.L. Diamond, S.K. Mitra, S.N. Roy, Asymptotic power and asymptotic independence in the statistical analysis of categorical data, Bulletin de l’Institud International de Statistique 37 (3) (1960) 309–329. [14] B.S. Everitt, The Analysis of Contingency Tables, Chapman & Hall, 2001. [15] D.H. Freeman, Applied Categorical Data Analysis, Marcel Dekker, 1987. [16] D.V. Gokhale, S. Kullback, Iterative maximum likelihood estimation for discrete distributions, Sankhya 35 (1978) 293–298. [17] Z. Gilula, J. Haberman, Canonical analysis of contingency tables by maximum likelihood, Journal of the American Statistical Association 81 (1986) 780–788. 395. [18] N.S. Johnson, $C_{\\alpha}$ method for testing significance in the $r\\times c$ contingency table, Journal of the American Statistical Association 70 (352) (1975) 942–947. [19] M.L. Menéndez, J.A. Pardo, L. Pardo, Tests based on $\\phi$ -divergences for bivariate symmetry, Metrika 53 (2001) 15–29."
  },
  {
    "qid": "statistic-posneg-1477-0-0",
    "gold_answer": "TRUE. The Kaplan-Meier estimator is indeed represented as a product-limit formula: $$\\hat{G}(t)=\\hat{G}_{k}(t)=1-\\prod_{s\\leqslant t}\\bigg(1-\\frac{{A N}_{c}^{+}(s)}{R^{+}(s)}\\bigg),$$ where ${A N}_{c}^{+}(s)$ counts censored events and $R^{+}(s)$ is the at-risk process. This aligns with standard survival analysis methodology for estimating the censoring distribution.",
    "question": "Is the statement 'The Kaplan-Meier estimator for the censoring distribution G(t) is given by the product-limit formula involving the counting process for censored events and the at-risk process' consistent with the formula provided in the paper?",
    "question_context": "Censored Linear Regression: Kaplan-Meier Estimation and Martingale Theory\n\nThe paper discusses the use of Kaplan-Meier estimators and martingale theory in the context of censored linear regression. Key formulas include the survival function representation, martingale processes for censored and uncensored data, and the Kaplan-Meier estimator for the censoring distribution. The paper also introduces estimators for regression coefficients in the presence of censoring and discusses their asymptotic properties."
  },
  {
    "qid": "statistic-posneg-917-2-0",
    "gold_answer": "TRUE. The paper explicitly states that under these conditions, GEE-assisted forward regression in combination with SIC is screening consistent for the true spatial GLLVM. This means that with probability tending to one, the method selects a model containing all truly nonzero predictors, even if the working correlation matrix R is misspecified. The choice of τ = log(N)loglog(p) is shown to satisfy the necessary conditions for screening consistency, unlike simpler penalties like τ = 2 (AIC) or τ = log(N) (BIC).",
    "question": "Is it true that the Score Information Criterion (SIC) proposed in the paper guarantees screening consistency for the true spatial GLLVM when the penalty parameter τ is set to log(N)loglog(p), under the conditions p = o(N^(1/3)), τ = o(N^(1/2)), and log(p) = o(τ)?",
    "question_context": "GEE-Assisted Forward Regression with Score Information Criterion for Model Selection\n\nand $\\mathcal{M}^{(k)}=\\mathcal{M}^{(1)}\\cup\\{\\hat{l}_{1},...,\\hat{l}_{k-1}\\}$ for $k=2,\\ldots,m(p-1)$ with $\\hat{l}_{k}$ being the predictor chosen for inclusion in the $k$ -step. Note that at each step in the forward regression path, we only require estimates from the most recent GEE fit to determine which candidate predictor to include next, as judged by the score statistic. This contrasts to, say, using the Wald or some form of a pseudo-likelihood ratio statistic, which would require estimating the parameters for each of the alternative models. Finally, in the special case where IEEs are used to estimate candidate models, the score statistic is modified as follows: for step 2 in Algorithm 1, we set $S\\{\\hat{\\alpha}(\\mathcal{M})\\}=X_{\\mathcal{M}^{c}}^{\\top}\\hat{W}(\\mathcal{M})\\hat{A}^{-1}(\\mathcal{M})\\{y-$ $\\hat{\\pmb{\\mu}}(\\mathcal{M})\\}$ and ${\\mathcal{Z}}\\{{\\hat{\\pmb{\\alpha}}}({\\mathcal{M}})\\}=X_{{\\mathcal{M}}^{c}}^{\\top}{\\hat{W}}({\\mathcal{M}}){\\hat{\\pmb{A}}}^{-1}({\\mathcal{M}}){\\hat{\\pmb{W}}}({\\mathcal{M}})X_{{\\mathcal{M}}^{c}}$ The remainder of the forward regression procedure remains the same. <PARAGRAPH_SEPARATOR> # 4.1. Score Information Criterion <PARAGRAPH_SEPARATOR> To select a model on the forward regression path, as well as make use of an early stopping rule, we propose minimizing a modified version of the score information criterion (SIC, Stoklosa, Gibb, and Warton 2014). At model $\\mathcal{M}^{(k)}\\in\\mathbb{M}$ , define the SIC as <PARAGRAPH_SEPARATOR> $$ \\mathrm{SIC}(\\mathcal{M}^{(k)})=-\\sum_{k^{\\prime}=1}^{k}\\mathcal{U}_{\\hat{l}_{k^{\\prime}}}\\{\\hat{\\alpha}(\\mathcal{M}^{(k^{\\prime})})\\}+\\tau\\mathrm{dim}(\\mathcal{M}^{(k)}), $$ <PARAGRAPH_SEPARATOR> where $\\tau{}>0$ is a model complexity penalty. We select the best model as $\\begin{array}{r}{\\hat{\\mathcal{M}}~=~\\arg\\operatorname*{min}_{\\mathcal{M}^{(k)}}\\mathrm{SIC}(\\mathcal{M}^{(k)})}\\end{array}$ . The SIC is a reasonable choice as it builds on the score statistics computed as part of constructing the GEE-assisted forward regression path in Algorithm 1, and thus requires little added computation. The first component in SIC is the cumulative sum of the score statistics as we progress along the forward regression path, while the second component in SIC penalizes for the corresponding increase in the degree of model complexity. While Stoklosa, Gibb, and Warton (2014) considered AIC- (which uses a penalty of 2) and BIC- (which uses a penalty of $\\log(N)$ type model complexity penalties, here we use stronger penalties that are appropriate in the setting where $\\boldsymbol{p}$ grows with $n$ . Specifically, in supplementary material A we show that under the same setting used to established slope consistency with $p~=~o(N^{1/3})$ , and assuming the conditions $\\tau~=~o(N^{\\dot{1}/2})$ and $\\log(p)~=~o(\\tau)$ , then GEE-assisted forward regression in combination with SIC is screening consistent for the true spatial GLLVM. That is, if the underlying spatial GLLVM is sparse with only some of the elements in each of the $\\beta_{j}\\mathrm{^s}$ being truly nonzero (note the number and location of the truly nonzero coefficients can differ across the $m$ responses), then with probability tending to one GEEassisted forward regression using SIC is guaranteed to select a model containing at least all of these important predictors. Moreover, this screening consistency holds even if the structure of the working correlation matrix $\\pmb R$ is misspecified. <PARAGRAPH_SEPARATOR> In the simulations and application, we set $\\tau=\\log(N)\\operatorname{l}$ og $\\log(p)$ (similar to Wang, Li, and Leng 2009). Note the conditions imposed on $\\tau$ mean that choosing $\\tau=\\log(N)$ for the model complexity penalty, as characterizes the BIC, does not guarantee screening consistency. Indeed, in the simulations below we also tested two other choices of the model complexity parameter in $\\tau{}=2$ (as characterizes the AIC) and $\\tau~=~\\log(N)$ , and perhaps not surprisingly found both performed worse than our proposed choice of $\\tau~=~\\log(N)\\log\\log(p)$ . At the same time however, we stress this particular choice of $\\tau$ is by no means definitive; other, perhaps more data driven choices are possible (see for instance Bai, Rao, and Wu 1999, in a different context using information criteria), although they would likely increase detract from the computational efficiency of the proposed approach. We offer a more detailed discussion regarding the choice of $\\tau$ in Supplementary Material C. <PARAGRAPH_SEPARATOR> Finally, as seen in Algorithm 1, note that SIC can also be used as an early stopping rule to further reduce computation time. Specifically, in Step 3 of Algorithm 1, we only include a new predictor and proceed on the forward solution path if it reduces the current value of SIC. Otherwise, we terminate the solution path early, and select the last fitted candidate GEE. We employ this approach in both our simulations and application below. It is important to emphasize that the use of an early stopping rule based on SIC is theoretically justified, in the sense that it asymptotically still guarantees screening consistency. We provide further discussion of this, as well as empirical studies to support the SIC-based early stopping rule, in supplementary material B."
  },
  {
    "qid": "statistic-posneg-2002-0-4",
    "gold_answer": "FALSE. The prior distribution on the loading matrix W is specified as independent Gaussian priors for each entry w_jk, with mean zero and variance ϕ⁻¹, not as a matrix-variate normal distribution. The entries are independent and identically distributed (i.i.d.) according to N(0, ϕ⁻¹).",
    "question": "Is the prior distribution on the loading matrix W specified as a matrix-variate normal distribution with mean zero and covariance matrix ϕ⁻¹I?",
    "question_context": "Bayesian Probabilistic Principal Component Analysis (PPCA) with Closed-Form Marginal Likelihood\n\nIn this section, we present a prior structure that leads to a closed-form expression for the marginal likelihood of PPCA. We consider the regular PPCA model already defined in (1), where $\\mathbf y_{i}\\sim\\mathcal N(0,\\mathbf I_{d})$ , W is a $p\\times d$ parameter matrix, and $\\varepsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I}_{p})$ . We rely on a Gaussian prior distribution over the loading matrix W and a gamma prior distribution over the noise variance $\\sigma^{2}$ . Specifically, we use a gamma prior $\\sigma^{2}\\sim\\mathrm{Gamma}(a,b)$ with hyperparameters $a>0$ and $b>0$ together with i.i.d. Gaussian priors for the entries of the loading matrix $w_{j k}\\sim\\mathcal{N}(0,\\phi^{-1})$ for $j\\in\\left\\{1,\\ldots,p\\right\\}$ and $k\\in\\{1,\\ldots,d\\}$ with some precision hyperparameter $\\phi>0$ . Within the framework of Bayesian model uncertainty (Kass & Raftery, 1995), the posterior probabilities of models can be written as, for all $d\\in\\left\\{1,\\ldots,p\\right\\}$ , where $p(\\mathbf{X}|a,b,\\phi,\\mathcal{M}_{d})=\\prod_{i=1}^{n}\\int_{\\mathbb{R}^{d\\times p}\\times\\mathbb{R}^{+}}p(\\mathbf{x}_{i}|\\mathbf{W},\\sigma,\\mathcal{M}_{d})p(\\mathbf{W}|\\phi)p(\\sigma|a,b)d\\mathbf{W}d\\sigma,$ is the marginal likelihood (or evidence) of the data under conditional independence (Kass & Steffey, 1989). Note that this expression also involves model prior probabilities—in this paper, we will simply consider a uniform prior $\\forall d\\in\\{1,\\dots,p\\},\\quad p(\\mathcal{M}_{d})\\propto1.$ Computing the high-dimensional integral of the marginal likelihood usually comes at the price of various approximations (Bayesian, 1999; Hoyle, 2008; Minka, 2000) or expensive sampling (Hoff, 2007). However, with our specific choice of priors, and imposing a constraint on their respective hyperparameters, we obtain a closed-form expression for the marginal likelihood. Theorem 1. Let $d\\in\\{1,\\ldots,p\\}$ . Under the normal-gamma prior with $b=\\phi/2,$ , the log-marginal likelihood of model $\\mathcal{M}_{d}$ is given by where $K_{\\nu}$ is the modified Bessel function of the second kind of order $\\nu\\in\\mathbb{R}$ . A detailed proof of this theorem is given in the next subsection. Note that the modified Bessel function $K_{\\nu}$ can be evaluated using most statistical computing software. For instance, we used in our experiments the R package Bessel (Mäechler, 2013). To the best of our knowledge, this result is the first computation of the marginal likelihood of a PPCA model. It is worth mentioning that, in a slightly different context, Ando (2009) also derived the marginal likelihood of a factor analysis model, with Student factors. Similarly, Bouveyron, Latouche, and Mattei (2018) derived the exact marginal likelihood of the noiseless PPCA model, in order to obtain sparse PCs."
  },
  {
    "qid": "statistic-posneg-1421-2-1",
    "gold_answer": "FALSE. The context indicates that the univariate ESS for $Y^{(1)}$ is 5263, which is much lower than the true multivariate ESS of 55,188. The slowest-mixing component $Y^{(1)}$ has the lowest univariate ESS due to its high autocorrelation, but the multivariate ESS, which accounts for cross-correlations, is significantly higher.",
    "question": "Does the context suggest that the univariate effective sample size (ESS) for the slowest-mixing component $Y^{(1)}$ is higher than the multivariate ESS?",
    "question_context": "Multivariate Output Analysis for Markov Chain Monte Carlo\n\nRecall from Theorem 1 that as $\\epsilon$ decreases to zero, the coverage probability of confidence regions created at termination converges to the nominal level. This is demonstrated in Fig. 2, where we present the coverage probability over 1000 replications as $-\\epsilon$ increases. <PARAGRAPH_SEPARATOR> # 5.3. Vector autoregressive process <PARAGRAPH_SEPARATOR> Consider the vector autoregressive process of order 1. For $t=1,2,\\ldots$ <PARAGRAPH_SEPARATOR> $$ Y_{t}=\\Phi Y_{t-1}+\\epsilon_{t}, $$ <PARAGRAPH_SEPARATOR> where $Y_{t}\\in\\mathbb{R}^{p}$ , $\\Phi$ is a $p\\times p$ matrix, $\\epsilon_{t}\\overset{\\mathrm{iid}}{\\sim}N_{p}(0,\\Omega)$ , and $\\Omega$ is a $p\\times p$ positive-definite matrix. We set $Y_{0}$ to be the zero vector. The matrix $\\Phi$ determines the nature of the correlation in the process. This Markov chain has invariant distribution $F=N_{p}(0,V)$ , where vec $(V)=(I_{p^{2}}-\\Phi\\otimes{\\bf\\bar{\\Phi}})^{-1}\\mathrm{vec}(\\Omega)$ <PARAGRAPH_SEPARATOR> with $\\otimes$ denoting the Kronecker product, and is geometrically ergodic when the spectral radius of $\\Phi$ is less than 1 (Tjøstheim, 1990). <PARAGRAPH_SEPARATOR> Consider the goal of estimating the mean of $F$ , $\\operatorname{E}_{F}(Y)=0$ with ${\\bar{Y}}_{n}$ . Then <PARAGRAPH_SEPARATOR> $$ \\Sigma=(I_{p}-\\Phi)^{-1}V+V(I_{p}-\\Phi)^{-1}-V. $$ <PARAGRAPH_SEPARATOR> Let $p=5$ , $\\Phi=\\mathrm{diag}(0.9,0.5,0.1,0.1,0.1)$ , and $\\Omega$ be the ar(1) covariance matrix with autocorrelation 0.9. Since the first eigenvalue of $\\Phi$ is large, the first component mixes slowest. We sample the process for $10^{5}$ iterations and in Fig. 3 present the autocorrelation plot for $Y^{(1)}$ and $\\overset{\\cdot}{Y}^{(3)}$ and the cross-correlation plot between $\\overset{\\cdot}{Y}^{(1)}$ and $Y^{(3)}$ in addition to the trace plot for $Y^{(1)}$ . Notice that $Y^{(1)}$ exhibits higher autocorrelation than $Y^{(3)}$ and there is significant crosscorrelation between $Y^{(1)}$ and $Y^{(3)}$ . Figure 3 also displays joint confidence regions for $Y^{(1)}$ and $Y^{(3)}$ . Recall that the true mean is $(0,0)$ and is present in all three regions, but the ellipse produced by multivariate batch means has significantly smaller volume than the univariate batch means boxes. The orientation of the ellipse is determined by the cross-correlations witnessed in Fig. 3. <PARAGRAPH_SEPARATOR> We set $n^{*}=1000$ , $b_{n}=\\lfloor n^{1/2}\\rfloor$ and $\\epsilon$ in $\\{0.05,0.02,0.01\\}$ , and at termination calculate the coverage probabilities and effective sample size. Results are presented in Table 3. As $\\epsilon$ decreases, termination time increases and coverage probabilities tend to the $90\\%$ nominal level for each method. Also, the uncorrected methods produce confidence regions with undesirable coverage probabilities and thus are not of interest. Consider $\\epsilon=0.02$ in Table 3. Termination for multivariate batch means is at $8.8\\times10^{4}$ iterations compared to $1.1\\times10^{6}$ for Bonferroni-corrected univariate batch means, with effective sample sizes of $4.8\\times10^{4}$ and $5.7\\times10^{4}$ , respectively. This is because the leading component $Y^{(1)}$ mixes much slower than the other components and defines the behaviour of the univariate effective sample size. <PARAGRAPH_SEPARATOR> The true univariate and multivariate effective sample sizes can be obtained in closed form in this case since both $\\Sigma$ and $V$ are known. For a Monte Carlo sample size of $10^{5}$ , the univariate effective sample sizes for the five components are 5263, 33 333, 81 818, 81 818, and 81 818, while the true ess is 55 188. In the absence of cross-correlations, ess is the geometric mean of the univariate ${\\mathrm{ESS}}_{i}$ , which is 39 494 here. Thus, by accounting for the cross-correlation structure of $V$ and $\\Sigma$ , we see an increase in effective sample size."
  },
  {
    "qid": "statistic-posneg-1081-0-0",
    "gold_answer": "FALSE. The condition $\\underset{N}{\\operatorname*{lim}}\\frac{K N_{t r a i n}^{2}}{N}\\frac{\\zeta_{1,N_{t r a i n}}}{\\zeta_{N_{t r a i n},N_{t r a i n}}}=0$ is not strictly necessary for asymptotic normality. While it helps ensure the variance terms vanish appropriately, the key condition is the convergence of the U-statistic's variance to a finite limit, which can occur under weaker assumptions. The paper uses this condition to simplify the proof and ensure clean asymptotic results, but it is not the minimal requirement for normality.",
    "question": "Is the condition $\\underset{N}{\\operatorname*{lim}}\\frac{K N_{t r a i n}^{2}}{N}\\frac{\\zeta_{1,N_{t r a i n}}}{\\zeta_{N_{t r a i n},N_{t r a i n}}}=0$ necessary for the asymptotic normality of the U-statistic in the random forest-based two-sample test?",
    "question_context": "Asymptotic Properties of Random Forest-Based Two-Sample Tests\n\nThe paper analyzes the asymptotic properties of random forest-based two-sample tests, focusing on the decision rule's level conservation and power. Key formulas include the asymptotic normality of the U-statistic and the conditions under which the test conserves the level and achieves consistent power."
  },
  {
    "qid": "statistic-posneg-2084-1-2",
    "gold_answer": "FALSE. The context clearly derives this result in Appendix B, showing that the variance of $B_0$ is indeed $\\frac{2}{n}\\mathrm{tr}(\\mathbf{T}\\Sigma)^2$. This follows from the properties of quadratic forms and the normal distribution assumptions.",
    "question": "Is the variance of the quadratic form $B_0$ given by $\\operatorname{Var}(B_0) = \\frac{2}{n}\\mathrm{tr}(\\mathbf{T}\\Sigma)^2$ incorrect?",
    "question_context": "Cumulants and Covariance Structures in Quadratic and Bilinear Forms\n\nThe paper discusses the properties of quadratic and bilinear forms under normal distribution assumptions, providing detailed derivations for their cumulants and covariance structures. Key formulas include the rth cumulant of quadratic forms, the representation of bilinear forms as quadratic forms, and various covariance expressions between different forms."
  },
  {
    "qid": "statistic-posneg-519-0-0",
    "gold_answer": "TRUE. According to Table 1, GCV has a lower root fitting MSE (15.363) and root prediction MSE (15.514) compared to ASP-U (15.675 and 15.871) and ASP-A (15.719 and 15.870). This indicates that GCV provides better fitting and prediction accuracy. However, the proposed asympirical methods are faster in terms of CPU time, as shown by the significantly lower values for ASP-U (0.030s) and ASP-A (0.790s) compared to GCV (40.560s).",
    "question": "Is it true that the generalized cross-validation method (GCV) outperforms the proposed asympirical methods (ASP-U and ASP-A) in terms of fitting and prediction mean squared errors, as suggested by the results in Table 1?",
    "question_context": "Smoothing Spline ANOVA Model Performance Comparison\n\nThe paper compares different smoothing parameter selection methods for smoothing spline ANOVA models in large samples. The model considered is a cubic tensor product smoothing spline ANOVA model with the functional ANOVA decomposition: $$\\eta(x)=\\eta_{\\vartheta}+\\sum_{j=1}^{42}\\eta_{j}(x_{\\langle j\\rangle}),$$ where $\\eta_{\\alpha}$ is a constant function and $\\eta_{1}(x_{\\langle1\\rangle}),\\dots,\\eta_{42}(x_{\\langle42\\rangle})$ denote the main-effect functions for 42 selected features. The number of basis functions for all methods is taken to be $\\bar{1}0n^{2/9}$. The performance of methods like ASP-U, ASP-A, GAM-GCV, REML, BAM, and GCV is compared in terms of R2, fitting MSE, prediction MSE, and CPU time."
  },
  {
    "qid": "statistic-posneg-1051-0-2",
    "gold_answer": "FALSE. The effectiveness of the bias-correction procedure for the intercept estimate depends on the distribution of the data below the boundary. For example, in the case of a linear boundary with uniformly distributed data, the bias-correction improves the intercept estimate but has little or no effect on the slope estimate due to the symmetry of the asymptotic distribution of the slope estimate. The procedure's effectiveness may vary with different data distributions, such as bimodal distributions.",
    "question": "Is the bias-correction procedure for the intercept estimate effective regardless of the distribution of the data below the boundary?",
    "question_context": "Construction of Bias-Corrected Boundary Estimates and Confidence Bands\n\nIn this section we present the results of three different numerical experiments. The first demonstrates the construction of bias-corrected boundary estimates and $95\\%$ upper confidence bands for simulated data-sets having either linear or quadratic boundaries and any one of several different distributions. The second experiment estimates coverages of nominal $95\\%$ confidence bands in the case of a linear boundary. The third experiment addresses boundary estimation for a real data-set from productivity frontier analysis. For those data, several bias-corrected boundary estimates as well as $95\\%$ upper confidence bands are presented.<PARAGRAPH_SEPARATOR>Before proceeding to the results of these experiments we briefly discuss construction of upper confidence bands. Two distinct methods were employed. The first is based on a ``template'' curve which is scaled until it exceeds (for all $x\\in{\\mathcal{I}}$ ) the desired proportion of the simulated boundaries derived from the asymptotic distribution approximation $\\hat{g}-g\\approx\\hat{\\nu}^{-1}\\xi$ . (In all the numerical experiments presented here we assume that $\\alpha=1.$ ) For example, the simplest, ``constant template'' results in a vertical shift of the maximum likelihood estimate, and so equals $\\hat{g}(x)+d,$ , where $d$ is defined as the smallest value satisfying the equation<PARAGRAPH_SEPARATOR>$$ P\\{\\operatorname*{max}_{x\\in\\mathcal{I}}(-\\hat{\\nu}^{-1}\\xi)<d\\}=0.95. $$<PARAGRAPH_SEPARATOR>Another possible template is ${\\hat{g}}(x)+d s(x)$ , where the function $s(x)$ is an estimate of the standard deviation of the boundary estimate at $x$ . However, since the template curve is extremely ``flexible'' in nature, and so can adopt rather complex shapes, this procedure tends to be conservative in small samples.<PARAGRAPH_SEPARATOR>The second method of confidence band construction is based on using a ``ranking'' function, $R(\\theta)$ , for boundary estimates, and setting the upper confidence band equal to the upper envelope of those boundaries corresponding to the set of $\\theta$ values defined by<PARAGRAPH_SEPARATOR>$$ \\theta=\\{\\theta:R(\\theta)<r_{0.95}\\}, $$<PARAGRAPH_SEPARATOR>where $r_{0.95}$ is the 95th percentile of the distribution of $R(-\\hat{\\nu}^{-1}\\xi)$ . Using this method, the height of the $95\\%$ upper confidence band at $x$ would be defined by<PARAGRAPH_SEPARATOR>$$ {\\hat{g}}(x)+\\operatorname*{min}_{\\theta\\in\\Theta}g(x\\mid\\theta). $$<PARAGRAPH_SEPARATOR>One possible choice of $R$ is $R(\\theta)=G(\\theta)$ , the latter defined in Section 2, representing the area below the boundary. It has intuitive appeal by virtue of its association with the maximum likelihood estimation procedure. Alternatively we could use the are above the maximum likelihood estimate, leading to:<PARAGRAPH_SEPARATOR>$$ R(\\theta)=\\int_{A}\\left\\{g(x\\mid\\theta)-\\hat{g}(x)\\right\\}d x,\\qquad\\mathrm{where}\\quad A=\\left\\{x:g(x\\mid\\theta)-\\hat{g}(x)>0\\right\\}. $$<PARAGRAPH_SEPARATOR>It is also possible to construct methods based on a hybrid of these two schemes. However, care must be taken to avoid creating methods which choose an overly complex set of $95\\%$ of the simulated curves, lest the procedure have poor coverage accuracy. Finally, we point out that all our numerical experiments rely on being able to generate curves from the distribution of the random vector $D$ . In principle, this requires calculation of an infimum over an infinite collection of random variables. However, there exists an algorithm which enables the infimum to be computed from a relatively small (although random) finite number of these quantities. The algorithm is available from the authors.<PARAGRAPH_SEPARATOR># Bias-Corrected Boundary Estimates and $95\\%$ Upper Confidence Bands<PARAGRAPH_SEPARATOR>We start by examining simulated data below the true linear boundary, $g(x)=1+2x$ . In each of the following procedures, 1500 simulations were used to estimate means and quantiles of the asymptotic distribution of the maximum likelihood boundary estimator. Figure 1a shows a simulated data-set of size $n=25$ , uniformly distributed below its boundary. The figure depicts the true boundary, the maximum likelihood estimate and the biascorrected maximum likelihood estimate, calculated under the assumption that the data are uniformly distributed below the boundary. With this assumption we do not need to estimate $\\mu$ . Figure 1b depicts another plot of a simulated data-set, this time of size $n=50$ , where the data are uniformly distributed below the boundary $g(x)=1+2x$ . In this plot, however, the bias-corrected estimate is calculated by estimating $\\hat{\\mu}$ , using bandwidths $h_{1}=0.05$ and $h_{2}=0.5$ . Figure 1c presents a similar situation with $n=60$ , but there, the abscissa distribution is bimodal rather than uniform, with modes at 0.25 and 0.67. For each of these plots, biascorrection clearly improves the intercept estimate. Of course, there is little or no improvement in the bias of the slope estimate, in the case of a linear boundary with uniformly distributed data, since the asymptotic distribution of the slope estimate is symmetric about its true value (see Theorem 2.2)."
  },
  {
    "qid": "statistic-posneg-445-0-0",
    "gold_answer": "FALSE. The local linear-additive estimator is designed to achieve the optimal convergence rate of order O(n^{-4/(4+p)}) regardless of whether the model is additive or nonadditive. It adapts to both local and global additivity automatically.",
    "question": "The local linear-additive estimator achieves the optimal convergence rate of order O(n^{-4/(4+p)}) only when the model is globally nonadditive. True or False?",
    "question_context": "Local Linear-Additive Estimation for Nonparametric Regression\n\nThe paper discusses local linear-additive estimation for nonparametric regression, combining local linear and local additive estimators to adapt to local and global additivity. The estimator is designed to achieve optimal convergence rates whether the model is additive or nonadditive."
  },
  {
    "qid": "statistic-posneg-1923-3-0",
    "gold_answer": "TRUE. The context explicitly states in Theorem 3 that for each C > 0, the supremum of the absolute difference between the estimated error rate (êrr) and the true error rate (err) converges to 0 in probability. This is mathematically represented as sup_{β,r;C} |êrr(β1, β2, R1, R2) - err(β1, β2, R1, R2)| → 0 in probability. This convergence is a direct result of the consistency properties derived under Property 4, which includes conditions on the boundedness of parameters and the uniqueness of the minimizers.",
    "question": "Is the statement 'The estimator (êrr) for the error rate converges in probability to the true error rate (err) as sample sizes n0 and n1 diverge' supported by the formulas and theorems in the context?",
    "question_context": "Consistency of Error Rate Minimization in Tiered Classifiers\n\nThe paper discusses a tiered classifier approach where the error rate is minimized through a combination of feature selection and parameter optimization. The error rate is defined as a weighted sum of misclassification probabilities for two populations, Π0 and Π1. The estimators for the error rate and the optimal parameters are shown to be consistent under certain conditions."
  },
  {
    "qid": "statistic-posneg-1953-4-2",
    "gold_answer": "FALSE. The CLE should separate the deterministic drift (based on reaction rates) and the stochastic diffusion (based on noise). The provided drift term (c_1x_1,t - c_2x_1,tx_2,t, c_1x_1,t) seems plausible, but the diffusion term is not fully justified in the context. The diffusion matrix should be derived from the hazard function and stoichiometry matrix, which is not clearly shown in the given context.",
    "question": "The CLE for the aphid growth model includes a drift term (c_1x_1,t - c_2x_1,tx_2,t, c_1x_1,t) and a diffusion term involving Brownian motion. Does this formulation correctly represent the deterministic and stochastic components of the system dynamics?",
    "question_context": "Stochastic Kinetic Models: Aphid Growth and Lotka-Volterra Dynamics\n\nThe paper presents stochastic kinetic models for aphid growth and Lotka-Volterra dynamics, detailing the stoichiometry matrices, hazard functions, and associated differential equations for the Chemical Langevin Equation (CLE) and Linear Noise Approximation (LNA). The models describe the evolution of system states over time, incorporating reaction rates and stochastic noise."
  },
  {
    "qid": "statistic-posneg-937-0-0",
    "gold_answer": "TRUE. The formula shows that $O_{\\mathrm{ratio}}$ is a weighted average of the positive likelihood ratio (LR+) and negative likelihood ratio (LR-), with weights given by the sampling ratio $R$ and $(1-R)$, respectively. This reflects the design's reliance on the surrogate's ability to correctly identify cases (LR+) and non-cases (LR-), weighted by how much the design over-represents surrogate positives.",
    "question": "Is it true that for a rare outcome, the formula $O_{\\mathrm{ratio}}(R,Z)\\approx(R)(L R+)+(1-R)(L R-)$ implies that $O_{\\mathrm{ratio}}$ is a weighted average of the positive and negative likelihood ratios of the surrogate $Z$?",
    "question_context": "Surrogate-Guided Sampling (SGS) Design Efficiency and Likelihood Ratios\n\nThe denominator of $O_{\\mathrm{ratio}}$ is the expected odds of cases for samples collected with SRS, and is less than 1 for outcome with prevalences less than $50\\%$ . The numerator is the expected odds of cases for samples collected with SGS designs. Therefore, $O_{\\mathrm{ratio}}$ is a single estimate of design effect on sample outcome prevalence, and can be interpreted as the expected increase in cases comparing SGS to SRS, with higher values indicating that SGS provides more case enrichment, and $O_{\\mathrm{ratio}}>1$ indicating improvement using SGS relative to SRS. An interesting property of $O_{\\mathrm{ratio}}$ is the connection to likelihood ratios (LRs) of the enrichment surrogate. Of note, LRs of a diagnostic test can be interpreted as slopes of Receiver Operating Characteristics (ROC) curves, are related to positive and negative predictive values (PPV and NPV), but are invariant to outcome prevalence (Choi, 1998). Therefore, by framing enrichment surrogates $Z$ as “prior tests” of outcome $Y$ , we may gain insight into what types of variables are the best surrogates for sampling. <PARAGRAPH_SEPARATOR> # PROPOSITION 2.1 Properties of $O_{\\mathrm{ratio}}$ . <PARAGRAPH_SEPARATOR> Let an SGS design of sample size $n$ be defined with surrogate $Z$ and sampling ratio $R=P(Z=1|S=1)$ ), where $R=0.50$ corresponds to a “balanced” SGS 1:1 design. Let $Z$ has operating characteristics: $Z_{\\mathrm{sens}}:=$ $P(Z=1|Y=1)$ ), $Z_{\\mathrm{spec}}=P(Z=0|Y=0)$ . Then, if the outcome is rare $(P(Y=1)\\approx0)$ ), then <PARAGRAPH_SEPARATOR> $$ O_{\\mathrm{ratio}}(R,Z)\\approx(R)(L R+)+(1-R)(L R-). $$ <PARAGRAPH_SEPARATOR> COROLLARY 2.1 For a given Z, Oratio ∝ R. Over the set of possible Z, Oratio ∝ Zsens and Oratio ∝ 1 −1Zspec . <PARAGRAPH_SEPARATOR> Details of Proposition 2.1 and Corollary 2.1 are in Supplementary Material B available at Biostatistics online. For a given surrogate, higher values of $O_{\\mathrm{ratio}}$ can be achieved by over-representing surrogate positives. Over the range of possible surrogates, a small change in surrogate specificity can have a much higher impact on $O_{\\mathrm{ratio}}$ compared to the same change in surrogate sensitivity. Figure 1 demonstrates the impact of surrogate operating characteristics on $O_{\\mathrm{ratio}}$ for a fixed sampling ratio of $R=0.50$ (i.e., SGS 1:1 ratio of cases to control, a “balanced” design) and prevalence of $10\\%$ . An SGS design with $O_{\\mathrm{ratio}}>2$ , may be obtained by using a surrogate with (sensitivity, specificity) of $(30\\%,90\\%)$ or $(90\\%,80\\%$ ), where in order to maintain the same sample case–control odds a small decrease of surrogate specificity required a much larger increase in surrogate sensitivity. In fact, very high information designs, such as $O_{\\mathrm{ratio}}>3$ , can only be achieved when surrogate specificity is at least $90\\%$ . <PARAGRAPH_SEPARATOR> ![](images/b0224c8b98d0cd482477b2eb83b4ed8441373becdb407c60c75470385df3e6ba.jpg)  Fig. 1. $O_{\\mathrm{ratio}}$ values for surrogates of different marginal sensitivity and specificity, based on a fixed $R=0.50$ and an outcome with prevalence of $10\\%$ ."
  },
  {
    "qid": "statistic-posneg-1329-0-0",
    "gold_answer": "TRUE. The context explicitly states that Laurie's recursive procedure is used to solve this equation, and it is noted for its efficiency, requiring O(n) divisions. This method is contrasted with Wilson's approach, which requires O(n²) divisions, making Laurie's method more efficient for large n.",
    "question": "Is the equation H(x)G(x⁻¹) + H(x⁻¹)G(x) = W(x) correctly solved using the recursive procedure by Laurie (1980) as described in the context?",
    "question_context": "Cramér-Wold Factorization and Recursive Solutions in ARMA Processes\n\nThe context discusses auxiliary algorithms for solving specific equations related to polynomial functions in the context of ARMA processes. The key equations involve polynomial functions G(x), H(x), and W(x), and their relationships. The recursive procedure by Laurie (1980) is highlighted for its efficiency, requiring O(n) divisions compared to O(n²) in Wilson (1979). The equations include transformations and operations such as Y(J) ← X(J) - P*X(M+1-J) and H(x) = G(x) + W(x)W(x⁻¹)."
  },
  {
    "qid": "statistic-posneg-2049-1-0",
    "gold_answer": "TRUE. The text explicitly states that Design 2.4.1 has a breakdown number of eight, meaning it can handle up to three drop-outs without becoming disconnected. However, if four or more subjects drop out (as in Design 2.4.3), the design becomes disconnected, and no unbiased estimators exist for the treatment contrasts, despite having equal replications of treatments A and B.",
    "question": "In the context of cross-over designs, is it true that Design 2.4.1 becomes disconnected and loses the ability to provide unbiased estimators for treatment contrasts ($\\tau_{A}-\\tau_{B}$ and $\\rho_{A}-\\rho_{B}$) if four or more subjects drop out in the third or fourth periods?",
    "question_context": "Robustness of Cross-Over Designs to Subject Drop-Outs\n\nDesign 2.4.1. From Jones and Kenward (2015)<PARAGRAPH_SEPARATOR>![](images/83093d034e75a83c10a3adf1926b7eb928755351785fa89606e49f50ab74a9f5.jpg)  \nDesign 2.4.2. Hedayat-Stufken Design   \nDesign 2.4.3. Drop-out from Design 2.4.1<PARAGRAPH_SEPARATOR>Each of these two designs has 19 degrees of freedom available for the residual error variance. Each design is robust to the possibility that up to three subjects drop out of the study in the third or fourth period. However the breakdown number for Design 2.4.1 is eight, consequently the experiment based on Design 2.4.1 is at risk if four or more subjects leave prematurely. For example, Design 2.4.3 is the eventual design after four subjects dropped out in the third period, where the symbol ‘*’ signifies a missing observation. Design 2.4.3 is disconnected. No unbiased estimator of the direct treatment contrast $\\tau_{A}-\\tau_{B}$ exists even though twelve replications of treatment A and twelve replications of treatment $B$ have been recorded successfully. Furthermore no unbiased estimator of the carry-over treatment contrast $\\rho_{A}-\\rho_{B}$ exists.<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l r l r l r l}{{\\normalcolor A}}&{{\\normalcolor B}}&{{\\normalcolor A}}&{{\\normalcolor B}}&{{\\normalcolor A}}&{{\\normalcolor B}}&{{\\normalcolor A}}&{{\\normalcolor B}}\\\\ {{\\normalcolor B}}&{{\\normalcolor A}}&{{\\normalcolor B}}&{{\\normalcolor A}}&{{\\normalcolor B}}&{{\\normalcolor A}}&{{\\normalcolor B}}&{{\\normalcolor A}}\\\\ {*}&{{\\normalcolor A}}&{{\\normalcolor*}}&{{\\normalcolor A}}&{{\\ast}}&{{\\normalcolor A}}&{{\\ast}}&{{\\normalcolor A}}\\\\ {*}&{{\\normalcolor B}}&{{\\normalcolor*}}&{{\\normalcolor B}}&{{\\ast}}&{{\\normalcolor B}}&{{\\ast}}&{{\\normalcolor B}}\\end{array}$$<PARAGRAPH_SEPARATOR>The unpleasant outcome described here is avoided with Design 2.4.2. In this case no drop-out activity in the third or fourth periods will result in a disconnected eventual design. This is indicated by specifying the breakdown number $m_{D}=\\infty$ . Given the assumption that all subjects remain with the study for two periods then contrasts $\\tau_{A}-\\tau_{B}$ and $\\rho_{A}-\\rho_{B}$ have unbiased estimators, irrespective of subject drop-out behaviour in periods three and four. The Hedayat–Stufken design does not have the risk due to drop-out in the third or fourth periods that is associated with Design 2.4.1.<PARAGRAPH_SEPARATOR># 3. Criteria for cross-over design assessment<PARAGRAPH_SEPARATOR># 3.1. Introductory comments"
  },
  {
    "qid": "statistic-posneg-443-1-0",
    "gold_answer": "TRUE. The joint density function correctly includes the product of differences (λ_i-λ_j) to account for the ordering of the eigenvalues (0<λ₁<⋯<λₚ<∞) and the term ∏_{i=1}^p λ_i^m/(1+λ_i)^{m+n+p+1} to reflect the distribution of the eigenvalues of S₁S₂⁻¹. This form is consistent with the known results for the joint density of eigenvalues in multivariate statistics, particularly for the generalized T² distribution.",
    "question": "Is the joint density function f(λ₁,...,λₚ) correctly specified to include the product of differences (λ_i-λ_j) and the term ∏_{i=1}^p λ_i^m/(1+λ_i)^{m+n+p+1}?",
    "question_context": "Laplace Transform and Density Function of U^(p) in Hotelling's Generalized T^2 Distribution\n\nThe joint density function of λ₁,λ₂,...,λₚ, the characteristic roots of S₁S₂⁻¹, has the form [18] f(λ₁,...,λₚ)=C(ϕ,m,n)∏_{i=1}^p λ_i^m/(1+λ_i)^{m+n+p+1}∏_{i>j}(λ_i-λ_j), 0<λ₁<⋯<λₚ<∞, where C(p̂,m,n)=π^{p̂/2}∏_{i=1}^p Γ(½(2m+2n+p̂+i+2))/{Γ(½(2m+i+1))Γ(½(2n+i+1))Γ(½i)}. Then U^(p)=∑_{i=1}^p λ_i=tr(S₁S₂⁻¹), and the Laplace transform of U^(p) is L(t;p̂,m,n)=E(exp(-t∑_{i=1}^p λ_i))=C∫⋯∫ exp(-t∑_{i=1}^p λ_i)∏_{i=1}^p λ_i^m/(1+λ_i)^{m+n+p+1}∏_{i>j}(λ_i-λ_j)∏_{i=1}^p dλ_i."
  },
  {
    "qid": "statistic-posneg-1969-0-0",
    "gold_answer": "TRUE. The paper states that under the null hypothesis H_0^(M), and assuming large enough cell counts, the M-moment score test statistic X_M^2 has an approximate chi-squared distribution with M(I-1) degrees of freedom. This is derived from MPH (multinomial–Poisson homogeneous) theory and standard likelihood theory, as referenced in the paper.",
    "question": "Is it true that the M-moment score test statistic X_M^2 follows an approximate chi-squared distribution with M(I-1) degrees of freedom under the null hypothesis H_0^(M)?",
    "question_context": "M-moment Score Test for Independence in Contingency Tables\n\nThe paper introduces an M-moment score test for independence in contingency tables, focusing on the equality of the first M conditional moments across rows. The test uses a relaxed-null hypothesis and a Pearson-form statistic to assess independence, with the test statistic following an approximate chi-squared distribution under the null hypothesis."
  },
  {
    "qid": "statistic-posneg-1618-0-4",
    "gold_answer": "FALSE. The paper shows in Proposition 3 that the density involves conditional distributions (F₁|u₂,...,u_{d-1} and F_d|u₂,...,u_{d-1}) and their derivatives, along with the copula density c and the density f_{2,...,d-1}.",
    "question": "Is it correct that the density of a pair Lévy copula can be expressed purely in terms of the marginal densities without involving any conditional distributions?",
    "question_context": "Vine Constructions of Lévy Copulas\n\nThe central theorem for the construction is Theorem 2. It states that two $(d\\mathrm{~-~}1)$ -dimensional Lévy copulas with overlapping $(d-2)$ -dimensional margins may be coupled to a $d\\cdot$ -dimensional Lévy copula by a new, 2-dimensional distributional copula. Ensured by vine constructions (see Bedford and Cooke [7]) and starting at $(d-1)=2$ , Theorem 2 therefore enables to sequentially construct Lévy copulas out of 2-dimensional dependence functions, i.e., 2-dimensional distributional copulas and Lévy copulas. Before we state the theorem, for convenience, we recall some definitions which can be found, e.g., in Ambrosio et al. [2]."
  },
  {
    "qid": "statistic-posneg-534-0-2",
    "gold_answer": "TRUE. The global AIC criterion evaluates the overall model fit (using $\\hat{\\sigma}^2 = \\|\\mathbf{Y} - \\tilde{\\mathbf{Y}}\\|^2/n$) and complexity (via $\\operatorname{tr}(H)/n$) across all observations. Unlike the local AIC, it is not restricted to a specific region $R_k$ and instead optimizes the global parameters $h$ and $\\lambda$ for the entire estimator.",
    "question": "The global AIC criterion $AIC(h, \\lambda) = \\log(\\hat{\\sigma}^2) + 2\\operatorname{tr}(H)/n$ is used to select the global bandwidth $h$ and regularization parameter $\\lambda$ after fitting the local N-W-additive estimator. Does this criterion operate on the entire dataset rather than within individual regions?",
    "question_context": "Local Linear-Additive Estimation and Bandwidth Selection via AIC\n\nThe paper discusses a method for selecting local regularization parameters and bandwidths in nonparametric regression using an AIC-type criterion. The approach involves splitting the support of covariates into rectangular regions, fitting local additive estimators within each region, and smoothing the selected parameters. The AIC criterion is used to choose optimal bandwidths and regularization parameters both locally and globally."
  },
  {
    "qid": "statistic-posneg-814-0-2",
    "gold_answer": "FALSE. While simulated annealing is designed to escape local maxima by allowing probabilistic transitions, it is not guaranteed to find the global maximum. The success depends on the cooling schedule and the amount of computation time, and even then, it may only approximate the global maximum.",
    "question": "Is the simulated annealing method guaranteed to always find the global maximum of $P(x|y)$?",
    "question_context": "Markov Random Fields and Image Reconstruction\n\nWe shall be dealing with rectangular arrays of pixels but it is notationally convenient, for the moment, merely to suppose that the (two-dimensional) region S has been partitioned into $\\pmb{n}$ pixels, labelled in some manner by the integers $i=1,2,\\ldots,n$ . Each pixel can take one of $c$ colours, labelled $1,2,\\ldots,c,$ with $c$ finite. We assume there are no deterministic exclusions, so that the minimal sample space is $\\Omega=\\{1,2,\\ldots,c\\}^{n}$ : in Section 4, we briefly describe an exception and also the case of continuous “colours\", which we then refer to as \"intensities\". An arbitrary colouring of S will be denoted by $x=(x_{1},x_{2},\\ldots,x_{n}),$ where $x_{i}$ is the corresponding colour of pixel i. We write $x^{*}$ for the true but unknown scene and interpret this as a particular realization of a random vector $X=(X_{1},X_{2},...,X_{n}) $ where $X_{i}$ assigns colour to pixel i. We let $y_{i}$ denote the observed record at $i$ and $y$ the corresponding vector, interpreted as a realization of a random vector, $Y=(Y_{1},Y_{2},\\ldots,Y_{n})$ ; recall that $Y_{i}$ may itself have several components. For definiteness, we take $Y_{i}$ to be continuous but the discrete case, perhaps where $Y_{i}$ is itself a colour, is entirely analogous. We use $P(\\cdot)$ and, on occasion, $P_{T}(\\cdot)$ to denote probabilities of named events."
  },
  {
    "qid": "statistic-posneg-295-0-1",
    "gold_answer": "TRUE. The estimation aligns with the model's structure: (1) $\\hat{\\tau}_{j n}^{2}$ computes the variance of batch-specific residual means ($\\bar{r}.{j k n}$) around their overall mean ($\\bar{r}.{j.n}$), thus quantifying how much probe effects vary across batches (between-batch variability). (2) $\\hat{\\sigma}_{j n}^{2}$ computes the average squared deviation of individual residuals ($r_{i j k n}$) from their batch-specific means ($\\bar{r}_{.j k n}$), thus quantifying variability within each batch (within-batch variability). This decomposition is consistent with the model's assumption that $\\gamma_{j k n}$ (batch-specific probe effect) and $\\varepsilon_{i j k n}$ (within-batch error) are independent sources of variation.",
    "question": "The paper estimates the variance components $\\tau_{j n}^{2}$ and $\\sigma_{j n}^{2}$ using the residuals $\\bar{r_{i j k n}}=\\bar{Y_{i j k n}}-(\\hat{\\theta}_{i n}+\\hat{\\phi}_{j n})$. The formulas are $\\hat{\\tau}_{j n}^{2}=\\frac{1}{K}\\sum_{k}(\\bar{r}.{j k n}-\\bar{r}.{j.n})^{2}$ and $\\hat{\\sigma}_{j n}^{2}=\\frac{1}{I K}\\sum_{k}\\sum_{i}(r_{i j k n}-\\bar{r}_{.j k n})^{2}$. Is it accurate to say that $\\hat{\\tau}_{j n}^{2}$ captures between-batch variability while $\\hat{\\sigma}_{j n}^{2}$ captures within-batch variability?",
    "question_context": "Probe-Level Variability Modeling in Frozen Robust Multiarray Analysis (fRMA)\n\nThe paper presents a probe-level model for microarray data analysis, accounting for batch effects and probe-specific variability. The model is given by: $$Y_{i j k n}=\\theta_{i n}+\\phi_{j n}+\\gamma_{j k n}+\\varepsilon_{i j k n}.$$ Here, $\\theta_{i n}$ represents the sample effect, $\\phi_{j n}$ the probe effect, $\\gamma_{j k n}$ the batch-specific probe effect (random effect with variance $\\tau_{j n}^{2}$), and $\\varepsilon_{i j k n}$ the within-batch variability (with probe-specific variance $\\sigma_{j n}^{2}$). The paper describes a robust procedure to estimate these parameters and variance components using residuals from the model fit."
  },
  {
    "qid": "statistic-posneg-43-0-2",
    "gold_answer": "FALSE. The matrix $A^T$ is not diagonal as it contains non-zero off-diagonal elements $\\lambda_{21}, \\lambda_{42}, \\lambda_{52}, \\lambda_{73},$ and $\\lambda_{83}$, which represent factor loadings connecting different latent variables to observed variables.",
    "question": "Is the matrix $A^T$ in the measurement equation $\\pmb{\\mu}_i = \\pmb{u} + \\pmb{A}\\pmb{\\xi}_i$ a diagonal matrix?",
    "question_context": "Bayesian Analysis of Non-Linear Structural Equation Models with Non-Ignorable Missing Data\n\nThe paper presents a Bayesian analysis of non-linear structural equation models with non-ignorable missing data. It discusses various missingness mechanism models and compares them using path sampling to compute Bayes factors. The models include logistic regression models for missing data mechanisms and a non-linear structural equation model for latent variables. The analysis is applied to a real-world dataset from the World Values Survey, focusing on female respondents in Russia."
  },
  {
    "qid": "statistic-posneg-1733-1-0",
    "gold_answer": "FALSE. The FWSA mechanism does not follow a winner-take-all approach as described. Instead, it adjusts weights proportionally based on the ratio b_n/a_n for each feature, normalized by the sum of all such ratios. The final weights are a weighted average of the previous weights and the adjustment margin, as shown in the formula: $$w_{n}^{(s+1)}=\\frac{1}{2}\\left(w_{n}^{(s)}+\\varDelta w_{n}^{(s)}\\right).$$ This ensures that all features contribute to the clustering, albeit with different weights, rather than having a single feature dominate.",
    "question": "Is the statement 'The FWSA mechanism assigns a weight of 1 to the feature with the highest b_n/a_n ratio and 0 to all other features' true based on the proposed weight adjustment formula?",
    "question_context": "Feature Weight Self-Adjustment (FWSA) Mechanism in K-means Clustering\n\nThe paper proposes a feature weight self-adjustment (FWSA) mechanism embedded into the K-means algorithm to improve clustering quality. The mechanism iteratively updates feature weights based on the ratio of between-cluster separations to within-cluster separations for each feature. The weight adjustment is given by: $$\\Delta w_{n}^{(s)}=\\frac{b_{n}^{(s)}/a_{n}^{(s)}}{\\displaystyle\\sum_{n=1}^{N}\\left(b_{n}^{(s)}/a_{n}^{(s)}\\right)}\\quad\\mathrm{for}n=1,2,\\ldots,N.$$ The adjusted weights are then normalized to ensure they sum to one."
  },
  {
    "qid": "statistic-posneg-278-0-2",
    "gold_answer": "TRUE. When $\\eta \\rightarrow \\infty$, the function $\\phi_{\\eta}(a)$ reduces to $a$, making the estimator $\\hat{\\delta}_i = \\hat{\\delta}_{i-1} + a_i$ a regular EWMA without bounds. This eliminates the restriction $\\delta \\geq \\delta_{\\min}$, allowing the estimator to adapt freely to large shifts, as discussed in the context of overcoming steady-state inertia.",
    "question": "Is the condition $\\eta \\rightarrow \\infty$ in the Markovian-type EWMA estimator $\\phi_{\\eta}(a)$ equivalent to removing the restriction $\\delta \\geq \\delta_{\\min}$?",
    "question_context": "Adaptive CUSUM Control Chart with Variable Sampling Intervals\n\nThe paper discusses the design and performance of Variable Sampling Interval (VSI) Adaptive CUSUM control charts for detecting shifts in process mean. It introduces a Markovian model for computing Average Run Length (ARL) and Average Time to Signal (ATS) metrics, utilizing transition probabilities between states defined by the current CUSUM statistic and estimated process mean. Key formulas include the transition probability calculations and the steady-state ATS evaluation."
  },
  {
    "qid": "statistic-posneg-2027-0-3",
    "gold_answer": "FALSE. Explanation: MRSB is a relative measure, as it compares each model's VaR predictions to the mean of all models' predictions. Its value depends on the specific set of models included; adding or removing models alters the mean and thus the MRSB. It is not an absolute measure of efficiency.",
    "question": "The mean relative scaled bias (MRSB) measures the deviation of a model's scaled VaR predictions from the average across all models. Is it correct to say that MRSB is an absolute measure of efficiency, unaffected by the choice of models included in the comparison?",
    "question_context": "Value-at-Risk (VaR) Forecasting Accuracy and Testing\n\nThe paper discusses the testing of VaR prediction models using likelihood ratio tests for unconditional coverage, independence of violations, and conditional coverage. It introduces the Boolean sequence of VaR violations, the empirical downfall probability, and the mean relative scaled bias (MRSB) for comparing model efficiency."
  },
  {
    "qid": "statistic-posneg-1430-5-0",
    "gold_answer": "FALSE. Explanation: For $t=3/10$, the first inequality becomes $1.55D^{-1/2}n^{(3/10+1)/2} = 1.55D^{-1/2}n^{13/20} \\leqslant \\beta_{n} \\leqslant 2.46D^{-1/2}n^{13/20}$, which corresponds to the rate $n^{13/20}$ (not $n^{3/10}$) as suggested by (3.22). The second inequality becomes $62.58D^{-1}n^{3/10} \\leqslant \\beta_{n} \\leqslant 99D^{-1}n^{3/10}$, which corresponds to the rate $n^{3/10}$ (not $n^{13/20}$) as suggested by (3.23). Thus, the claim is inconsistent with the given inequalities.",
    "question": "Is the claim that the formula (3.22) suggests the rate $n^{3/10}$ for $\\beta_{n}$ while (3.23) gives the rate $n^{13/20}$ consistent with the given inequalities $1.55D^{-1/2}n^{(t+1)/2}\\leqslant\\beta_{n}\\leqslant2.46D^{-1/2}n^{(t+1)/2}$ and $62.58D^{-1}n^{t}\\leqslant\\beta_{n}\\leqslant99D^{-1}n^{t}$ when $t=3/10$?",
    "question_context": "Asymptotic Rate Analysis for Nonempty Bins in Density Estimation\n\nWe will discuss a small simulation that illustrates the asymptotic nature and the practical relevance of some of the results of this section. In Example 1 we showed that for a density $f$ with a compact support, $\\beta_{n}$ has asymptotically the rate $\\delta_{n}^{-d}$ . Since such a result could be trivially ``inferred'' simply by counting the number of bins that intersect the support of $f,$ one might ask whether Theorems 3.2 and 3.7 have any practical relevance in this case.<PARAGRAPH_SEPARATOR>Let us consider the density $f$ depicted in Fig. 2. The density has the support [1, 100], it equals $C x^{-2}$ on [2, 99], $C\\approx1.51$ , and it is four times continuously differentiable. Take $\\delta_{n}=D n^{-t}$ . Then, dropping the $1+o(1)$ , the formula (3.17) gives the estimates<PARAGRAPH_SEPARATOR>![](images/683083d1791b257064aad6719a6f7cc581e7f89106cbcc6f7680cd8eedd32378.jpg)  FIG. 3. The (base 10) logarithm of $\\beta_{n}$ is plotted against the logarithm of $n$ (solid line). The bin size was $\\delta_{n}{=}n^{-3/10}$ . The dashed lines are the logarithms of the left and the right sides of (3.23), while the dash-dotted lines are the logarithms of the left and the right sides of (3.22).<PARAGRAPH_SEPARATOR>$$62.58D^{-1}n^{t}\\leqslant\\beta_{n}\\leqslant99D^{-1}n^{t}.$$<PARAGRAPH_SEPARATOR>However, it turns out that due to the rather long tail of $f,$ there is a range of moderate values of $n$ for which it is in fact better to use the more precise formulas (3.4) and (3.12). Substituting (3.20) and (3.21) in (3.4) and (3.12) one gets the estimates<PARAGRAPH_SEPARATOR>$$1.55D^{-1/2}n^{(t+1)/2}\\leqslant\\beta_{n}\\leqslant2.46D^{-1/2}n^{(t+1)/2}.$$<PARAGRAPH_SEPARATOR>Taking $\\delta_{n}$ proportional to $h_{n}^{1+2/s}$ , $s=4$ , gives $t=3/10$ . The formula (3.22) then suggests the rate $n^{3/10}$ for $\\beta_{n}$ while (3.23) gives the quite different rate $n^{13/20}$ . Figure 3 shows for $D=1$ how the faster rate is appropriate for a wide range of values of $n$ while the slower rate manifests itself eventually as $n$ gets sufficiently large. The value of $\\beta_{n}$ was estimated by drawing samples of size $n$ from $f$ and computing the average number of nonempty bins in 100 trials. The variance of the estimates was negligible.<PARAGRAPH_SEPARATOR>![](images/cc2fb52d1868874e90bce623dac316b41324e47f0af3ae0941ca3002b64ac7c6.jpg)  FIG. 4. The (base 10) logarithm of $\\beta_{n}$ is plotted against the logarithm of $n$ (solid line). The bin size was $\\delta_{n}=10n^{-1/5}$ . The dashed lines are the logarithms of the left and the right sides of (3.23) while the dash-dotted lines are the logarithms of the left and the right sides of (3.22).<PARAGRAPH_SEPARATOR>The ranges of $n$ for which the two different rates are valid depend on the multiplicative constant $D$ and the power $t$ that determine the bin size. Figure 4 shows a second simulation with $\\delta_{n}$ proportional to $h_{n}$ (note the discussion in Subsection 2.2 on such a choice). The rates $n^{1/5}$ and $n^{3/5}$ are now suggested for $\\beta_{n}$ by (3.22) and (3.23), respectively."
  },
  {
    "qid": "statistic-posneg-653-0-0",
    "gold_answer": "FALSE. The paper does not explicitly state that c_N^h(k) is always positive-definite. While it mentions that c_N^*(k) is not always positive-definite, leading to potential negative spectral estimates, the focus for c_N^h(k) is on reducing bias and improving asymptotic properties, not necessarily guaranteeing positive-definiteness.",
    "question": "Is the tapered covariance estimate c_N^h(k) always positive-definite, unlike the unbiased estimate c_N^*(k)?",
    "question_context": "Edge Effects and Efficiency in Tapered Whittle Estimation for Stationary Random Fields\n\nThe paper discusses edge effects in spatial statistics and introduces tapered Whittle estimates to mitigate bias in covariance estimation for stationary random fields. The tapered covariance estimate is defined using a taper function h(u), and the asymptotic properties of the tapered Whittle estimator are analyzed, including bias and efficiency under certain assumptions."
  },
  {
    "qid": "statistic-posneg-311-1-0",
    "gold_answer": "TRUE. The condition $\\frac{V_{i12}}{V_{i11}}=\\frac{\\bar{V}_{12}}{\\bar{V}_{11}}$ implies that the ratio of the off-diagonal element to the diagonal element in the top row of $V_{i}$ matches the corresponding ratio in $\\bar{V}$. This means the top row of $V_{i}$ is proportional to the top row of $\\bar{V}$, as the proportionality constant would be the same for both elements. This is a necessary and sufficient condition for no borrowing of strength, as it ensures the residuals $(u_i - \\hat{u}_i)$ are zero, making $B(V_i^{-1}) = 0$.",
    "question": "Is the condition for no borrowing of strength, $\\frac{V_{i12}}{V_{i11}}=\\frac{\\bar{V}_{12}}{\\bar{V}_{11}}$, equivalent to requiring that the top row of $V_{i}$ is proportional to the top row of $\\bar{V}$?",
    "question_context": "Borrowing-of-Strength in Bivariate Meta-Analysis\n\nIn bivariate meta-analysis, with only one secondary outcome, we can obtain reasonably simple explicit expressions for all the quantities that were discussed in the previous section. In particular, the finding that borrowing of strength depends on differences between the $V_{i}\\mathbf{s}$ can be given a constructive interpretation in terms of residuals in a regression model. <PARAGRAPH_SEPARATOR> In the bivariate case, suppose that the variance matrix $V_{i}$ of $y_{i}=(y_{i1},y_{i2})^{\\mathrm{T}}$ is <PARAGRAPH_SEPARATOR> $$ V_{i}=\\left(\\begin{array}{c c}{{\\sigma_{i}^{2}}}&{{\\rho_{i}\\sigma_{i}\\nu_{i}}}\\\\{{\\rho_{i}\\sigma_{i}\\nu_{i}}}&{{\\nu_{i}^{2}}}\\end{array}\\right). $$ <PARAGRAPH_SEPARATOR> So $(\\sigma_{i},\\nu_{i})$ are the standard errors of $(y_{i1},y_{i2}),\\rho_{i}$ is the correlation between them, and the inverse of $V_{i}$ is <PARAGRAPH_SEPARATOR> $$ V_{i}^{-1}=\\frac{1}{1-\\rho_{i}^{2}}\\left(\\begin{array}{c c}{{\\sigma_{i}^{-2}}}&{{-\\rho_{i}\\sigma_{i}^{-1}\\nu_{i}^{-1}}}\\\\{{-\\rho_{i}\\sigma_{i}^{-1}\\nu_{i}^{-1}}}&{{\\nu_{i}^{-2}}}\\end{array}\\right). $$ <PARAGRAPH_SEPARATOR> Adding equation (18) over the $n$ studies, and taking the inverse, gives the harmonic mean <PARAGRAPH_SEPARATOR> $$ {\\bar{V}}=n\\Omega={\\frac{n}{s_{11}s_{22}-s_{12}^{2}}}\\left({s_{22}}\\quad{s_{12}}\\right) $$ <PARAGRAPH_SEPARATOR> where $(s_{11},s_{22},s_{12})$ are weighted between-studies sums of squares and products of the outcome accuracies $(\\sigma_{i}^{-1},\\nu_{i}^{-1})$ , <PARAGRAPH_SEPARATOR> $$ s_{11}=\\sum\\frac{\\sigma_{i}^{-2}}{1-\\rho_{i}^{2}}, $$ <PARAGRAPH_SEPARATOR> $$ s_{22}=\\sum\\frac{\\nu_{i}^{-2}}{1-\\rho_{i}^{2}}, $$ <PARAGRAPH_SEPARATOR> $$ s_{12}=\\sum\\frac{\\rho_{i}}{1-\\rho_{i}^{2}}\\sigma_{i}^{-1}\\nu_{i}^{-1}, $$ <PARAGRAPH_SEPARATOR> with weights depending on different functions of the within-study correlations $\\rho_{i}$ . As expected, each of these quantities retains the feature of a harmonic average. <PARAGRAPH_SEPARATOR> Apart from these differences in the weights, equations (20)–(22) are like the second-order absolute sample moments of the $n$ pairs $(\\bar{\\sigma_{i}^{-1}},\\nu_{i}^{-\\bar{1}})$ , suggesting a through-the-origin linear regression model in which we can examine the extent to which a study’s primary accuracy $\\sigma^{-1}$ can be predicted from its secondary accuracy $\\nu^{-1}$ . Allowing for the different weights, consider predicting $u_{i}$ from $v_{i}$ , where <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{u_{i}=\\displaystyle\\frac{\\rho_{i}}{(1-\\rho_{i}^{2})^{1/2}}\\sigma_{i}^{-1},}}\\\\ {{v_{i}=\\displaystyle\\frac{1}{(1-\\rho_{i}^{2})^{1/2}}\\nu_{i}^{-1}.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> If we plot the $n$ observed values of $u_{i}$ against the corresponding values of $v_{i}$ , the least squares slope through the origin is <PARAGRAPH_SEPARATOR> $$ \\frac{\\sum u_{i}v_{i}}{\\sum v_{i}^{2}}=\\frac{s_{12}}{s_{22}}, $$ <PARAGRAPH_SEPARATOR> and so the least squares prediction line is <PARAGRAPH_SEPARATOR> $$ \\hat{u}=\\frac{s_{12}}{s_{22}}v=\\frac{s_{12}}{(1-\\rho^{2})^{1/2}s_{22}}\\nu^{-1}. $$ <PARAGRAPH_SEPARATOR> Requiring the regression line to go though the origin is a natural requirement, since if we know that a study has a very small sample size then we know in advance that both $u$ and $v$ will be close to 0. The definitions of $u$ and $v$ in expression (23) have assumed complete data, but studies with missing data can also be included as in Section 2.2. If only the primary outcome estimate in the ith study is observed, then we take both $\\nu_{i}^{-1}$ and $\\rho_{i}$ to be 0, and so $u_{i}=v_{i}=0$ . If only the secondary outcome estimate is observed, we take $\\sigma_{i}^{-1}$ and $\\rho_{i}$ to be 0, leading to $u_{i}=0$ and $v_{i}=\\nu_{i}^{-1}$ . <PARAGRAPH_SEPARATOR> The plot of the $n$ values of $u_{i}$ against their predicted values $\\hat{u}_{i}$ turns out to be closely related to the borrowing-of-strength function $B(V_{i}^{-1})$ that was defined in Section 2. Using equations (18) and (19), and evaluating the required matrix terms explicitly, we obtain <PARAGRAPH_SEPARATOR> $$ T(V_{i}^{-1})=\\frac{[\\Omega V_{i}^{-1}\\Omega]_{11}}{\\Omega_{11}}=\\frac{s_{22}^{2}\\sigma_{i}^{-2}-2s_{12}s_{22}\\rho_{i}\\sigma_{i}^{-1}\\nu_{i}^{-1}+s_{12}^{2}\\nu_{i}^{-2}}{s_{22}(s_{11}s_{22}-s_{12}^{2})(1-\\rho_{i}^{2})}. $$ <PARAGRAPH_SEPARATOR> Rewriting $\\nu_{i}$ and $\\sigma_{i}$ in terms of $u_{i}$ and $v_{i}$ , and completing the square, gives <PARAGRAPH_SEPARATOR> $$ T(V_{i}^{-1})=\\Omega_{11}\\left\\{\\frac{1-\\rho_{i}^{2}}{\\rho_{i}^{2}}u_{i}^{2}+\\left(u_{i}-\\frac{s_{12}}{s_{22}}v_{i}\\right)^{2}\\right\\}. $$ <PARAGRAPH_SEPARATOR> The first term in the outer brackets is just $\\sigma_{i}^{-2}$ , which is proportional to the univariate variance contribution of the primary outcome, and so the borrowing of strength is just the second term <PARAGRAPH_SEPARATOR> $$ B(V_{i}^{-1})=\\Omega_{11}\\left(u_{i}-\\frac{s_{12}}{s_{22}}v_{i}\\right)^{2}=\\Omega_{11}(u_{i}-\\hat{u}_{i})^{2}. $$ <PARAGRAPH_SEPARATOR> Thus $B(V_{i}^{-1})$ is proportional to the squared residual of the point $(\\hat{u}_{i},u_{i})$ from the diagonal prediction line $u=\\hat{u}$ . For any other study with inverse variance $V^{-1}$ , $B(V^{-1})$ is similarly proportional to the squared residual of its point $(\\hat{u},u)$ from the line and so indicates the (approximate) decrease in var $(\\hat{\\beta}_{1})$ which we would obtain if we were to add this study to the meta-analysis. The proportionality factor is <PARAGRAPH_SEPARATOR> $$ \\Omega_{11}=\\mathrm{var}(\\hat{\\beta}_{1})=\\frac{s_{22}}{s_{11}s_{22}-s_{12}^{2}}. $$ <PARAGRAPH_SEPARATOR> If the ith study has missing data, $(\\hat{u}_{i},u_{i})$ is either $(0,0)$ when the secondary outcome estimate is missing, or . $\\dot{((s_{12}/s_{22})\\nu_{i}^{-1})},0)$ when the primary outcome estimate is missing. In the first case, <PARAGRAPH_SEPARATOR> the point is always on the line and so, as expected, there can be no borrowing of strength. In the second case, the point is down on the horizontal axis and so will generally have a non-zero residual and so, again as expected, will contribute at least some borrowing of strength. <PARAGRAPH_SEPARATOR> The $(\\hat{u}_{i},u_{i})$ plot is easier to interpret if we first scale $u_{i}$ and $\\hat{u}_{i}$ by the factor $\\Omega_{11}^{1/2}$ , giving <PARAGRAPH_SEPARATOR> $$ w_{i}=\\Omega_{11}^{1/2}u_{i}=\\left(\\frac{s_{22}}{s_{11}s_{22}-s_{12}^{2}}\\right)^{1/2}\\frac{\\rho_{i}\\sigma_{i}^{-1}}{(1-\\rho_{i}^{2})^{1/2}} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\hat{w}_{i}=\\Omega_{11}^{1/2}\\hat{u}_{i}=\\left\\{\\frac{s_{12}^{2}}{s_{22}(s_{11}s_{22}-s_{12}^{2})}\\right\\}^{1/2}\\frac{\\nu_{i}^{-1}}{(1-\\rho_{i}^{2})^{1/2}}. $$ <PARAGRAPH_SEPARATOR> We call the scatter plot of $w_{i}$ against $\\hat{w}_{i}$ the borrowing-of-strength plot. Now the ith squared residual from the diagonal line, $(w_{i}-\\hat{w}_{i})^{2}$ , is equal to $\\overset{\\cdot}{B}(\\overset{\\cdot}{V}_{i}^{-1})$ . The combined variance contributions of the secondary outcomes in the meta-analysis are indicated by the scatter of the points about the diagonal regression line. If the points all lie on the line then $B(V_{i}^{-1})=0$ for all $i$ and so $E=1$ . More generally, we can show from the earlier formulae that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{1-E=\\sum(w_{i}-\\hat{w}_{i})^{2},}\\end{array} $$ <PARAGRAPH_SEPARATOR> and so $1-E$ is equal to the residual sum of squares of the points in the borrowing-of-strength plot. <PARAGRAPH_SEPARATOR> To aid interpretation of the borrowing-of-strength plot, equation (26) means that, for efficiency $E$ , the root-mean-squared distance of the points from the diagonal line $w=\\hat{w}$ is <PARAGRAPH_SEPARATOR> $$ \\bar{d}=\\left(\\frac{1-E}{n}\\right)^{1/2}. $$ <PARAGRAPH_SEPARATOR> For example, to achieve $90\\%$ efficiency, the root-mean-squared distance is $\\bar{d}=(10n)^{-1/2}$ . This is indicated on the borrowing-of-strength plot by the two parallel lines <PARAGRAPH_SEPARATOR> $$ w=\\hat{w}\\pm\\left(\\frac{1}{10n}\\right)^{1/2}. $$ <PARAGRAPH_SEPARATOR> These lines give a visual benchmark for interpreting residuals in terms of efficiency. If the points are predominantly inside, or predominantly outside, these lines, then the efficiency of univariate meta-analysis is likely to be greater than, or less than, 0.9. As noted previously in Section 2.1, an efficiency of $90\\%$ indicates that the information that is gained from the secondary outcomes in multivariate meta-analysis is like the extra information which would be available in univariate meta-analysis if we had an additional $n/9$ studies. <PARAGRAPH_SEPARATOR> Equation (25) also gives us the necessary and sufficient condition for a study to give no borrowing of strength. If $(\\hat{u}_{i},u_{i})$ lies on the line, then $u_{i}=(s_{12}/s_{22})v_{i}$ and so <PARAGRAPH_SEPARATOR> $$ \\frac{\\rho_{i}\\sigma_{i}\\nu_{i}}{\\sigma_{i}^{2}}=\\frac{s_{12}}{s_{22}}. $$ <PARAGRAPH_SEPARATOR> The left-hand side of equation (28) is the ratio of the covariance element in $V_{i}~(V_{i12})$ to its primary diagonal element $V_{i11}$ , whereas the right-hand side is the ratio of the corresponding elements of $\\Omega$ , or of $\\bar{V}$ . For no borrowing of strength these are equal, and so <PARAGRAPH_SEPARATOR> $$ \\frac{V_{i12}}{V_{i11}}=\\frac{\\bar{V}_{12}}{\\bar{V}_{11}}. $$ <PARAGRAPH_SEPARATOR> Previously we noted that a study with $V_{i}=k\\bar{V}$ for some scalar constant $k$ gives no borrowing of strength. This is a sufficient but not necessary condition—all we need is that the top row (or left-hand column) of $V_{i}$ is proportional to the top row (or left-hand column) of $\\bar{V}$ . In particular, there is no requirement on the secondary variance $\\nu_{i}^{2}$ per se. We show in Section 4 that this generalizes to any number of secondary outcomes. <PARAGRAPH_SEPARATOR> The borrowing-of-strength plot also illustrates two other aspects of borrowing of strength which were discussed in Section 2. Firstly, for studies in the meta-analysis, $B(V_{i}^{-\\overline{{1}}})$ is the proportional contribution of the ith secondary outcome estimate to var. $\\bar{\\hat{(\\beta}_{1})}$ (the direct interpretation), but, for a study outside the meta-analysis, $B(V^{-1})$ is only the approximate (large $n$ ) contribution which the secondary outcome estimate would make if this study were added to the meta-analysis (the add-one-in interpretation). We see the nature of this approximation in the borrowing-of-strength plot. The line $w=\\hat{w}$ is the least squares line of best fit (through the origin) for the $n$ points $(\\hat{w}_{i},w_{i})$ . But, if we add in the new study, the value of the scale factor Ω11 will change, affecting the co-ordinates for all the studies. So the residual of the new point from the line fitted by least squares to the enhanced data will not be the same as the residual from the line calculated from the original $n$ studies alone. If $n$ is large then adding one more study will have only a small effect on the fitted line, and so these two residuals will be similar. <PARAGRAPH_SEPARATOR> Secondly, we have noted the linear property of the function $B(V^{-1})$ in equation (14). If we multiply $V^{-1}$ by $k$ then both $w$ and $\\hat{w}$ are scaled by the factor $\\sqrt{k}$ and so the squared residual from the diagonal line is scaled by the original factor $k$ , which means that $B(k\\bar{V^{-1}})=k B(V^{-1})$ as required. If $V^{-1}$ gives no borrowing of strength then the point will simply move up or down the diagonal line according to the value of $k$ ."
  },
  {
    "qid": "statistic-posneg-1810-0-0",
    "gold_answer": "FALSE. The formula is part of the analysis but does not directly determine the convergence rate. The convergence rate is determined by the total variation distance behavior described in the context, which shows a sharp cut-off at log $n + c n$ iterations, not by this specific formula.",
    "question": "Is the formula $$ a={\\frac{1}{2\\theta}}+{\\frac{1}{4\\theta}}\\Bigg({\\frac{1}{\\theta}}-\\theta\\Bigg), $$ used to determine the convergence rate of the Metropolis-Hastings chain in the symmetric group example?",
    "question_context": "Convergence Analysis of Gibbs Sampler and Metropolis-Hastings Chains\n\nThe following more specific questions were sparked by earlier discussants’ suggestions to improve convergence. In the proposal to use a different random orthonormal basis at each cycle, when might it help to use a stratified random sample of bases to ensure better coverage? Is there any benefit in the idea of combining Chris Jennison's impressive use of a range of power transformations with the eigenanalysis of Besag and Green? Within some parameterized family, could a transformation be sought which in some sense optimally trades off speed of convergence against accuracy of estimation? <PARAGRAPH_SEPARATOR> Persi Diaconis and Jeffrey S. Rosenthal (Harvard University, Cambridge): The asymptotic rate of convergence to stationarity is determined by the second-largest eigenvalue (Besag and Green, Section 2). This can be deceptive. <PARAGRAPH_SEPARATOR> To explain this, we describe a study of Diaconis and Hanlon (1992) where Metropolis-Hastings chains can be explicitly diagonalized. On the symmetric group, consider sampling from $P(x)=C_{\\theta}\\theta^{d(x,x_{0})}$ where $d(\\l,\\l)$ is a metric (like Kendall's tau), $x_{0}$ is a fixed permutation, $0{<}\\theta{<}1$ and $C_{\\theta}$ is a normalizing constant. A base chain is run by repeatedly transposing random pairs. If this chain is run for an log $n+c n$ iterations, with <PARAGRAPH_SEPARATOR> $$ a={\\frac{1}{2\\theta}}+{\\frac{1}{4\\theta}}\\Bigg({\\frac{1}{\\theta}}-\\theta\\Bigg), $$ <PARAGRAPH_SEPARATOR> then the total variation distance of the chain from $P$ goes to O in an explicit way if $c$ is large, whereas the distance to $P$ is essentially 1 if $c$ is negative. We see a sharp cut-off at an log $\\pmb{n}$ ; this has occurred in many examples (see, for example, Diaconis (1988)). If we only use the second eigenvalue in this example, we can only conclude that order $n^{2}\\log n$ steps suffice. For $n=52$ ， $\\pmb{n}$ log $n\\doteq200$ , whereas $n^{2}$ log $n{\\doteq}11000$ <PARAGRAPH_SEPARATOR> In Rosenthal (1991a,b), ideas related to Harris recurrence are used to obtain explicit bounds on the variation distance to equilibrium for the Gibbs sampler for certain models. In particular, in Rosenthal (1991b) the Gibbs sampler is analysed for a standard variance components model. The model involves a location parameter ${\\pmb\\mu}$ and $K$ parameters $\\theta_{1},...,\\theta_{K}$ which are independently and identically distributed (IID) normal $(\\mu,\\sigma_{\\theta}^{2})$ . For each $\\pmb{\\theta}_{i}$ , there are $J$ observations $Y_{i1},...,Y_{i J}$ which are IID normal $(\\theta_{i},\\sigma_{e}^{2})$ ： The distribution of $(\\sigma_{\\theta}^{2},\\sigma_{e}^{2},\\mu,\\dot{\\theta}_{1},...,\\theta_{K})$ is to be estimated.Itis shown that the Gibbs sampler for this model (with conjugate priors) will converge in $C(1+\\log K/\\log J)$ iterations, where $c$ is a positive constant, independent of $\\boldsymbol{\\jmath}$ and $K$ ,which depends on the priors and the data. This rate is essentially sharp, and the techniques used seem broadly applicable. <PARAGRAPH_SEPARATOR> Another class of techniques for proving convergence results involves bounds on eigenvalues by using the underlying geometry of the chain. They were pioneered by computer scientists (see, for example, Jerrum and Sinclair (1989)) to study randomized algorithms and have been developed (Lawler and Sokal, 1988; Diaconis and Stroock, 1991; Diaconis and Saloff-Coste, 1992) to give essentially sharp rates in a variety of messy problems."
  },
  {
    "qid": "statistic-posneg-96-0-0",
    "gold_answer": "TRUE. The NT discrepancy function is justified as a generalized distance measure, but it does not yield efficient estimators under nonnormality due to its reliance on higher-order moments that complicate asymptotic expansions and efficiency conditions.",
    "question": "Is the claim that the NT discrepancy function under nonnormality does not give efficient estimators but represents a generalized distance, as stated in the paper, accurate?",
    "question_context": "Discrepancy Functions in Asymptotic Expansions of Estimator Distributions\n\nIn this paper, the discrepancy function $F_{\\mathrm{NT}}$ (see (2.2)) based on NT has been exclusively dealt with, including some modification on the original discrepancy function $F_{\\mathrm{NT}}^{*}$ (see (2.1)). The NT discrepancy function under nonnormality does not give efficient estimators, though the use of the function has justification since the function represents a generalized distance. The discrepancy function of fully generalized least squares (GLS) estimation is defined as $$\\boldsymbol{F}_{\\mathrm{GLS}}=(\\mathbf{u}-\\boldsymbol{\\uptau})^{\\prime}\\hat{\\boldsymbol{\\Omega}}^{-1}(\\mathbf{u}-\\boldsymbol{\\uptau}),$$ where $\\hat{\\pmb\\Omega}$ is a consistent estimator of $\\pmb{\\Omega}$ . Since $\\hat{\\pmb\\Omega}$ includes sample moments up to the fourth order, the asymptotic expansions of the distributions of the GLS estimators may be intractable. As addressed earlier, the ULS discrepancy function $F_{{\\mathrm{ULS}}}$ is quite simple and tractable. The necessary corrections to have the asymptotic expansions of the distributions of the ULS estimators are to derive the partial derivatives of $F_{{\\mathrm{ULS}}}$ with respect to $\\bar{\\bf x}$ and s. Since the partial derivatives of $(1/2)\\mathrm{tr}\\{(\\mathbf{S}-\\Sigma)^{2}\\}$ are available (see e.g., Ogasawara [23]), the partial derivatives of $F_{\\mathrm{ULS}}^{\\#}=F_{\\mathrm{ULS}}-(1/2)\\mathrm{tr}\\{({\\bf S}-{\\bf\\Sigma}{\\Sigma})^{2}\\}=(\\bar{\\bf x}-{\\bf\\Sigma}{\\bf\\Sigma}{\\bf\\Sigma}{\\bf\\Sigma}^{}\\mathrm{-}{\\bf\\Sigma}{\\bf\\Sigma}{\\bf\\Sigma}{\\bf\\Sigma})$ with $\\hat{F}_{\\mathrm{ULS}}^{\\#}=(\\bar{\\bf x}-\\hat{\\pmb{\\upmu}})^{\\prime}(\\bar{\\bf x}-\\hat{\\pmb{\\upmu}})$ are shown as follows: $$\\begin{array}{r l}&{\\begin{array}{r l}&{\\frac{\\partial\\hat{F}_{\\parallel,S}^{\\mu}}{\\partial\\hat{\\Theta}}=-2\\frac{\\partial\\hat{\\hat{\\Psi}}^{\\prime}}{\\partial\\hat{\\Theta}}(\\bar{\\kappa}-\\hat{\\upmu}),\\quad\\quad\\frac{\\partial\\hat{F}_{\\parallel,S}^{\\mu}}{\\partial\\hat{\\pi}}=2(\\bar{\\bf x}-\\hat{\\\\\\mu}),} &{\\frac{\\partial^{2}\\hat{F}_{\\parallel,S}^{\\mu}}{\\partial\\hat{\\theta}_{i}\\partial\\hat{\\theta}}=-2\\frac{\\partial^{2}\\hat{\\hat{\\theta}}^{\\prime}}{\\partial\\hat{\\theta}_{i}\\partial\\hat{\\theta}}(\\bar{\\kappa}-\\hat{\\upmu})+2\\frac{\\partial\\hat{\\hat{\\theta}}^{\\prime}}{\\partial\\hat{\\theta}}\\frac{\\partial\\hat{\\hat{\\theta}}}{\\partial\\hat{\\theta}_{j}},\\quad\\quad\\frac{\\partial^{2}\\hat{F}_{\\parallel,S}^{\\mu}}{\\partial\\hat{\\theta}_{i}\\partial\\hat{\\bf x}}=-2\\frac{\\partial\\hat{\\hat{\\theta}}}{\\partial\\hat{\\theta}_{i}},} &{\\frac{\\partial^{2}\\hat{F}_{\\parallel,S}^{\\mu}}{\\partial\\hat{\\theta}_{i}\\partial\\hat{\\theta}_{j}\\partial\\hat{\\theta}_{k}}=-2\\frac{\\partial^{3}\\hat{\\hat{\\psi}}^{\\prime}}{\\partial\\hat{\\theta}_{i}\\partial\\hat{\\theta}_{j}\\partial\\hat{\\theta}_{k}}(\\bar{\\kappa}-\\hat{\\upmu})+2\\sum_{\\partial\\hat{\\theta}_{i}\\partial\\hat{\\theta}_{j}}^{3}\\frac{\\partial^{2}\\hat{\\bf\\psi}_{\\theta}^{\\prime}}{\\partial\\hat{\\theta}_{i}\\partial\\hat{\\theta}_{j}},\\quad\\frac{\\partial^{3}\\hat{\\bf\\psi}_{\\theta_{i}}^{\\mu}}{\\partial\\hat{\\theta}_{i}\\partial\\hat{\\theta}_{j}\\partial\\hat{\\bf x}}=-2\\frac{\\partial^{2}\\hat{\\bf\\psi}_{\\theta}^{\\prime}}{\\partial\\hat{\\theta}_{i}\\partial\\hat{\\theta}_{j}},} &{\\frac{\\partial^{4}\\hat{F}_{\\parallel,S}^{\\mu}}{\\partial\\hat{\\theta}_{i}\\partial\\hat{\\theta}_{j}\\partial\\hat{\\theta}_{k}}\\frac{\\partial}{\\partial\\hat{\\theta}_{i}}\\Big|_{"
  },
  {
    "qid": "statistic-posneg-1034-0-1",
    "gold_answer": "TRUE. The relative efficiency (RE) formula shows that V(ȳ_b)/V(ȳ_b^(ν)) > 100% for all sample sizes (e.g., 540% for n=3, 18923.44% for n=100). This is due to the term E_d(1/ν) > 1/n, which reduces the variance of the sufficient bootstrap estimator compared to the conventional one (V(ȳ_b) = s_y²/n). The efficiency gain arises because sufficient bootstrapping accounts for distinct units, reducing redundancy in resampling.",
    "question": "Does the variance formula for the sufficient bootstrapping sample mean, V(ȳ_b^(ν)) = [E_d(1/ν) - 1/n]s_y², imply that it is always more efficient than conventional bootstrapping?",
    "question_context": "Sufficient Bootstrapping vs. Conventional Bootstrapping: Bias and Efficiency Comparison\n\nThe paper compares the performance of sufficient bootstrapping and conventional bootstrapping in estimating sample statistics such as mean, standard deviation, and coefficient of variation. It demonstrates that sufficient bootstrapping can yield less biased estimates and higher efficiency compared to conventional bootstrapping. Key formulas include the expected values and variances of estimators under both methods, as well as the relative bias and relative efficiency metrics."
  },
  {
    "qid": "statistic-posneg-988-0-1",
    "gold_answer": "FALSE. For a VARMA model to be invertible, the roots of det{Θ(z)} must lie outside the unit disk, not inside. This ensures that the MA component can be inverted to express the innovations ε_t as a convergent series of past observations Y_t.",
    "question": "For a VARMA model to be invertible, the roots of det{Θ(z)} must lie inside the unit disk. True or False?",
    "question_context": "Structured VARMA Models and Exact Maximum Likelihood Estimation\n\nThe paper discusses structured VARMA models, focusing on echelon forms based on Kronecker indices and scalar component models (SCM). It covers I(1) VARMA models, their transformation to stationary VARMA models, and the evaluation of exact likelihood using recurrence equations. The paper also compares exact maximum likelihood estimation with conditional methods."
  },
  {
    "qid": "statistic-posneg-679-0-1",
    "gold_answer": "FALSE. Theorem 3.3 states that the asymptotic distribution holds for any pre-determined finite constants $K$ and $D$, but this is contingent on conditions (C1)–(C5) being satisfied. The result does not universally hold for arbitrary choices of $K$ and $D$ without these underlying regularity conditions governing the functional data structure and eigenvalue behavior.",
    "question": "Does the asymptotic distribution $T_{n}\\stackrel{d}{\\rightarrow}\\chi^{2}(D K(K+1)/2)$ under $H_{0}$ hold for any choice of $K$ and $D$ as long as they are finite?",
    "question_context": "Consistency and Asymptotic Behavior in Functional Quadratic Regression\n\nWhen the predicator $X_{i}$ is Gaussian, all $\\xi_{i j}{'s}$ are normal. Thus (C5) is satisfied for any $C\\le3$ . All conditions (C1)–(C5) are common in functional data analysis. <PARAGRAPH_SEPARATOR> Let $d_{n}=\\lVert\\hat{C}_{X}-C_{X}\\rVert$ . Denote $\\tilde{K}_{n}=\\operatorname*{min}\\{j\\ge1:\\lambda_{j}\\le2d_{n}\\}-1$ . It is clear that $d_{n}\\to0$ and thus $\\tilde{K}_{n}\\rightarrow\\infty$ as $n\\to\\infty$ . The following theorem shows the consistency of the proposed estimators. <PARAGRAPH_SEPARATOR> Theorem 3.1. Let truncations $K=K(n)\\to\\infty$ and $D=D(n)\\to\\infty$ , which satisfy $K\\leq\\tilde{K}_{n}$ and <PARAGRAPH_SEPARATOR> $$ \\frac{1}{\\sqrt{n}}\\sum_{k=1}^{D}\\sum_{m=1}^{K}\\frac{1}{\\lambda_{m}}(\\frac{1}{\\delta_{X,m}}+\\frac{1}{\\delta_{Y,k}})\\rightarrow0. $$ <PARAGRAPH_SEPARATOR> Then under conditions (C1)–(C5), we have $\\Vert\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}\\Vert=o_{p}(1).$ If we further assume that <PARAGRAPH_SEPARATOR> $$ \\frac{1}{\\sqrt{n}}\\sum_{k=1}^{D}\\sum_{l=1}^{K}\\left(\\frac{\\sqrt{\\sigma}_{k}}{\\lambda_{l}^{3}\\delta_{X,l}}+\\sum_{m=1}^{l}\\frac{1}{\\lambda_{m}\\lambda_{l}}(\\frac{1}{\\delta_{X,l}}+\\frac{1}{\\delta_{X,m}}+\\frac{1}{\\delta_{Y,k}})\\right)\\rightarrow0, $$ <PARAGRAPH_SEPARATOR> then $\\|\\hat{\\gamma}-\\gamma\\|=o_{p}(1)$ . <PARAGRAPH_SEPARATOR> To illustrate availability of assumption (15), we consider a common setting in FPCA. <PARAGRAPH_SEPARATOR> Remark 2. Similar to formula (3.2) in Hall and Horowitz (2007), suppose that <PARAGRAPH_SEPARATOR> $$ \\lambda_{j}-\\lambda_{j+1}\\geq C_{1}j^{-\\nu_{1}-1},\\quad\\sigma_{j}-\\sigma_{j+1}\\geq C_{2}j^{-\\nu_{2}-1} $$ <PARAGRAPH_SEPARATOR> for $j\\geq1$ and $\\nu_{1},\\nu_{2}>1$ . This implies that $\\lambda_{j}\\ge C_{3}j^{-\\nu_{1}}$ , $\\delta_{X,j}\\geq C_{1}j^{-\\nu_{1}-1}$ and $\\delta_{Y,j}\\geq C_{2}j^{-\\nu_{2}-1}$ , which yield that <PARAGRAPH_SEPARATOR> $$ \\frac{1}{\\sqrt{n}}\\sum_{k=1}^{D}\\sum_{m=1}^{K}\\frac{1}{\\lambda_{m}}(\\frac{1}{\\delta_{X,m}}+\\frac{1}{\\delta_{Y,k}})\\leq\\frac{C_{4}}{\\sqrt{n}}\\sum_{k=1}^{D}\\sum_{m=1}^{K}m^{\\nu_{1}}(m^{\\nu_{1}+1}+k^{\\nu_{2}+1}). $$ <PARAGRAPH_SEPARATOR> Hence we can always find suitable $K=K(n)\\to\\infty$ , $D=D(n)\\to\\infty$ such that (15) holds. (16) can be verified similarly. <PARAGRAPH_SEPARATOR> Our next theorem ensures the prediction consistency. <PARAGRAPH_SEPARATOR> Theorem 3.2. Suppose that all conditions of Theorem 3.1 hold. As $n\\to\\infty$ , we have <PARAGRAPH_SEPARATOR> $$ \\int_{\\mathcal{T}}\\Big(\\hat{Y}^{*}(t)-E(Y^{*}(t)|X^{*})\\Big)^{2}d t\\overset{P}{\\rightarrow}0. $$ <PARAGRAPH_SEPARATOR> Finally we investigate the asymptotic behaviors of the testing statistic $T_{n}$ . <PARAGRAPH_SEPARATOR> Theorem 3.3. Suppose conditions (C1)–(C5) are satisfied. For any pre-determined constants $K$ and $D$ we have <PARAGRAPH_SEPARATOR> $$ T_{n}\\stackrel{d}{\\rightarrow}\\chi^{2}(D K(K+1)/2) $$ <PARAGRAPH_SEPARATOR> under $H_{0}$ and $T_{n}{\\overset{P}{\\to}}\\infty$ if $\\pmb R\\neq0$ , where $\\pmb{R}=(\\pmb{R}_{1}^{T},\\dots,\\pmb{R}_{D}^{T})^{T}$ . <PARAGRAPH_SEPARATOR> Remark 3. In Theorem 3.3, the asymptotic distribution of $T_{n}$ depends on pre-specified finite $K$ and $D$ . The proposed test is consistent when the alternative hypothesis satisfies $\\pmb R\\neq0$ , which means that the projection of $\\gamma$ on the subspace spanned by $\\left\\{\\psi_{l}\\otimes\\psi_{m}\\otimes\\phi_{k}:1\\leq l,m\\leq K,1\\leq k\\leq l\\right.$ is not zero."
  },
  {
    "qid": "statistic-posneg-1220-1-2",
    "gold_answer": "FALSE. For a nonstationary functional process, such as a random walk model $X_{i} = X_{i-1} + \\varepsilon_{i}$, the growth rate of $N_{i}^{u}$ is $O(\\sqrt{n})$, not $O(\\log i)$. This is derived from Proposition 2, where $\\frac{N_{n}^{u}}{\\sqrt{n}}$ converges in distribution to a random variable $G_{1}$.",
    "question": "Is the growth rate of $N_{i}^{u}$ for a nonstationary functional process $O(\\log i)$?",
    "question_context": "Functional Record Analysis in Time Series\n\nThe paper discusses the properties of functional records in time series analysis. It defines upper and lower functional records and studies their behavior over time. The counting process for upper functional records, $N_{i}^{u}$, is analyzed under different conditions: independent and identically distributed (iid) sequences, stationary functional time series, and nonstationary functional processes. The growth rates of $N_{i}^{u}$ are derived for these cases, showing $O(\\log i)$ for iid and stationary series, and $O(\\sqrt{n})$ for nonstationary processes. The paper also provides visualizations and theoretical results supporting these growth rates."
  },
  {
    "qid": "statistic-posneg-171-0-4",
    "gold_answer": "TRUE. The empirical estimator of the conditional copula C_y(u) is constructed using Nadaraya-Watson weights w_i,n and indicator functions based on the empirical marginal distributions F_j,n,y(X_ij). This non-parametric approach leverages kernel smoothing to account for the functional covariate Y.",
    "question": "Is the empirical estimator of the conditional copula C_y(u) constructed using Nadaraya-Watson weights and indicator functions based on the empirical marginal distributions?",
    "question_context": "Estimation of Extreme Multivariate Expectiles with Functional Covariates\n\nThe paper discusses the estimation of multivariate expectiles with functional covariates, focusing on conditional marginal distributions, quantile functions, and tail dependence functions. It introduces a framework for estimating extreme multivariate expectiles using conditional copulas and tail dependence functions, with applications in risk analysis and capital allocation."
  },
  {
    "qid": "statistic-posneg-492-0-1",
    "gold_answer": "FALSE. The transformations ${\\tilde{\\boldsymbol{\\beta}}}=\\Sigma^{\\frac{1}{2}}{\\boldsymbol{\\beta}}$ and $\\tilde{\\mathbf{X}}=\\mathbf{X}(\\boldsymbol{\\Sigma}^{1/2})^{-}$ can indeed be used to express the standard penalized least squares problem in the form $({\\bf y}-{\\bf X}\\beta)^{\\prime}({\\bf y}-{\\bf X}\\beta)+\\frac{1}{c}\\beta^{\\prime}\\Sigma\\beta$. This is because the transformations ensure that $\\mathbf{X}{\\boldsymbol{\\beta}}={\\tilde{\\mathbf{X}}}{\\tilde{\\boldsymbol{\\beta}}}$ and $\\operatorname{Cov}(\\tilde{\\boldsymbol{\\beta}})=c\\mathbf{I}$, effectively converting the problem into the desired form.",
    "question": "The standard penalized least squares problem $({\\bf y}-{\\bf X}\\beta)^{\\prime}({\\bf y}-{\\bf X}\\beta)+\\frac{1}{c}\\beta^{\\prime}\\beta$ cannot be transformed into the form $({\\bf y}-{\\bf X}\\beta)^{\\prime}({\\bf y}-{\\bf X}\\beta)+\\frac{1}{c}\\beta^{\\prime}\\Sigma\\beta$ using the transformations ${\\tilde{\\boldsymbol{\\beta}}}=\\Sigma^{\\frac{1}{2}}{\\boldsymbol{\\beta}}$ and $\\tilde{\\mathbf{X}}=\\mathbf{X}(\\boldsymbol{\\Sigma}^{1/2})^{-}$. True or False?",
    "question_context": "Bayesian Variable Selection with Different Penalty Functions in Nonparametric Regression\n\nThe paper discusses the application of a computational framework to different priors for regression coefficients, corresponding to different penalty functions. Specifically, it compares the standard penalized least squares problem with a roughness penalty used in smoothing splines. The standard problem is formulated as $({\\bf y}-{\\bf X}\\beta)^{\\prime}({\\bf y}-{\\bf X}\\beta)+\\frac{1}{c}\\beta^{\\prime}\\beta$, while the roughness penalty is written as $\\beta^{\\prime}\\pmb{\\Sigma}\\beta$ for some symmetric positive-semidefinite matrix $\\Sigma$, corresponding to the prior $\\beta\\sim N(0,\\pmb{\\Sigma}^{-})$. The paper also discusses spatial adaptation in the regression function, showing how the Bayesian framework can allow for spatial variability."
  },
  {
    "qid": "statistic-posneg-1037-0-3",
    "gold_answer": "FALSE. While the paper defines h(zi, xi) = exp(zi^Tγ) + [φ/(φ + exp(xi^Tβ))]^φ, this is not a standard formulation for combining zero-inflation and count components in ZINB models. Typically, the zero-inflation probability would be modeled separately from the count component's mean parameter. The given formula appears to mix components in an unconventional way that doesn't clearly represent either the zero-inflation probability or the count mean. A more standard approach would model the zero-inflation probability as logistic and the count mean as exp(xi^Tβ).",
    "question": "The combined hazard function h(zi, xi) is defined as exp(zi^Tγ) + [φ/(φ + exp(xi^Tβ))]^φ. Does this formulation correctly combine the zero-inflation and count components of the model?",
    "question_context": "Zero-Inflated Negative Binomial Regression: Influence Diagnostics\n\nThe authors derive formulas for obtaining the second-order partial derivatives of the log-likelihood function in a zero-inflated negative binomial (ZINB) regression model. The model includes functions for zero-inflation probabilities (g1, g2), a combined hazard function (h), and logistic components (v). The perturbation schemes (case-weight, explanatory variable, and simultaneous) are used to assess the influence of observations on parameter estimates."
  },
  {
    "qid": "statistic-posneg-652-0-0",
    "gold_answer": "FALSE. The covariance formula provided is complex and involves multiple integrals, but the context does not provide sufficient derivation steps or justification for the specific form of the covariance. The formula is stated as part of the theorem, but without detailed proof or derivation in the given context, we cannot confirm its correctness solely based on the provided information.",
    "question": "Is the covariance formula for $Y_j$ and $Y_k$ correctly derived as a combination of integrals involving the fourth-order spectrum $f_4$ and the functions $\\phi_j$?",
    "question_context": "Central Limit Theorem for Stationary Random Fields\n\nWe state a central limit theorem for the statistic $J_{N}(\\phi)$ needed for the proof of Theorem 2. THEOREM A1. Suppose that $X_{t}$ , for $t\\in Z^{d}.$ . fulfls Assumption 3, Assumption 1 holds where the smoothness parameter $\\rho$ depends on $N$ with $\\rho_{N}\\rightarrow\\rho_{0}$ and $\\rho_{N}^{-1}=o(N^{2/d-\\frac{1}{2}})$ , and $\\phi_{j}$ $(j=1,\\ldots,k)$ are continuous functions. Then $\\{N^{\\{d}_{J_{N}(\\phi_{j})\\}}$ for $j=1,\\ldots,k$ converges weakly to a Gaussian random vector $\\{Y_{j}\\}$ with mean zero and $$\\begin{array}{r l}&{\\mathsf{c o v}\\left(Y_{j},Y_{k}\\right)=(2\\pi)^{d}\\bigg(\\displaystyle\\int_{0}^{1}\\{h_{\\rho_{0}}(x)\\}^{4}d x\\bigg/\\bigg[\\displaystyle\\int_{0}^{1}\\{h_{\\rho_{0}}(x)\\}^{2}d x\\bigg]^{2}\\bigg)^{d}}\\ &{\\qquad\\times\\left[\\displaystyle\\int_{\\Pi^{2d}}\\phi_{i}(\\alpha_{1})\\phi_{j}(-\\alpha_{2})f_{4}(\\alpha_{1},-\\alpha_{1},\\alpha_{2})d\\alpha_{1}d\\alpha_{2}\\right.}\\ &{\\qquad\\quad\\left.+\\displaystyle\\int_{\\Pi^{d}}\\phi_{1}(\\alpha)\\{\\phi_{2}(\\alpha)+\\phi_{2}(-\\alpha)\\}f(\\alpha)^{2}d\\alpha\\right],}\\end{array}$$ where $f_{4}$ is the fourth-order spectrum of the process. Proof. The proof consists of the following two steps. By generalizing the arguments of Bolthausen (1982) we first prove a central limit theorem for the empirical covariances $c_{N}^{h}(k)$ , and by using the Cesaro sums of the $\\phi_{j}$ we then conclude from this to the weak convergence of $\\{N^{\\{d}_{J_{N}\\{\\phi_{j}}\\}}\\}$ for $j=1,\\ldots,k.$ Details may be obtained from the authors upon request. 口 Alternatively, the result may also be proved by a cumulant method under the assumptions that all moments of the process exist and the cumulant spectra fulfill certain smoothness conditions, for example by using a modified version of Theorem 3.1 of Brillinger (1970)."
  },
  {
    "qid": "statistic-posneg-266-2-0",
    "gold_answer": "TRUE. The estimator $\\widehat c_{i}^{2}(t,s)$ is constructed as an unbiased estimator for $c_{i}^{2}(t,s)$ under the condition that $n_{i} \\geq 4$. This is ensured by the normalization factor $\\frac{1}{n_{i}(n_{i}-1)(n_{i}-2)(n_{i}-3)}$ and the summation over all distinct quadruples $(u, v, w, z)$, which accounts for the unbiasedness property.",
    "question": "Is the estimator $\\widehat c_{i}^{2}(t,s)$ unbiased for $c_{i}^{2}(t,s)$ under the condition $n_{i} \\geq 4$?",
    "question_context": "Consistent Estimation of Null Variance in Hypothesis Testing\n\nProposition 1. Suppose that the assumptions in Lemma 1 and Assumptions 1–4 are fulfilled. Then (4) holds. <PARAGRAPH_SEPARATOR> # 4. Estimating the null variance <PARAGRAPH_SEPARATOR> In order to get a critical region for testing $H_{0}$ using the result in Corollary 1, we need a consistent estimator of $\\sigma_{0k}$ . Recall that $\begin{array}{r}{\\sigma_{0k}^{2}=k\times v a r_{0}(T_{k,L i n})=2/k\\sum_{i=1}^{k^{-}}\theta_{i}/n_{i}(\\stackrel{.}{n_{i}}-1).\\sigma_{0}^{2}}\\end{array}$ $\\sigma_{0k}^{2}$ is unknown because $\theta_{1},\\ldots,\theta_{k}$ are unknown quantities. From expression (1), $\theta_{i}$ can be unbiasedly  estimated by replacing $c_{i}^{2}(t,s)$ with an unbiased estimator. Assume that $n_{i}\\geq4$ $1\\leq i\\leq k$ , then $c_{i}^{2}(t,s)$ can be unbiasedly estimated by <PARAGRAPH_SEPARATOR> $$ \\widehat c_{i}^{2}(t,s)=\\frac{1}{n_{i}(n_{i}-1)(n_{i}-2)(n_{i}-3)}\\sum_{\\substack{1\\leq u\neq v\neq w\neq z\\leq n_{i}}}h(X_{i u},X_{i v};t,s)h(X_{i w},X_{i z};t,s), $$ <PARAGRAPH_SEPARATOR> where $\begin{array}{r}{h(X_{i u},X_{i v};t,s)=\\frac12\\{X_{i u}(t)-X_{i v}(t)\\}\\{X_{i u}(s)-X_{i v}(s)\\}.}\\end{array}$ Note that <PARAGRAPH_SEPARATOR> $$ \\widehat{c_{i}^{2}}(t,s)=\\frac{1}{n_{i}(n_{i}-1)}\\sum_{1\\leq u\neq v\\leq n_{i}}h(X_{i u},X_{i v};t,s)S_{i(u,v)}^{2}(t,s), $$ <PARAGRAPH_SEPARATOR> uwsheefrule $\begin{array}{r}{S_{i(u,v)}^{2}(t,s)=\\frac{1}{n_{i}-3}\\sum_{w\neq u,v}\\{X_{i w}(t)-\bar{X}_{i(u,v)}(t)\\}\\{X_{i w}(s)-\bar{X}_{i(u,v)}(s)\\}}\\end{array}$ , and $\begin{array}{r}{\bar{X}_{i(u,v)}(t)=\\frac{1}{n_{i}-2}\\sum_{w\neq u,v}X_{i w}(t).}\\end{array}$ Formula (6) is for computational purposes. Let $\begin{array}{r}{\\hat{\\sigma}_{0k}^{2}=\bar{(2/k)}\\sum_{i=1}^{k}\\hat{{\theta}}_{i}\bar{/}n_{i}({n}_{i}-1)}\\end{array}$ , where $\begin{array}{r c l}{\\widehat{\theta}_{i}}&{=}&{\\int\\int\\widehat{c_{i}^{2}}(t,s)d t d s}\\end{array}$ . Next proposition shows that, under some mild assumptions, the quotient 0/ onverges in probability ˆto 1. <PARAGRAPH_SEPARATOR> Proposition 2. Suppose that the assumptions in Lemma 1 and Assumptions 1–3 are fulfilled, and that $n_{i}\\geq4,1\\leq i\\leq k.$ Then $\\hat{\\sigma}_{0k}^{2}/\\sigma_{0k}^{2}\\xrightarrow{P}1$ . <PARAGRAPH_SEPARATOR> As an immediate consequence of Corollary 1 and Proposition 2, we have the following result."
  },
  {
    "qid": "statistic-posneg-2111-0-3",
    "gold_answer": "TRUE. The depth contours $D_{k}$ are convex, and they are nested such that $D_{k+1}\\subseteq D_{k}$ for each $k$. This property is derived from the definition of halfspace depth and the convexity of the intersection of halfspaces.",
    "question": "Are the depth contours $D_{k}$ always convex and nested within each other?",
    "question_context": "Halfspace Depth and Its Properties in Multivariate Data Analysis\n\nTake any data set $X_{n}=\\{\\mathbf{x}_{i};i=1,...,n\\}$ with data points $\\mathbf{x}_{i}=(x_{i1},...,x_{i p})^{\\prime}$ $\\in\\mathbb{R}^{p}$ . This data set determines an empirical distribution ${\\hat{P}}_{n}$ which is the discrete probability distribution on $\\mathbb{R}^{p}$ given by $\\hat{P}_{n}(A)=\\#\\left\\{\\mathbf{x}_{i}\\in A\\right\\}/n$ . When the sample size $n$ is given, ${\\hat{P}}_{n}$ characterizes the data set $X_{n}$ . <PARAGRAPH_SEPARATOR> Tukey (1975) and Donoho and Gasko (1992) defined the halfspace depth of an arbitrary point ${\\bf\\uptheta}=(\\theta_{1},...,\\theta_{p})^{\\prime}\\in\\mathbb{R}^{p}$ relative to $X_{n}$ as the smallest number of data points in any closed halfspace with boundary hyperplane through %. We also call this the location depth, and it can be written as <PARAGRAPH_SEPARATOR> $$ l d e p t h(\\ \\ 6;X_{n})=\\operatorname*{min}_{\\|\\mathbf{u}\\|=1}\\#\\left\\{i;\\mathbf{u}^{\\prime}\\mathbf{x}_{i}\\leqslant\\mathbf{u}^{\\prime}\\mathbf{\\uptheta}\\right\\}, $$ <PARAGRAPH_SEPARATOR> where u ranges over all vectors in $\\mathbb{R}^{p}$ with $\\left\\|\\mathbf{u}\\right\\|=1$ . Interestingly, (1.1) is affine invariant. That is, if we consider a regular matrix $A\\in\\mathbb{R}^{p\\times p}$ and some vector $\\ensuremath{\\mathbf{b}}\\in\\mathbb{R}^{p}$ , it holds that <PARAGRAPH_SEPARATOR> $$ l d e p t h(A\\Theta+\\ensuremath{\\mathbf{b}};A X_{n}+\\ensuremath{\\mathbf{b}})=l d e p t h(\\ensuremath{\\mathbf{\\Theta}};X_{n}) $$ <PARAGRAPH_SEPARATOR> due to the fact that halfspaces are mapped to halfspaces. <PARAGRAPH_SEPARATOR> Since (1.1) is defined for any $\\mathbf{\\uptheta}\\in\\mathbb{R}^{p}$ we call it the depth function. Its values are nonnegative integers. When $p=1$ we have $u\\in\\{-1,1\\}$ so we can write (1.1) as <PARAGRAPH_SEPARATOR> $$ l d e p t h_{1}(\\theta;X_{n})=\\operatorname*{min}\\{n\\hat{F}_{n}(\\theta;X_{n}),n\\hat{F}_{n}(-\\theta;-X_{n})\\}, $$ <PARAGRAPH_SEPARATOR> where $\\hat{F}_{n}(\\theta;X_{n})=\\#\\left\\{x_{i}\\leqslant\\theta\\right\\}/n$ is the usual empirical cdf. Figure 1(a) shows the depth function of a univariate data set with $n=30$ . The data values were randomly generated from a $\\chi_{6}^{2}$ -distribution. The depth function clearly reflects the skewness. The increasing part of the function coincides with $\\hat{F}_{n}$ , whereas the decreasing part coincides with the empirical cdf of the image of the data under an affine transformation $x\\to a x+b$ with $a<0$ . Figure 1(b) is the depth function of a bivariate data set ( $p=2,$ ). The two variables are the batting average and the number of home runs of 14 baseball teams in the American League in 1987 (Moore and McCabe 1989; the data are also available at http:\\\\\\\\lib.stat.cmu.edu\\\\\\\\DASL\\\\\\\\). For any dimension $p\\geqslant2$ it holds that <PARAGRAPH_SEPARATOR> $$ l d e p t h}(\\ \\mathbf{\\Theta};X_{n})=\\operatorname*{min}_{\\|\\mathbf{u}\\|=1}{l d e p t h}_{1}(\\mathbf{u}^{\\prime}\\mathbf{\\Theta}\\mathbf{\\Theta};\\mathbf{u}^{\\prime}X_{n}), $$ <PARAGRAPH_SEPARATOR> which can also be written as <PARAGRAPH_SEPARATOR> $$ \\operatorname*{min}_{g\\in\\mathcal{A}}l d e p t h_{1}([g(\\mathbf{\\theta})]_{1};[g(X_{n})]_{1})=\\operatorname*{min}_{g\\in\\mathcal{A}}n\\hat{F}_{n}([g(\\mathbf{\\theta})]_{1};[g(X_{n})]_{1}), $$ <PARAGRAPH_SEPARATOR> where $\\mathcal{A}$ is the set of all affine transformations of $\\mathbb{R}^{p}$ and $[g(\\mathbf{\\theta})]_{1}$ denotes the first component of $g({\\bf\\theta})$ . Therefore, location depth can be seen as a natural affine equivariant generalization of the univariate empirical cdf. The usual multivariate empirical cdf is not affine equivariant because it depends on the coordinate system used. <PARAGRAPH_SEPARATOR> The depth contours defined as <PARAGRAPH_SEPARATOR> $$ D_{k}=\\{\\pmb{\\uptheta}\\in\\mathbb{R}^{p};l d e p t h(\\pmb{\\uptheta};X_{n})\\geqslant k\\} $$ <PARAGRAPH_SEPARATOR> are convex, and $D_{k+1}\\subseteq D_{k}$ for each $k$ . The outermost contour $D_{1}$ is the convex hull of the data set. Each data set also has an innermost depth contour $D_{k^{*}}$ where $k^{*}$ is the maximum of the function ldepth $\\iota(\\boldsymbol{\\mathbf{\\theta}};\\boldsymbol{X}_{n})$ over all $\\mathbf{\\uptheta}\\in\\mathbb{R}^{p}$ . Therefore, the complete set of contours is $D_{k^{*}}\\subseteq D_{k^{*}-1}\\subseteq\\cdots\\subseteq$ $D_{2}\\subseteq D_{1}$ . Figure 2 shows such a collection of contours. The depicted data set (from the Wall Street Journal of March 1, 1984, and provided at http:\\\\\\\\lib.stat.cmu.edu\\\\\\\\DASL\\\\\\\\) gives the 1983 TV advertising budget of several well-known companies, in millions of dollars. The second variable is based on a survey, where people had to cite a commercial for that product they had seen in the past week. The number of retained impressions (in millions) are plotted on the vertical axis. We see that the depth contours reflect the shape of the data set. <PARAGRAPH_SEPARATOR> From definition (1.1) we can derive an equivalent expression for the $k$ th depth contour: <PARAGRAPH_SEPARATOR> Lemma 1. <PARAGRAPH_SEPARATOR> $$ D_{k}=\\bigcap_{\\scriptstyle A\\in{\\mathcal{A}}(n-k+1)}A, $$ <PARAGRAPH_SEPARATOR> where $\\mathcal{A}(m)$ is the set of all closed halfspaces containing at least m data points. <PARAGRAPH_SEPARATOR> Proof. We first prove $\\subseteq$ . If % does not belong to the intersection there exists a closed halfspace in $\\mathcal{A}(n-k+1)$ which does not contain %, hence % belongs to an open halfspace containing at most $n-(n-k+1)=k-1$ observations. Therefore ldepth $\\iota(\\boldsymbol\\Theta;X_{n})\\leqslant k-1$ , and thus $\\mathbf{\\uptheta}\\notin D_{k}$ . On the other hand, suppose that $\\mathbf{\\uptheta}\\notin D_{k}$ . Then ldepth $(\\mathbf{\\theta}\\in X_{n})<k$ , hence % belongs to a closed halfspace $A$ containing fewer than $k$ data points. The complement of $A$ is an open halfspace containing at least $n-k+1$ data points, from which we immediately obtain a closed halfspace that does not contain % and has at least $n-k+1$ data points. Therefore also $\\supseteq$ holds. K"
  },
  {
    "qid": "statistic-posneg-729-0-2",
    "gold_answer": "FALSE. The paper discusses the importance of choosing appropriate weights for the probability measures in the log quasi-likelihood function. The weights should reflect the different mixing rates of the chains, and the choice of weights can significantly impact the performance of the estimator. The optimal weights are not arbitrary and should be chosen carefully to improve estimation accuracy.",
    "question": "Is the choice of weights for the probability measures in the log quasi-likelihood function arbitrary and does not affect the estimation of the normalizing constants?",
    "question_context": "Multiple-Chain Importance Sampling with Unknown Normalizing Constants\n\nThe paper discusses a method for estimating unknown normalizing constants using multiple Markov chains. The key formulas include the log quasi-likelihood function, the estimator for the normalizing constants, and the asymptotic distributions of these estimators. The method involves a two-stage process where preliminary chains are used to estimate the normalizing constants, and these estimates are then used in shorter chains to compute the final estimates."
  },
  {
    "qid": "statistic-posneg-989-0-0",
    "gold_answer": "TRUE. The perpendicularity condition $\\beta_{0}^{\\mathrm{T}}\\theta_{0} = 0$ ensures identifiability by preventing an arbitrary rotation of the parameters that could leave the model unchanged. Specifically, without this condition, the model could be reparameterized by rotating $\\beta_{0}$ and $\\theta_{0}$ in a way that preserves the linear combination $\\beta_{0}^{\\mathrm{T}}X + \\phi(\\theta_{0}^{\\mathrm{T}}X)$, leading to multiple parameter sets producing the same model output. The normalization $\\|\\theta_{0}\\|=1$ further removes scale ambiguity in the single-index component.",
    "question": "In the extended partially linear single-index model $$y=\\beta_{0}^{\\mathrm{{T}}}X+\\phi(\\theta_{0}^{\\mathrm{{T}}}X)+\\varepsilon,$$ the condition that $\\beta_{0}$ and $\\theta_{0}$ are perpendicular ensures the identifiability of the model parameters. Is this statement true, and why?",
    "question_context": "Extended Partially Linear Single-Index Models: Structure and Applications\n\nThe paper introduces an extended version of a generalized partially linear single-index model, which takes the form $$y=\\beta_{0}^{\\mathrm{{T}}}X+\\phi(\\theta_{0}^{\\mathrm{{T}}}X)+\\varepsilon,$$ where $E(\\varepsilon|X)=0$ and $E\\varepsilon^{2}=\\sigma^{2}$, $\\beta_{0}$ and $\\theta_{0}$ are unknown vector parameters and $\\phi(.)$ is an unknown function. To ensure estimability, the authors assume that $\\beta_{0}$ and $\\theta_{0}$ are perpendicular to each other with $\\|\\theta_{0}\\|=1$ and its first nonzero element positive. The model includes many existing models as special cases, such as the single-index model and the partially linear model. The paper also discusses the application of the model to nonlinear time series data, exemplified by the analysis of annual sunspot numbers."
  },
  {
    "qid": "statistic-posneg-111-0-0",
    "gold_answer": "FALSE. The assumption $\\pi_{1A}^{(1)}=\\pi_{1A}^{(0)}$ is used to simplify the derivation of $\\hat{\\pi}_{1C}^{(1)}$ as $\\hat{\\pi}_{1C A}^{(1)}-\\hat{\\pi}_{1A}^{(0)}$, but the estimator $\\hat{P R}$ can still be valid under different assumptions or with additional adjustments. The core validity of the estimator relies on the correct identification and estimation of complier proportions, not strictly on this equality assumption.",
    "question": "The estimator $\\hat{P R}$ is derived under the assumption that $\\pi_{1A}^{(1)}=\\pi_{1A}^{(0)}$. Is this assumption necessary for the validity of the estimator?",
    "question_context": "Estimation of Proportion Ratio in Treatment Compliance Analysis\n\nThe paper discusses the estimation of the proportion ratio (PR) in a clinical trial setting where patients are assigned to either an experimental (G=1) or standard treatment (G=0). The MLE for PR is derived as $\\hat{P R}=\\hat{\\pi}_{1C}^{(1)}/\\hat{\\pi}_{1C}^{(0)}=(\\hat{\\pi}_{1C A}^{(1)}-\\hat{\\pi}_{1A}^{(0)})/(\\hat{\\pi}_{1C N}^{(0)}-\\hat{\\pi}_{1N}^{(1)})$. The variance of this estimator is also provided using the delta method."
  },
  {
    "qid": "statistic-posneg-153-0-0",
    "gold_answer": "FALSE. The MLE for PR is correctly specified as $\\hat{P R}=(\\hat{p}_{1C A}^{(1)}/\\hat{\\gamma}^{(1)}-\\hat{p}_{1A}^{(0)}/\\hat{\\gamma}^{(0)})/(\\hat{p}_{1C N}^{(0)}/\\hat{\\gamma}^{(0)}-\\hat{p}_{1N}^{(1)}/\\hat{\\gamma}^{(1)})$ only under the assumption that missingness depends solely on the assigned treatment (MAR assumption). However, if missingness also depends on the received treatment (a more general MAR assumption), the correct MLE should account for this by using $\\hat{\\eta}_{R}^{(G)}$ instead of $\\hat{\\gamma}^{(G)}$, leading to $\\hat{P R}=(\\hat{p}_{1C A}^{(1)}/\\hat{\\eta}_{1}^{(1)}-\\hat{p}_{1A}^{(0)}/\\hat{\\eta}_{1}^{(0)})/(\\hat{p}_{1C N}^{(0)}/\\hat{\\eta}_{0}^{(0)}-\\hat{p}_{1N}^{(1)}/\\hat{\\eta}_{0}^{(1)})$.",
    "question": "Is the MLE for the proportion ratio (PR) correctly specified as $\\hat{P R}=(\\hat{p}_{1C A}^{(1)}/\\hat{\\gamma}^{(1)}-\\hat{p}_{1A}^{(0)}/\\hat{\\gamma}^{(0)})/(\\hat{p}_{1C N}^{(0)}/\\hat{\\gamma}^{(0)}-\\hat{p}_{1N}^{(1)}/\\hat{\\gamma}^{(1)})$ when considering missing outcomes?",
    "question_context": "Estimation of Proportion Ratio with Missing Outcomes\n\nFor convenience in the following discussion, we let $p_{i J}^{(1)}$ $(i=1,2,3$ and $J=C A,N)$ denote the corresponding cell probability in the above table. For example, we define $p_{i C A}^{(1)}=\\gamma^{(1)}\\pi_{i C A}^{(1)}$ for $i=1,2$ , $p_{3C A}^{(1)}=(1-\\gamma^{(1)})\\pi_{+C A}^{(1)}$ etc. These also imply that the marginal probabilities: $p_{i+}^{(1)}=\\gamma^{(1)}\\pi_{i+}^{(1)}$ for $i=1,2$ , and $p_{3+}^{(1)}=(1-\\gamma^{(1)})$ . Note that the probability $\\gamma^{(1)}$ of a non-missing outcome for a randomly selected patient assigned to the experimental treatment simply equals p(1+)+ p(2+) . Similarly, we can summarize the data structure for patients assigned to the standard treatment in the following table: <PARAGRAPH_SEPARATOR> We let $p_{i J}^{(0)}$ denote the corresponding cell probability in the above table. Note that the probability $\\gamma^{(0)}$ of nonmissing outcome for patients assigned to the standard treatment equals $p_{1+}^{(0)}+p_{2+}^{(0)}$ . Further, in terms of the parameters $\\gamma^{(G)}$ and $p_{i J}^{(G)}$ , we can easily show that the parameter $P R(1)$ of interest can be expressed as <PARAGRAPH_SEPARATOR> $$ P R=(p_{1C A}^{(1)}/\\gamma^{(1)}-p_{1A}^{(0)}/\\gamma^{(0)})/(p_{1C N}^{(0)}/\\gamma^{(0)}-p_{1N}^{(1)}/\\gamma^{(1)}). $$ <PARAGRAPH_SEPARATOR> Suppose that we randomly assign $n_{G}$ patients to the experimental treatment ( $\\mathit{\\Pi}.G=1\\mathit{\\check{\\Psi}},$ and control treatment (G = 0), respectively. Let ni( JG) denote the observed frequency corresponding to the cell probability $p_{i J}^{(G)}$ in the assigned treatment $G$ . The MLE for $p_{i J}^{(G)}$ is $\\hat{p}_{i J}^{(G)}=n_{i J}^{(G)}/n_{G}$ . Because $\\gamma^{(G)}=p_{1+}^{(G)}+p_{2+}^{(G)}$ , we can estimate $\\gamma^{(G)}$ by $\\hat{\\gamma}^{\\left(G\right)}=\\hat{p}_{1+}^{\\left(G\right)}+\\hat{p}_{2+}^{\\left(G\right)}.$ . These lead to the MLE for $P R$ as <PARAGRAPH_SEPARATOR> $$ \\hat{P R}=(\\hat{p}_{1C A}^{(1)}/\\hat{\\gamma}^{(1)}-\\hat{p}_{1A}^{(0)}/\\hat{\\gamma}^{(0)})/(\\hat{p}_{1C N}^{(0)}/\\hat{\\gamma}^{(0)}-\\hat{p}_{1N}^{(1)}/\\hat{\\gamma}^{(1)}). $$"
  },
  {
    "qid": "statistic-posneg-171-0-3",
    "gold_answer": "FALSE. The paper does not claim that the conditional marginal quantile function q_j,y(alpha) is discontinuous in alpha. On the contrary, it assumes that the conditional marginal distributions F_j,y are continuous, which implies that the quantile functions q_j,y(alpha) are also continuous in alpha.",
    "question": "Does the paper claim that the conditional marginal quantile function q_j,y(alpha) is discontinuous in alpha?",
    "question_context": "Estimation of Extreme Multivariate Expectiles with Functional Covariates\n\nThe paper discusses the estimation of multivariate expectiles with functional covariates, focusing on conditional marginal distributions, quantile functions, and tail dependence functions. It introduces a framework for estimating extreme multivariate expectiles using conditional copulas and tail dependence functions, with applications in risk analysis and capital allocation."
  },
  {
    "qid": "statistic-posneg-1358-0-1",
    "gold_answer": "FALSE. The inverse Rosenblatt transform is analytically available for Clayton copulas but not for Gumbel copulas. For Gumbel copulas, the paper mentions that numerical root finding was used to handle the lack of an analytical solution, specifically for the purpose of timing comparisons.",
    "question": "Is the inverse Rosenblatt transform analytically available for both Clayton and Gumbel copulas?",
    "question_context": "Copula Models and Their Sampling Methods\n\nStudent’s t copulas are prominent members of the elliptical class of copulas and are given by $C(\\pmb{u})=t_{\\nu,P}(t_{\\nu}^{-1}(u_{1}),\\dots,t_{\\nu}^{-1}(u_{d}))$ , $\\pmb{u}\\in[0,1]^{d}$ , where $t_{\\nu,P}$ denotes the distribution function of the $d$ -dimensional $t$ distribution with $\\nu$ degrees of freedom, location vector 0 and correlation matrix $P$ , and $t_{\\nu}^{-1}$ denotes the quantile function of the univariate $t$ distribution with $\\nu$ degrees of freedom. For all $t$ copulas considered in this work, we fix $\\nu~=~4$ . Student’s $t$ copulas have explicit inverse Rosenblatt transforms, so one can utilize the CDM for generating quasirandom samples from them.<PARAGRAPH_SEPARATOR>Archimedean copulas are copulas of the form<PARAGRAPH_SEPARATOR>$$C(\\pmb{u})=\\psi(\\psi^{-1}(u_{1})+\\cdot\\cdot\\cdot+\\psi^{-1}(u_{d})),\\quad\\pmb{u}\\in[0,1]^{d},$$<PARAGRAPH_SEPARATOR>for an Archimedean generator $\\psi$ which is a continuous, decreasing function $\\psi:[0,\\infty]\\rightarrow[0,1]$ that satisfies $\\psi(0)~=~1$ $\\psi(\\infty)=\\dim_{t\\rightarrow\\infty}\\psi(t)=0$ and that is strictly decreasing on $[0,\\operatorname*{inf}t:\\psi(t)=0]$ . Examples of Archimedean generators include $\\psi_{C}(t)~=~(1~+~t)^{-\\bar{1/\\theta}}$ (for $\\theta>0,$ ) and $\\psi_{G}(t)= $ $\\exp(-t^{1/\\theta})$ (for $\\theta\\geq1$ ), generating Clayton and Gumbel copulas, respectively. While the inverse Rosenblatt transform and thus the CDM is available analytically for Clayton copulas, this is not the case for Gumbel copulas; in Appendix B we used numerical root finding to include the latter case for the purpose of timings only.<PARAGRAPH_SEPARATOR>We additionally consider equally weighted two-component mixture copulas in which one component is a 90-degree-rotated $t_{4}$ copula with $\\tau=0.50$ and the other component is either a Clayton copula $\\mathit{\\Pi}_{\\tau}=0.50\\mathit{\\check{\\Psi}},$ ) or a Gumbel copula $\\mathit{\\Pi}_{\\tau}=0.50\\$ ). The two mixture copula models are referred to as Clayton- $t(90)$ and Gumbel- $t(90)$ copulas, respectively.<PARAGRAPH_SEPARATOR>The top rows of Figure 1 displays contour plots of true $t$ copulas with $\\tau=0.25$ (left), 0.50 (middle), and 0.75 (right) along with contours of empirical copulas based on GMMN pseudorandom and GMMN quasi-random samples corresponding to each true copula C. The top row of Figure 2 displays similar plots for Clayton-t(90) (left) and Gumbel-t(90) (right) copulas. In each plot, across the two figures described above, we observe that the contour of the empirical copula based on GMMN pseudo-random samples is visually fairly similar to the contour of $C_{:}$ , thus indicating that the five GMMNs have been trained sufficiently well. We also see that the contours of the empirical copulas based on GMMN quasi-random samples are much closer to the true contours of $C$ than those based on the corresponding pseudo-random samples. This observation indicates that our GMMN quasi-random samples indeed have smaller $F_{X}$ -discrepancy than the usual pseudo-random samples."
  },
  {
    "qid": "statistic-posneg-926-1-3",
    "gold_answer": "TRUE. Theorem 1 establishes that under Assumption 1 and in a Bernoulli design, $\\tau_{\\mathrm{AOE}}(\\pi)=\\tau_{\\mathrm{INF}}(\\pi)$. This connection provides an alternative interpretation of the AIE as the difference between the infinitesimal policy effect and the ADE, i.e., $\\tau_{\\mathrm{AIE}}(\\pi)=\\tau_{\\mathrm{INF}}(\\pi)-\\tau_{\\mathrm{ADE}}(\\pi)$.",
    "question": "Is it correct to say that the infinitesimal policy effect $\\tau_{\\mathrm{NF}}(\\boldsymbol\\pi)$ is equivalent to the AOE $\\tau_{\\mathrm{AOE}}(\\pi)$ in a Bernoulli design, as stated in Theorem 1?",
    "question_context": "Average Direct and Indirect Causal Effects Under Interference\n\nWe study different experimental designs using the potential outcomes model. The main difference between a setting with interference and the standard Neyman–Rubin model is that potential outcomes for the ith unit may also depend on the intervention given to the jth unit with $j\\neq i$ (e.g., Hudgens & Halloran, 2008; Aronow & Samii, 2017). For convenience, we use the shorthand $Y_{i}(w_{j}=x;W_{-j})$ to denote the potential outcome we would observe for the ith unit if we were to assign the jth unit to treatment status $x\\in\\{0,1\\}$ and maintain all units, but the $j$ th at their realized treatments $W_{-j}\\in\\{0,1\\}^{n-1}$ . Expectations $E$ are over the treatment assignment only; potential outcomes are held fixed."
  },
  {
    "qid": "statistic-posneg-830-0-1",
    "gold_answer": "FALSE. Explanation: The condition $\\mathcal{M}_{2}$ is a multiplicative no-interaction condition, but it is not the only possible definition of no interaction. The additive condition $\\mathcal{A}$ is another valid definition. The choice between additive and multiplicative models depends on the underlying biological or statistical model being used. Therefore, while $\\mathcal{M}_{2} = 1$ indicates no interaction under a multiplicative model, it does not necessarily imply no interaction under an additive model. The presence or absence of interaction is model-dependent.",
    "question": "Does the condition $\\mathcal{M}_{2}: \\frac{\\pi_{00}\\pi_{11}}{\\pi_{01}\\pi_{10}} = 1$ imply that there is no interaction between the two exposure factors $\\pmb{A}$ and $\\pmb{B}$?",
    "question_context": "Synergism and Interaction in Binary Exposure Factors\n\nTable 1 is taken from a report of a U.N. Committee (1982) and provides estimates of the risk per year, multiplied by $10^{5}$ , of getting lung cancer in four male populations, cross-classified according to two binary exposure factors $\\pmb{A}$ and $B_{;}$ where $A_{0}$ indicates a nonsmoker, $A_{1}$ a heavy smoker, $B_{0}$ a control and $B_{1}$ a uranium worker. If the risks per year in Table 1 were converted into lifetime probabilities, the probabilities would be close to being proportional to the risks. We shall work with the lifetime probabilities $\\pi_{j k}$ of getting disease $\\pmb{D}$ in the populations exposed to $(A_{j},B_{k})$ for $j=0,1,k=0,1$ , as shown in Table 2. <PARAGRAPH_SEPARATOR> Table 1. Risks per year $(\times10^{5})$ of lung cancer, cross-classified  by  smoking  factor $\\pmb{A}$ and radiationfactor $\\pmb{B}$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{B_{0}}&{{}\\quad B_{1}}\\\\ {A_{0}}&{{}\\quad\\pi_{00}}\\\\ {A_{1}}&{{}\\quad\\pi_{10}}\\end{array}\\quad\\pi_{11} $$ <PARAGRAPH_SEPARATOR> The approach of this paper is to extend the definition of attributable risk from one to two binary exposure factors. In so doing we encounter natural definitions of synergistic and other kinds of joint action between the two exposure factors. <PARAGRAPH_SEPARATOR> In the literature on two binary exposure factors a great deal of discussion is devoted to concepts of synergism, interaction and no interaction between the two factors. The leading definitions of no interaction, one additive and two multiplicative, are <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathcal{A}:\\pi_{00}-\\pi_{01}-\\pi_{10}+\\pi_{11}=0,}\\\\ &{\\mathcal{A}_{1}:\\displaystyle\\frac{(1-\\pi_{00})(1-\\pi_{11})}{(1-\\pi_{01})(1-\\pi_{10})}=1,}\\\\ &{\\mathcal{M}_{2}:\\displaystyle\\frac{\\pi_{00}\\pi_{11}}{\\pi_{01}\\pi_{10}}=1.}\\end{array} $$ <PARAGRAPH_SEPARATOR> It is well known that $\\mathcal{A}$ and $\\mathcal{M}_{1}$ are approximately equivalent when the $\\pi_{j k}$ are small, as they usually are. While this is not the place to review the extensive literature on these no-interaction models, we shall show how our new approach throws new light on them, in $\\S\\S3{\\cdot}3{-}3{\\cdot}5.$ <PARAGRAPH_SEPARATOR> We begin with a discussion of attributable risk for one binary exposure factor."
  },
  {
    "qid": "statistic-posneg-1176-10-0",
    "gold_answer": "FALSE. The state-space model is incorrectly specified because the matrix $A(\\theta)$ does not correctly represent the autoregressive structure of the increments. The correct specification should reflect the relationship $L_{k_{l+1}}^{S} = L_{k_{l}}^{S} + \\theta_{3}(L_{k_{l}}^{S} - L_{k_{l-1}}^{S}) + \\epsilon_{l}$, which is not accurately captured by the given $A(\\theta)$ matrix. The matrix $Q(\\theta)$ also does not correctly represent the covariance structure of the noise term $\\pmb{\\nu}_{k_{l}}$.",
    "question": "Is the state-space model ${\\pmb x}_{k_{l}}=A(\\pmb{\\theta}){\\pmb x}_{k_{l-1}}+{\\pmb\\nu}_{k_{l}}$ correctly specified given the definitions of $A(\\theta)$ and $Q(\\theta)$ for modeling the trajectory increments?",
    "question_context": "State-Space Model for Trajectory Imputation\n\nIn Section 5, we used sampling from the smoothing distribution (FFBS) of the direction of flights to impute a missing part of the trajectory. Our approach can be detailed as follows. Consider $k_{l}$ to be a sequence of indices such that $R_{k_{l}}=f$ . Then we have\n\n$$\n\\Delta_{k_{l}}^{S}=L_{k_{l+1}}^{S}-L_{k_{l}}^{S}.\n$$\n\nUnder standard parametrisation this means that\n\n$$\n\\Delta_{k_{l}}^{S}=\\theta_{3}\\Delta_{k_{l-1}}^{S}+\\epsilon_{l},\n$$\n\nwhere $\\epsilon_{l}\\sim\\mathcal{n}(0,\\theta_{4}I)$ . Because of the previous equation we can also write it as\n\n$$\nL_{k_{l+1}}^{S}-L_{k_{l}}^{S}=\\theta_{3}(L_{k_{l}}^{S}-L_{k_{l-1}}^{S})+\\epsilon_{l}.\n$$\n\nThus, if we now define $x_{l}=[L_{k_{l}}^{S},\\Delta_{k_{l-1}}^{S}].$ , we can write that\n\n$$\n\\begin{array}{r}{{\\pmb x}_{k_{l}}=A(\\pmb{\\theta}){\\pmb x}_{k_{l-1}}+{\\pmb\\nu}_{k_{l}},}\\end{array}\n$$\n\nwhere $\\pmb{\\nu}_{k_{l}}\\sim n(\\pmb{O},\\pmb{Q}(\\pmb{\\theta}))$ with\n\n$$\nA(\\theta)=\\left[\\begin{array}{c c c c}{1}&{\\theta_{1}}&{0}&{0}\\ {0}&{0}&{1}&{\\theta_{1}}\\ {0}&{\\theta_{1}}&{0}&{0}\\ {0}&{0}&{0}&{\\theta_{1}}\\end{array}\\right]\\quad\\mathrm{and}\\quad Q(\\theta)=\\theta_{2}\\left[\\begin{array}{c c c c}{1}&{1}&{0}&{0}\\ {0}&{0}&{1}&{1}\\ {1}&{1}&{0}&{0}\\ {0}&{0}&{1}&{1}\\end{array}\\right].\n$$"
  },
  {
    "qid": "statistic-posneg-1454-0-0",
    "gold_answer": "TRUE. The paper states that the asymptotic distribution of the vector of Lagrange multipliers d under the null hypothesis H0 is given by n^(-1/2)d ~ N(0, A - CB^(-1)C^T), where A is the information matrix for the additional parameters, B is the information matrix for the null model parameters, and C is the matrix of mixed second derivatives of the log-likelihood function with respect to the additional and null model parameters.",
    "question": "Is the asymptotic distribution of the vector of Lagrange multipliers d given by n^(-1/2)d ~ N(0, A - CB^(-1)C^T) under the null hypothesis H0?",
    "question_context": "Lagrange-Multiplier Tests for ARMA Models\n\nThe paper discusses the use of Lagrange-multiplier tests for ARMA models, focusing on overfitting and the procedure for testing whether the null model adequately explains the data. It introduces alternative models and derives the necessary quantities for performing the Lagrange-multiplier test, including the vector of Lagrange multipliers and the information matrix."
  },
  {
    "qid": "statistic-posneg-1092-2-1",
    "gold_answer": "FALSE. The regularization method described does not subtract $\\frac{\\hat{\\sigma}_{b}^{2}}{\\lambda_{1}}$ from each singular value $\\lambda_{l}$ and $\\frac{\\hat{\\sigma}_{w}^{2}}{\\nu_{1}}$ from each singular value $\\nu_{q}$. Instead, it subtracts $\\frac{\\hat{\\sigma}_{b}^{2}}{\\lambda_{l}}$ from each singular value $\\lambda_{l}$ and $\\frac{\\hat{\\sigma}_{w}^{2}}{\\nu_{q}}$ from each singular value $\\nu_{q}$. This means the shrinkage is applied individually to each singular value based on its own magnitude, not uniformly based on the first singular value. This approach ensures that smaller singular values, which are more likely to be noise, are shrunk more aggressively than larger ones.",
    "question": "The regularization method described in the context applies a nonlinear transformation to the singular values of $W_{b}$ and $W_{w}$ by subtracting $\\frac{\\hat{\\sigma}_{b}^{2}}{\\lambda_{1}}$ from each singular value $\\lambda_{l}$ and $\\frac{\\hat{\\sigma}_{w}^{2}}{\\nu_{1}}$ from each singular value $\\nu_{q}$. Is this statement true?",
    "question_context": "Multilevel Singular Value Decomposition for Mixed Data Imputation\n\nThe imputation methods described in Sections 3.1 and 3.2 require to select two parameters: the number of between and within components $Q_{b}$ and $Q_{w}$ . Furthermore, they must be selected from an incomplete dataset. This is far from trivial, especially in the case of categorical variables. In fact, even in the complete case and without multilevel structure, not many options are available. Consequently, we advocate the use of cross-validation to select these components. More precisely, for quantitative data, leave-one-out cross-validation consists in removing each observed value $y_{k,i_{k},j}$ of the data matrix $Y$ one at a time. Then, for a fixed number of dimensions $Q_{b}$ and $Q_{w}$ , we predict its value using the multi-level method obtained from the dataset that excludes this cell (using the iterative MLPCA on the incomplete dataset). The predicted value is denoted $\\left(\\hat{y}_{k,i_{k},j}^{-k i_{k}j}\\right)^{(Q_{b},Q_{w})}$ . Finally, the prediction error is computed and the operation repeated for all observed cells in $Y$ and for a number of dimensions varying for $Q_{b}$ from 0 to $\\operatorname*{min}(K-2,p-1)$ and for $Q_{w}$ from 0 to $\\operatorname*{min}(n-K,p-1)$ . The numbers $Q_{b}$ and $Q_{w}$ that minimize the mean square error of prediction (MSEP) are kept: $$\\begin{array}{r l}{\\lefteqn{\\mathrm{MSEP}(Q_{b},Q_{w})}}\\ &{=\\frac{1}{n_{P}}\\sum_{k=1}^{K}\\sum_{i_{k}=1}^{n_{k}}\\sum_{j=1}^{P}\\bigg(y_{k,i_{k},j}-\\Big(\\hat{y}_{k,i_{k},j}^{-k i_{k}j}\\Big)^{(Q_{b},Q_{w})}\\bigg)^{2}.}\\end{array}$$ This method is computationally costly, especially when the number of cells is large, since it requires to perform the iterative multilevel algorithm for each cell and for each number of between and within components. To reduce the computational cost, we implement a $k$ -fold approach which consists in removing more than one value in the dataset, for instance, $5\\%$ of the cells and predict them simultaneously, combined with parallel computing and fast implementation of SVD. The same approach is used for categorical and mixed data using the coding with the indicator matrix of dummy variables and the iterative MLMCA and MLFAMD algorithms."
  },
  {
    "qid": "statistic-posneg-1974-0-1",
    "gold_answer": "FALSE. While P_l = ∞ indeed forces β_l → 0 under H_I (infinite precision), the Gamma hyperprior on P_l under H_D is not primarily for Bayes Factor stability. Instead, it hierarchically models uncertainty in the precision of non-zero β_l, with shape/rate parameters (υ_l/2 + β_l/2, etc.) adapting to the data. The Bayes Factor sensitivity concern is addressed through the model selection framework (comparing Pr(M_D|y) vs. Pr(M_I|y)), not the hyperprior choice.",
    "question": "The precision matrix P for testing evolutionary independence (H_I) vs. dependence (H_D) is block-diagonal with P_lI_p_l for non-zero β_l components. Does setting P_l = ∞ for H_I correctly enforce β_l = 0, and is the Gamma hyperprior on P_l (when H_D is true) justified to mitigate Bayes Factor sensitivity?",
    "question_context": "Bayesian Hierarchical Phylogenetic Model for Evolutionary Interaction Testing\n\nThe paper describes a two-way Bayesian hierarchical phylogenetic model (HPM) to test for evolutionary interactions between gene regions in HIV patients undergoing ENF therapy. The model estimates patient-specific and gene-specific parameters (e.g., tree topology, branch lengths, nucleotide substitution rates) using hierarchical priors. The key formulas include the log-Normal and Dirichlet priors for continuous parameters and a structured precision matrix for testing independence vs. dependence hypotheses in evolutionary tree outcomes."
  },
  {
    "qid": "statistic-posneg-2051-2-0",
    "gold_answer": "TRUE. Explanation: The 95% posterior interval for β2 (race) is [0.3282,0.9796], which does not include 0, indicating a statistically significant positive effect. For β4 (peak), the interval is [-0.2780,-0.2414], which is entirely negative and excludes 0, showing a statistically significant negative effect. This aligns with the clinical interpretation that higher peak values (better eardrum movement) indicate less chance of fluid, while race appears to be a significant predictor in this model.",
    "question": "Based on the parameter estimates β̂=(0.4249,0.6485,0.0009,-0.2598)⊤, does the model suggest that race (β2) has a statistically significant positive effect on the probability of ear fluid presence, while peak height (β4) has a statistically significant negative effect?",
    "question_context": "Hierarchical Logistic Regression for Ear Fluid Status Prediction\n\nThe study models the probability of ear fluid status in children as a function of race, age, peak tympanogram height, and other covariates using a hierarchical logistic regression model with random effects. The model is specified as Pr{Y_ij=1|b_i,β} = ϕ(β_1 + race*β_2 + age*β_3 + peak*β_4 + (ear,peak)*b_i), where b_i are normally distributed random effects with covariance matrix D. The analysis uses Gibbs sampling and MCEM algorithms for parameter estimation, with non-informative priors on β and D."
  },
  {
    "qid": "statistic-posneg-1462-14-1",
    "gold_answer": "FALSE. The probability formula $$\\mathbb{P}\\left(P_{i}=\\frac{j}{n+1},P_{i^{\\prime}}=\\frac{j^{\\prime}}{n+1}\\right)=\\frac{1}{n+2}\\cdot\\frac{1}{n+1}\\mathbb{1}_{\\{j=j^{\\prime}\\}}+\\left(1-\\frac{1}{n+2}\\right)\\cdot\\frac{1}{n+1}\\cdot\\frac{1}{n+1}$$ does not directly imply an invariant correlation of $1/(n+2)$. The invariant correlation is a property of the exchangeable case of the model (14), but the formula itself does not explicitly derive or confirm this correlation value. The reasoning provided in the context is insufficient to conclude that the correlation is indeed $1/(n+2)$ based solely on this probability expression.",
    "question": "Does the probability $\\mathbb{P}\\left(P_{i}=\\frac{j}{n+1},P_{i^{\\prime}}=\\frac{j^{\\prime}}{n+1}\\right)$ correctly reflect an invariant correlation of $1/(n+2)$ when $m=2$? Explain the reasoning.",
    "question_context": "Tail-Dependence Matrix and Conformal p-Values Analysis\n\nProof of Proposition 10. Since the statement on $\\kappa_{g}$ -matrices is obvious, we will show that $\\mathbf{X}$ has the tail-dependence matrix $R$ . Without loss of generality, we can assume that the identical marginal of $\\mathbf{X}$ is the standard uniform distribution. Fix $i,j\\in[d]$ Since $\\lambda_{i j}=\\lambda_{j i}$ , Proposition 7 leads to $$\\lambda_{i j}=\\frac{\\lambda_{i j}+\\lambda_{j i}}{2}=\\operatorname*{lim}_{u\\downarrow0}\\frac{C_{i j}(u,u)+C_{j i}(u,u)}{2u}=\\operatorname*{lim}_{u\\downarrow0}\\frac{r_{i j}M(u,u)+(1-r_{i j}){\\cal{I}}(u,u)}{u}=r_{i j}.\\quad\\bigtriangledown$$ Proof of Proposition 11. Since $S_{n+1},\\ldots,S_{n+d}$ are independent conditional on the null training sample ${\\mathcal{D}}=\\{S_{i}:i\\in[n]\\}$ , we have $$\\mathbb{P}\\left(P_{i}=\\frac{j_{i}}{n+1},i\\in\\mathcal{K}\\right)=\\mathbb{E}_{\\mathcal{D}}\\left[\\mathbb{P}\\left(P_{i}=\\frac{j_{i}}{n+1},i\\in\\mathcal{K}\\bigg|\\mathcal{D}\\right)\\right]=\\mathbb{E}_{\\mathcal{D}}\\left[\\prod_{i\\in\\mathcal{K}}\\mathbb{P}\\left(P_{i}=\\frac{j_{i}}{n+1}\\bigg|\\mathcal{D}\\right)\\right]=\\mathbb{E}_{\\mathcal{D}}\\left[\\prod_{i\\in\\mathcal{K}}\\left(S_{(j_{i})}-\\sum_{i,i=1}\\nu_{i}\\right)\\right]$$ for $j_{i}\\in[n+1],$ where $0=S_{(0)}<S_{(1)}<\\cdots<S_{(n)}<S_{(n+1)}=1$ are the order statistics of the null training sample $(S_{1}\\ldots,S_{n})$ . Since the conformal $\\mathfrak{p}$ -values are independent of the distribution of scores, we can assume without loss of generality that $S_{1},\\ldots,S_{n}$ are independently distributed of the standard uniform distribution. Let $T_{j}=S_{(j)}-S_{(j-1)}$ , $j\\in[n+1]$ . Then $(T_{1},\\dots,T_{n+1})$ follows the Dirichlet distribution with the parameter vector $\\mathbf{1}_{n+1}\\in\\mathbb{R}^{n+1}$ . Therefore, $$\\mathbb{E}_{\\mathcal{D}}\\left[\\prod_{i\\in\\mathcal{K}}\\left(S_{(j_{i})}-S_{(j_{i}-1)}\\right)\\right]=\\mathbb{E}_{\\mathcal{D}}\\left[T_{1}^{N_{1}}\\dots T_{n+1}^{N_{n+1}}\\right]=\\frac{B(1+N_{1},\\dots,1+N_{n+1})}{B(\\mathbf{1}_{n+1})}=\\frac{N_{1}!\\dots N_{n+1}!}{(n+m)(n+m-1)\\dots(n+1)},$$ where $B$ is the $(n+1)$ -dimensional Beta function. In particular, when $m=1$ , we have $\\mathbb{P}(P_{i}=j/(n+1))=1/(n+1),j\\in[n+1]$ . When $m=2$ , we have $$\\mathbb{P}\\left(P_{i}=\\frac{j}{n+1},P_{i^{\\prime}}=\\frac{j^{\\prime}}{n+1}\\right)=\\frac{1}{n+2}\\cdot\\frac{1}{n+1}\\mathbb{1}_{\\{j=j^{\\prime}\\}}+\\left(1-\\frac{1}{n+2}\\right)\\cdot\\frac{1}{n+1}\\cdot\\frac{1}{n+1},$$ which is the exchangeable case of the model (14) and thus has the invariant correlation $1/(n+2)$ . □"
  },
  {
    "qid": "statistic-posneg-749-0-1",
    "gold_answer": "TRUE. The paper explicitly mentions that Σ(β) characterizes the covariance structure of the model and is critical for high-dimensional analysis. This matrix is derived from the population counterparts of the empirical quantities used in the Hessian of the partial likelihood, ensuring theoretical validity in asymptotic settings.",
    "question": "The population counterpart of the Hessian matrix, Σ(β), is used to characterize the covariance structure of the model and plays a critical role in high-dimensional analysis. True or False?",
    "question_context": "High-dimensional Cox Model with Lasso Penalty: Sign Consistency and Estimation\n\nThe paper discusses the Cox proportional hazards model in a high-dimensional setting where the number of covariates p is much larger than the number of non-zero coefficients s0. The model uses a counting process framework with a Lasso penalty to induce sparsity. Key components include the partial likelihood function, its gradient and Hessian, and population counterparts for theoretical analysis. The primal-dual witness technique is employed to establish sign consistency and estimation error bounds."
  },
  {
    "qid": "statistic-posneg-982-0-0",
    "gold_answer": "FALSE. The correlation between $y_{1,i}$ and $y_{2,i}$ is not solely due to the correlation in the errors. The outcomes $y_{1,i}$ and $y_{2,i}$ are also correlated through the shared index functions $(\\alpha_{1}x_{1,i}+\\alpha_{2}x_{2,i}+\\alpha_{3}x_{3,i})$, which appear in both $f_{1}(v)$ and $f_{2}(\\upsilon)$. The errors $\\epsilon_{1,i}$ and $\\epsilon_{2,i}$ introduce additional correlation, but the primary source of correlation comes from the shared dependence on the index functions.",
    "question": "In the simulation study, the random errors for the two outcomes are generated from a bivariate normal distribution with heteroscedastic variances ($\\sigma_{2}^{2}=\\delta\\sigma_{1}^{2}$) and a correlation coefficient $\\rho=0.50$. Is it correct to assume that the correlation between $y_{1,i}$ and $y_{2,i}$ is solely due to the correlation in the errors $\\epsilon_{1,i}$ and $\\epsilon_{2,i}$?",
    "question_context": "Multivariate Nonlinear Regression Simulation Study\n\nWe conducted a simulation study to evaluate the numerical characteristics of the proposed method. We chose $L=2$ and let the two index functions be $f_{1}(v)=v^{2}\\sin(v)$ and $f_{2}(\\upsilon)=\\upsilon e^{\\upsilon}$ . We generated two correlated response outcomes $\\mathbf{y}_{1},\\mathbf{y}_{2}$ from the following model. For each $i\\in\\{1,\\ldots,n\\}$ , $$\\begin{array}{r l}&{\\left\\{y_{1,i}=(\\alpha_{1}x_{1,i}+\\alpha_{2}x_{2,i}+\\alpha_{3}x_{3,i})^{2}\\sin(\\alpha_{1}x_{1,i}+\\alpha_{2}x_{2,i}+\\alpha_{3}x_{3i})+\\beta_{1}z_{i}+\\epsilon_{1i},\\right.}\\ &{\\left\\{y_{2,i}=(\\alpha_{1}x_{1,i}+\\alpha_{2}x_{2,i}+\\alpha_{3}x_{3,i})\\exp(\\alpha_{1}x_{1,i}+\\alpha_{2}x_{2,i}+\\alpha_{3}x_{3,i})+\\beta_{2}z_{i}+\\epsilon_{2,i},\\right.}\\end{array}$$ where the independent variables within the index functions, $x_{1,i},x_{2,i}$ and $x_{3,i},$ are assumed to be independent and following a uniform distribution $\\mathcal{U}(0,\\pi)$ . We generated a binary variable $z_{i}$ from a Bernoulli distribution with $\\operatorname*{Pr}(z_{i}=1)=0.3$. The random errors $\\epsilon_{i}=(\\epsilon_{1,i},\\epsilon_{2,i})^{\\top}\\sim\\mathcal{N}(\\pmb{0},\\pmb{\\Sigma})$ , where the correlation coefficient between the two outcomes was $\\rho$ , and $\\sigma_{1}^{2}$ and $\\sigma_{2}^{2}=\\delta\\sigma_{1}^{2}$ , which represented a heteroscedasticity in the two outcomes. Table 1 Summary of parameter estimates over 200 simulation runs: true parameters $(\\alpha_{1},\\alpha_{2},\\alpha_{3})~=~1/{\\sqrt{14}}(2,-1,3)~=$ (0.5345, −0.2673, 0.8018), $\\beta_{1}=10$ , $\\beta_{2}=-30$ , $\\rho=0.50$ , $\\delta=0.9,\\sigma=2.$"
  },
  {
    "qid": "statistic-posneg-1726-7-0",
    "gold_answer": "TRUE. The conditional density is derived using Bayes' rule, where the likelihood of kₜ given ψₜ and past values is Gaussian, and the prior is the density of ψₜ. The exponential term represents the Gaussian likelihood, which depends on the squared deviation of kₜ from its conditional mean, scaled by the variance σ².",
    "question": "Is the conditional density of ψₜ given by the formula proportional to the prior density of ψₜ multiplied by an exponential term involving the squared deviation of kₜ from its mean, as derived using Bayes' rule?",
    "question_context": "Bayesian Extraction of Returns to Scale Path in Economic Growth Models\n\nThe paper discusses the estimation of the density of ψ₁ to derive an extracted path for ψₜ. The model is given by kₜ = ψₜkₜ₋₁ + aₜ′, where aₜ′ = σϵₜ + ϕ₁(kₜ₋₁ - ψₜ₋₁kₜ₋₂) + ϕ₂(kₜ₋₂ - ψₜ₋₂kₜ₋₃) + (1 - ϕ₁ - ϕ₂)a_∞. The distribution of kₜ given (ψₜ, ψₜ₋₁, ψₜ₋₂, kₜ₋₁, kₜ₋₂, kₜ₋₃) is Gaussian with mean (ψₜ + ϕ₁)kₜ₋₁ + (ϕ₂ - ϕ₁ψₜ₋₂)kₜ₋₂ - ϕ₂ψₜ₋₂kₜ₋₃ + (1 - ϕ₁ - ϕ₂)a_∞ and variance σ². Using Bayes' rule, the conditional density of ψₜ is derived as proportional to the prior density of ψₜ multiplied by an exponential term involving the squared deviation of kₜ from its mean. The paper then discusses the extraction and smoothing of the ψₜ path and checks for frequency patterns in the extracted sequence."
  },
  {
    "qid": "statistic-posneg-412-2-2",
    "gold_answer": "TRUE. For parametric families, the metric entropy $M(\\varepsilon)$ is of order $\\log(1/\\varepsilon)$, leading to $\\varepsilon_{n}^{2}$ of order $\\log n/n$. This term is indeed negligible compared to nonparametric rates such as $n^{-4/5}$, as $\\log n/n$ decays faster than $n^{-4/5}$ for large $n$.",
    "question": "Is it correct that when $\\varSigma$ is a parametric family, the term $\\varepsilon_{n}^{2}$ is of order $\\log n/n$, which is negligible compared to nonparametric rates like $n^{-4/5}$?",
    "question_context": "Adaptive Risk Bounds in Nonparametric Estimation with Known Variance Classes\n\nCase 3: $\\sigma^{2}(\\boldsymbol{x})$ is known to be in a class of variance functions. More generally than the above two cases, let $\\varSigma$ be a class of variance functions bounded above and below. Let $M(\\varepsilon)$ be the $L_{2}(P_{X})$ metric entropy of $\\varSigma$ ; i.e., $M(\\varepsilon)$ is the logarithm of the smallest size of an =-cover of $\\varSigma$ under the $L_{2}(P_{X})$ distance. Let $\\varepsilon_{n}$ be determined by $M(\\varepsilon_{n})/N=\\varepsilon_{n}^{2}$ . Take $\\underline{{\\psi}}$ to be a uniform weight on a covering set, i.e., $\\omega_{k}=e^{-M(\\varepsilon_{n})}$ . This choice of $\\varepsilon_{n}$ gives a good trade-off between $(1/N)\\log(1/\\omega_{j})$ and $\\Vert{\\boldsymbol{\\sigma}}^{2}-{\\boldsymbol{\\sigma}}_{k}^{2}\\parallel^{2}$ . We have the following corollary. Corollary 2. For any given $\\varDelta$ , class of variance function $\\varSigma$ , and weight $\\underline{{\\pi}}$ , we can construct a single estimation procedure $\\delta^{*}$ such that for any underlying regression function u with $\\|u\\|_{\\infty}\\leqslant A$ and $\\sigma^{2}\\in\\Sigma$ , we have $$ \\mathsf{R}((u,\\sigma);n;\\delta^{*})\\leqslant C_{A,\\bar{\\sigma}}\\big\\{\\varepsilon_{n}^{2}+\\operatorname*{inf}_{j}\\left(\\frac{1}{N}\\log\\frac{1}{\\pi_{j}}+\\frac{1}{N}\\sum_{l=n-N+1}^{n}R((u,\\sigma);l;\\delta_{j})\\right)\\le C_{A,\\bar{\\sigma}}\\big\\}. $$ Remark. If $\\sigma$ is known to be in any one in a collection of families of variance functions $\\{\\boldsymbol{\\it{\\Sigma}}_{l},\\boldsymbol{\\it{l}}\\geqslant1\\}$ , then a similar adaptation risk bound holds with an additional penalty term $(1/N)\\log(1/\\varsigma_{l})$ , where $\\left\\{\\varsigma_{l};l\\geqslant1\\right\\}$ is the weight on $\\{\\boldsymbol{\\it{\\Sigma}}_{l},l\\geqslant1\\}$ . If $\\varSigma$ is a parametric family (i.e., of finite dimension) as in Case 2, then $M(\\varepsilon)$ is usually of order $\\log(1/\\varepsilon)$ , resulting in $\\varepsilon_{n}^{2}$ of order log $n/n$ , which is negligible for a nonparametric rate of convergence. Thus for nonparametric estimation, we lose little not knowing the variance function in a parametric class. In contrast, when $\\varSigma$ is a nonparametric class, we may pay a rather high price. For instance, if $\\varSigma$ consists of all nondecreasing functions, then $M(\\varepsilon)$ is of order $1/\\varepsilon$ , and $\\varepsilon_{n}^{2}$ determined accordingly is of order $n^{-2/3}$ , which is damaging for a fast nonparametric rate, e.g., $n^{-4/5}$ . From Theorem 1, by mixing different procedures, we have a single procedure that shares the advantages of the mixed procedures automatically in terms of the risk. Besides the $L_{2}$ risk that we consider, other performance measures (e.g., $L_{\\infty}$ risk or local risks) are useful from both theoretical and practical points of view. It then is of interest to investigate whether similar adaptation procedures exist for other loss functions and if not, what the prices are one needs to pay for adaptation."
  },
  {
    "qid": "statistic-posneg-1541-0-1",
    "gold_answer": "FALSE. The truncation product-limit estimator $\\hat{S}(t)$ ignores $\\mathcal{L}_{M}$ by maximizing only the conditional likelihood $\\mathcal{L}_{C}$. In contrast, the proposed estimator $\\tilde{S}_{A}(t)$ incorporates information from both truncation time $A$ and residual survival time $V$ (via $\\tilde{Q}$ and $\\tilde{K}$), effectively utilizing additional data that relates to the marginal distribution of $A$. This is evidenced by the construction of $\\tilde{Q}(t)$ and $\\tilde{K}(t)$ from pooled data $(a_i, \\delta_i^*)$ and $(\\tilde{v}_i, \\delta_i)$, which leverages the shared marginal density $f_A(t) = f_V(t)$ under length-biased sampling.",
    "question": "Does the proposed estimator $\\tilde{S}_{A}(t) = \\prod_{u\\in[0,t]}\\left\\{1-\\frac{d\\tilde{Q}(u)}{\\tilde{K}(u)}\\right\\}$ ignore the information in the marginal likelihood $\\mathcal{L}_{M}$ as the truncation product-limit estimator does?",
    "question_context": "Nonparametric Estimation for Length-Biased and Right-Censored Data\n\nCondition 2. The incidence of disease onset occurs over calendar time at a constant rate, that is, $W^{0}$ has a constant density function. <PARAGRAPH_SEPARATOR> Assume that the sampling time $\\xi$ is noninformative; that is, $\\xi$ is independent of $(W^{0},T^{0})$. A prevalent population consists of individuals with the disease who have not experienced the failure event at the sampling time. Hence an individual would be qualified for enrolment only if $T^{0}\\geqslant\\xi-W^{0}>0$. We let $W$ denote the time of the incidence of the disease and $T$ denote the time from the disease incidence to the failure event. The superscript 0 is dropped to emphasize that the failure time $T$ in the prevalent population must exceed $\\xi-W$ and is thus a left truncated random variable. The probability distribution of $(W,T)$ is the same as the probability distribution of $(W^{0},T^{0})$ conditional on $T^{0}\\geqslant\\xi-W^{0}>0$. <PARAGRAPH_SEPARATOR> Let $A=\\xi-W$ be the truncation time from disease incidence to sampling time and let $V=T-A$ be the residual survival time from the sampling time. Let $f(t)$ and $S(t)$ denote the marginal density function and survival function of $T^{0}$ in the general population. Denote by $f_{T}(t)$, $f_{A}(t)$ and $f_{V}(t)$ the marginal density functions of the random variables $T$, $A$ and $V$ defined in the prevalent population, and by $S_{T}(t),S_{A}(t)$ and $S_{V}(t)$ the corresponding survival functions. Under Conditions 1 and 2, $(A,T)$ has the joint density function (Lancaster, 1990, Ch. 3) <PARAGRAPH_SEPARATOR> $$ f_{A,T}(a,t)=\\frac{f(t)}{\\mu}I(t>a>0), $$ <PARAGRAPH_SEPARATOR> where $\\textstyle\\mu=E(T^{0})=\\int_{0}^{\\infty}u f(u)d u$. It is easy to see that the failure time $T$ in the prevalent population has a length-biased density function, <PARAGRAPH_SEPARATOR> $$ f_{T}(t)=\\frac{t f(t)}{\\mu}I(t>0). $$ <PARAGRAPH_SEPARATOR> Moreover, $A$ and $V=T-A$ share the same marginal density function, <PARAGRAPH_SEPARATOR> $$ f_{A}(t)=f_{V}(t)=\\frac{S(t)}{\\mu}~I(t>0). $$ <PARAGRAPH_SEPARATOR> In practice, the observation of failure time could be further complicated by right censoring. Let $C$ be the time from study enrolment to censoring. We assume that $C$ is independent of $(\\xi,W,T)$. However, the survival time $T=A+V$ and the total censoring time $A+C$ are dependent, as they share the same $A$. In other words, the survival time $T$ is subject to informative censoring. The observed data include independent and identically distributed copies of $(W,A,\\tilde{V},\\Delta)$, where $\\tilde{V}=\\operatorname*{min}(V,C)$ and $\\Delta=I(V\\leqslant C)$."
  },
  {
    "qid": "statistic-posneg-1524-0-0",
    "gold_answer": "TRUE. The mean function $\\mu(B)$ is indeed defined as the average of the point-level mean function $\\mu(s)$ over the region $B$, as shown by the formula $\\mu(B) = |B|^{-1}\\int_{B}\\mu(s)\\mathrm{d}s$. This represents the expected value of the aggregated process $Y(B)$ over the region $B$.",
    "question": "Is the mean function $\\mu(B)$ correctly defined as the average of the point-level mean function $\\mu(s)$ over the region $B$?",
    "question_context": "Comparison of Gaussian Geostatistical Models and Gaussian Markov Random Fields\n\nIn many applications in spatial and environmental epidemiology, data concerning a spatial process of interest are often observed at different spatial resolutions, and the overall problem of incompatible spatial data has been encountered very commonly when relating two spatial variables with different supports. For example, in studies of the association between air pollution exposure and adverse health effects, relevant health outcomes are usually available as areal data due to confidentiality while the pollution data are available as a point level [6,19]. To investigate the relationships between two variables with different spatial resolutions, the mismatch problem in the support of the two variables needs to be resolved. One common solution to this spatial misalignment problem is to aggregate the point-referenced data to the area level, and create a common support for both variables. Once the point-referenced data are aggregated to the relevant level, the process representing the aggregated data is modeled using integrals of spatial continuous process [14,15]. Consider a continuous Gaussian process $Y(s)$ with mean function $\\mu(s)$ and covariance function $c(s_{i},s_{j})$ for $s_{i},s_{j}\\subset D\\in R^{d}$ , where $s_{i}$ is a location in a fixed domain $D$ . The aggregated process over a region $B$ , $\\begin{array}{r}{Y(B)=\\int_{B}Y(s)\\mathrm{d}s}\\end{array}$ has a multivariate normal distribution with mean function $\\mu(B)$ and covariance function $\\Sigma(B_{1},B_{2})$ , $$\\begin{array}{l}{\\displaystyle\\mu(B)=E(Y(B))=|B|^{-1}\\int_{B}\\mu(s)\\mathrm{d}s}\\ {\\sum(B_{1},B_{2})=\\mathrm{cov}(Y(B_{1}),Y(B_{2}))=|B_{1}|^{-1}|B_{2}|^{-1}\\int_{B_{1}}\\int_{B_{2}}c(s_{1},s_{2})\\mathrm{d}s_{1}\\mathrm{d}s_{2},}\\end{array}$$ where $|B_{i}|$ denotes the area of a region $B_{i}$ for $i=1,2$ . Modeling aggregated data using these spatial integrals requires lots of computation. Hence, instead of using the aggregated models directly in modeling aggregated point-referenced data, it is becoming very common to use Gaussian Markov random fields."
  },
  {
    "qid": "statistic-posneg-1816-7-1",
    "gold_answer": "FALSE. The context clearly states that the theory only provides results regarding the asymptotic behavior of the variational estimator $\\hat{\\theta}_{n}$ of the structural parameters $\\theta_{0}$ and does not cover the behavior of the variational parameters $\\hat{\\psi}_{i;}$. The paper acknowledges the importance of understanding these parameters for confidence regions with valid coverage for $Z_{i}$ but leaves this as a topic for future research.",
    "question": "Does the paper's asymptotic theory cover the behavior of the variational parameters $\\hat{\\psi}_{i;}$ governing the unit-specific variational conditional distributions of the latent variable $Z_{i}$ given the observed data $X_{i}$?",
    "question_context": "Variational Inference in Mixture Models: Asymptotic Theory and Estimator Consistency\n\nWe also used the asymptotic theory to propose a sandwich covariance estimator to provide calibrated confidence regions of variational estimators and a one-step correction to the variational estimator. Both of these methods work well when the variational estimator is consistent, and in fact the one-step correction exceeded our expectations by correcting some of the bias in fixed-effect variational parameter estimators in a logistic regression model with random quadratics.<PARAGRAPH_SEPARATOR>Our theory is limited to models which are iid at some level. While this includes many hierarchical and longitudinal models, it excludes models for fully dependent time series, spatial data, and dyadic data. Extending the theory to cover those cases could be a fruitful next step. Additionally, we made the simplifying assumption that the variational class is of fixed and finite dimension, but our theory could be extended to other variational classes.<PARAGRAPH_SEPARATOR>Our theory only provides results regarding the asymptotic behavior of the variational estimator $\\hat{\\theta}_{n}$ of the structural parameters $\\theta_{0}$ . It does not cover the behavior of the variational parameters $\\hat{\\psi}_{i;}$ , which govern the unit-specific variational conditional distributions of the latent variable $Z_{i}$ given the observed data $X_{i}$ . The behavior of these parameters is important in settings where confidence regions with valid coverage are desired for the $Z_{i}$ . This is the case, for instance, when the $Z_{i}$ correspond to specific fixtures of the real world, such as counties or schools. However, we note that, since the variational family typically does not contain the true conditional distribution, it may be difficult to provide regions with good coverage of $Z_{i}$ using variational inference. By contrast, as we have demonstrated, $\\hat{\\theta}_{n}$ may be consistent even when the variational family does not contain the true conditional. This was one reason we chose to focus on the asymptotic behavior of $\\hat{\\theta}_{n}$ . Nevertheless, we would conjecture that $\\hat{\\hat{\\psi}}_{i}$ given $X_{i}$ converges in distribution to<PARAGRAPH_SEPARATOR>$$\\underset{\\psi\\in\\Psi}{\\arg\\operatorname*{max}}\\int\\left(\\log\\frac{p_{\\bar{\\theta}}(X_{i},z)}{q(z;\\psi)}\\right)q(z;\\psi)d z$$<PARAGRAPH_SEPARATOR>for each $i,$ where $\\bar{\\theta}$ is the limit in probability of the variational estimator $\\hat{\\theta}_{n}$ . We leave further discussion along these lines to future work.<PARAGRAPH_SEPARATOR>We developed our theory in the context of a fixed variational family of conditional distributions. A natural question is whether our theory provides insight about which variational families yield consistency. Unfortunately, it appears to be difficult to address this question in a general manner. As a simple example, it would intuitively seem that if a given class of variational distributions yields a consistent estimator, then any enlargement of the class should also yield a consistent estimator. However, it is not clear whether this is true based on our theory. This would be an important topic of future research."
  },
  {
    "qid": "statistic-posneg-1559-0-0",
    "gold_answer": "FALSE. While the condition $\\mathbf{max}_{1\\leqslant i\\leqslant n}n b_{n i}^{2}\\to0$ a.e. is necessary, it is not sufficient on its own. The asymptotic normality of the KSV estimator also requires the variance-covariance matrix $\\Sigma(\\tau)$ to be well-defined and finite for $\\tau\\in[0,+\\infty]$, the tail conditions R1-R5, and the condition $\\mathsf{s u p}_{0<t}E[\\varepsilon_{i}-t|\\varepsilon_{i}>t]<\\infty$. These additional conditions ensure the proper handling of the tail of the remainder terms in the martingale representation and the finiteness of the variance-covariance matrix.",
    "question": "Is the condition $\\mathbf{max}_{1\\leqslant i\\leqslant n}n b_{n i}^{2}\\to0$ a.e. sufficient to ensure the asymptotic normality of the KSV estimator in the censored linear model?",
    "question_context": "Asymptotic Normality of Censored Linear Regression Estimators\n\nSince we are working with the random design, we assume Condition D1 throughout the rest of the paper. Before we state the main theorem, we pause here to observe that Condition D1 implies, among other things, that $\\mathbf{max}_{1\\leqslant i\\leqslant n}n b_{n i}^{2}\\to0$ a.e. and $\\mathrm{max}_{1\\leqslant i\\leqslant n}n b_{n i}^{2}X_{i}^{2}\\to0$ a.e. (see, e.g., Problem 8 of Chow and Teicher (1987, Sect. 5.2)). <PARAGRAPH_SEPARATOR> In addition to the design conditions, we need the following regularity conditions to handle the tail of the remainder terms in martingale representation. <PARAGRAPH_SEPARATOR> Towards this, let <PARAGRAPH_SEPARATOR> $$ C(t)=\\int_{0}^{\\prime}\\frac{1}{1-\\bar{F}}\\frac{d G}{(1-G)^{2}}, $$ <PARAGRAPH_SEPARATOR> where ${\\cal\\overline{{F}}}=\\operatorname*{lim}_{n\\to\\infty}\\left(1/n\\right)\\sum{\\cal F}_{i}$ . The conditions are <PARAGRAPH_SEPARATOR> R1. For some $\\tau>0$ <PARAGRAPH_SEPARATOR> $$ \\operatorname*{sup}_{n}\\int_{\\tau}^{\\infty}\\left[\\sum b_{n_{i}}t f_{i}(t)\\right]^{2}d C(t)<\\infty. $$ <PARAGRAPH_SEPARATOR> R2. For some $\\tau>0$ <PARAGRAPH_SEPARATOR> $$ E\\int_{\\tau}^{\\infty}\\left|B(C(t))\\bar{h}(t)\\right|d t<\\infty, $$ <PARAGRAPH_SEPARATOR> where $\\begin{array}{r}{\\bar{h}(t)=\\operatorname*{lim}\\sum b_{n i}t f_{i}(t).}\\end{array}$ <PARAGRAPH_SEPARATOR> R3. <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{t\\rightarrow\\infty}\\operatorname*{lim}_{n\\rightarrow\\infty}n\\sum{b_{n i}^{2}}\\frac{[\\int_{t}^{\\infty}s d F_{i}(s)]^{2}}{1-H_{i}(t)}=0. $$ <PARAGRAPH_SEPARATOR> R4. For any $\\tau>0$ $f_{i}(t)/(1-H_{i}(t))$ is of bounded variation on $[-\\tau,\\tau]$ uniformly in $i=1,2,\\ldots$ <PARAGRAPH_SEPARATOR> R5.For any $\\tau>0$ <PARAGRAPH_SEPARATOR> $$ \\int_{\\tau}^{\\infty}\\sqrt{n\\sum b_{m i}^{2}\\frac{t^{2}f_{i}^{2}(t)}{1-H_{i}}}d t\\leqslant C<\\infty. $$ <PARAGRAPH_SEPARATOR> A few comments are in order regarding Conditions R1-R5 before we proceed with the statement of the theorem. Condition R1 is equivalent to requiring that the variance of $\\begin{array}{r}{\\int_{\\tau}^{\\infty}\\sum b_{n i}t f_{i}(t)\\mathop{d}B(C(t))}\\end{array}$ be finite uniformly in n. Here the time changed Brownian motion $B(C(t))$ appears naturally as the limit of the Kaplan-Meier process. A condition of this type also appears in Gill (1983). A sufficient condition for R1 is that $X$ be bounded and for some $C>0$ and $\\varepsilon>0$ <PARAGRAPH_SEPARATOR> $$ \\operatorname*{sup}_{t>\\tau}\\frac{t^{2}\\bar{f}^{2}(t)}{(1-\\bar{F})(1-G)^{1+\\varepsilon}}<C, $$ <PARAGRAPH_SEPARATOR> where $\\bar{f}$ is the density of ${\\ \\boldsymbol{\\bar{F}}}.$ The condition R4 is seen to hold if either (i) the hazard function is of bounded variation on $\\mathbb{R}^{1}$ or (ii) the covariate $X$ is bounded with probability one. The tail conditions $\\mathbb{R}2,\\textbf{R}3$ ,and R5 will hold if for all $i=1,2,\\ldots$ <PARAGRAPH_SEPARATOR> $$ 1-G(t)>K(1-F_{i})\\qquad\\mathrm{for}\\quad K>0,0<\\beta<1, $$ <PARAGRAPH_SEPARATOR> and uniformly in $i$ <PARAGRAPH_SEPARATOR> $$ \\int_{\\tau}^{\\infty}\\frac{t d F_{i}(t)}{(1-F_{i})^{\\alpha}}<\\infty\\qquad\\mathrm{with}\\quad\\alpha=\\frac{1+\\beta}{2}. $$ <PARAGRAPH_SEPARATOR> The condition R3 will hold if, in addition to Assumption 2 of Theorem 3.1, $t^{2}(1-F_{i}(t))/(1-G(t))\\to0$ as $t\\rightarrow\\infty$ .Since <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{n\\displaystyle\\sum b_{n i}^{2}\\displaystyle\\frac{[\\int_{\\tau}^{\\infty}s d F_{i}(s)]^{2}}{1-H_{i}(t)}=n\\sum b_{n i}^{2}\\displaystyle\\frac{[t(1-F_{i})+\\int_{t}^{\\infty}\\left(1-F_{i}\\right)d s]^{2}}{(1-F_{i})(1-G)}}} {{~\\leqslant2t^{2}\\displaystyle\\sum n b_{n i}^{2}\\displaystyle\\frac{1-F_{i}}{1-G}+2\\sum n b_{n i}^{2}\\displaystyle\\frac{[\\int_{t}^{\\infty}\\left(1-F_{i}\\right)d s]^{2}}{(1-F_{i})^{2}}\\displaystyle\\frac{1-F_{i}}{1-G}}} {{~\\leqslant K\\displaystyle\\sum n b_{n i}^{2}\\displaystyle\\frac{1-F_{i}}{1-G}+K t^{2}\\sum n b_{n i}^{2}\\displaystyle\\frac{1-F_{i}}{1-G}.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> It is clear that the above will $\\rightarrow0$ as $t\\rightarrow\\infty$ if the said condition holds. Roughly speaking, these conditions require two properties: that $F_{i}$ have reasonably small tails and $G$ have a suitably heavier tail compared to $F_{i}$ For example, if $1-F_{i}(t)=e^{\\mathrm{~}\\lambda_{i}t}$ and $1-G(t)=e^{\\mathrm{~\\d~}\\eta t}$ with $\\lambda_{i}<M<\\eta$ then these conditions hold. <PARAGRAPH_SEPARATOR> THEOREM 3.1. In the censored linear model (1.1), the KSV estimator (3.1) is asymptotically normally distributed, i.e., <PARAGRAPH_SEPARATOR> $$ \\sqrt{n}\\binom{\\hat{\\alpha}-\\alpha}{\\hat{\\beta}-\\beta}(T^{n})\\overset{\\mathcal{L}}{\\longrightarrow}N\\bigg(0,\\sum(\\infty)\\bigg), $$ <PARAGRAPH_SEPARATOR> provided <PARAGRAPH_SEPARATOR> 1. the variance-covariance matrix $\\Sigma\\left(\\tau\\right)$ (see definition below)is welldefined and finitefor $\\tau\\in[0,+\\infty]$ and $\\begin{array}{r}{\\sum\\left(\\tau\\right)\\xrightarrow{\\tau\\rightarrow\\infty}\\sum\\left(\\infty\\right)}\\end{array}$ <PARAGRAPH_SEPARATOR> 2. $\\mathsf{s u p}_{0<t}E[\\varepsilon_{i}-t|\\varepsilon_{i}>t]<\\infty.$   3. the tail conditions R1-R5 given above hold. <PARAGRAPH_SEPARATOR> The variance-covariance matrix $\\Sigma(\\tau)=(V_{i j}(\\tau))$ is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\displaystyle V_{22}(\\tau)=\\operatorname*{lim}n\\sum{b_{n i}^{2}}\\int_{-\\infty}^{\\tau}\\left(\\frac{t}{1-G(t)}-\\frac{\\int_{s}^{\\tau}s d F_{i}(s)}{1-H_{i}(t)}\\right)^{2}\\left[1-G(t)\\right]d F_{i}(t)}} {{\\displaystyle~+\\operatorname*{lim}n\\sum{}\\int_{-\\infty}^{\\tau}\\left(\\frac{\\sum b_{n j}\\int_{s}^{\\tau}s d F_{j}(s)}{\\sum{[1-H_{j}(t)]}}\\right.}} {{\\displaystyle~\\left.-\\frac{b_{n i}\\int_{t}^{\\tau}s d F_{i}(s)}{1-H_{i}(t)}\\right)^{2}\\left[1-F_{i}(t)\\right]d G(t)}}\\end{array} $$ <PARAGRAPH_SEPARATOR> $V_{\\textrm{\\scriptsize11}}(\\tau)=s a m e$ as $V_{22}$ except $b_{n i}b_{n j}$ are now $a_{n i}$ and $a_{n j}$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{V_{12}(\\tau)=V_{21}(\\tau)=\\operatorname*{lim}n\\sum a_{n i}b_{n i}\\displaystyle\\int_{-\\infty}^{\\tau}\\left(\\frac{t}{1-G(t)}\\right.}&{} {\\left.-\\frac{\\int_{t}^{\\tau}s d F_{i}(s)}{1-H_{i}(t)}\\right)^{2}[1-G(t)]d F_{i}(t)}&{} {+\\operatorname*{lim}n\\sum_{-\\infty}\\displaystyle\\int_{c_{n i}-a_{n i};b_{n i}}^{\\tau}\\left(\\frac{\\sum c_{n j}\\int_{t}^{\\tau}s d F_{j}(s)}{\\sum[1-H_{j}(t)]}\\right.}&{} {\\left.-\\frac{c_{n i}\\int_{t}^{\\tau}s d F_{i}(s)}{1-H_{i}(t)}\\right)[1-F_{i}(t)]d G(t).}\\end{array} $$"
  },
  {
    "qid": "statistic-posneg-1979-5-1",
    "gold_answer": "FALSE. The estimating function $\\psi_1(\\theta)$ is derived from the composite likelihood approximation, which coincides with the Poisson process likelihood in the limit. While $\\psi_1(\\theta) = 0$ is unbiased for Poisson processes due to the Campbell theorem, this property does not generally hold for other spatial point processes (e.g., Gibbs or Cox processes) unless specific conditions are met. For non-Poisson processes, alternative estimating functions (e.g., based on the Papangelou conditional intensity $\\lambda_\\theta$) are typically required to ensure unbiasedness.",
    "question": "Is the estimating function $$\\psi_{1}(\\theta)=\\sum_{u\\in\\mathbf{x}}{\\mathrm{d}}\\log\\rho_{\\theta}(u)/{\\mathrm{d}}\\theta-\\int_{W}({\\mathrm{d}}\\log\\rho_{\\theta}(u)/{\\mathrm{d}}\\theta)\\rho_{\\theta}(u){\\mathrm{d}}u$$ unbiased for a general spatial point process, not just Poisson processes?",
    "question_context": "Composite Likelihood Estimation for Spatial Point Processes\n\nIn section 8.1, estimating functions based on the (conditional) intensity function are motivated heuristically as limits of composite likelihood functions (Lindsay, 1988) for Bernoulli trials concerning absence or presence of points within infinitesimally small cells partitioning the observation window. Section 8.2 considers minimum contrast or composite log likelihood type estimating functions based on second-order properties. In case of minimum contrast estimation, the parameter estimate minimizes the distance between a non-parametric estimate of a second-order summary statistic and its theoretical expression.<PARAGRAPH_SEPARATOR># 8.1. Estimating functions based on intensities<PARAGRAPH_SEPARATOR>For a given parametric model with parameter $\\theta$ , suppose that the intensity function $\\rho_{\\theta}$ is expressible in closed form. Consider a finite partitioning $C_{i},\\quad i\\in I$ , of the observation window $W$ into disjoint cells $C_{i}$ of small areas $\\left|{C_{i}}\\right|$ , and let $u_{i}$ denote a representative point in $C_{i}$ . Let $N_{i}=\\mathbf{1}[N(C_{i})>0]$ and $p_{i}(\\theta){=}\\mathbf{P}_{\\theta}(N_{i}{=}1)$ . Then $p_{i}(\\theta)\\approx\\rho_{\\theta}(u_{i})\\vert C_{i}\\vert$ , and the composite likelihood based on the $N_{i}$ , $i\\in I$ , is<PARAGRAPH_SEPARATOR>$$\\prod_{i\\in I}p_{i}(\\theta)^{N_{i}}(1-p_{i}(\\theta))^{(1-N_{i})}\\approx\\prod_{i}(\\rho_{\\theta}(u_{i})|C_{i}|)^{N_{i}}(1-\\rho_{\\theta}(u_{i})|C_{i}|)^{1-N_{i}}.$$<PARAGRAPH_SEPARATOR>We neglect the factors $\\left|{C_{i}}\\right|$ in the first part of the product, since they cancel when we form likelihood ratios. In the limit, under suitable regularity conditions and when the cell sizes $|C_{i}|$ tend to zero, the log composite likelihood becomes<PARAGRAPH_SEPARATOR>$$\\sum_{u\\in\\mathbf{x}}\\log\\rho_{\\theta}(u)-\\int_{W}\\rho_{\\theta}(u)\\mathrm{d}u$$<PARAGRAPH_SEPARATOR>which coincides with the log likelihood function (42) in the case of a Poisson process. The corresponding estimating function is given by the derivative<PARAGRAPH_SEPARATOR>$$\\psi_{1}(\\theta)=\\sum_{u\\in\\mathbf{x}}{\\mathrm{d}}\\log\\rho_{\\theta}(u)/{\\mathrm{d}}\\theta-\\int_{W}({\\mathrm{d}}\\log\\rho_{\\theta}(u)/{\\mathrm{d}}\\theta)\\rho_{\\theta}(u){\\mathrm{d}}u.$$<PARAGRAPH_SEPARATOR>By the Campbell theorem (4), $\\psi_{1}(\\theta)=0$ is an unbiased estimating equation, and it can easily be solved using, e.g. spatstat, provided $\\rho_{\\theta}$ is on a log linear form. For Cox processes, as exemplified in example 11 below, the solution may only provide an estimate of one component of $\\theta$ , while the other component may be estimated by another method. Asymptotic properties of estimates obtained using (47) are derived in Schoenberg (2004) and Waagepetersen (2007).<PARAGRAPH_SEPARATOR>For a Gibbs point process, it is more natural to consider the Papangelou conditional intensity $\\lambda_{\\theta}$ . Hence, we redefine $p_{i}(\\theta){=}\\mathrm{P}_{\\theta}(N_{i}=1|\\mathbf{X}\\backslash C_{i}){\\approx}\\lambda_{\\theta}(u_{i},\\mathbf{X}\\backslash C_{i})|C_{i}|$ . In this case the limit of $\\begin{array}{r}{\\log\\prod_{i}(p_{i}(\\theta)/|C_{i}|)^{N_{i}}(1-p_{i}(\\theta))^{(1-N_{i})}}\\end{array}$ becomes<PARAGRAPH_SEPARATOR>$$\\sum_{u\\in\\mathbf{x}}\\log\\lambda_{\\theta}(u,\\mathbf{x})-\\int_{W}\\lambda_{\\theta}(u,\\mathbf{x})\\mathrm{d}u$$<PARAGRAPH_SEPARATOR>which is known as the log pseudo-likelihood function (Besag, 1977; Jensen & Møller, 1991). By the GNZ formula (31), the pseudo-score<PARAGRAPH_SEPARATOR>$$s(\\theta)=\\sum_{u\\in\\mathbf{x}}\\mathrm{d}\\log\\lambda_{\\theta}(u,\\mathbf{x})/\\mathrm{d}\\theta-\\int_{W}(\\mathrm{d}\\log\\lambda_{\\theta}(u,\\mathbf{x})/\\mathrm{d}\\theta)\\lambda_{\\theta}(u,\\mathbf{x})\\mathrm{d}u$$<PARAGRAPH_SEPARATOR>provides an unbiased estimating equation $s(\\theta)=0$ . This can be solved using spatstat if $\\lambda_{\\theta}$ is on a log linear form (Baddeley & Turner, 2000).<PARAGRAPH_SEPARATOR>Example 11 (Estimation of the intensity function for tropical rainforest trees). For both the log Gaussian Cox process model in example 1 and the inhomogeneous Thomas process model in example 3, the intensity function is of the form $\\exp(z(u)(\\widetilde{\\beta}_{1},\\beta_{2},\\beta_{3})^{\\top})$ , where $\\widetilde{\\boldsymbol{\\beta}}_{1}=\\sigma^{2}/2+\\boldsymbol{\\beta}_{1}$ for the log Gaussian Cox process and $\\tilde{\\boldsymbol{\\beta}}_{1}{=}\\log(\\kappa\\boldsymbol{\\alpha})$ for the inhomogeneous Thomas process. Using the estimating function (47) and spatstat, we obtain $(\\hat{\\tilde{\\beta}}_{1},\\hat{\\beta}_{2},\\hat{\\beta}_{3})=(-4.989,0.021$ , 5.842), where ${\\hat{\\boldsymbol{\\beta}}}_{2}$ and $\\hat{\\boldsymbol{\\beta}}_{3}$ are smaller than the posterior means obtained with the Bayesian approach in example 10. The estimate of course coincides with the MLE under the Poisson process with the same intensity function. Estimates of the clustering parameters, i.e. $(\\sigma^{2},\\alpha)$ respectively $(\\kappa,\\omega)$ , may be obtained using minimum contrast estimation, see example 13.<PARAGRAPH_SEPARATOR>Assuming $(\\hat{\\boldsymbol{\\beta}}_{2},\\hat{\\boldsymbol{\\beta}}_{3})$ is asymptotically normal (Waagepetersen, 2007), we obtain approximate $95\\%$ confidence intervals $[-0.018,0.061]$ and [0.885, 10.797] for $\\beta_{2}$ and $\\beta_{3}$ , respectively. Under the Poisson process model much more narrow approximate $95\\%$ confidence intervals [0.017, 0.026] and [5.340, 6.342] are obtained."
  },
  {
    "qid": "statistic-posneg-1923-0-0",
    "gold_answer": "FALSE. Explanation: The ratio R(X|β) = D(X,𝒳₁|β)/D(X,𝒳₀|β) measures the relative distance of X to 𝒳₁ compared to 𝒳₀. A larger R(X|β) means X is farther from 𝒳₁ relative to 𝒳₀, implying X is closer to 𝒳₀ and thus more representative of Π₀. However, the question incorrectly states the implication. A larger R(X|β) actually implies X is less representative of Π₁ (not necessarily more representative of Π₀, unless the distances are explicitly compared). The correct interpretation is that a larger R(X|β) suggests X is more likely from Π₀ because it is relatively closer to 𝒳₀.",
    "question": "In the distance-based classifier, the ratio R(X|β) = D(X,𝒳₁|β)/D(X,𝒳₀|β) is used to assign X to Π₀ if R(X|β) > r. Is it correct to say that a larger R(X|β) implies X is more representative of Π₀ than Π₁, given the definition of D(x,𝒳ₖ|β)?",
    "question_context": "Distance-Based Classification with Weighted Features\n\nThe paper discusses two approaches to constructing classifiers for assigning a new data vector X to one of two populations, Π₀ or Π₁. The first approach uses a logit model where the probability pr(Π₀|X) is modeled as a logistic function of α + βᵀX. The second approach is distance-based, using measures D(x,𝒳ₖ|β) to compute the distance from x to training samples 𝒳ₖ, and then forming a ratio R(X|β) to decide classification. Two examples of distance measures are provided, both weighted by a parameter vector β with non-negative components summing to 1. The classifier assigns X to Π₀ if R(X|β) exceeds a threshold r, and to Π₁ otherwise."
  },
  {
    "qid": "statistic-posneg-2086-13-0",
    "gold_answer": "FALSE. The formula is correctly derived as the arithmetic mean of the deformed images $f_{i}\\{\\Phi_{i}(t)\\}$, not the original images. The deformation maps $\\Phi_{i}$ align each image's landmarks to the average landmark configuration $\\overline{x}$ before averaging, ensuring the average image is a meaningful composite. This process accounts for spatial differences between individual images, making the result more than a simple pixel-wise arithmetic mean.",
    "question": "Is the formula for the average image $\\overline{{f}}\\left(t\\right)=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}\\{\\Phi_{i}(t)\\}$ correctly derived from the context, and does it imply that the average image is simply the arithmetic mean of the individual images?",
    "question_context": "Image Averaging and Merging via Landmark Deformation\n\n(a) by a piecewise linear mapping based on the Delaunay triangulation of landmarks for the first image and (b) the thin plate spline mapping (see Bookstein (1989) and Mardia et al. (1991)). <PARAGRAPH_SEPARATOR> If the $\\{y_{i}\\}$ differ only slightly from a linear transformation of the $\\{x_{i}\\},$ then both these deformations will be bijective (i.e. no folding), the usual situation in practice. <PARAGRAPH_SEPARATOR> We can use this approach in averaging images (e.g. of faces) by considering the average $\\vec{x}$ of landmark vectors $x_{i}$ and constructing the map $\\Phi_{i}\\colon\\overline{{x}}\\rightarrow x_{i}$ where $\\Phi$ is applied componentwise. Then the average image is <PARAGRAPH_SEPARATOR> $$ \\overline{{f}}\\left(t\\right)=\\frac{1}{n}\\sum_{i=1}^{n}f_{i}\\{\\Phi_{i}(t)\\} $$ <PARAGRAPH_SEPARATOR> which has landmarks ${\\overline{{x}}}.$ . This theme seems to underlie the so-called average faces (e.g. Craw and Cameron (1991)). Also, two objects can be merged by forming $\\overline{{x}}=\\alpha x_{1}+(1-\\alpha)x_{2}$ $\\scriptstyle0<\\alpha<1$ ,and then, using the same argument, the merged image is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\overline{{f}}_{\\alpha}(t)=\\alpha f_{1}\\{\\Phi_{1}(t)\\}+\\left(1-\\alpha\\right)f_{2}\\{\\Phi_{2}(t)\\}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> A famous example of such merging of faces is Margaret Thatcher and Michael Heseltine (Benson and Perrett,1991)."
  },
  {
    "qid": "statistic-posneg-2109-2-4",
    "gold_answer": "TRUE. The paper concludes the derivation process with the single case where Σβᵣ = n-1, represented by the sequence {1, 1, ..., 1, 0}. This marks the end of the iterative process for generating sequences of betas.",
    "question": "Is the final case in the derivation process when Σβᵣ = n-1 represented by the sequence {1, 1, ..., 1, 0}?",
    "question_context": "Lexicographical Ordering of Sequences in Paired Comparisons\n\nThe paper discusses the distribution of the number of circular triads in paired comparisons, focusing on the lexicographical ordering of sequences. The sequences are derived recursively, starting from the largest first element. The method involves deriving subsequent sequences from the previous ones using specific equations. The correctness of the sequences is checked by comparing the results obtained from different sections of the methodology."
  },
  {
    "qid": "statistic-posneg-1143-2-0",
    "gold_answer": "TRUE. The formula infx(m0(x)/ϕσ(x))=π0+(1−π0)(f(0)/ϕσ0(0))=π0 is derived under the assumption that f(0)=0. This simplifies the expression to π0, confirming the statement.",
    "question": "Is the statement 'For σ=σ0, the infimum of m0(x)/ϕσ(x) equals π0' correct based on the provided formulas and assumptions?",
    "question_context": "Consistency of the Maximizing π-Type Estimator in Mixture Models\n\nWe have proposed two estimators for the mixing proportion and investigated their performance using simulations and real data. The estimator that maximizes π performs better than the EM-type estimator when the two densities overlap or in multi-dimensional cases. Both estimators perform well when the two densities in the mixture are well separated, even if f is not unimodal or f is not smooth. <PARAGRAPH_SEPARATOR> For the maximizing π-type estimator, it is easy to make assumptions on f that lead to a consistent estimator for π. Let us assume first that the mixture m0=π0ϕσ0+(1−π0)f is known. We assume that f(0)=0 and there exists a sequence xn such that as n→∞,xn2→∞ and |f(xn)/ϕσ0(xn)|≤C for some constant C. Then, for σ>σ0, <PARAGRAPH_SEPARATOR> infx(m0(x)/ϕσ(x))=infx((ϕσ0(x)/ϕσ(x))[π0+(1−π0)(f(x)/ϕσ0(x))])=0, <PARAGRAPH_SEPARATOR> because ϕσ0(xn)/ϕσ(xn)=(σ/σ0)exp(−(xn2/2)(1/σ02−1/σ2))→0, as n→∞ since xn2→∞. For σ<σ0, <PARAGRAPH_SEPARATOR> infx(m0(x)/ϕσ(x))≤m0(0)/ϕσ(0)=(π0σ)/σ0<π0. <PARAGRAPH_SEPARATOR> For σ=σ0, <PARAGRAPH_SEPARATOR> infx(m0(x)/ϕσ(x))=π0+(1−π0)(f(0)/ϕσ0(0))=π0. <PARAGRAPH_SEPARATOR> Therefore, <PARAGRAPH_SEPARATOR> π̂=supσinfx(m(x)/ϕσ0)=π0. <PARAGRAPH_SEPARATOR> This suggests that a consistent estimator for m would also lead to a consistent estimator for π. There are many articles about the consistency of kernel density estimation. (See, for example, Hall and Marron (1995), Einmahl and Mason (2005), Hardle (1991).) <PARAGRAPH_SEPARATOR> We can see that the maximizing π-type estimator performs better than the EM-type estimator when the two densities overlap significantly. This is because, when μ=0, the area around zero is the best place to estimate the mixing proportion π if the mixed density f has a very small value at zero. The parameters (π,σ) obtained by the maximizing π-type estimator will make πϕσ(x) fit better around zero. <PARAGRAPH_SEPARATOR> For multi-dimensional cases, we reduce the problem to a one-dimensional one by looking at the distance to the center. There is a trade-off between the loss of information due to the reduction of dimensionality and the quality of the density estimation for high dimensions. The choice of bandwidth size is more sensitive in the multi-dimensional case than in the one-dimensional case. Although we try to select the bandwidth size which maximizes π automatically, the best way is to check the plots of mixture density with several bandwidths and pick the appropriate bandwidth size (see Cleveland (1993)). Also note that the performance of our proposed estimators depends on the number of observations. If we do not have enough observations, then the quality of kernel density estimation becomes poor and this makes the bias bigger. Therefore we need enough observations to estimate the mixing proportion accurately or we have to use a fully parametric model."
  },
  {
    "qid": "statistic-posneg-1516-1-1",
    "gold_answer": "FALSE. The multiple outside transmissions model is specifically designed to handle cases where not all individuals in a cluster are observed. It allows for unobserved cases by projecting transmission trees into connected sub-trees originating from an abstract outside infector, thus relaxing the stringent assumption of complete observation required by the base model.",
    "question": "Does the multiple outside transmissions model require all individuals in a cluster to be observed for accurate parameter estimation?",
    "question_context": "Branching Process Models for Infectious Disease Transmission Risk Factors\n\n![](images/d4325469391c936f620ffe90f3db4c7dc5e645dbd95d1f8b82e2f1691924a21d.jpg)  \nFigure 2. Unique transmission trees for an observed cluster of size three where two individuals are smear negative and one is smear positive. The first number indexes the generation number and the second is the order within that generation. There are six possible transmission trees when we consider the individuals, conditioned on their smear statuses, to be indistinguishable from one another (e.g., there is no observable difference between individuals B and C). Underneath the trees, we show the label(s) of the individuals that each tree can have. Labels in blue show the first path and labels in green show the second path for the shown tree, if we consider the individuals to be distinguishable from one another. There are 12 unique transmission trees when we uniquely label the individuals.\n\n<PARAGRAPH_SEPARATOR>\n\nto the observed data as a cluster $\\mathcal{C}$ to emphasize that no transmission paths are known. We know only the size, $|{\\mathcal{C}}|$ , of the transmission tree and the covariates of the individuals $\\mathbf{X}_{i}$ for $i=1,\\ldots,|\\mathcal{C}|$ within the transmission tree.\n\n<PARAGRAPH_SEPARATOR>\n\nLet $T$ be a transmission tree of size $|{\\mathcal{C}}|$ and $\\tau_{c}$ be the set of all such trees of size $|{\\mathcal{C}}|$ with the given covariates $X_{1},\\dots,X_{|{\\mathcal{C}}|}$ distributed among the nodes of the transmission tree. The likelihood for this cluster $({\\mathcal{L}}({\\mathcal{C}}))$ is the sum over the likelihoods of all permissible transmission trees of size $|{\\mathcal{C}}|$ ,\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\mathcal{L}(\\mathcal{C})=\\sum_{T\\in\\mathcal{T}_{\\mathcal{C}}}L_{T}(\\mathcal{C}),\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nwhere $L_{T}(\\mathcal{C})$ is the likelihood for a given transmission tree and is the product of geometric variables where $N_{i}$ is the number of individuals infected by individual $i,$\n\n<PARAGRAPH_SEPARATOR>\n\n$$\nL_{T}(\\mathcal{C})=\\prod_{i=1}^{|\\mathcal{C}|}\\mathcal{P}_{i}^{N_{i}}(1-\\mathcal{P}_{i}).\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nA limitation of the base model is that it requires a stringent assumption: complete observation of a cluster. For example, consider the transmission tree (left) in Figure 3. In this transmission tree, individuals A and E are not detected. With these individuals missing, it is impossible for our base model to reconstruct the projected transmission trees shown in the middle image, where edges between unobserved individuals have been erased. The image in the middle is not the only possible projection of the transmission of seven individuals into five; one could argue that a projection that also includes an edge between C and G is more accurate. Regardless of the projection chosen, our base model can only generate transmission trees that, for example, will connect individual B to the other observed individuals. As such, we know our estimates for any parameters will be biased as individual B’s infection should be independent from any of the observed individuals. We propose a solution to this issue with an approach we call the multiple outside transmissions model.\n\n<PARAGRAPH_SEPARATOR>\n\nAssuming that the clusters are mutually independent, the total likelihood for $M$ observed clusters is the product of the likelihoods of individual clusters. We can then estimate $\\beta$ as the maximum likelihood estimate (MLE), $\\hat{\\boldsymbol{\\beta}}$ , over the likelihood of the observed clusters,\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\hat{\\pmb{\\beta}}=\\arg\\operatorname*{max}_{\\pmb{\\beta}}\\prod_{m=1}^{M}\\mathcal{L}(C_{m}).\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nA feature of our model shown in Equation (2) is that it can be extended to multiple covariates through the logit function $p_{i}=\\mathrm{logit}^{-1}\\left(\\mathbf{X_{i}}{\\pmb\\beta}\\right)$ for a covariate matrix $\\mathbf{X}$ and a vector of covariates $\\beta$ . This formulation allows nested sets of parameters to be sequentially tested, for example, via a likelihood ratio test.\n\n<PARAGRAPH_SEPARATOR>\n\n# 2.2. The Multiple Outside Transmissions Model\n\n<PARAGRAPH_SEPARATOR>\n\nCompared to the base model, the multiple outside transmissions model allows for a more flexible set of transmission trees, and as a consequence, it does not require all individuals be observed in a cluster. The idea is that any transmission tree with unobserved cases can be projected into connected sub-trees whose primary infector is from some abstract individual outside the observed cluster. For example, in Figure 3 (middle) we can instead view the group of six observed individuals as three singletons and one transmission tree of size three, which are then “glued” together such that they originate from some abstract outside infection, which is portrayed in Figure 3 (right). In this new approximation, we have four infections $(B,C,D,G)$ resulting directly from the abstract outside infector, which is why we call this model the multiple outside transmissions model."
  },
  {
    "qid": "statistic-posneg-2049-2-2",
    "gold_answer": "TRUE. The context states that $\\lambda\\in\\mathcal{R}_{\\mathrm{plan}}$ (the column space of $X^{\\prime}$) if and only if $\\lambda^{\\prime}S_{X}^{\\prime}=\\lambda^{\\prime}$. This is a necessary and sufficient condition for estimability of $\\lambda^{\\prime}\\theta$, as it ensures the existence of a weight vector $w$ such that $\\lambda^{\\prime}=w^{\\prime}X$.",
    "question": "Does the condition $\\lambda^{\\prime}=\\lambda^{\\prime}S_{X}^{\\prime}$ imply that $\\lambda^{\\prime}\\theta$ is estimable only if $\\lambda^{\\prime}$ lies in the column space of $X^{\\prime}$?",
    "question_context": "Robust Assessment of Two-Treatment Higher-Order Crossover Designs\n\nIt follows that if many designs are under consideration then the design $D$ with breakdown number $\\boldsymbol{\\mathrm{max}}(m_{D})$ is more robust than the competing designs and should be preferred on grounds of robustness. When several designs have the same largest breakdown number $\\boldsymbol{\\mathrm{max}}(m_{D})$ then efficiency considerations should apply to these designs. Evidently a useful preliminary step is to aim to identify those designs that possess this largest breakdown number. <PARAGRAPH_SEPARATOR> Definition 4. Given a cross-over design $D$ , let $D_{\\mathrm{min}}$ denote the eventual design which remains after all s subjects drop-out after completing two periods. Then $D_{\\mathrm{min}}$ is termed the minimal design associated with $D$ . <PARAGRAPH_SEPARATOR> Assume without loss of generality that the ordering of the rows of $X$ is such that $X_{*}$ , the submatrix of $X$ consisting of the first 2s rows, corresponds to the first two periods of the design. Then $X_{*}$ can be written as <PARAGRAPH_SEPARATOR> $$ X_{*}=\\left[X_{\\operatorname*{min}}\\mathrm{~}0_{2s}^{*}\\right] $$ <PARAGRAPH_SEPARATOR> where $X_{\\mathrm{min}}$ is the minimal design matrix and $0_{2s}^{*}$ is a $2s\\times(p-2)$ matrix consisting of zero elements. Clearly the rows of $X_{*}$ form a subspace ${\\mathcal{R}}_{*}\\subseteq{\\mathcal{R}}_{\\mathrm{plan}}$ . Using an argument of Godolphin and Godolphin (2017), it can be shown that $\\dim(\\mathcal{R}_{*})=\\operatorname{rank}\\left(X_{\\operatorname*{min}}\\right)$ , consequently there are two possible cases to consider. If the minimal design matrix $X_{\\mathrm{min}}$ is connected, i.e. it has maximal rank $s+3$ , then the deficiency in the dimension $\\mathrm{dim}\\big(\\mathcal{R}_{*}\\big)$ compared to $\\dim\\left(\\mathcal{R}_{\\mathfrak{p l a n}}\\right)$ is due solely to the fact that no measurements are made in the final $p-2$ periods which are missing in the minimal design. This implies that no drop-out activity from the design $D$ which occurs in these final $p-2$ periods will result in a disconnected eventual design, i.e. $D$ is perpetually connected. On the other hand if $X_{\\mathrm{min}}$ is disconnected then there will be other eventual designs that include $X_{\\mathrm{min}}$ which are also disconnected. <PARAGRAPH_SEPARATOR> # 3.3. Choosing a perpetually connected design <PARAGRAPH_SEPARATOR> Clearly if a perpetually connected design exists then it should be considered seriously for selection. However, it sometimes happens that the set $\\mathcal{D}$ of perpetually connected designs which are available may contain several members. In these circumstances a choice of planned design from $\\mathcal{D}$ is sensible. Considerable attention has been given to design selection based on various optimality criteria which assume no drop-out will arise: see the useful reviews in Jones and Kenward (2015, Chapter 3), and Chow and Liu (2008, Chapters 9-10). <PARAGRAPH_SEPARATOR> In addition the authors suggest making use of a complementary method of choice between members of $\\mathcal{D}$ that does not appear to be available easily in many software packages. This procedure gives the explicit forms for the unbiased estimators of ${\\tau_{A}}-{\\tau_{B}}$ and $\\rho_{A}-\\rho_{B}$ as linear forms in the observations Y , for each competing member of $\\mathcal{D}$ The method is based on an established alternative test of estimability, described by Searle (1971, Section 5.4). Suppose that $\\lambda\\in{\\mathfrak{A}}$ , where $\\varLambda$ is the estimability space defined in (3). Then $\\lambda^{\\prime}\\theta$ is estimable so it follows from Proposition 1 that there is a $w$ of size $n\\times1$ , referred to as the weight vector, such that <PARAGRAPH_SEPARATOR> $$ \\lambda^{\\prime}=w^{\\prime}X. $$ <PARAGRAPH_SEPARATOR> Let $(X^{\\prime}X)^{-}$ be a generalized inverse of $X^{\\prime}X$ , i.e. $(X^{\\prime}X)^{-}$ satisfies $X^{\\prime}X(X^{\\prime}X)^{-}X^{\\prime}X=X^{\\prime}X$ . Then $P_{X}=X(X^{\\prime}X)^{-}X^{\\prime}$ is the orthogonal projection operator on the column space of $X$ and <PARAGRAPH_SEPARATOR> $$ P_{X}X=X\\Rightarrow X^{\\prime}=X^{\\prime}P_{X}=X^{\\prime}X(X^{\\prime}X)^{-}X^{\\prime} $$ <PARAGRAPH_SEPARATOR> after noting that $P_{X}$ is symmetric. Thus if $S_{X}$ is defined as $S_{X}=X^{\\prime}X(X^{\\prime}X)^{-}$ then $S_{X}X^{\\prime}=X^{\\prime}$ , i.e. $S_{X}$ is a projection operator on the column space of $X^{\\prime}$ , which is $\\mathcal{R}_{\\sf p l a n}$ . This implies that $\\lambda\\in\\mathcal{R}_{\\mathrm{plan}}$ if and only if $\\lambda^{\\prime}S_{X}^{\\prime}=\\lambda^{\\prime}$ . From this condition it follows that, <PARAGRAPH_SEPARATOR> whenever $\\lambda^{\\prime}\\theta$ is estimable <PARAGRAPH_SEPARATOR> $$ \\lambda^{\\prime}=\\lambda^{\\prime}S_{X}^{\\prime}=\\lambda^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}X=w^{\\prime}X, $$ <PARAGRAPH_SEPARATOR> taking account of (6), where the weight vector is given by <PARAGRAPH_SEPARATOR> $$ w^{\\prime}=\\lambda^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}. $$ <PARAGRAPH_SEPARATOR> Searle (1971, page 185) remarks that ‘‘the derivation of a vector $\\lambda$ satisfying $\\lambda^{\\prime}=w^{\\prime}X$ may not always be easy’’, however the improvement in methods for computing the expression (8) in the intervening years has probably made this comment somewhat pessimistic. In particular, it follows from (8) that expressions for the least-squares estimators of the estimable contrasts $\\tau_{A}-\\tau_{B}$ and $\\rho_{A}-\\rho_{B}$ are given by <PARAGRAPH_SEPARATOR> $$ \\widehat{\\tau_{A}}-\\widehat{\\tau_{B}}=w_{\\tau}^{\\prime}Y=\\lambda_{\\tau}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}Y\\mathrm{and}\\widehat{\\rho_{A}}-\\widehat{\\rho_{B}}=w_{\\rho}^{\\prime}Y=\\lambda_{\\rho}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}Y $$ <PARAGRAPH_SEPARATOR> respectˆivelyˆ, where $\\lambda_{\\tau}$ and $\\lambda_{\\rho}$ are the $(p+s+4)\\times1$ vectors <PARAGRAPH_SEPARATOR> $$ \\lambda_{\\tau}=[1-1000_{p+s}^{\\prime}]^{\\prime}\\mathrm{and}\\lambda_{\\rho}=[001-10_{p+s}^{\\prime}]^{\\prime}. $$ <PARAGRAPH_SEPARATOR> These estimators (9) are specified uniquely as weighted sums of the elements of $Y$ and are, of course, unbiased. The leastsquares estimators of $\\tau_{A}-\\tau_{B}$ and $\\rho_{A}-\\rho_{B}$ have sampling variances <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\mathsf{v a r}(\\widehat\\tau_{A}-\\widehat\\tau_{B})=w_{\\tau}^{\\prime}w_{\\tau}\\sigma^{2}\\quad\\mathrm{and}\\mathsf{v a r}(\\widehat\\rho_{A}-\\widehat\\rho_{B})=w_{\\rho}^{\\prime}w_{\\rho}\\sigma^{2},}\\end{array} $$ <PARAGRAPH_SEPARATOR> respectively.These results can be summarized as follows: <PARAGRAPH_SEPARATOR> Proposition 2. If the design $D$ is connected then the weighted coefficients of the least-squares estimators of the direct effects contrast and the carry-over effects contrast are given by $w_{\\tau}^{\\prime}=\\lambda_{\\tau}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}$ and $w_{\\rho}^{\\prime}=\\lambda_{\\rho}^{\\prime}(X^{\\prime}X)^{-}X^{\\prime}$ respectively, where $\\lambda_{\\tau}^{\\prime}$ , $\\lambda_{\\rho}^{\\prime}$ are given by (10). The least squares contrast estimators are given by (9) and their sampling variances are given by (11) <PARAGRAPH_SEPARATOR> It should be remarked that Proposition 2 also applies to an eventual cross-over design $D_{e}$ in the event of missing observations due to subject drop-out, provided that $D_{e}$ is connected. The proposition is therefore useful for establishing the weights $w$ for eventual designs that occur after subject drop-out from a perpetually connected design. Furthermore it is evident that a choice between competing perpetually connected designs in $\\mathcal{D}$ can be made by comparing the corresponding sampling variances for the direct effects var $\\dot{\\tau}(\\widehat{\\tau_{A}}-\\widehat{\\tau_{B}})$ , specified in (11), which give the usual measure of precision of the estimator in each case."
  },
  {
    "qid": "statistic-posneg-1760-2-0",
    "gold_answer": "TRUE. The table shows the relative lengths (RL) of the shortest prediction intervals compared to the standard ones, with values ranging from 0.84 to 0.89 and an average of 0.86. This means the shortest intervals are, on average, 14% (1 - 0.86) shorter than the standard intervals.",
    "question": "Is the statement 'The shortest prediction intervals are on average about 14% shorter than the standard prediction intervals' supported by the data in Table 3?",
    "question_context": "Comparison of Prediction Interval Lengths in Statistical Modeling\n\nThe paper compares the lengths of 95% standard and shortest prediction intervals, both plug-in and calibrated versions, at the first ten locations of a validation set. The results show that the shortest prediction intervals are on average about 14% shorter than the standard prediction intervals. The coverage probabilities of these intervals are also discussed, with the calibrated shortest prediction intervals having coverage close to the nominal 95%."
  },
  {
    "qid": "statistic-posneg-1121-0-1",
    "gold_answer": "FALSE. The equality $\\frac{1}{2}\\sum_{(i,j)\\in\\mathcal{E}}(f_{i}-f_{j})(g_{i}-g_{j})w_{i j}=f^{\\top}\\mathbf{L}_{\\mathsf{W}}\\mathbf{g}$ holds for any vectors $\\pmb f,\\pmb g\\in\\mathbb{R}^{D}$, as stated in the context. This is a general property of the Laplacian matrix and does not depend on specific choices of $\\pmb f$ and $\\pmb g$.",
    "question": "The equality $\\frac{1}{2}\\sum_{(i,j)\\in\\mathcal{E}}(f_{i}-f_{j})(g_{i}-g_{j})w_{i j}=f^{\\top}\\mathbf{L}_{\\mathsf{W}}\\mathbf{g}$ holds only for specific choices of $\\pmb f$ and $\\pmb g$ in $\\mathbb{R}^{D}$. Is this statement correct?",
    "question_context": "Graph Laplacian and Variable Relationships in Compositional Data Analysis\n\nIn this subsection we use the notation $(f_{1},\\dots,f_{D})\\in\\mathbb{R}^{D}$ and $(g_{1},\\dots,g_{D})\\in\\mathbb{R}^{D}$ for two sets of variables, in order to avoid any confusion with the compositional case. We define a graph as a fixed pair $(\\nu,\\mathbf{W})$ , where $\\nu:=\\{1,\\dots,D\\}$ denotes a set of indices, and $\\mathbf{W}=(w_{i j})_{1\\le i,j\\le D}\\in\\mathbb{R}^{D\\times D}$ is a symmetric matrix with zero diagonal and non-negative entries, corresponding to weights between indices. Graphs are useful to model the relation between variables $(f_{1},\\dots,f_{D})\\in\\mathbb{R}^{D}$ . The idea is that the bigger a weight $w_{i j}$ , the bigger the relationship between the two variables $f_{i}$ and $f_{j}$ is. Whenever $w_{i j}$ is zero there is no relation. <PARAGRAPH_SEPARATOR> The edge-set of a graph is defined as $\\mathcal{E}:=\\{(i,j)\\mid w_{i j}\\neq0\\}\\subset\\mathcal{V}^{2}$ . We write in the following $i\\sim j$ whenever $(i,j)\\in\\mathcal{E}$ We say that there exists a path from the vertex $i$ to the vertex $j$ if there are vertices $i_{1},\\ldots,i_{k}$ , with $i=i_{1},j=i_{k}$ , and $w_{i_{1}i_{2}}\\neq0$ $\\mathrm{~:~}0,w_{i_{2}i_{3}}\\neq0,\\dots,w_{i_{k-1}i_{k}}\\neq0$ . A subset of indices $\\{i_{1},\\dots,i_{k}\\}\\subset\\mathcal{V}$ is called connected if there is a path from each vertex in the subset to another vertex in the subset. If the subset is equal to $\\nu$ then the graph is said to be connected, otherwise we say it is disconnected. The set of vertices $\\nu$ can always be written as a union of its connected components $\\nu=\\cup_{m=1}^{M}\\nu_{m}$ , with disjoint sets $\\nu_{1},\\dots,\\nu_{M}$ , for any graph. Such a decomposition can be obtained by starting with one vertex, say $v_{1}=1$ , and looking for all other vertices connected with $v_{1}$ through a path to obtain $\\nu_{1}$ . Deleting $\\nu_{1}$ from $\\nu$ we can restart the procedure to get $\\nu_{2}$ , and so on. <PARAGRAPH_SEPARATOR> An important analytical tool for finite graphs is the so called Laplacian-matrix, defined as <PARAGRAPH_SEPARATOR> $$ \\mathbf{L}_{\\mathsf{W}}:=\\mathrm{diag}\\left(\\sum_{j=1}^{D}w_{1j},\\hdots,\\sum_{j=1}^{D}w_{D j}\\right)-\\mathbf{W}, $$ <PARAGRAPH_SEPARATOR> where diag is the diagonal matrix of the corresponding entries. The definition of $\\mathbf{L}_{\\mathbf{W}}$ is motivated by the following key equality <PARAGRAPH_SEPARATOR> $$ \\frac{1}{2}\\sum_{(i,j)\\in\\mathcal{E}}(f_{i}-f_{j})(g_{i}-g_{j})w_{i j}=f^{\\top}\\mathbf{L}_{\\mathsf{W}}\\mathbf{g}, $$ <PARAGRAPH_SEPARATOR> for any $\\pmb f,\\pmb g\\in\\mathbb{R}^{D}$ , see Merris [37]. <PARAGRAPH_SEPARATOR> The properties of $\\mathbf{L}_{\\mathbf{W}}$ have been analyzed extensively. In this paper we will need the following standard results in spectral graph theory, see Mohar [38] for a proof. <PARAGRAPH_SEPARATOR> Lemma 1. Assume that $\\mathbf{W}$ is symmetric with zero diagonal and non-negative entries, then:"
  },
  {
    "qid": "statistic-posneg-1355-4-1",
    "gold_answer": "TRUE. The penalty term in the SIC criterion is $\\frac{\\ln n}{2n}(m_{\\lambda}+1)$, where $m_{\\lambda}$ is defined as the total number of selected FPCs ($m_{\\lambda}=\\sum_{j:\\tilde{\\upsilon}_{\\lambda,j}\\neq0}L_{j,c p\\nu}$). This means the penalty increases with the number of selected FPCs, as confirmed by the context's discussion of Example 2, where more FPCs were selected, leading to a larger penalty term.",
    "question": "Does the SIC criterion formula $$ \\mathrm{SIC}(\\lambda)=\\ln\\left(\\frac{1}{2n}\\sum_{i=1}^{n}\\Bigl(Y_{i}-\\sum_{j=1}^{p}\\sum_{k=1}^{L_{j,c p v}}\\hat{\\xi}_{i,j k}\\hat{v}_{j k}\\Bigr)^{2}\\right)+\\frac{\\ln n}{2n}(m_{\\lambda}+1) $$ include a penalty term that increases with the number of selected functional principal components (FPCs)?",
    "question_context": "Robust Shrinkage Estimation and Selection for Functional Data\n\nTo show the robustness of our proposal, we compare the results under square loss function to those of our proposal which uses LAD loss. In our simulations, ‘‘L.’’ and ‘‘G.’’ denote the proposed functional LAD-Lasso method (FLL) in this paper and the method using least square loss function with group lasso penalty (FGL), respectively. <PARAGRAPH_SEPARATOR> Taking into account computation efficiency of CPV-SIC and its good performance in Example 1 as shown in Section 4.2, we adopt the CPV-SIC criterion to select the tuning parameters for both FLL and FGL. That is, both methods select $L_{j}^{\\prime}s$ by CPV criterion. Simulation results for other criteria of tuning parameter selection are similar and omitted for conciseness. <PARAGRAPH_SEPARATOR> Denoted by $\\mathbf{L}_{c p v}$ the L selected by CPV method. Given $\\mathbf{L}_{c p v}$ , SIC criterion for square loss function is defined as follows (He et al., 2002; Huang et al., 2014) <PARAGRAPH_SEPARATOR> $$ \\mathrm{SIC}(\\lambda)=\\ln\\left(\\frac{1}{2n}\\sum_{i=1}^{n}\\Bigl(Y_{i}-\\sum_{j=1}^{p}\\sum_{k=1}^{L_{j,c p v}}\\hat{\\xi}_{i,j k}\\hat{v}_{j k}\\Bigr)^{2}\\right)+\\frac{\\ln n}{2n}(m_{\\lambda}+1), $$ <PARAGRAPH_SEPARATOR> where $\\begin{array}{r}{m_{\\lambda}=\\sum_{j:\\tilde{\\upsilon}_{\\lambda,j}\\neq0}L_{j,c p\\nu}}\\end{array}$ with $\\tilde{\\boldsymbol{v}}_{\\lambda,j}=(\\hat{\\boldsymbol{v}}_{j k},1\\le k\\le L_{j,c p\\nu})^{T}$ <PARAGRAPH_SEPARATOR> We first c ompare the RMSE of FLL and FGL. By Table 7, the RMSEs of FGL are slightly smaller than FLL under normal distribution. However, as the distribution tail of error term becomes heavy, the FLL will have better performance in terms of RMSE. Now we turn to the variable selection of these two methods. Clearly, both methods can select the nonzero variables, but the FGL is opt to select more insignificant variables into the model, which results in much larger value in false positive (FP). Particularly, when the error is from Cauchy distribution, which is heavy tailed, we see from Table 7 that FLL is much better than FGL in terms of both RMSE and variable selection. It demonstrates FLL is insensitive to heavy-tailed error. Table 8 presents the simulation results for Example 2. With normal errors, FGL is slightly better than FLL. When the error follows $t_{3}$ whose tail is heavier than normal, FLL outperforms FGL in terms of RMSE and slightly worse than FGL in terms of FP. In case of Cauchy error, FGL cannot select nonzero predictors, and consequently RMSEs are much large. Note that the penalty term in (16) involves a term $m_{\\lambda}+1$ , where $m_{\\lambda}$ is the total number of FPCs selected. Compared with Example 1, more FPCs (about 10) are selected in Example 2. Thus compared to Example 1, the penalty term in Example 2 is much larger, and FGL tends to select less variables into the model. <PARAGRAPH_SEPARATOR> # 4.4. Real data"
  },
  {
    "qid": "statistic-posneg-869-0-2",
    "gold_answer": "TRUE. Under the null hypothesis $H_{0}: y_{T i} = y_{C i}$, $\\delta_{i j} = 0$ for all $i, j$, so $A = 0$ and thus $A/\\{m(N-m)\\}=0$.",
    "question": "Is the attributable proportion $A/\\{m(N-m)\\}$ equal to zero under the null hypothesis of no treatment effect?",
    "question_context": "Attributable Effects in Randomized Experiments Using Rank Sum Statistics\n\nThis section considers pivotal inference for attributable effects based on the Wilcoxon rank sum test and the Mann–Whitney $U$ -test. It is convenient to assume, at first, that the $N$ potential responses to control are not tied, $y_{C(N)}>...>y_{C(1)}$ , and to discuss adjustments for ties separately. Ties do not present technical problems, but it is easier to interpret the attributable effect when ties are either absent or rare. Write\n\n$$\n\\begin{array}{r}{\\tilde{W}_{i j}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if~}y_{T i}>y_{C j},}\\ {0}&{\\mathrm{otherwise},}\\end{array}\\right.\\quad W_{i j}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if~}y_{C i}>y_{C j},}\\ {0}&{\\mathrm{otherwise},}\\end{array}\\right.}\\end{array}\n$$\n\nso that $W_{i i}=0$ and $\\begin{array}{r}{q_{i}=1+\\sum_{j=1}^{N}W_{i j}}\\end{array}$ is the rank of $y_{C i}$ among the $N$ potential responses to control, $y_{C j}$ , for $j=1,\\ldots,N.$ . Write $\\delta_{i j}=\\tilde{W}_{i j}-W_{i j}$ . Since the treatment is assumed to have a nonnegative effect, $y_{T i}\\geqslant y_{C i}$ , it follows that $\\delta_{i j}\\geqslant0$ . Under the null hypothesis of no treatment effect, $H_{0}:y_{T i}=y_{C i}$ , for $i=1,\\ldots,N,$ it follows that $\\delta_{i j}=\\tilde{W}_{i j}-W_{i j}=0$ for all $i,j.$ If $\\delta_{i j}=1$ , then exposure of subject $i$ to the treatment would cause subject $i$ to have a higher response than subject $j$ would have under the control, whereas $i$ would have had a response no higher than subject $j$ had they both received the control. Although there are $N^{2}$ different $\\delta_{i j}$ , the maximum value of $\\textstyle\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\delta_{i j}$ is not $N^{2}$ but rather $N+N(N-1)/2$ because all $N$ of the $\\delta_{i i}$ could equal 1, but $1=W_{i j}+W_{j i}$ so that $1\\geqslant\\delta_{i j}+\\delta_{j i}$ for $i\\neq j.$ .\n\nNow $\\tilde{W}_{i j}$ is observed only if subject $i$ received the treatment and subject $j$ received the control, that is only if $Z_{i}=1$ and $Z_{i}=0$ , so that many $\\tilde{W}_{i j}$ are not observed, but the Mann–Whitney $U$ -statistic $\\begin{array}{r}{V=\\sum_{i=1}^{N}\\sum_{j=1}^{N^{'}}Z_{i}(1-Z_{j})\\tilde{W}_{i j}}\\end{array}$ is obser ved. As is well known, the Mann–Whitney statistic differs by a constant from Wilcoxon’s rank sum statistic. Under the null hypothesis of no treatment effect in a randomised experiment, $\\tilde{W}_{i j}=W_{i j}$ , so that then $V$ would equal $\\begin{array}{r}{\\sum_{i=1}^{N}\\sum_{j=1}^{N}Z_{i}(1-Z_{j})W_{i j}}\\end{array}$ which in turn equals $-m(m+1)/2+\\Sigma Z_{i}q_{i}$ which has the conventional null randomisation distribution of the Mann–Whitney statistic. If we write $\\begin{array}{r}{A=\\sum_{i=1}^{N}\\sum_{j=1}^{N}Z_{i}(1-Z_{j})\\delta_{i j}.}\\end{array}$ , it follows that $\\begin{array}{r}{V-A=\\sum_{i=1}^{N}\\sum_{j=1}^{N}Z_{i}(\\bar{1}-Z_{j})W_{i j}}\\end{array}$ is a pivot which, under the alternative hypothesis, has the null distribution of the Mann– Whitney statistic, with expectation $m(N-m)/2$ and variance $m(N-m)(N+1)/12$ . The quantity $A/\\{m(N-m)\\}$ is the proportion of the $m(N-m)$ possible comparisons of one of the $m$ treated subjects with one of the $N-m$ controls in which exposure to the treatment caused the treated subject to have a higher response than the control, where this treated subject would not have had a higher response than this control had they both been exposed to the control. Under the null hypothesis of no treatment effect, $H_{0}\\colon y_{T i}=y_{C i}$ , for $i=1,\\ldots,N.$ , the attributable proportion is $A/\\{m(N-m)\\}=0$ ."
  },
  {
    "qid": "statistic-posneg-2059-0-1",
    "gold_answer": "FALSE. While penalty terms (e.g., AIC/BIC) are generally necessary to prevent overfitting in most PPCA settings, Bouveyron et al. (2011) proved that regular maximum likelihood (without penalty) is consistent for dimension selection in the isotropic PPCA model. This is because the isotropic constraint ($\\sigma^2$ shared across dimensions) inherently regularizes the problem, making the likelihood criterion alone sufficient to identify the true dimensionality asymptotically. Thus, the penalty is not strictly required in this specific case.",
    "question": "The model selection criterion for PPCA, $d^{*}\\in\\underset{d\\in\\{1,\\ldots,p-1\\}}{\\mathrm{argmax}}\\{\\mathrm{log}p({\\mathbf X}|{\\mathbf W}_{\\mathrm{ML}},\\sigma_{\\mathrm{ML}},\\mathcal{M}_{d})-\\mathrm{pen}(d)\\}$, always requires a penalty term $\\mathrm{pen}(d)$ to avoid overfitting, even in the isotropic PPCA case. Is this statement correct?",
    "question_context": "Probabilistic PCA and Model Selection via Penalized Likelihood\n\nLet us assume that a centered independent and identically distributed (i.i.d.) sample $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}\\in$ $\\mathbb{R}^{p}$ is observed that we aim at projecting onto a $d$ -dimensional subspace while retaining as much variance as possible. All the observations are stored in the $n\\times p$ matrix $\\mathbf{X}{=}(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})^{T}$ . <PARAGRAPH_SEPARATOR> # 2.1 Probabilistic PCA <PARAGRAPH_SEPARATOR> The PPCA model $\\mathcal{M}_{d}$ assumes that, for all $i\\in\\{1,\\ldots,n\\}$ , each observation is driven by the following generative model <PARAGRAPH_SEPARATOR> $$ \\mathbf{x}_{i}=\\mathbf{W}\\mathbf{y}_{i}+\\boldsymbol\\varepsilon_{i}, $$ <PARAGRAPH_SEPARATOR> where $\\mathbf y_{i}\\sim\\mathcal N(0,\\mathbf I_{d})$ is a low-dimensional Gaussian latent vector, $\\mathbf{w}$ is a $p\\times d$ parameter matrix called the loading matrix, and $\\varepsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I}_{p})$ is a Gaussian noise term. <PARAGRAPH_SEPARATOR> This model is an instance of factor analysis and was first introduced by Lawley (1953). Tipping and Bishop (1999) then presented a thorough study of this model. In particular, expanding a result of Theobald (1975), they proved that this generative model is indeed equivalent to PCA in the sense that the PCs of $\\mathbf{X}$ can be retrieved using the maximum likelihood (ML) estimator ${\\bf{W}}_{\\mathrm{{ML}}}$ of W. More specifically, if A is the $p\\times d$ matrix of ordered principal eigenvectors of $\\mathbf{X}^{T}\\mathbf{X}$ and if $\\pmb{\\Lambda}$ is the $d\\times d$ diagonal matrix with corresponding eigenvalues, we have <PARAGRAPH_SEPARATOR> $$ \\mathbf{W}_{\\mathrm{ML}}=\\mathbf{A}(\\mathbf{A}-\\sigma^{2}\\mathbf{I}_{d})^{1/2}\\mathbf{R}, $$ <PARAGRAPH_SEPARATOR> where $\\mathbf{R}$ is an arbitrary orthogonal matrix. <PARAGRAPH_SEPARATOR> Under this sound probabilistic framework, dimension selection can be recast as a model selection problem, for which standard techniques are available. We review a few important ones in the next subsection. <PARAGRAPH_SEPARATOR> # 2.2 Model selection for PPCA <PARAGRAPH_SEPARATOR> The problem of finding an appropriate dimension can be seen as choosing a “best model\" within a family of models $({\\mathcal{M}}_{d})_{d\\in\\{1,...,p-1\\}}.$ . A first popular approach would be to use likelihood penalization, leading to the choice <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{d^{*}\\in\\underset{d\\in\\{1,\\ldots,p-1\\}}{\\mathrm{argmax}}\\{\\mathrm{log}p({\\mathbf X}|{\\mathbf W}_{\\mathrm{ML}},\\sigma_{\\mathrm{ML}},\\mathcal{M}_{d})-\\mathrm{pen}(d)\\},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where pen is a penalty which grows with $d$ . These methods include the popular Akaike information criterion (AIC, (Akaike, 1974)), the Bayesian information criterion (BIC, (Schwarz, 1978)), or other refined approaches (Bai & Ng, 2002). However, their merits are mainly asymptotic, and our main interest in this paper is to investigate non-asymptotic scenarios. While the penalty term is usually necessary to avoid selecting the largest model, under a constrained PPCA model, called isotropic PPCA, Bouveyron et al. (2011) proved that regular ML was surprisingly consistent. While the theoretical optimality of this method is also asymptotic, the fact that it directly maximizes a likelihood criterion which is not derived based on asymptotic considerations makes it of particular interest within the scope of this paper."
  },
  {
    "qid": "statistic-posneg-233-4-0",
    "gold_answer": "TRUE. The survivor function is indeed specified as a semiparametric model with a nonparametric baseline survivor function $S_0(t)$ and a parametric part involving basis functions $\\mathbb{B}_d(t)$ and coefficients $\\beta$. This allows for flexible modeling of time-varying effects while maintaining interpretability.",
    "question": "In the context of the paper, the survivor function $S(t|A_i) = S_0(t)\\exp\\{-A_i^T\\mathbb{B}_d(t)\\beta\\}$ is correctly specified as a semiparametric model where $S_0(t)$ is nonparametric and $\\mathbb{B}_d(t)$ is a set of basis functions. True or False?",
    "question_context": "Semiparametric Estimation of Time-Varying Coefficients in Survival Analysis\n\nThe paper discusses semiparametric estimation methods for time-varying coefficients in survival analysis, focusing on the use of kernel regression and basis functions to model the survivor function. Key formulas include the estimation of the survivor function $S(t|A_i)$ and the asymptotic properties of the estimators."
  },
  {
    "qid": "statistic-posneg-1952-0-0",
    "gold_answer": "FALSE. The stoichiometry matrix S should represent the net change in species counts per reaction. For the aphid growth model, the given matrix does not align with typical reaction schemes where each column corresponds to a reaction and each row to a species. The correct stoichiometry matrix should have columns representing reactions and rows representing species, which is not evident from the provided context.",
    "question": "In the aphid growth model, the stoichiometry matrix S is given as a 2x2 matrix with elements (1, -1) in the first row and (1, 0) in the second row. Does this matrix correctly represent the net change in species counts for the reactions described in the model?",
    "question_context": "Stochastic Kinetic Models: Aphid Growth and Lotka-Volterra Dynamics\n\nThe paper presents stochastic kinetic models for aphid growth and Lotka-Volterra dynamics, detailing the stoichiometry matrices, hazard functions, and associated differential equations for the Chemical Langevin Equation (CLE) and Linear Noise Approximation (LNA). The models describe the evolution of system states over time, incorporating reaction rates and stochastic noise."
  },
  {
    "qid": "statistic-posneg-612-0-0",
    "gold_answer": "TRUE. The condition number CN(m) is defined as the ratio of the maximum to the minimum eigenvalues of matrix A, and the selection of m is constrained by the condition that log10 CN(m) must be less than or equal to √n. This ensures numerical stability while allowing the largest possible number of weights.",
    "question": "Is the condition number CN(m) used to select the number of weights m in the density estimation model by ensuring that log10 CN(m) is bounded by √n?",
    "question_context": "Model Selection Criteria in Unimodal Density Estimation\n\nWe present a novel procedure for selecting the number of weights by examining the condition number of A. Since A is a normal matrix the condition number is evaluated by, $\\mathsf{C N}(m)=\\left\\lceil\\frac{\\lambda_{\\operatorname*{max}}(A)}{\\lambda_{\\operatorname*{min}}(A)}\\right\\rceil$ , where $\\lambda_{\\operatorname*{max}}(A)$ and $\\lambda_{\\mathrm{min}}(A)$ are the maximum and minimum eigenvalues of A respectively. We select $m$ by including the largest number of weights possible while still bounding $\\log_{10}\\mathrm{CN}(m)$ by $\\sqrt{n}$ . <PARAGRAPH_SEPARATOR> # 2.1.2. AIC and BIC <PARAGRAPH_SEPARATOR> We also present more traditional methods for selecting the number of weights, specifically, the Akaike information criterion (AIC) and Bayesian information criteria (BIC). In the realm of density estimation, these criteria are given by, <PARAGRAPH_SEPARATOR> $\\mathsf{A I C}(m)=-2\\sum_{i=1}^{n}\\log[\\widehat{f}_{m}(x_{i},\\widehat{\\omega}_{m})]+2(m-1)$, <PARAGRAPH_SEPARATOR> $\\mathrm{BIC}(m)=-2\\sum_{i=1}^{n}\\log[\\hat{f}_{m}(x_{i},\\widehat{\\omega}_{m})]+\\log(n)(m-1)$, <PARAGRAPH_SEPARATOR> where $m$ is the number of weights, $\\hat{f}_{m}$ is the estimated density using $m$ weights, $\\widehat{\\omega}_{m}$ is the vector of estimated weights, and $x_{i}$ for $i=1,\\ldots,n$ are the observations. The effective dimension of the weight vector $\\widehat{\\omega}_{m}$ is $m-1$ since the weights are constrained to sum to one. <PARAGRAPH_SEPARATOR> A variation of Theorem 3.1 in Babu et al. (2002) shows that $\\hat{f}_{m}$ will converge uniformly to $f$ for $2\\leq m\\leq(n/\\log n)$ as $n\\rightarrow\\infty$ , so we implement both the AIC and BIC methods by estimating the density with $m$ weights ranging from 2 to $\\lceil n/\\log n\\rceil$ . The AIC or BIC is calculated for each fit, then the density estimate with the lowest AIC or BIC is selected as the best estimate. <PARAGRAPH_SEPARATOR> # 2.2. Comparison of criterion for selecting the number of weights <PARAGRAPH_SEPARATOR> We compare these three methods of selecting $m$ by performing a small simulation study. We generate data from a mixture of Beta densities with a fixed vector of weights, which follows the required format of our density estimation model. We then generate density estimates using the AIC, BIC, and CN criterion to select m. Our goal is to find the method which provides the best density estimate in terms of a low root mean integrated squared error (RMISE) and selects the true number of weights of the Beta mixture. The RMISE is approximated as follows: <PARAGRAPH_SEPARATOR> $\\mathrm{RMISE}=\\mathrm{E}\\left[\\Vert\\hat{f}_{\\widehat{m}}-f\\Vert_{2}\\right]\\approx\\frac{1}{N}\\sum_{\\ell=1}^{N}\\sqrt{d\\sum_{j=1}^{J}\\left[\\hat{f}_{\\widehat{m}}^{(\\ell)}(x_{j})-f(x_{j})\\right]^{2}}$, <PARAGRAPH_SEPARATOR> where $J=100,\\operatorname*{Pr}[x_{1}\\leq X\\leq x_{J}]=0.999$ , and $d=(x_{j}-x_{j-1})=\\frac{x_{J}-x_{1}}{J-1}$ for all $j\\geq2$ . In the above expression, $\\hat{f}_{\\widehat{m}}^{(\\ell)}$ denotes the estimated density at the ℓth sample for $\\ell=1,\\ldots,N$ with $\\widehat{m}$ chosen by one of the three criteria. <PARAGRAPH_SEPARATOR> We examine a symmetric and right-skewed Beta mixture each with 7 weights (i.e. $m=7$ ). The weight vectors are fixed to be $\\omega_{1}~=~\\{0.05,0.1,0.2,0.3,0.2,0.1,0.05\\}$ for the symmetric distribution, and $\\omega_{2}=\\{0.1,0.4,0.25,0.15,$ 0.05, 0.03, 0.02} for the right-skewed distribution. We generate $N~=~1000$ Monte Carlo (MC) samples of size $n=$ 15, 30, 100, and 500 for each Beta mixture. Table 1 displays the MC estimated RMISE values as well as the average estimate of m across the MC samples. Statistically significant lower RMISE values, based on a paired $t$ -test, are in bold. <PARAGRAPH_SEPARATOR> The AIC method has the lowest average RMISE in most situations. Unfortunately, none of the methods have average number of weight estimates that are close to 7. The CN method appears to select the number of weights at around $\\sqrt{n}$ , which is much larger than 7 for the cases of $n=100$ and 500. Both the AIC and BIC underestimate $m$ for all sample sizes with the AIC having slightly larger estimates. Overall it appears the CN criterion is a better option for samples with fewer observations while the AIC performs the best for larger samples of size 100 and 500."
  },
  {
    "qid": "statistic-posneg-1644-0-1",
    "gold_answer": "FALSE. The variance $\\sigma_{s}^{2}$ is actually modeled as an exponential function of store characteristics and non-observable factors: $$\\sigma_{s}^{2}=\\exp{(\\lambda^{\\top}\\phi_{s}+\\varepsilon_{s})}.$$ This nonlinear transformation ensures that the variance is always positive, which is necessary for a valid Gaussian distribution, but it does not represent a linear relationship between store characteristics and variance.",
    "question": "The variance of the Gaussian distribution for store utilities, $\\sigma_{s}^{2}$, is modeled as a linear function of store characteristics $\\phi_{s}$ and non-observable characteristics $\\varepsilon_{s}$. Is this formulation consistent with the assumption that store attraction is influenced by both observable and unobservable factors?",
    "question_context": "Bayesian Spatial Interaction Model for Competitive Facility Location\n\nThe paper introduces a Bayesian spatial interaction model (BSIM) to analyze customer demand and optimize facility locations. The model uses truncated Gaussian distributions to represent customer utilities for visiting stores, incorporating socio-demographic characteristics and spatial distances. The optimal facility location problem is formulated as an integer nonlinear programming problem with budget constraints, aiming to maximize revenue or market share."
  },
  {
    "qid": "statistic-posneg-2003-0-1",
    "gold_answer": "FALSE. The inequality $\\|\\pmb{\\zeta}\\|_{\\gamma}\\leq\\|\\pmb{\\zeta}\\|_{1}$ holds only for $\\gamma \\geq 1$, but the second part $\\|\\pmb{\\zeta}\\|_{1}\\leq\\kappa^{1/2}\\|\\pmb{\\zeta}\\|_{2}$ is a specific property of the $L_1$ and $L_2$ norms and does not generalize to all $\\gamma \\geq 1$. The correct inequality for $\\gamma \\geq 1$ is $\\|\\pmb{\\zeta}\\|_{\\gamma} \\leq \\kappa^{1/\\gamma - 1/2} \\|\\pmb{\\zeta}\\|_{2}$ under certain conditions.",
    "question": "Does the inequality $\\|\\pmb{\\zeta}\\|_{\\gamma}\\leq\\|\\pmb{\\zeta}\\|_{1}\\leq\\kappa^{1/2}\\|\\pmb{\\zeta}\\|_{2}$ hold for any vector $\\pmb{\\zeta}$ of dimension $\\kappa$ and any $\\gamma \\geq 1$?",
    "question_context": "Consistency and Variable Selection in Quantile Varying Coefficient Models\n\nThe paper discusses the consistency and variable selection properties of penalized estimators in quantile varying coefficient models. It presents several lemmas and theorems to establish the asymptotic properties of the estimators, including their convergence rates and the conditions under which certain coefficients are set to zero with probability approaching one. The mathematical derivations involve high-dimensional probability bounds and optimization theory."
  },
  {
    "qid": "statistic-posneg-970-1-0",
    "gold_answer": "FALSE. While the condition $\\mathrm{max}_{j\\in S^{c}}|\\check{z}_{j}|<1$ (strict dual feasibility) is necessary for ensuring the uniqueness of the Lasso solution and correct support recovery, it is not sufficient on its own. The PDW method also requires the sign consistency condition $\\check{z}_{S}=\\mathrm{sign}(\\pmb{\\beta}_{1}^{o})$ to hold, ensuring that the signs of the estimated non-zero coefficients match the true signs. Additionally, the invertibility of $\\ddot{\\ell}_{S S}(\\pmb{\\beta}_{S},\\pmb{0})$ is required for the uniqueness of the oracle estimator $\\check{\\pmb{\\beta}}_{S}$. Thus, multiple conditions must be satisfied simultaneously for the PDW argument to guarantee support recovery.",
    "question": "In the Primal-Dual Witness (PDW) method for Lasso support recovery, the condition $\\mathrm{max}_{j\\in S^{c}}|\\check{z}_{j}|<1$ is sufficient to guarantee that the Lasso solution is unique and recovers the true support $S$. True or False?",
    "question_context": "Lasso Support Recovery via Primal-Dual Witness Method\n\nWe now outline the main steps of the PDW, which we will use to establish the support recovery property for the program (3). Define the active set $S=S(\\bar{\\pmb\\beta}_{0})=\\{j:\\beta_{j}^{0}\\neq0\\}$ , and its supplementary $S^{c}=\\{j:{\\bar{\\beta}}_{j}^{0}=0\\}$ . Without loss of generality, we assume that the last $p-s_{0}$ components of ${\\dot{\\boldsymbol{\\beta}}}^{0}$ are 0, so we can write ${\\pmb\\beta}^{0}=(({\\pmb\\beta}_{1}^{0})^{\\top},{\\pmb0}^{\\top})^{\\top}$ . A vector $z\\in\\mathbb{R}^{p}$ is a subgradient for the $\\ell_{1}$ -norm evaluated at $\\beta$ , denoted $z\\in\\partial\\|\\beta\\|_{1}$ , if the elements satisfy the following relation: $z_{j}=\\mathrm{sign}(\\beta_{j})$ if $\\beta_{j}\\neq0$ and $z_{j}\\in[-1,1]$ otherwise, where $\\mathrm{sign}(\\beta_{j})=1$ , 0, or $-1$ if $\\beta_{j}>0,=0$ or $<0$ , respectively. <PARAGRAPH_SEPARATOR> The key steps of the PDW argument are as follows. <PARAGRAPH_SEPARATOR> Step 1: Define a biased oracle estimator as $\\check{\\pmb\\beta}=(\\check{\\pmb\\beta}_{S},\\mathbf0)$ , where <PARAGRAPH_SEPARATOR> $$ \\check{\\pmb{\\beta}}_{S}=\\operatorname*{min}_{\\pmb{\\beta}_{S}\\in\\mathbb{R}^{S}}\\{\\ell\\left(\\pmb{\\beta}_{S},\\mathbf{0}\\right)+\\lambda\\|\\pmb{\\beta}_{S}\\|_{1}\\}. $$ <PARAGRAPH_SEPARATOR> Here we impose the additional constraint such that $S(\\check{\\pmb\\beta})\\subseteq S({\\pmb\\beta}^{0})=S$ The solution to this restricted convex program (5) is guaranteed to be unique under the invertibility condition on $\\ddot{\\ell}_{S S}(\\pmb{\\beta}_{S},\\pmb{0})$ . Note that here $\\check{\\boldsymbol{\\beta}}$ is just a theoretical construction assuming the true sparsity structure were known, but not a real estimator. <PARAGRAPH_SEPARATOR> Step 2: We choose $\\check{z}_{S}\\in\\mathbb{R}^{S}$ as an element of the subdifferential of the $\\left\\Vert\\cdot\\right\\Vert_{1}$ norm evaluated at $\\check{\\boldsymbol{\\beta}}_{S}$ . By definition of subgradient, we see that $\\|\\check{z}_{S}\\|_{\\infty}\\leq1$ . <PARAGRAPH_SEPARATOR> Step 3: We solve for a vector $\\check{z}_{S^{c}}\\in\\mathbb{R}^{p-s_{0}}$ to satisfy the zero subgradient condition <PARAGRAPH_SEPARATOR> $$ \\dot{\\ell}(\\check{\\pmb{\\beta}})+\\lambda\\check{z}=0, $$ <PARAGRAPH_SEPARATOR> where $\\check{z}=(\\check{z}_{S},\\check{z}_{S^{c}})$ . Then we check whether or not the dual feasibility condition $|\\check{z}_{j}|\\le1$ for all $j\\in S^{c}$ is satisfied. To ensure uniqueness, we need to check strict dual feasibility of $\\check{z}_{S}$ , i.e., $\\mathrm{max}_{j\\in S^{c}}|\\check{z}_{j}|<1$ . <PARAGRAPH_SEPARATOR> Step 4: We check whether the sign consistency condition $\\check{z}_{S}=\\mathrm{sign}(\\pmb{\\beta}_{1}^{o})$ is satisfied. <PARAGRAPH_SEPARATOR> Note that, in high dimensions, the Lasso program (3) is not guaranteed to be strictly convex, so there may be multiple solutions generated by (3). Note that the justification of the PDW argument mainly relies on some basic knowledge of convex optimization such as Kuhn–Tucker conditions, as shown in Lemma 5 in the Appendix."
  },
  {
    "qid": "statistic-posneg-621-1-0",
    "gold_answer": "FALSE. While the text states that the range is 'always narrower than similar ranges calculated from Behrens's solution,' this is not necessarily verifiable from the given formula alone. The comparison depends on the specific terms and coefficients in both solutions, which are not fully detailed here. The claim may be contextually true for the cases considered by the author, but it is not universally provable without explicit comparative analysis of both methods' full expansions.",
    "question": "Is the claim that the proposed solution's interval estimate for $\\pmb{\\eta}$ is always narrower than Behrens's solution correct based on the provided formula?",
    "question_context": "Generalization of Student's Problem with Multiple Population Variances\n\nI have, myself, quite definite views on these questions (particularly on the usage of the word ‘fiducial’) but do not feel that I need express them at any great length here. I disagree with Fisher, but this divergence of opinion must already have become apparent in the way I have defined the feld within which I make my probability inferences about $\\pmb{\\eta}$ It appears to me to be quite artificial to restrict our view to one which, even in a limited sense, fixes $\\pmb{\\vartheta}_{\\pmb{\\mathscr{i}}}^{\\pmb{\\mathscr{2}}}$ . It is true that, in the two-sample problem, we have to draw our inferences from the unique pair of samples observed, or, more precisely, from the statistics $\\overline{{x}}_{1},\\overline{{x}}_{2},s_{1}^{2}$ &nd $\\pmb{\\vartheta_{2}^{2}}$ which they provide. These statistics are our only data for the purpose of making inferences, but we add something to these data in the interpretation when we regard the samples as being drawn randomly from hypothetical normal populations. Once having embarked on this method of interpretation, we should stick to it consistently throughout. The sampling variations of $\\pmb{\\vartheta}_{\\pmb{\\mathscr{i}}}^{\\pmb{\\mathscr{i}}}$ should be taken into account only by & direct use of the probability distributions as given by our equation (l) and not by any inversion such as is involved in Fisher's conception of the fiducial distribution of $\\sigma_{i}^{2}$ .As we have seen, it is quite possible to make probability statements about the difference between the population means without making any reference whatever either to inverse probability or to fiducial distributions. <PARAGRAPH_SEPARATOR> The distinction between the procedure which Fisher advocates and one which averages overthe $\\mathbf{s^{2}}$ distributions has, of course, been stressed by most of the writers who have contributed papers on the subject, from whatever viewpoint (e.g. Bartlett, 1936, p. 566, and Yates, 1939.) What has been lacking hitherto, however, is a solution, analogous to Gosset's single sample solution, which makes complete use of the information contained in the data provided. Bartlett indicated one particular way in which probability inferences about the difference between two population means might be made, but was careful to point out that the problem of making the best possible inferences (in the theoretical sense of utilizing all the information in the data to its full extent) was still an open one. There has indeed been some doubt expressed whether a fully satisfactory solution from this point of view existed at all. I believe, however, that the one I advance above in equation (1l). and develop in equation (2l), meets all the requirements that one can reasonably expect. <PARAGRAPH_SEPARATOR> Whatever conclusion the reader may come to on these matters, however, he will probably wish to know how, in the numerical details, this solution will differ from that of Behrens. This will be more easily seen when some tables become available, but fortunately certain comparisons can already be made. For Fisher (194l, p. 155) has provided a scries expansion of the Behrens solution. In our notation, and with $\\pmb{k}=\\mathbf{2}$ ,this may be written, to order $1/f_{i}$ a8follow8: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathbf{\\Phi}^{\\star\\prime}}\\ &{h(\\pmb{\\mathscr{s}}^{\\bot})=\\xi\\sqrt{(\\lambda_{1}\\mathscr{s}_{1}^{\\bot}+\\lambda_{2}\\mathscr{s}_{2}^{\\bot})}\\left[1+\\frac{(1+\\xi^{\\mathbf{2}})}{4}\\frac{\\bigl(\\frac{\\lambda_{1}^{\\mathfrak{s}}\\mathscr{s}_{1}^{\\bot}}{f_{1}}+\\frac{\\lambda_{2}^{\\mathfrak{s}}\\mathscr{s}_{2}^{\\bot}}{f_{1}}\\bigr)}{(\\lambda_{1}\\mathscr{s}_{1}^{\\bot}+\\lambda_{2}\\mathscr{s}_{2}^{\\bot})^{\\mathfrak{s}}}+\\left(\\frac{1}{f_{1}}+\\frac{1}{f_{2}}\\right)\\frac{\\lambda_{1}\\lambda_{2}\\mathscr{s}_{1}^{\\bot}\\mathscr{s}_{2}^{\\mathbf{2}}}{(\\lambda_{1}\\mathscr{s}_{1}^{\\bot}+\\lambda_{2}\\mathscr{s}_{1}^{\\bot})^{\\mathfrak{s}}}\\right].}\\end{array} $$ <PARAGRAPH_SEPARATOR> Even to this order, this differs from our equation (21) in the inclusion of an extra term. In other words, although the two solutions are the same when samples are large enough to adopt the large-sample normal approximation, they differ immediately we take into account the first corrective term, i.e. they differ as soon a8 we begin to attach any importance to 'Studentization'. <PARAGRAPH_SEPARATOR> 6. $\\scriptstyle A n$ interval estimate for $\\pmb{\\eta}$ .We have shown in $\\S\\S2$ and 3 how to calculate a value $h(\\vartheta_{1}^{\\Omega},\\vartheta_{\\Im}^{\\Omega},...,\\vartheta_{k}^{\\Omega},P)$ , depending on the observed variances $\\vartheta_{1}^{2},\\vartheta_{8}^{8},\\ldots,\\vartheta_{k}^{8}$ such that the probability i8 $P$ that $(y-\\eta)<h(s_{1}^{\\tt Q},s_{2}^{\\tt Q},\\ldots,s_{k}^{\\tt Q},P)$ . This provides a method of testing the consistency of an observed $\\pmb{y}$ with a prescribed value $\\pmb{\\eta}$ <PARAGRAPH_SEPARATOR> When the question is not whether any particular given $\\pmb{\\eta}$ is contradicted by the data, but rather one of estimating $\\pmb{\\eta}$ and at the same time of providing & measure of the uncertainty of the estimate, the further step required is immediate. For, as in the case of a single sample, the order of the words in our probability statement can be changed so that it becomes--the probability is $\\mathcal{P}$ that $\\pmb{\\eta}$ is greater than $\\{y-h(s_{1}^{\\tt Q},s_{\\tt B}^{\\tt Q},s_{k}^{\\tt Q},P)\\}$ . An interval estimate for $\\pmb{\\eta}$ is then obtained by taking two levels $P_{1}$ &nd $P_{8}$ for $P$ . Thus the probability is $(P_{1}-P_{2})$ that $\\pmb{\\eta}$ lies between $\\{y-h(\\vartheta_{1}^{\\tt Q},\\vartheta_{\\tt A}^{\\tt B},\\ldots,\\vartheta_{k}^{\\tt Q},P_{1})\\}$ &nd $\\{y-h(s_{1}^{\\tt S},s_{2}^{\\tt S},...,s_{k}^{\\tt S},P_{2})\\}$ <PARAGRAPH_SEPARATOR> If $P_{\\mathfrak{s}}=(1-P_{1})$ the range will be symmetrically placed about $\\pmb{y}$ . Thus, for example, if $P_{1}=0{\\cdot}85$ and $P_{\\perp}=0{\\cdot}05$ , the chance will be $90\\%$ that $\\pmb{\\eta}$ lies within the range <PARAGRAPH_SEPARATOR> $$ y\\pm1\\cdot6449\\sqrt{(\\scriptstyle\\sum\\lambda_{i}\\vartheta_{1}^{2})}\\left[1+\\frac{1+(1\\cdot6449)^{2}}{4}\\frac{\\left(\\scriptstyle\\sum\\frac{\\lambda_{i}^{2}\\vartheta_{4}^{4}}{f_{i}}\\right)}{(\\scriptstyle\\sum\\lambda_{i}\\vartheta_{4}^{2})^{\\mathfrak{L}}}+\\mathrm{etc.}\\right]. $$ <PARAGRAPH_SEPARATOR> It may be noted, incidentally, that this range is always narrower than similar ranges calculated from Behrens's solution."
  },
  {
    "qid": "statistic-posneg-588-0-1",
    "gold_answer": "FALSE. While the method of regarding each response value as missing in turn and estimating the residual mean square can be more robust in some cases, it does not guarantee higher power in all scenarios. The power of any detection method depends on the specific nature of the spurious values and the underlying model assumptions. The paper mentions that Anscombe's method may have diminished power when a spurious value is included in the model fitting, but the alternative method's performance is context-dependent and not universally superior.",
    "question": "Does the method of regarding each response value as missing in turn and estimating the residual mean square guarantee higher power in detecting spurious values compared to Anscombe's method?",
    "question_context": "Detection of Spurious Values in Unreplicated Factorial Experiments\n\nA strong case for fully replicated experiments can thus be made when there is prior evidence of frequent spurious values. In practice, however, experimenters are reluctant to spend much time in replication, although the statistician should always urge partial duplication. At the very least the first treatment combination should be repeated at the end of the experiment. <PARAGRAPH_SEPARATOR> The present paper is largely concerned with unreplicated experiments in which the residual variance is estimated from the pooled effects of high-order interactions. The significance of main-effects and low-order interactions is usually tested against this residual variance, but in some cases the experimenter may have a well-founded prior estimate of the run-to-run variance, ${\\pmb\\sigma}^{2}$ <PARAGRAPH_SEPARATOR> A number of methods for identifying spurious values in this situation were proposed at a meeting of the American Statistical Association in December 1959, and were subsequently published in Technometrics. Anscombe (1960) suggested tests based on the size of the largest individual residual, $|z_{M}|$ .Forknown $\\pmb{\\sigma}_{i}$ heproposed rejection of the corresponding response value $y_{M}$ when <PARAGRAPH_SEPARATOR> $$ \\{z_{M}\\vert>C\\sigma $$ <PARAGRAPH_SEPARATOR> and calculated the “insurance premium\" involved for various values of the multiplying factor C. He also extended the analysis to the case when only an estimate of $\\sigma_{:}$ based on a limited number of degrees of freedom, is available. Daniel (1960) studied the pattern of the residuals and also devised a significance test based on the size of the largest residual. He also pointed out that since a bad value enters into every factorial contrast (see, for example, Table 1), a half-Normal plot of the ordered contrasts would have too few contrasts near zero and hence that such a graph would constitute a useful detective device. A general procedure for calculating critical values of the maximum normed residual has recently been given by Stefansky (1972). <PARAGRAPH_SEPARATOR> The drawback of these tests based on the largest residual is that its identity and size depend on a model which has been fitted with a possible spurious value included. Consequently, for a given risk of false detection, the power of the test at revealing a truly spurious value is diminished. <PARAGRAPH_SEPARATOR> The method we have adopted, therefore, is to regard each response value in turn as a missing value and to estimate the treatment effects and the residual mean square for the assumed model from the remaining data. Then if a rogue is present in the set of response values, the analysis carried out when this value is currently regarded as missing will result in a much lower residual mean square. Hence the method consists of scanning the list of residual mean squares, identifying the lowest level and testing whether it shows a statistically significant reduction from the residual mean square obtained with all the given data."
  },
  {
    "qid": "statistic-posneg-1113-1-0",
    "gold_answer": "TRUE. When R̄ = 0, the formula for g reduces to g = (nθ(θ+2) + p - 1)/(nθ + p - 1), and f = (nθ + p - 1)²/(nθ(θ+2) + p - 1). Under this condition, the approximation simplifies to a chi-squared distribution, making it exact. This is because the mixture of chi-squared distributions for Y1 collapses to a single chi-squared distribution when R̄ = 0, aligning perfectly with the approximation's form.",
    "question": "Is the approximation U ≈ g(χ²_f / χ²_{n-p+1}) exact when R̄ = 0, as implied by the formulas for g and f?",
    "question_context": "Approximation to the Distribution of U and R Using Chi-Squared Mixtures\n\nThe paper discusses an approximation to the distribution of U, which is distributed as a ratio Y1/Y2, where Y1 is a mixture of chi-squared distributions and Y2 is independently distributed as χ²_{n-p+1}. The approximation suggested is U ≈ g(χ²_f / χ²_{n-p+1}), where g and f are constants determined by equating the first two moments of gχ²_f to those of Y1. The values of g and f are given by specific formulas involving n, θ, and p. The approximation is validated through comparisons with exact values and Fisher's z-transform, showing better performance in the right-hand tail of the distribution."
  },
  {
    "qid": "statistic-posneg-165-1-2",
    "gold_answer": "FALSE. The efficient score for length-biased data involves a different transformation of the density $g$ and uses $\\tilde{g}=u g(u)/\\int S(u)d(u)$ instead of $S(u)/\\int S(u)d(u)$. The score operator and the form of $R a(t)$ are adjusted accordingly, leading to a different efficient score expression.",
    "question": "The efficient score for $\\theta$ in the length-biased data setting is identical to that in the standard accelerated failure time model. Is this statement true?",
    "question_context": "Efficient Score Estimation in Semiparametric Accelerated Failure Time Models\n\nThe paper discusses efficient score estimation in semiparametric accelerated failure time models, focusing on the derivation of efficient scores and tangent spaces. The model is defined by the distribution $P_{\theta,g,h}^{*}$ with density $\\frac{d P_{\theta,g,h}^{*}}{d\\nu}(t,z)=e^{(\\theta-\\theta_{0})^{\\prime}z}g\\{e^{(\\theta-\\theta_{0})^{\\prime}z}t\\}h(z)$, where $\\theta_{0}$ is the true parameter value. The efficient score for $\\theta$ is derived using orthogonal projections onto tangent spaces for $S$ and $h$."
  },
  {
    "qid": "statistic-posneg-231-1-0",
    "gold_answer": "FALSE. While the paper uses $w=0.5$ as a default for equal contribution, it explicitly states that alternative weighting procedures are explored in Appendix C, implying context-specific adjustments may be optimal. The normalization by $\\operatorname*{max}_{k}$ terms also means the score is not a simple average but a relative performance metric scaled by pathway-specific maxima.",
    "question": "The combined pathway score $R_{\\mathrm{comb}}^{2}|_{k}$ is calculated by averaging the normalized $R^{2}$ values from the gene and metabolite models, with equal weights ($w=0.5$) always being the optimal choice for balancing contributions from both data sources.",
    "question_context": "Pathway Activity Scoring via Combined Gene and Metabolite Models\n\nPreliminaries: Center and scale the data. <PARAGRAPH_SEPARATOR> (a) Select active genes in pathway $k$ . (b) Regress the metabolites in pathway $k$ on the active genes under regularization. (c) Calculate a score (indicating level of activity) for pathway $k$ . <PARAGRAPH_SEPARATOR> The procedure for selection within pathways is displayed in Algorithm 1. In step (a) we select active genes in each pathway by comparing the null $(m_{0}^{i}$ , not active) and non-null $(m_{1}^{i}$ , active) gene models. For a given pathway $k$ and gene $i:a_{i k}=1$ , we have: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{m_{0}^{i}:g_{i t}=\\alpha_{i}+\\varepsilon_{i t},}\\ {\\quad}\\ {m_{1}^{i}:g_{i t}=\\alpha_{i}+x_{t}\\beta_{i}+\\varepsilon_{i t}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> A rate-distortion criterion (for details, see Appendix B of the supplementary material available at Biostatistics online) is used to choose the model for each gene. The rate-distortion theory, originally intended for data compression in the information theory field, can be adjusted to work as a model selection criterion in significance testing or cluster analysis on high-dimensional data (Jornsten, 2009). Briefly, the aim is to do model selection simultaneously for all genes in pathway $k$ by minimizing the overall distortion (the residual sum-of-squares) under the constraint that the total number of parameters used in the models must not exceed a certain bound. In effect this means that we are optimizing the number of parameters we pay for a certain explanatory power. The rate-distortion slope $S_{\\operatorname*{min}}^{k}$ (i.e. the bound for the number of parameters) <PARAGRAPH_SEPARATOR> that minimizes the overall Bayesian information criterion (BIC) is selected, and this determines the choice between $m_{0}$ and $m_{1}$ simultaneously for all genes in pathway $k$ : <PARAGRAPH_SEPARATOR> $$ S_{\\operatorname*{min}}^{k}=\\underset{S}{\\operatorname{argmin}}\\sum_{i:a_{i k}=1}\\mathrm{BIC}(m^{i}(S)). $$ <PARAGRAPH_SEPARATOR> In using BIC to select models, we make the assumption that the model errors are Gaussian. <PARAGRAPH_SEPARATOR> In step (b), we do model selection for the metabolites within pathway $k$ . Active genes (i.e. genes with $\\beta_{i}\\neq0$ ), as chosen in step (a), are allowed to work as potential predictors for metabolite $j$ . The number of genes may be large (and the number of replicates small), and hence we use regularization with the elasticnet penalty (Zou and Hastie, 2005), with a high level of sparsity, to select predictors. <PARAGRAPH_SEPARATOR> For each metabolite we have the null model $m_{0}^{j}$ and a set of increasingly complex models $m_{\\lambda}^{j}$ indexed by the elastic net penalty parameter (large penalty results in small models); for a given pathway $k$ and metabolite $j:a_{i k}b_{j k}=1$ : <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{{}}&{{m_{0}^{j}=m_{\\lambda=\\infty}^{j}:f_{j t}=\\tilde{\\alpha_{j}}+\\tilde{\\varepsilon}_{j t},}}\\ {{}}&{{m_{\\lambda}^{j}:f_{j t}=\\tilde{\\alpha_{j}}+\\displaystyle\\sum_{i:\\beta_{i}\\neq0}g_{i t}\\delta_{i j}(\\lambda)+\\tilde{\\varepsilon}_{j t}.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Rate-distortion is used to choose between the models $m_{\\lambda}^{j}$ , i.e. which $\\lambda$ to use, simultaneously for all metabolites. Instead of minimizing the overall BIC (which is unstable in a $p>n$ context), we maximize the overall predictive likelihood by using cross validation over the different rate-distortion slopes. The slope that minimizes the overall residual sum-of-squares is selected. <PARAGRAPH_SEPARATOR> In step (c), the overall distortion is calculated by extracting the residual sum-of-squares $\\mathrm{SS_{E}}$ , and the total sum-of-squares $\\mathrm{SS_{T}}$ in the gene and metabolite models. The coefficient of determination $R^{2}=1-$ $(\\mathrm{SS_{E}/S S_{T}},$ ) for both models (giving $R_{g}^{2}$ and $R_{m}^{2}$ ) is calculated and a weighted combined $R_{\\mathrm{comb}}^{2}$ for pathway $k$ is <PARAGRAPH_SEPARATOR> $$ R_{\\mathrm{comb}}^{2}|_{k}=w\\frac{R^{2}|_{k}^{g}}{\\operatorname*{max}_{k}\\{R^{2}|_{k}^{g}\\}}+(1-w)\\frac{R^{2}|_{k}^{m}}{\\operatorname*{max}_{k}\\{R^{2}|_{k}^{m}\\}}. $$ <PARAGRAPH_SEPARATOR> The weight parameter $0\\leqslant w\\leqslant1$ defines how much the gene and metabolite model influences the combined $R^{2}$ . If $w=1$ , only the gene model influences the choice, and conversely if $w=0$ , only the metabolite model does so. Equal contribution of the two data sources implies $w=0.5$ as a sensible choice, but we also explore an alternative weighting procedure in Appendix C of the supplementary material available at Biostatistics online. For all the simulations and the real data application presented in this paper, we use $w=0.5$ . <PARAGRAPH_SEPARATOR> To make the informed pathway decision, i.e. globally select the most active pathways, we could stop the procedure here, after one round, and then select a set of top-ranking pathways based on the $R^{2}$ scores. An alternative would be to run the estimation several rounds in a stepwise procedure, as described in Algorithm 2. In each round (repeat) of the stepwise procedure, one pathway is output as the most active. A set of active pathways can be picked by running several rounds of estimation. When performing repeated rounds of estimation, the data need to be re-processed before entering another iteration. First, the picked pathway $k^{\\prime}$ is removed from the set of pathways for which to do estimation in subsequent rounds. Secondly, the genes in pathway $k^{\\prime}$ for which the non-null model was chosen (step (a) in Algorithm 1) are excluded from the data set. Thirdly, the residuals from the metabolite model (step (b) in Algorithm 1) are calculated and used as responses in subsequent rounds. <PARAGRAPH_SEPARATOR> The stepwise procedure is repeated until we meet a stopping criterion, for example after a fixed number of rounds of estimation, or after the combined $R^{2}$ for any pathway falls under a certain threshold. A discussion on a reasonable cutoff for the $R^{2}$ -statistic is outlined in Section 3."
  },
  {
    "qid": "statistic-posneg-664-1-0",
    "gold_answer": "FALSE. While the condition f(p0, q0, r0) > f(p, q, r) for (p, q, r) ∈ R1 is part of the sufficient conditions to avoid errors, it is not alone sufficient. The function f must also satisfy condition B and be symmetric in p and q. Additionally, the paper mentions that conditions B and C1 are necessary and sufficient for no errors if the definition of errors is modified to include points along DE and AC. Thus, the condition alone is not sufficient without the other requirements.",
    "question": "Is the condition f(p0, q0, r0) > f(p, q, r) for (p, q, r) ∈ R1 sufficient to ensure no errors in test selection, given that the function f is symmetric in p and q and satisfies condition B?",
    "question_context": "Error-Free Test Selection in Diagnostic Key Construction\n\nThe paper discusses the construction of diagnostic keys using functions that avoid errors in test selection. It introduces regions R1 and R2 defined by inequalities involving test parameters p, q, and r. The function f(p, q, r) is required to be symmetric in p and q and must satisfy certain conditions to ensure no errors occur. The paper also explores extensions to polytomous attributes and introduces utility and entropy-based measures for test selection."
  },
  {
    "qid": "statistic-posneg-1574-0-0",
    "gold_answer": "FALSE. The formula appears to be a simplified representation of the estimation process but lacks explicit derivation from the underlying logistic model assumptions. The correct derivation should account for the properties of the UDTR (Up-and-Down Transformed Response) method and the logistic curve's slope parameter $\\tau = 1/\\beta$. The formula's validity depends on the independence of the two experiments for $L_p$ and $L_q$, which is not explicitly justified in the context. Additionally, the use of $\\overline{w}_p$ and $\\overline{w}_q$ as estimates of $L_p$ and $L_q$ requires further validation under the UDTR framework.",
    "question": "Is the formula $\\#=\\frac{1}{2}(\\overline{{w}}_{p}-\\overline{{w}}_{q})/l$ correctly derived for estimating $\\tau$ in the context of sequential estimation of quantal response curves using UDTR rules?",
    "question_context": "Sequential Estimation of Quantal Response Curves: Slope Estimation via UDTR\n\nTrue value ${\\bf\\lambda}=1{\\cdot}322$ ; asymptotic value $=1{\\cdot}647$ ! <PARAGRAPH_SEPARATOR> # 11. ESTIMATION OF THE SLOPE <PARAGRAPH_SEPARATOR> Consider estimation of ${\\boldsymbol{\\tau}}=1/\\beta.$ . We conduct two separate experiments, one to estimate $L_{p}$ , and the other to estimate ${\\cal L}_{{\\pmb{\\alpha}}},$ where $q=1-p,p>q$ , &nd in both cases we use the $\\overline{{\\pmb{w}}}$ -estimator with the UDTR. Then for the logistic curve (l), an estimate of $\\pmb{\\tau}$ i8 <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\#=\\frac{1}{2}(\\overline{{w}}_{p}-\\overline{{w}}_{q})/l,}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $l=\\log_{e}\\{p/(1-p)\\}$ , and $\\overrightarrow{w}_{p}$ &nd $\\overline{{w}}_{q}$ are estimates of $L_{p}$ and $L_{q}$ respectively. <PARAGRAPH_SEPARATOR> If the two experiments, for $L_{p}$ and $\\pmb{L_{q}},$ are independent, then the distribution and properties of $\\spadesuit$ can be obtained from those for $\\overline{{w}}_{\\pmb{\\mathscr{p}}}$ already obtained. <PARAGRAPH_SEPARATOR> Suppose, for example, we use the following two UDTR rules: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c c}{{L_{0\\cdot794}}}&{{L_{0\\cdot208}}}\\\\ {{}}&{{}}\\\\ {{U\\colon(0),(1,0),(1,1,0);}}&{{U\\colon(0,0,0);}}\\\\ {{}}&{{D\\colon(1,1,1);}}&{{D\\colon(1),(0,1),(0,0,1).}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Here $\\boldsymbol{l=1{\\cdot}3493}$ ,fromTable $\\l\\l\\mathbf{3}\\pmb{a}$ . We shall assume that the starting rule is used with both <PARAGRAPH_SEPARATOR> UDTR's, so that ${\\pmb{\\mathcal{E}}}({\\pmb{\\overline{{{w}}}}}_{\\pmb{\\mathcal{p}}})$ is obtained from equation (12). Table 1ll shows the effect of starting values and spacings on $\\ensuremath{\\mathbf{\\ensuremath{\\mathbf{\\mathit{\\delta\\phi}}}}}(\\hat{\\ensuremath{\\mathbf{\\mathit{\\delta\\phi}}}})$ , and we see that there is considerable bias if there is a large starting error, coupled with a small number of runs. <PARAGRAPH_SEPARATOR> Table 1l. The expectation of f"
  },
  {
    "qid": "statistic-posneg-1743-0-1",
    "gold_answer": "TRUE. The context states that to estimate θ, a variational class of conditional distributions over Zn consisting of all independent Gaussian distributions is considered. This is explicitly referred to as a Gaussian variational approximation (GVA), confirming that the random intercepts are modeled with independent Gaussian distributions in this approach.",
    "question": "Does the Gaussian variational approximation (GVA) assume independent Gaussian distributions for the random intercepts?",
    "question_context": "Variational Inference in Logistic Regression with Random Intercepts\n\nThe results indicate that variational estimators are not always consistent: in the first example the estimator is consistent for some parameters and not for others, and in the second example it is not consistent for any parameters. The first example also demonstrates that even when the variational estimator is consistent, it is not necessarily efficient. In either case the sandwich covariance matrix provides good confidence interval coverage rates and the one-step correction improves efficiency. The empirical evaluation of consistency correctly identifies inconsistency of the parameter vector as a whole, but not always inconsistency of individual parameters.<PARAGRAPH_SEPARATOR># 5.1. Logistic Regression With Random Intercepts<PARAGRAPH_SEPARATOR>First we consider logistic regression with random intercepts. Let $Z_{i}$ be a random intercept controlling each youth’s overall propensity for marijuana use, $\\mathrm{SEX}_{i}$ be an indicator that the youth is male, and $\\mathrm{AGE}_{i j}$ be youth i’s age at interview j. Denote ${p_{i j}}\\mathrm{~=~}P({Y_{i j}}\\mathrm{~=~1~}|\\mathrm{~}Z_{i},\\mathrm{SEX}_{i},\\mathrm{AGE}_{i j})$ . Our first model for marijuana usage is then<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}&{\\mathrm{|ogit}(p_{i j})=}\\ &{\\quad\\left\\{Z_{i}+\\beta_{0}+\\beta_{1}(\\mathrm{AGE}_{i j}/35)+\\beta_{2}(\\mathrm{AGE}_{i j}/35)^{2},\\right.\\mathrm{SEX}_{i}=0}\\ &{\\quad\\left.Z_{i}+\\beta_{3}+\\beta_{4}(\\mathrm{AGE}_{i j}/35)+\\beta_{5}(\\mathrm{AGE}_{i j}/35)^{2},\\right.\\mathrm{SEX}_{i}=1.}\\end{array}$$<PARAGRAPH_SEPARATOR>Each youth’s outcomes $Y_{i1},\\dots,Y_{i n_{i}}$ are assumed conditionally independent given $Z_{i},$ and we model $Z_{i}$ as iid $N(0,\\sigma^{2})$ . The parameter vector is $\\theta~=~(\\beta,\\log(\\sigma^{2}))$ . The inclusion of the quadratic effect of age is important because we expect that marijuana usage peaks some time in young adulthood and decreases thereafter. This model form is similar to that used in the analysis of age-crime curves (Fabio et al. 2011).<PARAGRAPH_SEPARATOR>To estimate $\\theta$ we consider a variational class of conditional distributions over $\\mathbf{Z}_{n}$ consisting of all independent Gaussian distributions. This is known as a Gaussian variational approximation (GVA). The variational parameters are $\\psi_{i}=(m_{i},\\log{s_{i}})$ , $m_{i}$ being the mean and $s_{i}$ the standard deviation of the variational conditional distribution of $\\gamma_{i}$ . The variational objective involves one-dimensional numerical integrals. To optimize the variational objective function we use a variational EM algorithm using the statistical software R (R Core Team 2018). We used the R package fastGHQuad (Blocker 2018) for numerical integration. In this case it is not possible to express the profiled objective function explicitly.<PARAGRAPH_SEPARATOR>To evaluate our methods, we conducted a simulation study based on the NLSY data. For each of 1000 simulations, we draw a bootstrap sample of youth. Conditional on these youth’s age and sex we simulated $Y_{1},\\dots,Y_{n}$ from the model, treating the variational estimate $\\hat{\\theta}_{n}$ for the data as the true parameter value $\\theta_{0}$ . We then estimated the model parameters and asymptotic covariance matrix using ML with the R package lme4 (Bates et al. 2015), the Gaussian variational approximation, and the one-step correction to the variational estimator. Finally, we used our proposed method to assess consistency of the variational algorithm at the estimated parameter value."
  },
  {
    "qid": "statistic-posneg-1074-1-3",
    "gold_answer": "FALSE. While the Fine & Gray model has lower MSE for the correct (cloglog) specification (Table 4), Table 3 shows it exhibits substantially larger bias than the pseudo-value methods when the true model is logistic (link function misspecification). The pseudo-value methods are more robust to link function choice, though they are generally more variable (higher MSE). The paper notes the Fine & Gray model 'is not overly robust to the choice of the link function.'",
    "question": "The Fine & Gray proportional hazards model outperforms the pseudo-value approach in all scenarios studied in the Monte Carlo simulation. Is this claim supported by the results?",
    "question_context": "Pseudo-Value Approach in Multistate Modeling and Competing Risks Analysis\n\nThe paper presents a general approach to multistate model probability estimation using pseudo-values, with applications to regression models for cumulative incidence functions in competing risks. The method involves defining pseudo-values based on unbiased estimators of state probabilities and modeling their dependence on covariates via generalized linear models. The approach is validated through Monte Carlo studies comparing different working covariance matrices and link functions."
  },
  {
    "qid": "statistic-posneg-1250-0-2",
    "gold_answer": "FALSE. The weighted version modifies the original Aitchison geometry by introducing weights w_{ij}, which selectively emphasize certain log-ratios. This changes the underlying geometry and is not guaranteed to maintain the original Hilbert space structure unless specific conditions on the weights are met. The paper's goal is to adapt the geometry based on graph structure, not preserve the original Aitchison geometry.",
    "question": "Is the weighted Aitchison inner product (1/2) * Σ_{i,j=1}^D ln(x_i/x_j) ln(y_i/y_j) w_{ij} a valid extension that maintains the Hilbert space structure of the original Aitchison geometry?",
    "question_context": "Compositional Data Analysis: Aitchison Geometry and Graph Theory Connections\n\nThe context discusses compositional data analysis (CoDa) and its mathematical foundations, focusing on the Aitchison geometry. It introduces operations like perturbation and powering, the Aitchison inner product, and transformations like the clr and ilr maps. The paper aims to extend CoDa tools by incorporating graph theory concepts, proposing a weighted version of the Aitchison inner product to emphasize certain log-ratios."
  },
  {
    "qid": "statistic-posneg-174-0-2",
    "gold_answer": "TRUE. The monotonicity assumption $(D(1)\\geq D(0))$ implies that no patient would receive the experimental treatment if assigned to the standard treatment but not if assigned to the experimental treatment (i.e., no defiers). Defiers are defined as patients with $D(1)=0$ and $D(0)=1$, which directly violates the monotonicity assumption. The context explicitly states that the monotonicity assumption is plausible because defiers are unlikely in practice, as they would refuse the proffered treatment and not participate in the study.",
    "question": "Is the monotonicity assumption $(D(1)\\geq D(0))$ violated if there are defiers in the population?",
    "question_context": "Proportion Ratio Estimation in Non-Compliance RCT with MAR Outcomes\n\nIn this paper, we focus the discussion on estimation of the $P R$ under a non-compliance RCT with outcomes missing at random (MAR). Using an approach similar to that proposed elsewhere (Brunner and Neumann, 1985; Bernhard and Compagnone, 1989), we derive in Sections 2.1–2.3 the maximum likelihood estimator (MLE) for the $P R$ under various assumptions, including the model in which there is no missingness, the model in which missingness depends on only assigned treatments, and the model in which missingness depends on both assigned and received treatments. In Section 2.4, we develop three asymptotic interval estimators in closed form for the $P R$ under the model in which missingness depends on assigned and received treatments. We use Monte Carlo simulation to study the performance of point estimators developed in Section 3.1 and the performance of the three asymptotic interval estimators in Section 3.2. We note that the point estimator for missing completely at random (MCAR) can be subject to a serious bias under various models with MAR. By contrast, we note that the point estimators developed in this paper can perform well when the number of patients per assigned treatment is large. Further, we find that the asymptotic interval estimator using Wald’s statistic tends to lose accuracy with respect to the coverage probability, while the interval estimator using the logarithmic transformation (Katz et al., 1978; Fleiss, 1981; Lui, 2002) tends to lose precision with respect to the average length. Finally, we note that an interval estimator, formed by an ad hoc combination of the previous two estimators, can perform well in almost all situations considered here. We apply the data taken from a multiple risk factor intervention trial for reducing the mortality of coronary heart disease (Multiple Risk Factor Intervention, 1982) in Section 3.3 to illustrate the bias of the point estimator for MCAR, and the use of the above point and interval estimators under the assumed MAR."
  },
  {
    "qid": "statistic-posneg-1595-0-1",
    "gold_answer": "TRUE. The simplification occurs because the moments of the normal distribution are easiest to calculate when the mean is 0 and the variance is 1. By transforming the data to z_i = (x_i - x̄)/s, the maximum likelihood estimates of the mean and variance become 0 and 1, respectively. This transformation leverages the group invariance property to simplify the calculation of the Rao's score statistic, leading to the given form. The paper explicitly derives this result in Example 1.",
    "question": "In the context of testing for skewness in the normal distribution, does the Rao's score statistic simplify to T = (1/6n)(Σz_i³)² when the data is transformed to have sample mean 0 and standard deviation 1?",
    "question_context": "Rao's Score Test for Goodness of Fit in Exponential Families\n\nThe paper discusses Rao's score test for departures from a regular exponential family model M(θ), where the probability density function is given by f(x;θ) = c(θ)exp{θᵀu(x)}. The test is based on a q×1 vector v(x) and involves the construction of a test statistic T that is invariant under group transformations. The paper provides detailed derivations and examples, including testing for skewness in the normal distribution and goodness of fit for the multivariate normal distribution."
  },
  {
    "qid": "statistic-posneg-1134-0-2",
    "gold_answer": "TRUE. The context provides the formula for the variance of $Y_{i}$ given $\\mu$ as $\\Sigma_{i}:=\\sigma_{e}^{2}I_{n_{i+}}+X_{i}D X_{i}^{\\mathrm{T}}$, where $D$ is a diagonal matrix of variances $\\sigma_{\\alpha}^{2},\\sigma_{\\beta}^{2},...,\\sigma_{\\beta}^{2}$. This correctly captures the variance structure of the nested random effects model.",
    "question": "Is the variance of $Y_{i}$ given $\\mu$ correctly expressed as $\\Sigma_{i}:=\\sigma_{e}^{2}I_{n_{i+}}+X_{i}D X_{i}^{\\mathrm{T}}$ in the context?",
    "question_context": "Nested Random Effects in Normal Linear Mixed Models\n\nColumns provide the 50th and 95th percentiles of Gelman & Rubin's (1992) variance inflation factor (G&R), and the value of the lag 1 sample autocorrelation (AcF) computed from the first of the five sampled chains. <PARAGRAPH_SEPARATOR> # 4. THE NESTED RANDOM EFFECTS NORMAL LINEAR MODEL <PARAGRAPH_SEPARATOR> Suppose there are additional levels of the hierarchy. For instance, we might denote the response for the $k$ th child in the $j$ th class of the ith school by $Y_{i j k}$ and model it as <PARAGRAPH_SEPARATOR> $$ Y_{i j k}=\\mu+\\alpha_{i}+\\beta_{i j}+\\gamma_{i j k}, $$ <PARAGRAPH_SEPARATOR> where $i=1,\\ldots,I,j=1,\\ldots,J$ and $k=1,\\ldots,n_{i j}$ At each level of the hierarchy we set up a linear model relating the terms in (4.1) to explanatory variables, i.e. <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\alpha_{i}=x_{i}^{\\mathrm{{T}}}\\alpha+\\varepsilon_{i},\\quad\\beta_{i j}=w_{i j}^{\\mathrm{{T}}}\\beta_{i}+\\lambda_{i j},\\quad\\gamma_{i j k}=z_{i j k}^{\\mathrm{{T}}}\\gamma_{i j}+\\xi_{i j k}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Here, $x_{i}$ denotes school-level covariates, $w_{i j}$ class-level covariates, and $z_{i j k}$ student-level covariates. Such models are referred to as multilevel or nested mixed linear models, e.g. Goldstein (1986), Longford (1987). Our centring ideas can be relevant here, applied in sequence from the highest level down. <PARAGRAPH_SEPARATOR> We illustrate using (4·1) with no covariates, assuming $\\varepsilon_{i j k}\\sim N(0,\\sigma_{e}^{2})$ $\\beta_{i j}\\sim N(0,\\sigma_{\\beta}^{2})$ and $\\alpha_{i}\\sim N(0,\\sigma_{\\alpha}^{2})$ , all independent. Define $\\rho_{i j}=\\mu+\\alpha_{i}+\\beta_{i j}$ whence $Y_{i j k}|\\rho_{i j}\\sim N(\\rho_{i j},\\sigma_{e}^{2})$ Define $\\eta_{i}=\\mu+\\alpha_{i}$ so that $\\rho_{i j}|\\eta_{i}\\sim N(\\eta_{i},\\sigma_{\\beta}^{2})$ and $\\eta_{i}|\\mu\\sim N(\\mu,\\sigma_{\\alpha}^{2})$ We assume a flat prior for $\\mu,$ and take $\\sigma_{\\epsilon}^{2},\\sigma_{\\beta}^{2}$ and $\\sigma_{\\alpha}^{2}$ as known. The centred parametrisation is thus $(\\mu,\\eta,\\rho)$ . Exact calculations are more complicated here since marginalisation produces dependence amongst the Yjk. <PARAGRAPH_SEPARATOR> Let <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{Y_{i j}^{\\mathsf{T}}=(Y_{i j1},\\hdots,Y_{i j n_{i j}}),\\quad Y_{i}^{\\mathsf{T}}=(Y_{i1}^{\\mathsf{T}},\\hdots,Y_{i j}^{\\mathsf{T}}),\\quad Y^{\\mathsf{T}}=(Y_{1}^{\\mathsf{T}},\\hdots,Y_{I}^{\\mathsf{T}}).}\\end{array} $$ <PARAGRAPH_SEPARATOR> Then, given $\\mu,$ the $Y_{i}$ are independent normal with $E(Y_{i}|\\mu)=\\mu1_{n_{i+}}$ and variance <PARAGRAPH_SEPARATOR> $$ \\mathrm{var}\\left(Y_{i}|\\mu\\right)=\\Sigma_{i}:=\\sigma_{e}^{2}I_{n_{i+}}+X_{i}D X_{i}^{\\mathrm{T}}. $$ <PARAGRAPH_SEPARATOR> Here $\\begin{array}{r}{n_{i+}=\\sum n_{i j},D=\\mathrm{diag}\\left(\\sigma_{\\alpha}^{2},\\sigma_{\\beta}^{2},...,\\sigma_{\\beta}^{2}\\right)}\\end{array}$ and $X_{i}$ .s $n_{i+}\\times(J+1)$ with first column all 1's, and $(j+1)$ th column $(j=1,\\ldots,J)$ having a 1 in each of the $n_{i j}$ rows corresponding to $Y_{i j k}\\left(k=1,\\ldots,n_{i j}\\right)$ , and ${0}\\mathfrak{s}$ elsewhere. From (4.2), <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{E(\\boldsymbol{\\mu}|\\boldsymbol{Y})=\\hat{\\boldsymbol{\\mu}}:=(\\boldsymbol{X}^{\\mathrm{T}}\\Sigma^{-1}\\boldsymbol{X})^{-1}\\boldsymbol{X}^{\\mathrm{T}}\\Sigma^{-1}\\boldsymbol{Y},\\quad\\mathrm{var}(\\boldsymbol{\\mu}|\\boldsymbol{Y})=(\\boldsymbol{X}^{\\mathrm{T}}\\Sigma^{-1}\\boldsymbol{X})^{-1}=(\\sum\\boldsymbol{X}_{i}^{\\mathrm{T}}\\Sigma_{i}^{-1}\\boldsymbol{X}_{i})^{-1}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Following $\\S2$ we investigate the $|B_{i}D^{-1}|,$ where $B_{i}=(\\sigma_{e}^{-2}X_{i}^{\\mathrm{T}}X_{i}+D^{-1})^{-1}.$ In Appendix 2 we show that if $\\sigma_{\\beta}^{2}\\rightarrow\\infty$ with $\\sigma_{\\alpha}^{2}$ and $\\sigma_{e}^{2}$ fixed, then $|B_{i}D^{-1}|\\to0$ , so centring will be preferred. If on the other hand $\\sigma_{e}^{2}\\to\\infty$ with $\\sigma_{\\alpha}^{2}$ and ${\\sigma}_{\\beta}^{2}$ fixed, then $|B_{i}D^{-1}|\\to1$ and the uncentred parametrisation will be better. <PARAGRAPH_SEPARATOR> We now describe a simulation study of (4.1) with $I=J=3$ $n_{i j}=5$ and the true value of $\\mu$ equal to 0. We are interested in comparing four possible parametrisations: uncentred $(\\mu,\\alpha,\\beta)$ , α's centred $(\\mu,\\eta,\\beta)$ , $\\beta$ 's centred $(\\mu,\\alpha,\\rho)$ , and both $\\pmb{\\alpha\\mathscr{s}}$ and $\\beta$ 's centred $(\\mu,\\eta,\\rho)$ After selecting fixed values for $\\sigma_{\\epsilon}^{2},\\sigma_{\\beta}^{2}$ and $\\sigma_{\\alpha}^{2}$ , designed to favour one parametrisation or another, we first generate ‘true' parameter values from the assumed priors, then data given these parameter values. A single Gibbs sampling chain is then run using the uncentred parametrisation for 10 000 iterations. The output from this chain is used to estimate the posterior correlation matrix among the $I J+I+1=13$ parameters under each of the four parametrisations. <PARAGRAPH_SEPARATOR> Table 2(a) tabulates the resulting estimated correlations arising under the four parametrisations for the case $\\sigma_{e}=10,\\sigma_{\\beta}=1$ and $\\sigma_{\\alpha}=1$ Since the model variability is large relative to that in both stages of the prior, we would expect the uncentred parametrisation to be best. This is in fact the case, with 75 of the 78 sample correlations falling in $(-0{\\cdot}1,0{\\cdot}1)$ Table 2(b) gives results for the case $\\sigma_{\\bullet}=1$ $\\sigma_{\\beta}=10$ and $\\pmb{\\sigma}_{\\pmb{\\alpha}}=1$ . Here, as predicted by our"
  },
  {
    "qid": "statistic-posneg-503-0-4",
    "gold_answer": "FALSE. The context states that the performance of the proposed method is comparable to that of the generalized cross-validation method when the signal-to-noise ratio is low, with log-transformed relative efficacies close to zero. The proposed method performs better than the generalized cross-validation method as the signal-to-noise ratio increases, not when it is low.",
    "question": "Does the proposed method perform better than the generalized cross-validation method when the signal-to-noise ratio is low?",
    "question_context": "Smoothing Spline ANOVA Models: Univariate and Multivariate Function Simulations\n\nWe simulated the data according to (1) using three univariate functions with different orders of smoothness. Univariate scenario 1: where $$\\eta_{u1}(x)=\\frac{1}{3}B_{20,5}(x)+\\frac{1}{3}B_{12,12}(x)+\\frac{1}{3}B_{7,30}(x),$$ $$B_{\\alpha,\\beta}(x)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\quad(0\\leqslant x\\leqslant1).$$ Univariate scenario 2: $$\\eta_{u2}(x)=10\\sin^{2}(2\\pi x)\\mathbb{1}_{(x\\leqslant\\frac{1}{2})},$$ where $\\mathbb{1}_{(x\\leqslant\\frac{1}{2})}$ is an indicator function that equals 1 for $x\\leqslant{\\frac{1}{2}}$ and equals 0 otherwise. Univariate scenario 3: $$\\eta_{u3}(x)=10\\times\\left\\{-x+2\\left(x-\\frac{1}{4}\\right)\\right\\}\\mathbb{1}_{(x\\geqslant\\frac{1}{4})}+2\\left(-x+\\frac{3}{4}\\right)\\mathbb{1}_{(x\\geqslant\\frac{3}{4})},$$ where $\\mathbb{1}_{(x\\geqslant1/4)}$ and $\\mathbb{1}_{(x\\geqslant3/4)}$ are two indicator functions that equal 1 when the conditions in parentheses are satisfied and equal 0 otherwise. We generated $x$ from a uniform distribution on [0, 1]. The generated data for three univariate functions with $\\mathrm{SNR}=1$ and three true function values are shown in Fig. 1. The log-transformed relative efficacies of the proposed method and the order-based method for the three scenarios are shown in Fig. 2. The skip method reduces to the generalized cross-validation method in the single smoothing parameter selection. The performance of the proposed method is comparable to that of the generalized cross-validation method when the signal-to-noise ratio is low, with logtransformed relative efficacies close to zero. The performance of our method is better than that of the generalized cross-validation method as the signal-to-noise ratio increases. Such behaviour may result from unstably estimated smoothing parameters based on subsamples when the signalto-noise ratio is low. Even though the order-based method performs well in some scenarios, such as univariate scenario 3, it is not reliable because of the large variability in most scenarios."
  },
  {
    "qid": "statistic-posneg-927-0-0",
    "gold_answer": "FALSE. Under the assumption of functional white noise, the empirical autocorrelation coefficient $\\hat{\\rho}_{h}$ does not necessarily converge to zero. Instead, the norm of the empirical autocovariance operator $\\|\\hat{C}_{h}\\|$ converges to a distribution described by a weighted sum of chi-squared random variables, as shown in Theorem 1. The empirical autocorrelation coefficient $\\hat{\\rho}_{h}$ is scaled by the empirical variance, and its behavior is governed by this limiting distribution, not by convergence to zero.",
    "question": "Is it true that under the assumption of functional white noise, the empirical autocorrelation coefficient $\\hat{\\rho}_{h}$ converges to zero as the sample size $T$ increases?",
    "question_context": "Functional Time Series Autocorrelation and Partial Autocorrelation Analysis\n\nThe paper discusses the analysis of functional time series (FTS) through autocorrelation and partial autocorrelation functions. It defines the autocovariance kernel $C_{h}(u,v)$ and its empirical estimator $\\hat{C}_{h}(u,v)$, as well as the partial autocovariance kernel $C_{h,h}(u,v)$. The paper also introduces the functional autocorrelation coefficient $\\rho_{h}$ and the functional partial autocorrelation function (FPACF) $\\rho_{h,h}$, along with their empirical estimators. Theoretical results are provided for the large sample behavior of these estimators under the assumption of white noise."
  },
  {
    "qid": "statistic-posneg-1146-0-0",
    "gold_answer": "TRUE. The context explicitly provides the AMISE formula as: AMISE(H) = n^{-1}|H|^{-1/2}R(K) + (1/4)μ_2(K)^2(vech^T H)Ψ_4(vech H). This is derived from the MISE approximation under the conditions that all entries in D^2f(x) are square integrable and H → 0, n^{-1}|H|^{-1/2} → 0 as n → ∞.",
    "question": "Is the statement 'The AMISE for multivariate kernel density estimation is given by n^{-1}|H|^{-1/2}R(K) + (1/4)μ_2(K)^2(vech^T H)Ψ_4(vech H)' true based on the provided context and formulas?",
    "question_context": "Multivariate Kernel Density Estimation: Bandwidth Matrix Selection and Convergence Rates\n\nThe choice of smoothing parameters is a problem of fundamental importance in kernel density estimation and related areas. Bandwidth selection for univariate kernel density estimation is the simplest form of this problem, and has been the subject of considerable research. Substantial advances been made leading to the development of bandwidth selectors which combine good practical performance with excellent asymptotic properties. See [4] for an overview. Progress in the case of multivariate case has been much slower. Nonetheless, the selection of bandwidth matrices is an important problem because of the utility of multivariate kernel density estimators in areas such as data visualization, nonparametric discriminant analysis and goodness of fit testing."
  },
  {
    "qid": "statistic-posneg-678-0-0",
    "gold_answer": "TRUE. The estimator $\\mathbf{\\hat{b}}_{\\mathrm{eb,gen}}$ is derived under the assumption that $\\mathbf{r}$ is a fixed effect, as indicated in the context where $\\Theta=\\mathbf{rr}^{\\mathrm{{T}}}\\times(\\Sigma+\\mathbf{rr}^{\\mathrm{{T}}})^{-1}$ minimizes the mean-squared error when $\\mathbf{r}$ is treated as a fixed effect. The form of the estimator combines the full and reduced model estimates with weights determined by $\\Lambda$, which is related to the fixed effect assumption.",
    "question": "Is the estimator $\\mathbf{\\hat{b}}_{\\mathrm{eb,gen}}=\\frac{\\Lambda}{1+\\Lambda}\\mathbf{\\hat{b}}_{\\mathrm{full}}+\\frac{1}{1+\\Lambda}\\mathbf{\\hat{b}}_{\\mathrm{red}}$ derived under the assumption that $\\mathbf{r}$ is a fixed effect?",
    "question_context": "Empirical Bayes Shrinkage Estimators in Statistical Modeling\n\nWe consider empirical Bayes-type shrinkage estimators, which are denoted $\\hat{\\mathbf{b}}_{\\mathrm{{eb}}}$ , that are pooled estimators based on the full and reduced models (Huntsberger, 1955), defined as $\\hat{\\mathbf{b}}_{\\mathrm{{eb}}}\\overset{\\cdot}{=}\\Theta\\times$ $\\mathbf{\\hat{b}}_{\\mathrm{full}}+(\\mathbf{I}_{p}-\\boldsymbol{\\Theta})\\times\\hat{\\mathbf{b}}_{\\mathrm{red}}$ . Here $\\mathbf{I}_{p}$ is the identity matrix of dimension $p$ , and $\\Theta$ is an unknown square matrix. We obtain $\\Theta$ as the matrix that minimizes the mean-squared error of $\\hat{\\mathbf{b}}_{\\mathrm{{eb}}}$ . <PARAGRAPH_SEPARATOR> # 3.1. r is a fixed effect <PARAGRAPH_SEPARATOR> Suppose that $\\mathbf{r}=\\mathbf{b}-\\mathbf{W}\\mathbf{d}$ is an unknown fixed effect. Let $\\Theta$ be a square matrix of dimension $p$ . In Appendix B we show that the mean-squared error is minimized when $\\Theta=\\mathbf{rr}^{\\mathrm{{T}}}\\times(\\Sigma+\\mathbf{rr}^{\\mathrm{{T}}})^{-1}$ . The resulting estimator, which is denoted $\\hat{\\mathbf{b}}_{\\mathrm{eb,gen}}$ , is given by <PARAGRAPH_SEPARATOR> $$ \\mathbf{\\hat{b}}_{\\mathrm{eb,gen}}=\\frac{\\Lambda}{1+\\Lambda}\\mathbf{\\hat{b}}_{\\mathrm{full}}+\\frac{1}{1+\\Lambda}\\mathbf{\\hat{b}}_{\\mathrm{red}}. $$ <PARAGRAPH_SEPARATOR> Suppose that we assume that $\\Theta$ is a diagonal matrix with different diagonal entries. The meansquared error of $\\hat{\\mathbf{b}}_{\\mathrm{{eb}}}$ is minimized when the $j$ th diagonal entry of $\\Theta$ is $\\theta_{j}=T_{j}/(1+T_{j})$ . This provides the componentwise empirical Bayes estimator, which is denoted $\\hat{\\mathbf{b}}_{\\mathrm{eb,comp}}$ , having $j$ th component: $\\hat{\\mathbf{b}}_{j,\\mathrm{eb,\\mathrm{comp}}}=\\boldsymbol{\\theta}_{j}\\times\\hat{\\mathbf{b}}_{j,\\mathrm{full}}+(1-\\bar{\\theta_{j}})\\mathbf{\\hat{b}}_{j,\\mathrm{red}}$ . <PARAGRAPH_SEPARATOR> These estimators also have striking parallels with empirical Bayes estimators that have been considered in recent genetic epidemiology applications (Luo et al., 2009; Chen et al., 2009). Our investigations establish a connection between the weights of these empirical Bayes estimators and the preliminary hypothesis tests. <PARAGRAPH_SEPARATOR> # 3.2. r is a random effect <PARAGRAPH_SEPARATOR> The reduced model provides erroneous estimates of $\\mathbf{b}$ when $\\mathbf{W}$ is inaccurate. In this case, the effects of the exposures are $\\mathbf{b}=\\mathbf{W}\\mathbf{d}+\\mathbf{r}$ , where $\\mathbf{r}$ is an unknown nuisance parameter, and can be interpreted as the residual effect. When there is uncertainty about the accuracy of W, r is not equal to 0. It is a random quantity that varies around 0. We specify an $N(\\mathbf{0},\\mathbf{A})$ distribution for this random effect. The marginal distribution of $\\hat{\\mathbf{r}}=\\hat{\\mathbf{b}}_{\\mathrm{full}}-\\hat{\\mathbf{b}}_{\\mathrm{red}}^{\\star}$ is $N(\\mathbf{0},{\\boldsymbol{\\Sigma}}+\\mathbf{A})$ . The minimum mean-squared estimator of $\\mathbf{r}$ is the posterior mean $E(\\mathbf{r}|\\hat{\\mathbf{r}}){=}\\mathbf{A}(\\Sigma+\\mathbf{A})^{-1}\\hat{\\mathbf{r}}$ . Note that ${\\bf I}_{p}-$ $\\mathbf{A}(\\Sigma+\\mathbf{A})^{-1}=\\bar{\\Sigma}(\\Sigma+\\mathbf{A})^{-1}$ . Therefore, if A can be estimated empirically from the observed data, <PARAGRAPH_SEPARATOR> J. Satagopan, Q. Zhou, S. Oliveria, S. Dusza, M. Weinstock, M. Berwick and A. Halpern we have <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\hat{\\bf b}_{\\mathrm{eb}}={\\hat{\\bf b}}_{\\mathrm{red}}+E({\\bf r}|\\hat{\\bf r})}\\ &{\\quad\\quad={\\hat{\\bf b}}_{\\mathrm{full}}-\\hat{\\bf r}+{\\bf A}(\\Sigma+{\\bf A})^{-1}\\hat{\\bf r}}\\ &{\\quad\\quad={\\hat{\\bf b}}_{\\mathrm{full}}-\\Sigma(\\Sigma+{\\bf A})^{-1}\\hat{\\bf r}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> We obtain three estimators based on the following assumptions. <PARAGRAPH_SEPARATOR> (a) A is a general symmetric matrix. A conservative estimate of A is rrT. Using this in equation (3), it can be seen that the resulting estimator is indeed $\\hat{\\mathbf{b}}_{\\mathrm{eb,gen}}$ .   (b) A is a diagonal matrix with unequal diagonal entries. Its conservative estimator is diag $(\\hat{\\mathbf{r}}\\hat{\\mathbf{r}}^{\\mathrm{T}})$ and the corresponding estimator is denoted $\\hat{\\mathbf{b}}_{\\mathrm{eb,non-ex}}$ .   (c) A is a diagonal matrix with equal diagonal entries, i.e. $\\mathbf{A}{=}\\mathbf{t}^{2}\\mathbf{I}_{p}$ . A conservative estimate of the prior variance $\\mathbf{t}^{2}$ is $\\hat{\\mathbf{r}}^{\\mathrm{T}}\\hat{\\mathbf{r}}/p$ . The corresponding estimator is denoted $\\hat{\\mathbf{b}}_{\\mathrm{eb,ex}}$ . <PARAGRAPH_SEPARATOR> The first two assumptions correspond to specifying a non-exchangeable prior distribution for r, whereas the third assumption is an exchangeable prior. These assumptions are also reflected in the subscripts ‘non-ex’ and ‘ex’ for the estimators. The choice of an exchangeable versus a non-exchangeable prior is not a testable hypothesis but simply a user-defined presumption since the likelihood does not contain information about the nature of the variance A (Lindley and Smith, 1972). In Section 4 we conduct simulations to compare the operating characteristics of the various estimators."
  },
  {
    "qid": "statistic-posneg-2065-2-0",
    "gold_answer": "FALSE. The expansion is correct up to the second term in ν⁻¹, but the question incorrectly states the second term as (1/6(4ν)²)(3Q⁴Pⁱᵛ - 2Q³P''' - 3Q²P'' + 3QP'). The correct second term is (1/4ν)(Q⁴P'' - QP'). The third term, which involves ν⁻², is given by (1/6(4ν)²)(3Q⁴Pⁱᵛ - 2Q³P''' - 3Q²P'' + 3QP').",
    "question": "Is the expansion for P_ν(Q) in powers of ν⁻¹ given by P_ν(Q) = P(Q) + (1/4ν)(Q⁴P'' - QP') + (1/6(4ν)²)(3Q⁴Pⁱᵛ - 2Q³P''' - 3Q²P'' + 3QP') correct up to the second term in ν⁻¹?",
    "question_context": "Expansion of the Studentized Integral in Powers of ν⁻¹\n\nThe paper discusses the expansion of the studentized integral in powers of ν⁻¹, where ν represents degrees of freedom. It presents a recurrence formula connecting P_ν(Q) and P_{ν-2}(Q), leading to an expansion for P_ν(Q) in powers of ν⁻¹. The expansion includes terms up to ν⁻⁴ and involves derivatives of P(Q). The paper also applies this expansion to the incomplete beta-function, providing a new formula in terms of the incomplete gamma-function and its derivatives."
  },
  {
    "qid": "statistic-posneg-1494-5-0",
    "gold_answer": "TRUE. The context explicitly states Proposition 1, which asserts the convergence of $S_{\\lfloor h^{2}t\\rfloor}/h$ to $\\Theta(t)$ as $h\\rightarrow\\infty$, where $\\Theta(t)$ is defined as a Brownian motion $W(t)$ with a drift term $-\\gamma t$ and an additional term $\\theta\\delta\\sigma m a x\\{0,t-\\tau^{*}\\}$ that accounts for the change in distribution of observations after time $\\tau^{*}$. The proposition is backed by the detailed setup of the problem, including the definitions of $S_{\\lfloor h^{2}t\\rfloor}$, $\\zeta$, and the convergence in the space $D[0,\\infty)$. The integral expression for $\\theta$ further supports the validity of the drift term in the limiting process.",
    "question": "Is the claim that the process $S_{\\lfloor h^{2}t\\rfloor}/h$ converges in distribution to $\\Theta(t)=W(t)-\\gamma t+\\theta\\delta\\sigma m a x\\{0,t-\\tau^{*}\\}$ as $h\\rightarrow\\infty$ supported by the given context and formulas?",
    "question_context": "Convergence of Signed-Sequential-Rank CUSUMs to Brownian Motion with Drift\n\nThe following result, which is a special case of Theorem 1 in Lombard (1981), forms the basis of the heuristic. Proposition 1. Let $h$ and $\\tau^{*}$ be positive numbers. For every $t>0,$ , denote the integer part of $h^{2}t$ by $\\lfloor h^{2}t\\rfloor$ and set $\\tau=\\lfloor h^{2}\\tau^{*}\\rfloor$ . Suppose the independent observations $X_{1},\\dots,X_{\\tau}$ have common cdf $F(x)$ while $X_{\\tau+1}$ , $X_{\\tau+2}$ , . . . have cdf $F(x-\\delta\\sigma/h)$ . With $\\xi_{i}$ from (2), set $$ S_{\\lfloor h^{2}t\\rfloor}=\\sum_{i=1}^{\\lfloor h^{2}t\\rfloor}(\\xi_{i}-\\zeta),t\\geq0 $$ where $\\zeta~=~\\gamma/h$ and $S_{0}~=~0$ . Then the continuous time process $S_{\\lfloor h^{2}t\\rfloor}/h$ , $t\\geq0$ converges in distribution as $h\\rightarrow\\infty$ to the continuous time process $$ \\Theta(t)=W(t)-\\gamma t+\\theta\\delta\\sigma m a x\\{0,t-\\tau^{*}\\} $$ where W denotes a standard Brownian motion and where $$ \\theta=-\\int_{-1}^{1}J(u)\\frac{f^{\\prime}(F^{-1}(\\frac{1+u}{2}))}{f(F^{-1}(\\frac{1+u}{2}))}d u. $$ Here, convergence in distribution is meant in the sense of weak convergence of probability measures on the space $D[0,\\infty)-s e e$ Billingsley (1999, Section 16). Straightforward calculation involving an integration by parts gives $\\theta=\\theta_{0}/\\sigma$ for $\\theta_{0}$ defined in (5). Then, upon evaluating (12) and (13) at $t=n/h^{2}$ , $1\\leq n\\leq\\tau$ and $t=(\\tau+k)/h^{2}$ , $k\\geq1$ and using the fact that $W(n/h^{2})$ and $W(n)/h$ , $n\\geq1$ have the same joint distributions, Proposition 1 suggests that the joint distributions of the partial sums $S_{n}$ , $n\\geq m$ that figure in the SSR CUSUM can be approximated by those of the sequence $$ \\Theta(n)=W(n)-\\zeta n+\\theta_{0}\\delta m a x\\{0,n-\\tau\\} $$ where $\\tau>m$ and $m$ is a ``large'' positive integer. Let $\\xi_{1}^{\\ast},\\hdots\\xi_{\\tau}^{\\ast}$ be i.i.d. normal(0, 1), let $\\xi_{\\tau+k}^{\\ast},k\\geq1$ be i.i.d. normal $(\\theta_{0}\\delta,1)$ and set $S_{n}^{*}=\\xi_{1}^{*}+\\cdot\\cdot\\cdot+\\xi_{n}^{*}-n\\zeta$ . Then the sequences $S_{n}^{*}$ , $n\\geq1$ and $\\Theta(n)$ , $n\\geq1$ are identically distributed. Thus, the joint distributions of the normal CUSUM based on $\\xi_{n}^{*}$ , $n\\geq1$ provide an approximation to the joint distributions of the SSR CUSUM based on $\\xi_{n}$ , $n\\geq1$ . This is the content of the heuristic."
  },
  {
    "qid": "statistic-posneg-1066-0-1",
    "gold_answer": "FALSE. While the condition on $K^{(r)}(\\mathbf{0})$ is necessary for the theorem's assumptions, the specific rate $n^{-4/(d+12)}$ is derived from the balance between the bias and variance terms in the AMSE expression for the plug-in selector. The condition on $K$ ensures the proper behavior of the higher-order derivatives in the bias calculation, but the rate itself comes from the dimensional scaling of these terms, not directly from the kernel properties.",
    "question": "Does the relative convergence rate $n^{-4/(d+12)}$ for the AMSE plug-in selector to $\\mathbf{H}_{\\mathrm{AMISE}}$ hold under the condition that the kernel function $K$ satisfies $K^{(r)}(\\mathbf{0})=1$ when $|r|=4$ with all even elements in $r$?",
    "question_context": "Asymptotic Convergence Rates for Bandwidth Matrix Selectors\n\nThe paper analyzes the asymptotic mean squared error (AMSE) of bandwidth matrix selectors in kernel density estimation. Key results include the decomposition of AMSE into asymptotic bias and variance components, and the derivation of convergence rates for plug-in selectors to the optimal bandwidth matrix $\\mathbf{H}_{\\mathrm{AMISE}}$. The main formulas relate the differential operators of the estimated and true AMISE criteria to the convergence behavior of the bandwidth matrix estimators."
  },
  {
    "qid": "statistic-posneg-1770-0-0",
    "gold_answer": "FALSE. While the product density formula $\\rho^{(n)}(u_{1},\\ldots,u_{n}) = \\rho(u_{1}) \\ldots \\rho(u_{n})$ reflects the lack of interaction between points, it does not imply independence in the usual statistical sense. The independence property of a Poisson process refers to the fact that the counts of points in disjoint regions are independent random variables, not the spatial locations themselves. The product density formula is a consequence of the Poisson process's complete spatial randomness, but the points are still generated by a stochastic process with specific spatial properties.",
    "question": "In a homogeneous Poisson process, the product density $\\rho^{(n)}(u_{1},\\ldots,u_{n})$ equals the product of the intensity function evaluated at each point, i.e., $\\rho^{(n)}(u_{1},\\ldots,u_{n}) = \\rho(u_{1}) \\ldots \\rho(u_{n})$. Does this imply that the points in a homogeneous Poisson process are independent?",
    "question_context": "Poisson and Cox Processes in Spatial Point Pattern Analysis\n\n(i) $N(B)$ is Poisson distributed with mean $\\mu(B)$ , (ii) conditional on $N(B)$ , the points in $\\mathbf{X}_{B}$ are i.i.d. with density proportional to $\\rho(u),u\\in B$ . <PARAGRAPH_SEPARATOR> Poisson processes are studied in detail in Kingman (1993). They play a fundamental role as a reference process for exploratory and diagnostic tools and when more advanced spatial point process models are constructed. <PARAGRAPH_SEPARATOR> If $\\rho(u)$ is constant for all $u\\in S$ , we say that the Poisson process is homogeneous. Realizations of the process may appear to be rather chaotic with large empty space and close pairs of points, even when the process is homogeneous. The Poisson process is a model for ‘no interaction’ or ‘complete spatial randomness’, since $\\mathbf{X}_{A}$ and $\\mathbf{X}_{B}$ are independent whenever $A,B\\subset S$ are disjoint. Moreover, <PARAGRAPH_SEPARATOR> $$ \\rho^{(n)}(u_{1},\\ldots,u_{n}){=}\\rho(u_{1}).\\ldots\\rho(u_{n}),\\quad g{\\equiv}1, $$ <PARAGRAPH_SEPARATOR> reflecting the lack of interaction. Stationarity means that $\\rho(u)$ is constant, and implies isotropy of $\\mathbf{X}$ . Note that another Poisson process results if we make an independent thinning of a Poisson process. <PARAGRAPH_SEPARATOR> Typically, a log linear model of the intensity function is considered (Cox, 1972), <PARAGRAPH_SEPARATOR> $$ \\log\\rho(u)=z(u)\\beta^{\\mathsf{T}}. $$ <PARAGRAPH_SEPARATOR> The independence properties of a Poisson process are usually not realistic for real data. Despite of this the Poisson process has enjoyed much popularity due to its mathematical tractability."
  },
  {
    "qid": "statistic-posneg-697-0-0",
    "gold_answer": "FALSE. The inequality is incorrectly stated as it does not account for all cross terms in the expansion of $||\\beta-\\hat{\\beta}||^{2}$. The correct expansion should include additional terms to capture the interactions between the different components of the error, which are omitted in the given inequality.",
    "question": "Is the inequality $||\\beta-\\hat{\\beta}||^{2} \\leq 4I_{1} + 4I_{2} + 4R_{\\beta}(K,D)^{2}$ correctly derived from the expansion of $||\\beta-\\hat{\\beta}||^{2}$?",
    "question_context": "Convergence Analysis in Functional Quadratic Regression\n\nAppendix A contains proofs of theoretical results stated in Section 3. Note that constant C may vary from lines and all of them are not related to $K$ or $D$. Proof of Theorem 3.1. We only show the result of $\\|\\beta-\\hat{\\boldsymbol{\\beta}}\\|$ since the derivation of $\\lVert\\boldsymbol\\gamma-\\hat{\\boldsymbol\\gamma}\\rVert$ is analogous. Definitions (3) and (9) yield that $||\\beta-\\hat{\\beta}||^{2}=||\\sum_{k=1}^{\\infty}\\sum_{m=1}^{\\infty}b_{m k}\\psi_{m}\\otimes\\phi_{k}-\\sum_{k=1}^{D}\\sum_{m=1}^{K}\\hat{b}_{m k}\\hat{\\psi}_{m}\\otimes\\hat{\\phi}_{k}||^{2}$."
  },
  {
    "qid": "statistic-posneg-266-5-2",
    "gold_answer": "TRUE. The variance formula $k \\times var(T_k)$ is correctly derived as a weighted sum of within-population ($\\theta_i$) and between-population ($\\theta_{ij}$) covariance terms. The weights reflect the sample sizes and the number of populations, ensuring the formula captures the correct scaling behavior of the test statistic. This derivation is consistent with the asymptotic theory presented in the paper and aligns with the expected behavior of the test under $H_0$.",
    "question": "Is the variance formula $k \\times var(T_k)$ correctly derived under the null hypothesis, accounting for both within-population and between-population covariance terms?",
    "question_context": "Testing Equality of Means in High-Dimensional Data\n\nThe paper discusses the asymptotic behavior of a test statistic $T_k$ for comparing means across $k$ populations, particularly focusing on scenarios where the number of populations $k$ is large. The test statistic's variance is derived under the null hypothesis $H_0$, and its asymptotic distribution is shown to depend on the covariance structures of the populations. The paper also compares the proposed test with existing methods, particularly those assuming normality, and evaluates their performance under various data distributions."
  },
  {
    "qid": "statistic-posneg-1047-0-2",
    "gold_answer": "TRUE. The context states that following Wand and Jones [12] and Duong and Hazelton [1], the pilot bandwidth matrix is constrained to be of the form G = g^2I, which simplifies the estimation process and is empirically supported when applied to pre-sphered data.",
    "question": "Is it true that the plug-in estimator ψ̌_r(G) uses a pilot bandwidth matrix G constrained to be of the form G = g^2I?",
    "question_context": "Multivariate Kernel Density Estimation: Bandwidth Matrix Selection and Convergence Rates\n\nThe choice of smoothing parameters is a problem of fundamental importance in kernel density estimation and related areas. Bandwidth selection for univariate kernel density estimation is the simplest form of this problem, and has been the subject of considerable research. Substantial advances been made leading to the development of bandwidth selectors which combine good practical performance with excellent asymptotic properties. See [4] for an overview. Progress in the case of multivariate case has been much slower. Nonetheless, the selection of bandwidth matrices is an important problem because of the utility of multivariate kernel density estimators in areas such as data visualization, nonparametric discriminant analysis and goodness of fit testing."
  },
  {
    "qid": "statistic-posneg-499-0-3",
    "gold_answer": "FALSE. While the formula is mentioned in the context, it does not correctly account for the reflection principle and the specific conditions of the game. The correct term should involve a different combinatorial expression that properly reflects the constraints of the game.",
    "question": "Does the formula $a_{n}^{p}=\\frac{\\left[n-2p\\right]}{n}c_{p}^{n}$ correctly represent the term of the modified Pascal's Triangle at $(n-2p,-n)$ under the condition that the game ceases if the gain vanishes?",
    "question_context": "Probability Analysis in the Game of Heads and Tails\n\nM. Levy, in his paper, investigates the behavior of the gain of one of the players in a game of heads and tails by considering arithmetic triangles that are modifications of Pascal's Arithmetic Triangle. The paper discusses the probabilities of certain events, such as the final gain and the extreme values of the gain during a set of games, using various combinatorial and asymptotic formulas."
  },
  {
    "qid": "statistic-posneg-226-1-0",
    "gold_answer": "TRUE. The context explicitly states that 'the $D$ -optimal design does not depend on $\\theta_{0}$' and provides the justification: 'Since, for any constant $r>0$ , $|M(\\xi,r\\theta)|=|M(\\xi^{\\prime},\\theta)|$ , where $\\xi^{\\prime}\\{(r x)\\}=\\xi\\{(x)\\}$'. This invariance under scaling of $\\theta_{0}$ confirms that the D-optimal design is independent of $\\theta_{0}$.",
    "question": "Is the statement 'The D-optimal design for the model $Y=\\theta_{0}(e^{-\\theta_{1}x}-e^{-\\theta_{2}x})+\\epsilon$ does not depend on the parameter $\\theta_{0}$' true based on the provided context?",
    "question_context": "D-optimal Designs for Nonlinear Pharmacokinetic Models\n\nCompartmental models are frequently used to model the profile of the drug concentration in plasma and tissue compartments over time. If we assume rapid drug equilibrium throughout the body and linear drug absorption and elimination, the drug concentration in the plasma compartment, when administered orally, might be adequately characterized by the one-compartment pharmacokinetic model. The expected plasma drug concentration is described by the function $$f(t)=\\frac{\\mathrm{Dose}\\times k_{a}}{V\\times(k_{a}-k_{e})}\\times(e^{-k_{e}t}-e^{-k_{a}t}),$$ where $k_{a}$ and $k_{e}$ are the absorption constant and elimination constant, respectively. Atkinson et al. (1993) and Atkinson $\\&$ Haines (1996) have considered the optimal designs for this model. In this section, we apply the sufficient condition in Theorem 1 to obtain some analytical results about the locally $D$ -optimal design. By reparameterization, we consider the model $$Y=\\theta_{0}(e^{-\\theta_{1}x}-e^{-\\theta_{2}x})+\\epsilon,$$ where the $\\epsilon\\mathbf{S}$ are independently $N(0,1),\\theta_{0}>0,0<\\theta_{1}<\\theta_{2}$ and $\\theta=(\\theta_{0},\\theta_{1},\\theta_{2})^{\\mathrm{T}}$ is the parameter vector of interest. The $D$ -optimal design is invariant under reparameterization. The Fisher information matrix for $\\theta$ is $\\begin{array}{r}{M(\\xi,\\theta)=\\int_{\\chi}h(x,\\theta)h(x,\\theta)^{\\mathrm{T}}d\\xi(x)}\\end{array}$ and $\\boldsymbol{d}(\\xi,x)=h(x,\\theta)^{\\mathrm{T}}\\boldsymbol{M}^{-1}(\\xi,\\theta)h(x,\\theta)$ for the nonsingular design $\\xi$ , where $h(x,\\theta)=(e^{-\\theta_{1}x}-e^{-\\theta_{2}x},-\\theta_{0}x e^{-\\theta_{1}x},\\theta_{0}x e^{-\\theta_{2}x})^{\\mathrm{T}}$ Let $m_{i j}$ denote the $(i,j)$ th element of $M^{-1}(\\xi,\\theta)$ . It is easy to show that $d(\\xi,0)=0$ and $\\begin{array}{r}{\\operatorname*{lim}_{x\\rightarrow\\infty}d(\\xi,x)=0}\\end{array}$ . For any design $\\xi$ based on $t\\geqslant3$ points $x_{1},\\ldots,x_{t}$ and corresponding probabilities $p_{1},\\ldots,p_{t}$ , $p_{i}>0$ , $\\textstyle\\sum_{i=1}^{t}p_{i}=1$ , let $M_{i j}$ be the minor corresponding to the $(i,j)$ th element of $M(\\xi,\\theta)$ . Then $\\begin{array}{r}{M_{23}=\\theta_{0}^{2}\\sum_{1\\leqslant i<j\\leqslant t}p_{i}p_{j}f(x_{i},x_{j})}\\end{array}$ , where $f(r,s)=\\{f_{1}(r,s)f_{2}(r,s)\\}/e^{2\\theta_{2}r+2\\theta_{2}s}$ , $f_{1}(r,s)=s e^{r(\\theta_{2}-\\theta_{1})}-$ $r e^{s(\\theta_{2}-\\theta_{1})}+r-s$ and $f_{2}(r,s)=s e^{s(\\theta_{2}-\\theta_{1})}-r e^{r(\\theta_{2}-\\theta_{1})}+(r-s)e^{(r+s)(\\theta_{2}-\\theta_{1})}$ . It can be shown that $f_{1}(r,s)$ and $f_{2}(r,s)$ always have the same sign. Hence, $m_{23}=-M_{23}/|M|<0$ . If we let $u(x)=m_{22}\\theta_{1}x^{2}e^{-2\\theta_{1}x}+m_{33}\\theta_{2}x^{2}e^{-2\\theta_{2}x}-m_{23}(\\theta_{1}+\\theta_{2})x^{2}e^{-(\\theta_{1}+\\theta_{2})x}$ , then $d^{\\prime}(\\xi,x)$ is a linear combination of $$\\left\\{e^{-2\\theta_{1}x},x e^{-2\\theta_{1}x},e^{-2\\theta_{2}x},x e^{-2\\theta_{2}x},e^{-(\\theta_{1}+\\theta_{2})x},x e^{-(\\theta_{1}+\\theta_{2})x},u(x)\\right\\}.$$ It follows from Karlin $\\&$ Studden (1966, p. 10) that each of the following is a $T$ -system: $$\\begin{array}{r l}&{\\left\\{e^{-2\\theta_{1}x},x e^{-2\\theta_{1}x},e^{-2\\theta_{2}x},x e^{-2\\theta_{2}x},e^{-(\\theta_{1}+\\theta_{2})x},x e^{-(\\theta_{1}+\\theta_{2})x},x^{2}e^{-2\\theta_{1}x}\\right\\},}\\ &{\\left\\{e^{-2\\theta_{1}x},x e^{-2\\theta_{1}x},e^{-2\\theta_{2}x},x e^{-2\\theta_{2}x},e^{-(\\theta_{1}+\\theta_{2})x},x e^{-(\\theta_{1}+\\theta_{2})x},x^{2}e^{-2\\theta_{2}x}\\right\\},}\\ &{\\left\\{e^{-2\\theta_{1}x},x e^{-2\\theta_{1}x},e^{-2\\theta_{2}x},x e^{-2\\theta_{2}x},e^{-(\\theta_{1}+\\theta_{2})x},x e^{-(\\theta_{1}+\\theta_{2})x},x^{2}e^{-(\\theta_{1}+\\theta_{2})x}\\right\\}.}\\end{array}$$ Since $m_{23}<0,u(x)$ is a linear combination of $x^{2}e^{-2\\theta_{1}x},x^{2}e^{-2\\theta_{2}x}$ and $x^{2}e^{-(\\theta_{1}+\\theta_{2})x}$ with positive coefficients. By Lemma 1, (3) is a $T$ -system, and thus $d^{\\prime}(\\xi,x)$ has at most six real roots. Hence, by Theorem 1, the $D$ -optimal design for $\\chi_{0b}$ is unique and minimally supported, and so is the $D$ -optimal design for $\\chi_{a b}$ if $0<a<b$ . It is easy to show that the $D$ -optimal design does not depend on $\\theta_{0}$ . Since, for any constant $r>0$ , $|M(\\xi,r\\theta)|=|M(\\xi^{\\prime},\\theta)|$ , where $\\xi^{\\prime}\\{(r x)\\}=\\xi\\{(x)\\}$ , without loss of generality we assume that $\\theta_{2}=1$ . For some values of $\\theta_{1}$ the support points of the $D$ -optimal designs are displayed in Table 1. In the case of $\\theta_{1}=0.1$ , for instance, the $D$ -optimal design for [0, 16] is a three-point design supported on $x_{1}^{*}=0{\\cdot}889$ , $x_{2}^{*}=3{\\cdot}824$ and $x_{3}^{*}=14{\\cdot}442$ . It can be shown that for a sufficiently large $b_{0}$ , the zeros of $d(\\xi,x)-3$ are all interior to $[0,b_{0}]$ ; for $\\theta_{1}=0.1$ this value may be taken as $b_{0}=16$ . The relationships among optimal designs for different design spaces established in Corollary 1 are also illustrated in Table 1. For example, the $D$ -optimal design for [0, 8] has a design point at $x=8$ since $x_{3}^{*}>8$ , while the $D$ -optimal design for [0·8, 16] is the same as that for [0, 16] since $0.8<x_{1}^{*}<x_{3}^{*}<16$ ; the $D$ -optimal design for [2, 16] has a support point at $x=2$ since $x_{1}^{*}<2$ , while the $D$ -optimal design for [2, 8] has both $x=2$ and $x=8$ in the support because $x_{1}^{*}<2<8<x_{3}^{*}$ . From part (i) of Corollary 1, the uniform design supported on $\\{x_{1}^{*},x_{2}^{*},x_{3}^{*}\\}$ is the $D$ -optimal design for $\\chi_{0b}$ for any $b>x_{3}^{*}$ . The support points of the locally $D$ -optimal designs are sensitive to the value of $\\theta_{1}$ particularly for small value of $\\theta_{1}$ ; see Fig. 1. To account for the uncertainty in the parameter value, Bayesian optimal designs could be an alternative choice (Chaloner $\\&$ Larntz, 1989)."
  },
  {
    "qid": "statistic-posneg-307-3-0",
    "gold_answer": "TRUE. The indicator function $I_{c}(Y,X,U,\\theta)$ explicitly enforces the condition that $Y_{i}=g(h^{\\operatorname{T}}X_{i}+s_{i})$ for all $i\\in\\{1,2,\\dots,n\\}$. This is a hard constraint in the model, meaning the likelihood is zero (and thus the posterior is zero) unless this condition is satisfied for every observation. This reflects a deterministic observation equation in the state-space model, where $Y_{i}$ is a strict function of the hidden state $X_{i}$ and seasonal component $s_{i}$.",
    "question": "Is the statement 'The indicator function $I_{c}(Y,X,U,\\theta)$ equals 1 only if the observation $Y_{i}$ is exactly equal to the deterministic transformation $g(h^{\\operatorname{T}}X_{i}+s_{i})$ for all $i$' true, given the model's likelihood specification?",
    "question_context": "Bayesian State-Space Modeling for Traffic Volume Analysis\n\nLet $Y=\\{Y_{1},\\ldots,Y_{n}\\}$ denote the entire observation process, $X=\\{X_{1},X_{2},\\ldots,X_{n}\\}$ the (hidden) state process and $U=(u_{1}^{1},\\dots,u_{48}^{1})$ the 48-period (daily) seasonal subcomponent of $\\left\\{s_{t}\\right\\}$ . Let $\\theta=(\\phi,\\sigma,\\beta_{w})$ denote the vector of remaining parameters to be estimated. Strictly speaking, the seasonal component (see equation (5)) has a period of 336, but we need only 49 parameters to specify it. Independent prior distributions to both $U$ and $\\theta$ are assigned, and they are denoted by $p(U)$ and $p(\\theta)$ respectively. <PARAGRAPH_SEPARATOR> Our goal is to construct a Markov chain $\\{M_{k}=(X^{(k)},U^{(k)},\\theta^{(k)})$ , $k=1,2,\\hdots\\}$ whose limiting distribution is the posterior distribution $p(X,U,\\theta|Y)~\\propto~p(Y,X,U,\\theta)$ . Here $X^{(k)}$ is a set $\\{\\bar{X^{(k)}},t=1,\\ldots,n\\}$ representing the $k$ th approximate sample from the marginal posterior distribution of the state process, with $U^{(k)}$ and ${\\boldsymbol{\\theta}}^{(k)}$ also representing $k$ th approximate draws from the marginal posteriors of the respective distributions. After an initial burn-in period, elements $(X^{(k)},U^{(k)},\\theta^{(k)})$ of the chain can be regarded as approximate (non-independent) draws from the desired posterior distribution. <PARAGRAPH_SEPARATOR> Before formally stating the sampling algorithm, we point out that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{p(Y,X,U,\\theta)=I_{c}(Y,X,U,\\theta)p(X,U,\\theta)=I_{c}(Y,X,U,\\theta)p(X|U,\\theta)p(U,\\theta)}}\\ {{\\mathrm{}}}\\ {{\\mathrm{}=I_{c}(Y,X,U,\\theta)p(X|\\theta)p(U)p(\\theta),}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $I_{c}(Y,X,U,\\theta)$ is equal to 1 if $Y_{i}=g(h^{\\operatorname{T}}X_{i}+s_{i})$ for $i\\in\\{1,2,\\dots,n\\}$ , and 0 otherwise. Furthermore, <PARAGRAPH_SEPARATOR> $$ p(X|\\theta)=p(X_{1}|\\theta)\\prod_{i=2}^{n}p(X_{i}|X_{i-1}), $$ <PARAGRAPH_SEPARATOR> where $X_{1}|\\theta\\sim N(0,\\Lambda)$ , $\\Lambda$ being the solution to $F\\Lambda F^{\\mathrm{T}}+\\Sigma=\\Lambda$ , and, for $i>1$ , $X_{i}|X_{i-1}\\sim$ $N(F X_{i-1},\\Sigma)$ . Therefore it is straightforward to compute the density $p(Y,X,U,\\theta)$ . <PARAGRAPH_SEPARATOR> Our MCMC sampler is as follows. <PARAGRAPH_SEPARATOR> Step 1: set $k=1$ . Choose some initial state $(X^{(1)},U^{(1)},\\theta^{(1)})$ satisfying $I_{c}(Y,X^{(1)},U^{(1)},\\theta^{(1)})=1$ : Step 2: replace $k$ by $k+1$ . Set $X^{(k)}=X^{(k-1)},U^{(k)}=U^{(k-1)}$ and $\\theta^{(k)}=\\theta^{(k-1)}$ . Step 3: for $i=1,2,\\hdots,n$ , replace the vector $X_{i}^{(k)}$ by a draw from its conditional distribution, given $Y$ , $\\{X_{j}^{(k)},j\\neq i\\}$ , $U^{(k)}$ and ${\\theta}^{(k)}$ ."
  },
  {
    "qid": "statistic-posneg-1653-0-0",
    "gold_answer": "TRUE. The doubly robust estimator $\\hat{\\mu}_{\\mathrm{DR}}$ is designed to be consistent (and thus asymptotically unbiased) if either the propensity score model $\\pi(W;\\alpha)$ or the outcome regression model $m(W;\\beta)$ is correctly specified. This property is derived from the estimator's construction, which combines inverse probability weighting (IPW) and regression adjustment. If the propensity score model is correct, the IPW component ensures consistency. If the outcome regression model is correct, the regression adjustment component ensures consistency. The estimator does not require both models to be correct simultaneously.",
    "question": "Is the doubly robust estimator $\\hat{\\mu}_{\\mathrm{DR}}$ unbiased if either the propensity score model $\\pi(W;\\alpha)$ or the outcome regression model $m(W;\\beta)$ is correctly specified, but not necessarily both?",
    "question_context": "Bayesian Doubly Robust Estimation with Exponentially Tilted Empirical Likelihood\n\nThe paper discusses a Bayesian approach to doubly robust estimation using exponentially tilted empirical likelihood. The models involve propensity score estimation via logistic regression and outcome regression via linear models. The target parameter is the population mean of the outcome variable, estimated using a doubly robust estimator that combines inverse probability weighting and regression adjustment. The paper compares different Bayesian methods, including the Bayesian exponentially tilted empirical likelihood (BETEL) approach and the method proposed by Saarela et al. (2016)."
  },
  {
    "qid": "statistic-posneg-1816-4-2",
    "gold_answer": "FALSE. Explanation: While the one-step correction estimator $\\hat{\\theta}_{n}^{(1)}$ can improve efficiency compared to the initial variational estimator $\\hat{\\theta}_{n}$, its asymptotic efficiency depends on the regularity conditions and the quality of the initial estimator. The one-step correction achieves the same asymptotic distribution as the MLE only if the initial estimator is consistent and the regularity conditions for the Newton-Raphson approximation hold.",
    "question": "Is the one-step correction estimator $\\hat{\\theta}_{n}^{(1)}$ guaranteed to be asymptotically efficient under all regularity conditions?",
    "question_context": "Sandwich Covariance Estimation in Variational Inference\n\nWe now discuss computation of consistent covariance estimators. Recall that in practice, $m(\\theta;X)$ is often not available in closed form. Fortunately, the derivatives of $m(\\theta;X)$ can be expressed in terms of the derivatives of $\\nu(\\theta,\\psi;X)$ , which are always available, because $\\nu(\\theta,\\psi;X)$ is a result of the model and variational family used. Thus, using the chain rule, the asymptotic variance can be estimated whether or not $m(\\theta;X)$ is available explicitly. We denote by $D_{\\theta}\\nu$ and $D_{\\psi}\\nu$ the first partial derivatives of $\\nu_{:}$ and $D_{\\theta\\theta}^{2}\\nu,D_{\\theta\\psi}^{2}$ , and $D_{\\psi\\psi}^{2}\\nu$ the second derivatives of $\\nu$ . <PARAGRAPH_SEPARATOR> Concerning $D_{\\theta}m(\\theta;x)$ , which appears in Equation (4), since $\\hat{\\psi}(\\boldsymbol{\\theta};\\boldsymbol{x})$ maximizes $\\nu$ for fixed $\\theta,x,D_{\\theta}m(\\theta;x)=D_{\\theta}$ $\\nu(\\theta,\\psi;x)|_{\\psi=\\hat{\\psi}(\\theta;x)}$ For $D_{\\theta}^{2}m(\\theta;x)$ in Equation (3), <PARAGRAPH_SEPARATOR> $$ D_{\\theta}^{2}m(\\theta;x)=\\left[D_{\\theta\\theta}^{2}\\nu-D_{\\theta\\psi}^{2}\\nu\\left(D_{\\psi\\psi}^{2}\\nu\\right)^{-1}D_{\\theta\\psi}^{2}\\nu^{T}\\right]_{\\psi=\\hat{\\psi}(\\theta;x)}, $$ <PARAGRAPH_SEPARATOR> as we show in the supplementary materials, where we abbreviate $\\nu(\\theta,\\psi;X)$ as $\\nu$ for presentation. Replacing the appropriate derivatives in the definition of $V(\\theta)$ with the above expressions and the population expectations with empirical ones gives a way to calculate the asymptotic covariance only knowing $\\nu(\\theta,\\psi;X)$ and its derivatives (which can be calculated numerically), as opposed to $m(\\theta;x)$ , the computation of which involves optimization. <PARAGRAPH_SEPARATOR> We now have $\\hat{A}_{n}(\\hat{\\theta}_{n})^{-1}\\hat{B}_{n}(\\hat{\\theta}_{n})\\hat{A}_{n}(\\hat{\\theta}_{n})^{-1}\\overset{\\mathrm{~P~}}{\\longrightarrow}V(\\bar{\\theta})$ where <PARAGRAPH_SEPARATOR> $$ {\\hat{A}}_{n}(\\theta)={\\frac{1}{n}}\\sum_{i=1}^{n}\\left[D_{\\theta\\theta}^{2}\\nu-D_{\\theta\\psi}^{2}\\nu\\left(D_{\\psi\\psi}^{2}\\nu\\right)^{-1}D_{\\theta\\psi}^{2}\\nu^{T}\\right]_{\\psi={\\hat{\\psi}}_{i},x=X_{i}}, $$ <PARAGRAPH_SEPARATOR> $$ \\hat{B}_{n}(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}\\left[\\left(D_{\\theta}\\nu\\right)\\left(D_{\\theta}\\nu\\right)^{T}\\right]_{\\psi=\\hat{\\psi}_{i},x=X_{i}}. $$ <PARAGRAPH_SEPARATOR> Equations (5) and (6) provide a formula for constructing an asymptotic covariance matrix for the variational estimator $\\hat{\\theta}_{n}$ . This covariance can be used to construct asymptotically calibrated Wald intervals, regions, and hypothesis tests about $\\theta_{0}$ if $\\bar{\\theta}=\\theta_{0}$ . Furthermore, the sandwich covariance is model-robust in the sense that it is valid even if $P_{0}\\notin\\mathcal{P}$ . <PARAGRAPH_SEPARATOR> For an MLE under correct model specification, $A(\\theta)=B(\\theta)$ and the asymptotic covariance reduces to $A(\\theta)^{-1}$ , the inverse Fisher information matrix. In this case the sandwich covariance is only needed for model-robust uncertainty estimation. However, when $m$ is not proportional to the log-likelihood, as is often true with variational inference, $A$ and $B$ are not necessarily equal even under correct model specification. Therefore, the sandwich covariance is necessary even if $P_{0}\\in\\mathcal{P}$ ."
  },
  {
    "qid": "statistic-posneg-1790-2-1",
    "gold_answer": "TRUE. The paper states that for $\\rho>0$ and $\\rho\\le\\alpha<1$, the maximum entropy copula gives less weight to the 'non-worse' case, where the first variable takes large values but stays less than the second variable. This is reflected in the inequality $\\mathbb{P}(\\alpha t\\leq X_{1}\\leq t|X_{2}=t)\\gg\\mathbb{P}(Y_{1}\\ge\\alpha Y_{2}|Y_{2}=t)$ for large t.",
    "question": "Does the maximum entropy copula give less weight to the 'non-worse' case when $\\rho>0$ and $\\rho\\le\\alpha<1$, meaning $\\mathbb{P}(\\alpha t\\leq X_{1}\\leq t|X_{2}=t)\\gg\\mathbb{P}(Y_{1}\\ge\\alpha Y_{2}|Y_{2}=t)$ for large t?",
    "question_context": "Comparison of Conditional Extreme Event Probabilities in Gaussian Copulas\n\nWe compare the conditional probabilities of extreme values of a pair of random variables $(X_{1},X_{2})$ which has bivariate normal distribution with standard normal marginals and correlation coefficient $\\rho$ , with a pair of random variables $(Y_{1},Y_{2})$ whose marginals are also standard normal, but has copula $c_{\\delta}$ , where $\\delta$ is the diagonal of the copula of $(X_{1},X_{2})$ . We compute the conditional probabilities $\\mathbb{P}(X_{1}\\geq\\alpha t|X_{2}=t)$ and $\\mathbb{P}(Y_{1}\\geq\\alpha t|Y_{2}=t)$ with $\\alpha\\geq1$ and consider their asymptotic behaviour when t goes to infinity. This comparison is motivated by consideration of correlated defaults in mathematical finance, see Section 10.8 in [24]. (Notice however the parameters of upper tail dependence of the two copulas are the same since they have the same diagonal.)"
  },
  {
    "qid": "statistic-posneg-2027-0-2",
    "gold_answer": "FALSE. Explanation: While LR_cc = LR_uc + LR_ind ~ χ²₂ tests both coverage and independence, a high P_cc alone does not guarantee both conditions are met. It merely indicates insufficient evidence to reject the joint null hypothesis. The individual P_uc and P_ind must also be examined to diagnose specific failures (e.g., incorrect coverage, dependence, or both).",
    "question": "The conditional coverage test (LR_cc) combines the unconditional coverage (LR_uc) and independence (LR_ind) tests. The test statistic is LR_cc = LR_uc + LR_ind ~ χ²₂. Is it correct to say that a high P_cc indicates the VaR model satisfies both correct coverage and independence of violations?",
    "question_context": "Value-at-Risk (VaR) Forecasting Accuracy and Testing\n\nThe paper discusses the testing of VaR prediction models using likelihood ratio tests for unconditional coverage, independence of violations, and conditional coverage. It introduces the Boolean sequence of VaR violations, the empirical downfall probability, and the mean relative scaled bias (MRSB) for comparing model efficiency."
  },
  {
    "qid": "statistic-posneg-1245-0-1",
    "gold_answer": "FALSE. Explanation: The context states that small values of $\\pmb{S}$ indicate local regularity, and for complete regularity, $\\pmb{S} = \\mathbf{0}$. Therefore, $\\pmb{S} = 0$ signifies complete regularity, not irregularity. The statistic $\\pmb{S}$ is the square of the coefficient of variation of the squared nearest neighbour distances, and a value of zero implies no variation, which is characteristic of a perfectly regular pattern.",
    "question": "A value of $\\pmb{S} = 0$ indicates complete irregularity in the spatial point pattern. True or False?",
    "question_context": "Test Statistics for Local Regularity in Spatial Point Patterns\n\nClearly some other approach is required. The obvious candidate is computer simulation, and this is the approach we adopted. <PARAGRAPH_SEPARATOR> # 3. TEST STATISTICS <PARAGRAPH_SEPARATOR> Let ${\\pmb v}_{1},...,{\\pmb v}_{\\pmb{\\pi}}$ denote the squared nearest neighbour distances of the $\\pmb{\\mathscr{n}}$ points in & given region. We consider the three test statistics: <PARAGRAPH_SEPARATOR> $$ D=\\frac{1}{n}\\sum_{i=1}^{n}v_{i},\\quad S=\\frac{1}{n-1}\\sum_{i=1}^{n}(v_{i}-D)^{2}/D^{2},\\quad G=\\left(\\prod_{i=1}^{n}v_{i}\\right)^{1/n}\\bigg/D. $$ <PARAGRAPH_SEPARATOR> Here $\\pmb{D}$ is the familiar mean squared distance discussed earlier. In the context of testing for local regularity, $\\pmb{S}$ has an obvious intuitive motivation. It is the square of the coefficient of variation of the squared nearest neighbour distances. Small values indicate local regularity ; for complete regularity $\\pmb{S}=\\mathbf{0}$ . Eberhardt (1967) proposed an essentially equivalent form of $\\pmb{S}$ but his statistic was based on distances, as opposed to squared distances. Finally $\\pmb{G}$ isthe ratio of the geometric mean to the arithmetic mean of the squared distances. It lies in the interval (0,1) and large values of ${\\pmb G}$ indicate local regularity, the maximum being attained at complete regularity; $\\pmb{G}$ was proposed by Moran (1951), in the context of renewal theory, a8 a test for a Poisson process against an alternative of a gamma distribution of times between events. He also based his statistic on the interval between events in time, rather than its square, as is natural in working in one dimension rather than two. <PARAGRAPH_SEPARATOR> The statistics $G$ and $s$ differ from $\\cal D$ in the important aspect that their infinite plane theory sampling distributions on the null hypothesis do not involve the density of the process, whereas that of $\\pmb{D}$ does. When points are dropped at random in a given area, the distributions of $\\pmb{G}$ and $\\pmb{S}$ depend on the number of points and on the shape, but not the size, of the area. The dependence on shape is not great, except for those which are very extreme; see $\\S8$ <PARAGRAPH_SEPARATOR> # 4. POWER OF $\\pmb{D}$ , $\\pmb{S}$ AND $\\pmb{G}$"
  },
  {
    "qid": "statistic-posneg-1310-0-2",
    "gold_answer": "TRUE. The paper confirms this by stating that the coverage probability of $C_{\\gamma}$ depends on the unknown parameters only through $\\eta^{\\prime}\\varSigma_{\\gamma}^{1}\\eta = \\theta^{\\prime}Q(Q^{\\prime}D_{n}^{1}Q)^{-1}Q^{\\prime}\\theta/\\sigma^{2} = \\Sigma n_{i}\\{\\theta_{i}-\\bar{\\theta}\\}^{2}/\\sigma^{2}$.",
    "question": "The coverage probability of the confidence set $C_{\\gamma}$ depends on the unknown parameters only through $\\Sigma n_{i}\\{\\theta_{i}-\\bar{\\theta}\\}^{2}/\\sigma^{2}$.",
    "question_context": "Improved Confidence Intervals for Comparisons and Contrasts in ANOVA\n\nThe paper discusses the construction of improved confidence intervals for comparisons and contrasts in the context of one-way analysis of variance (ANOVA). It compares the classic Scheffe intervals with improved intervals based on confidence sets centered at estimators that shrink towards zero. The improved intervals are shown to be uniformly shorter than the Scheffe intervals under certain conditions. The paper provides theoretical results and numerical evidence supporting the coverage properties of these improved intervals."
  },
  {
    "qid": "statistic-posneg-1294-1-0",
    "gold_answer": "TRUE. The proof is derived from the equalities: $$ \\langle(\\gamma K\\gamma)^{*}u,v\\rangle=\\langle u,\\gamma K\\gamma v\\rangle=\\langle K\\gamma v,\\gamma u\\rangle=\\langle\\gamma v,K^{*}\\gamma u\\rangle=\\langle(\\gamma K^{*}\\gamma)u,v\\rangle. $$ These equalities show that the adjoint of $\\gamma\\circ K\\circ\\gamma$ is indeed $\\gamma\\circ K^{*}\\circ\\gamma$, as required.",
    "question": "Is the statement $(\\gamma\\circ K\\circ\\gamma)^{*}=\\gamma\\circ K^{*}\\circ\\gamma$ true for any bounded endomorphism $K$ of $L^{2}(\\varOmega)$?",
    "question_context": "Conjugate Spectral Measures and Unitary Operators in L²(Ω)\n\nIn this section we define what is natural to name the conjugate of a spectral measure. We will need this notion in Section 7.4. If $K$ is a bounded endomorphism of $L^{2}(\\varOmega)$ , then it is the same for $\\gamma\\circ K\\circ\\gamma$ , and we get the following property. <PARAGRAPH_SEPARATOR> Proposition 16. If $K$ is a bounded endomorphism of $L^{2}(\\varOmega),$ then $(\\gamma\\circ K\\circ\\gamma)^{*}=\\gamma\\circ K^{*}\\circ\\gamma.$ <PARAGRAPH_SEPARATOR> Proof. It comes from the following equalities: <PARAGRAPH_SEPARATOR> $$ \\langle(\\gamma K\\gamma)^{*}u,v\\rangle=\\langle u,\\gamma K\\gamma v\\rangle=\\langle K\\gamma v,\\gamma u\\rangle=\\langle\\gamma v,K^{*}\\gamma u\\rangle=\\langle(\\gamma K^{*}\\gamma)u,v\\rangle. $$ <PARAGRAPH_SEPARATOR> Then it is easy to verify that, if $K$ is a projector (resp. a unitary operator of $L^{2}(\\varOmega))$ , then $\\gamma K\\gamma$ is a projector (resp. a unitary operator of $L^{2}(\\varOmega))$ , and that, if $\\varepsilon$ is a s.m. on $B_{I I^{k}}$ for $L^{2}(\\varOmega)$ , then the same happens for the mapping $A\\in B_{I I^{k}}\\mapsto$ $\\gamma\\circ{\\mathcal{E}}A\\circ\\gamma\\in{\\mathcal{P}}(L^{2}(\\varOmega))$ . This lets us define the conjugate of a s.m. <PARAGRAPH_SEPARATOR> Definition 12. We name conjugate of $\\varepsilon$ , s.m. on $B_{I I^{k}}$ for $L^{2}(\\varOmega)$ , the s.m. defined by $A\\in\\mathcal{B}_{\\varPi^{k}}\\mapsto\\gamma\\circ\\mathcal{E}A\\circ\\gamma\\in\\mathcal{P}(L^{2}(\\varOmega)).$ <PARAGRAPH_SEPARATOR> The following property defines the unitary operator associated with the conjugate of a s.m. <PARAGRAPH_SEPARATOR> Proposition 17. If $U$ is a unitary operator of associated s.m. $\\varepsilon$ , then the unitary operator $\\gamma U\\gamma$ has as associated s.m. $W_{I I}\\mathcal{E}_{c}$ , where $\\mathcal{E}_{c}$ is the conjugate s.m. of $\\varepsilon$ . <PARAGRAPH_SEPARATOR> Proof. Let $V$ be the unitary operator of associated s.m. $W_{I I}\\mathcal{E}_{c}$ , where $\\mathcal{E}_{c}$ is the conjugate of $\\varepsilon$ . From the recalls of Section 2, we can write <PARAGRAPH_SEPARATOR> $$ V X=\\int\\mathrm{e}^{i.1}\\mathrm{d}Z_{W_{\\cal T}\\mathcal{E}_{c}}^{X}=\\int\\mathrm{e}^{i.1}\\mathrm{d}W_{\\cal T}(Z_{\\mathcal{E}_{c}}^{X})=\\int\\mathrm{e}^{i.1}\\circ W_{\\cal T}\\mathrm{d}Z_{\\mathcal{E}_{c}}^{X}=\\int\\mathrm{e}^{i.-1}\\mathrm{d}Z_{\\mathcal{E}_{c}}^{X}, $$ <PARAGRAPH_SEPARATOR> and as $Z_{\\mathcal{E}_{c}}^{X}=\\gamma\\circ Z_{\\varepsilon}^{\\gamma X}$ , (2) can be completed by <PARAGRAPH_SEPARATOR> $$ V X=\\int\\operatorname{e}^{i.-1}\\mathsf{d}\\gamma\\circ Z_{\\varepsilon}^{X}=\\gamma(\\int\\operatorname{e}^{i.1}\\mathrm{d}Z_{\\varepsilon}^{\\gamma X})=\\gamma U\\gamma X, $$ <PARAGRAPH_SEPARATOR> this for any $X$ from $L^{2}(\\varOmega)$ , so the property stands. □ <PARAGRAPH_SEPARATOR> We can now generalize this property. <PARAGRAPH_SEPARATOR> Proposition 18. If $\\{U_{n};n~\\in~\\mathbb{Z}^{k}\\}$ is the group of the unitary operators deduced from $\\varepsilon$ , s.m. on $B_{I I^{k}}$ for $L^{2}(\\varOmega),$ then $\\{\\gamma U_{n}\\gamma;n\\in\\mathbb{Z}^{k}\\}$ is the group of the unitary operators of $L^{2}(\\varOmega)$ deduced from the s.m. $W_{\\varPi^{k}}\\mathcal{E}_{c}$ , where $\\mathcal{E}_{c}$ is the conjugate s.m. of $\\varepsilon$ ."
  },
  {
    "qid": "statistic-posneg-2017-0-1",
    "gold_answer": "FALSE. While the text states that the EBMLE estimator is 'closely related' to the James-Stein estimator in the homoscedastic case, it does not exactly reduce to it. The given form $\\hat{\\theta}_{t}^{\\hat{\\lambda}}=(1-\\frac{n k_{0}}{\\sum X_{t}^{2}})^{+}X_{t}$ is indeed similar in structure to the James-Stein estimator (both being shrinkage estimators), but the James-Stein estimator typically shrinks toward the overall mean (not necessarily zero) and has a different shrinkage factor derivation. The 'closely related' description is accurate, but the 'reduces to' claim would be too strong without additional constraints.",
    "question": "In the homoscedastic case where $k(A_t)=k_0$ and $g(A_t)=g_0$ for all $t$, does the EBMLE estimator of $\\lambda$ reduce to the positive-part James-Stein estimator as claimed in the text?",
    "question_context": "Empirical Bayes Estimation in Hierarchical Normal Models\n\nConsider two functions $g,k:\\mathcal{D}\\subseteq\\mathbb{R}^{k}\\to\\mathbb{R}^{+}$ . Let $X_{1},X_{2},\\ldots,X_{n}$ be a collection of independent normal variables with means $\\theta_{1},\\theta_{2},\\ldots,\\theta_{n}$ and variances $k(A_{1}),k(A_{2}),\\dots,k(A_{n})$ , respectively. Here, as in elsewhere, we assume $A_{t}s$ are some known and possibly distinct points in $\\mathcal{D}\\subseteq\\mathbb{R}^{k}$ . That is, $$X_{t}|\\theta_{t}\\sim N(\\theta_{t},k(A_{t})),\\quad t=1,2,\\dots,n.$$ Moreover, for given $_g$ , assume $\\theta_{t}\\boldsymbol{s}$ are independent and normally distributed with mean $\\mu$ and variance $\\lambda g(A_{t})$ . That is, $$\\theta_{t}\\sim N(\\mu,\\lambda g(A_{t})),\\quad t=1,2,\\dots,n.$$ Fλrgo(Am) kB(Aa)yes’ theorem, the posterior distribution of $\\theta_{t}$ is normal with mean $\\begin{array}{r}{\\frac{\\lambda g(A_{t})}{\\lambda g(A_{t})+k(A_{t})}X_{t}+\\frac{k(A_{t})}{\\lambda g(A_{t})+k(A_{t})}\\mu}\\end{array}$ and variance $\\frac{\\lambda g(A_{t})k(A_{t})}{\\lambda g(A_{t})+k(A_{t})}$ . It is easy to see that the marginal distribution of is $N(\\mu,\\lambda g(A_{t})+k(A_{t}))$ . In this marginal distribution the quantities $\\lambda$ and $\\mu$ are unknown hyperparameters and therefore, they need to be estimated."
  },
  {
    "qid": "statistic-posneg-1372-0-0",
    "gold_answer": "FALSE. The mapping $I$ is described as an affine isomorphism of ${\\mathfrak{M}}^{1}({\\mathfrak{N}}_{0})$ onto $\\mathfrak{N}$, not $\\Re$. The context explicitly states that $I$ is a continuous bijection of ${\\mathfrak{M}}^{1}({\\mathfrak{N}}_{0})$ onto $\\mathfrak{N}$ and thus a homeomorphism, with $\\mathfrak{N}$ being a Bauer simplex. The confusion arises from the earlier mention of $\\Re$, but the correct target of $I$ is $\\mathfrak{N}$.",
    "question": "Is the mapping $I$ defined as $I:=G\\circ H^{-1}$ correctly described as an affine isomorphism from ${\\mathfrak{M}}^{1}({\\mathfrak{N}}_{0})$ onto $\\Re$ based on the given context?",
    "question_context": "Operator-Semistable Laws and Bauer Simplex Properties\n\n[Theclosure $\\bar{Z}=\\left\\{{x}\\in V^{*};~|{x}|_{2}\\leqslant1\\leqslant|{B}^{-1}{x}|_{2}\\right\\}$ of $Z$ is_compact. If $x\\in\\bar{Z}\\backslash Z$ wemusthave $|B^{-1}x|_{2}=1$ and hence $y:=B^{-1}x\\in Z$ In view of (iv) this yields $\\xi_{x}=\\xi_{B y}=\\xi_{y}\\in\\mathfrak{N}_{0}$ . Hence $x\\rightarrow\\xi_{x}$ maps $\\bar{Z}$ onto $\\mathfrak{N}_{0}$ . Since this mapping is continuous in view of (iv) the assertion follows.] <PARAGRAPH_SEPARATOR> 2. Let $h(x):=\\xi_{x}$ for all $x\\in Z$ . In view of (iv) and (vi) $h$ is a continuous bijection of $Z$ onto $\\mathfrak{N}_{0}$ . Since $Z$ is $\\pmb{\\sigma}$ -compact $\\pmb{h}$ is a Borel isomorphism. Hence $\\pmb{h}$ extends to an affine isomorphism $\\pmb{H}$ of ${\\mathfrak{M}}^{1}(Z)$ onto $\\mathfrak{M}^{1}(\\mathfrak{N}_{0})$ Taking into account (vi) the mapping $\\scriptstyle I:=G\\circ H^{-1}$ is an affine isomorphism of ${\\mathfrak{M}}^{1}({\\mathfrak{N}}_{0})$ onto $\\Re$ <PARAGRAPH_SEPARATOR> 3. Let $f\\in\\Re(V^{*})$ and $\\kappa\\in\\mathfrak{M}^{1}(\\mathfrak{N}_{0})$ . Then an easy calculation yields <PARAGRAPH_SEPARATOR> $$ \\int_{V^{*}}f d I(\\kappa)=\\int_{\\mathfrak{R}_{0}}\\left(\\int_{V^{*}}f d\\xi\\right)\\kappa(d\\xi). $$ <PARAGRAPH_SEPARATOR> But $\\Re_{0}\\ni\\xi\\to\\int_{V^{*}}f d\\xi$ is continuous and hence bounded (since $\\mathfrak{N}_{0}$ is compact in view of part 1). Hence $I$ is a continuous bijection of ${\\mathfrak{M}}^{1}({\\mathfrak{N}}_{0})$ onto $\\mathfrak{N}$ and thus a homeomorphism (since ${\\mathfrak{M}}^{1}({\\mathfrak{N}}_{0})$ is compact). <PARAGRAPH_SEPARATOR> Now ${\\mathfrak{M}}^{1}({\\mathfrak{N}}_{0})$ is a Bauer simplex with extreme boundary $\\mathfrak{F}=\\left\\{\\varepsilon_{\\xi_{x}}\\colon x\\in Z\\right\\}$ [1, Corollary I1.4.2]. Hence $\\mathfrak{N}$ is a Bauer simplex too with extreme boundary $I(\\mathfrak{F})=\\mathfrak{N}_{0}$ <PARAGRAPH_SEPARATOR> # 5. ANALYTICITY OF DENSITIES OF OPERATOR-SEMISTABLE LAWS"
  },
  {
    "qid": "statistic-posneg-493-0-1",
    "gold_answer": "FALSE. The context explains that using diffuse priors (e.g., U(0, 10¹⁵)) for cᵢ leads to poor variable selection, a phenomenon known as Lindley’s paradox. Instead, the authors use data-based priors (lognormal) to improve variable selection performance.",
    "question": "Does the model use a uniform prior for the hyperparameters cᵢ to perform variable selection effectively?",
    "question_context": "Bayesian Nonparametric Probit Regression with Variable Selection\n\nThe article describes a Bayesian approach to nonparametric probit regression with variable selection. The model uses thin-plate basis functions to approximate the regression function and employs a hierarchical Bayesian framework to perform variable and component selection. The methodology is illustrated with simulated and real datasets, showing its effectiveness in identifying significant variables and interactions."
  },
  {
    "qid": "statistic-posneg-977-0-0",
    "gold_answer": "TRUE. The formula λ1 = |A| / (Π_{i=1}^q |A_ii|) is the correct likelihood ratio statistic for testing the hypothesis of multiple independence (H1: Σ_ij = 0 for i ≠ j) in complex multivariate normal populations. This statistic compares the determinant of the full covariance matrix A to the product of the determinants of its diagonal blocks A_ii, which would be equal under the null hypothesis of independence. The derivation follows from the likelihood ratio principle, where the numerator is the maximized likelihood under the null hypothesis (block-diagonal covariance) and the denominator is the maximized likelihood under the alternative (unrestricted covariance).",
    "question": "Is the likelihood ratio statistic for testing multiple independence (H1: Σ_ij = 0 for i ≠ j) correctly given by λ1 = |A| / (Π_{i=1}^q |A_ii|), where A is the sample covariance matrix?",
    "question_context": "Likelihood Ratio Tests for Covariance Structures in Complex Multivariate Normal Populations\n\nThe covariance matrices of complex multivariate normal populations play an important role in studying the spectral density matrices of stationary Gaussian multiple time series, because certain estimates of these matrices have approximately complex Wishart distributions. In this paper we study approximations to the distributions of the likelihood ratio tests for multiple independence, sphericity and multiple homogeneity of the covariance matrices, as well as the hypothesis that the covariance matrix is equal to a specifed matrix. Using the first four moments, distributions of certain powers of the test statistics are approximated by Pearson's type I distributions. The accuracy of the approximations is found to be sufficient for practical purposes."
  },
  {
    "qid": "statistic-posneg-991-0-4",
    "gold_answer": "TRUE. Explanation: The paper provides the formula n varα(β̂) = ½ ∑(1-Pj)/{Pj log²(Pj)}, where Pj = pr(Y=2) in sample j. This is the asymptotic variance of β̂ under the true model, as derived in the context.",
    "question": "Is the asymptotic variance of β̂ under the true model given by n varα(β̂) = ½ ∑(1-Pj)/{Pj log²(Pj)}?",
    "question_context": "Regression Analysis of Ordinal Data with Variability of Classification\n\nThis paper examines the effect of assuming common classification intervals when in fact there is variability among observations. This is done in the context of the proportional hazards regression model for ordinal data discussed by MeCullagh (1980). A class of models is proposed based on the introduction of a variability of classification into the proportional hazards model. The behaviour of the proportional hazards model in the presence of such variability is examined and the application of the new class of models to four examples is discussed."
  },
  {
    "qid": "statistic-posneg-1963-3-2",
    "gold_answer": "FALSE. The paper emphasizes that the use of common random numbers is critical because it produces a strong positive correlation between $\\sum d_j(\\theta)$ and $\\sum d_j(\\theta_0)$, leading to a more accurate estimate of the likelihood ratio.",
    "question": "Is the use of common random numbers in computing $d_j(\\theta)$ and $d_j(\\theta_0)$ unnecessary for accurate likelihood estimation?",
    "question_context": "Likelihood Estimation for Truncated Gaussian Spatial Processes\n\nThe paper discusses the estimation of parameters for a truncated Gaussian spatial process. The observed data is given by $Z(x_i) = W(x_i)^+$ where $W(\\cdot)$ is a Gaussian process with mean $m$ and covariance function $\\text{cov}(W(x), W(x')) = c K_a(x - x')$. The likelihood function for the parameters $\\theta = (m, c, a)'$ is derived, and methods for approximating this likelihood are presented, including the use of common random numbers to improve estimation accuracy."
  },
  {
    "qid": "statistic-posneg-574-0-0",
    "gold_answer": "FALSE. The SSRS impurity function measures the sum of squared differences between individual slope estimates ($\\hat{\\beta}_{t i}$) and the common slope estimate ($\\hat{\\beta}_{t}$) within a node. While it does reflect the deviation of individual slopes from a common trend, it does not directly account for the covariance structure of longitudinal data (captured in $\\Lambda$) or the subject-specific random effects ($\\alpha_i$). The negative restricted log-likelihood impurity $\\mathrm{R}(t)$ is more statistically rigorous for longitudinal data as it incorporates both the mean structure ($\\mu_t$) and covariance structure ($\\Lambda_t$). SSRS is simpler but may overlook important dependencies in longitudinal measurements.",
    "question": "Is the impurity function SSRS, defined as $\\mathsf{R}(t)=\\sum_{i\\in t}(\\hat{\\beta}_{t i}-\\hat{\\beta}_{t})^{2}$, a valid measure for assessing the homogeneity of slopes within a node in the context of longitudinal data modeling?",
    "question_context": "Tree-Structured Mixed-Effects Regression Modeling for Longitudinal Data\n\nWith $n$ independent subjects, often persons in practice, let $\\left({X}_{i},~{Y}_{i}\\right)$ be observations for subject $i$ where $Y_{i}$ is a vector on $q_{i}$ responses, $y_{i1},\\ldots,y_{i q_{i}}$ , at the $q_{i}$ follow-up time points and $X_{i}$ is its corresponding covariate matrix with $p$ predictors, $\\{(x_{i11},\\ldots,x_{i q_{i}1})^{T}$ , $\\mathbf{\\Sigma}...,(x_{i1p},...,x_{i q_{i}p})^{T}\\}$ , where $i=1,\\ldots,n$ . We consider a tree-structured regression model with $q_{i}$ response values in a real space $\\Upsilon_{i}\\subset\\mathbb{R}^{q_{i}}$ , where $q_{i}\\geq2$ . <PARAGRAPH_SEPARATOR> Our goal is to provide a predictive and interpretable rule to discover various trendsover-time and the patterns of $\\mathbf{Y}$ from $\\mathbf{X}$ . Figure 1 provides a motivating example for our development. At first, there is no relationship between the response and the time variable. However, different trends clearly become evident over time after splitting by gender. As we see in this simple example, we hope to find a variety of meaningful patterns over time after recursive partitioning. <PARAGRAPH_SEPARATOR> To build our tree-based model, we define the following mixed-effects model with a random intercept and a fixed time effect as a basic model <PARAGRAPH_SEPARATOR> $$ y_{i j}=\\alpha_{i}+\\beta(\\mathrm{time})_{i j}+\\epsilon_{i j}, $$ <PARAGRAPH_SEPARATOR> where $\\alpha_{i}$ is a random effect with ${\\bf N}(\\alpha_{0},\\sigma_{a}^{2})$ , $\\beta$ is a fixed unknown parameter, and $(\\mathrm{time})_{i j}$ is corresponding times at which the measurements are taken. It is assumed that the error term $\\epsilon=(\\epsilon_{11},\\ldots,\\epsilon_{1q_{1}},\\epsilon_{21},\\ldots,\\epsilon_{2q_{2}},\\epsilon_{n1},\\ldots,\\epsilon_{n q_{n}})^{\\prime}\\sim\\mathrm{N}(0,\\Lambda),$ where $\\Lambda$ is a certain covariance matrix. The basic model allows subjects to have the same slope at the same group (node), but they can have subject-specific intercepts even though they are in the same group. If it is not enough to explain trend-over-time by linearity, the model can be extended flexibly by <PARAGRAPH_SEPARATOR> $$ y_{i j}=\\alpha_{i}+\\sum_{k=1}^{K}\\beta_{k}(\\mathrm{time})_{i j}^{k}+\\epsilon_{i j}, $$ <PARAGRAPH_SEPARATOR> where $K$ is determined by the complexity of the data, for example, $K=2$ or 3. A low-order polynomial is usually used because of simplicity and interpretability. <PARAGRAPH_SEPARATOR> # 2.2 IMPURITY FUNCTION <PARAGRAPH_SEPARATOR> An impurity measure is the most fundamental factor in constructing a tree. When constructing a regression tree for longitudinal data, the previous algorithms employed the Mahalanobis distance by Segal (1992), the generalized estimating equation by Lee et al. (2005), or the mean dependency on the response by Siciliano and Mola (2000) as impurity functions. We aim to divide subjects into several groups, each of which has its own trend over time, when the slopes are heterogeneous between terminal nodes and homogeneous within terminal nodes. For this purpose, the impurity measures are not proper. Thus, we introduce new impurity functions to discover the large heterogeneity bottom subnodes. <PARAGRAPH_SEPARATOR> It is natural to consider the negative restricted log-likelihood as an impurity function. Let $y_{i}=(y_{i1}\\dots,y_{i q_{i}})$ be the observed sequence of measurements on the ith subject and $U$ be an $n q_{i}\\times1$ matrix of time variable with $\\{n(i-1)+j\\}$ th row of $(\\mathrm{time})_{i j}$ , where $i=1,\\ldots,n$ and $j=1,\\ldots,q_{i}$ . The negative restricted log-likelihood is defined as <PARAGRAPH_SEPARATOR> $$ \\mathrm{R}(t)=0.5[\\log|\\Lambda_{t}|+\\log|U^{\\prime}\\Lambda_{t}^{-1}U|+(y_{t}-\\mu_{t})^{\\prime}\\Lambda_{t}^{-1}(y_{t}-\\mu_{t})/2], $$ <PARAGRAPH_SEPARATOR> where $y_{t}=(y_{1}^{T},\\dots,y_{n}^{T})$ , $\\begin{array}{r}{n_{t}=\\sum_{i\\in t}q_{i}}\\end{array}$ , and $\\operatorname{E}(y_{t})=\\mu_{t}$ , $\\mathrm{var}(y_{t})=\\Lambda_{t}$ for node $t$ (Diggle et al. 2002). The smaller the im purity, the better the fit. However, this function does not account for trends over time. A more intuitive measure of impurity is to use the distance of subject-specific slopes from the common slope of all the subjects at each node. Let $\\beta_{t i}$ be a slope of subject $i$ and $\\beta_{t}$ be the common slope of all the subjects at node $t$ . We define the new impurity as <PARAGRAPH_SEPARATOR> $$ \\mathsf{R}(t)=\\sum_{i\\in t}(\\hat{\\beta}_{t i}-\\hat{\\beta}_{t})^{2}. $$ <PARAGRAPH_SEPARATOR> This measures the difference between the individual slopes and the common slope from fitting the basic model, and is small when the basic model, which is linear in time, is appropriate regardless of individuals’ intercepts. Thus, this measure reflects the goodness of fit of the basic model with a random intercept and a fixed slope for time. We call the impurity function the sum of slopes residual squares (SSRS). For the more flexible complex model in Equation (2), we can define the impurity as <PARAGRAPH_SEPARATOR> $$ R(t)=\\sum_{k=1}^{K}\\sum_{i\\in t}(\\hat{\\beta}_{t k i}-\\hat{\\beta}_{t k})^{2}, $$ <PARAGRAPH_SEPARATOR> where $\\hat{\\beta}_{t k i}$ and $\\hat{\\beta}_{t k}$ are the estimates of the kth-order individual and common slopes at node $t$ . <PARAGRAPH_SEPARATOR> # 2.3 FINDING SPLIT RULES <PARAGRAPH_SEPARATOR> A natural and naive approach for finding splits is to select a split yielding the greatest reduction of impurity. The simplest way to find a split is to evaluate the reduction of impurity, $\\Delta(t)=R(t)-[R(t_{L})+R(t_{R})]$ , over all possible splits, so as to select the best cutoff point that generates the largest value of $\\Delta(t)$ , where $R(t),R(t_{L})$ , and $R(t_{R})$ are the loss functions of node $t$ , its left branch $t_{L}$ , and its right branch $t_{R}$ , respectively. Because most existing tree algorithms such as CART and C4.5 (Quinlan 1993) use an exhaustive search (ES) algorithm for finding splits, they have issues over variable selection bias (Doyle 1973; Loh and Shih 1997). In addition, they suffer from impractical problems such as the substantial computational cost and end-cut preference. To solve these problems, Loh (2002) used the residual-based approach, and Hothorn and Lausen (2003) and Shih and Tsai (2004) used maximally selected splitting statistics and asymptotic distributions for changing points. However, the latter two approaches are computationally expensive. The manner in which the ES approach is especially vulnerable to both selection bias of split variable and computational cost is investigated for longitudinal data in Section 3. <PARAGRAPH_SEPARATOR> For selecting optimal split rules, we use residuals arising from fitting the above basic model to longitudinal data, like the statistical approach in the GUIDE classification tree (Loh 2009). We use such an RA approach, to solve the well-known problems mostly found in the ES approaches. Unlike ES, RA selects split rules by investigating the randomness of residuals after fitting the basic model to the data at each node. Selecting the split variable and split point (or set) separately achieves the negligibility of selection bias as well as the substantial reduction of the computing time. RA also reacts less sensitively to extreme cases and its split selection depends on the association between the response and predictor variables rather than the distributions of predictor variables, see Cho and Hong (2008). <PARAGRAPH_SEPARATOR> To apply the RA approach to longitudinal data, we define new residuals as <PARAGRAPH_SEPARATOR> $$ r_{i}={\\hat{\\beta}}_{i}-{\\hat{\\beta}}. $$ <PARAGRAPH_SEPARATOR> These are individual components of our new impurity residuals of SSRS. In general, if the fitted model is correct, the residuals should be randomly distributed over each predictor. The randomness of residuals means that the fitted model is sufficient to explain the data at the node so that no further split is needed. For Equation (2), we consider residuals as <PARAGRAPH_SEPARATOR> $$ r_{k i}=\\hat{\\beta_{k i}}-\\hat{\\beta_{k}}. $$ <PARAGRAPH_SEPARATOR> All the combinations of residual signs compose one variable and the RA approach is applied. For instance, consider a 2-degree polynomial and assume that positives $(+)$ and negatives $(-)$ are obtained from residuals minus the median of residuals for each $k$ . Then, $++,+-,-+,$ , and −− can be treated as four categories of a variable. <PARAGRAPH_SEPARATOR> We next separate the task of split selection into two parts: selecting the split variable and then finding the split set or point. Such a separation approach was originally proposed by Loh and Vanichsetakul (1988), and several improved or extended algorithms have been developed: improved FACT (Kademan, Loh, and Vanichsetakul 1989), SUPPORT (Chaudhuri et al. 1994), Quick, Unbiased, Efficient, Statistical Tree (QUEST) (Loh and Shih 1997), CRUISE (Kim and Loh 2001), GUIDE (Loh 2002, 2009), LOTUS (Chan and Loh 2004), and STUDI (Cho and Hong 2008). <PARAGRAPH_SEPARATOR> 2.3.1 Selection of Split Variable Using Main-Effect Test. For selecting the most significant variable for splitting at each node, we adopt the selection algorithm of GUIDE, which is efficient computationally and free of selection bias. Its selection is divided into three major parts: the main effect, the local interaction, and the linear split tests."
  },
  {
    "qid": "statistic-posneg-553-3-2",
    "gold_answer": "FALSE. The function $\\eta_{u3}(x)$ is not a simple piecewise linear function. It involves a combination of linear terms and indicator functions, but the exact form is more complex than a straightforward piecewise linear function. The given formula includes terms like $-x + 2(x - \\frac{1}{4})$ and $-x + \\frac{3}{4}$, which are linear, but the overall function is not purely piecewise linear due to the interaction with the indicator functions.",
    "question": "Is the function $\\eta_{u3}(x)$ correctly defined as a piecewise linear function with breaks at $x=\\frac{1}{4}$ and $x=\\frac{3}{4}$?",
    "question_context": "Smoothing Spline ANOVA Models: Univariate and Multivariate Function Simulations\n\nWe simulated the data according to (1) using three univariate functions with different orders of smoothness. Univariate scenario 1: where $$\\eta_{u1}(x)=\\frac{1}{3}B_{20,5}(x)+\\frac{1}{3}B_{12,12}(x)+\\frac{1}{3}B_{7,30}(x),$$ $$B_{\\alpha,\\beta}(x)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\quad(0\\leqslant x\\leqslant1).$$ Univariate scenario 2: $$\\eta_{u2}(x)=10\\sin^{2}(2\\pi x)\\mathbb{1}_{(x\\leqslant\\frac{1}{2})},$$ where $\\mathbb{1}_{(x\\leqslant\\frac{1}{2})}$ is an indicator function that equals 1 for $x\\leqslant{\\frac{1}{2}}$ and equals 0 otherwise. Univariate scenario 3: $$\\eta_{u3}(x)=10\\times\\left\\{-x+2\\left(x-\\frac{1}{4}\\right)\\right\\}\\mathbb{1}_{(x\\geqslant\\frac{1}{4})}+2\\left(-x+\\frac{3}{4}\\right)\\mathbb{1}_{(x\\geqslant\\frac{3}{4})},$$ where $\\mathbb{1}_{(x\\geqslant1/4)}$ and $\\mathbb{1}_{(x\\geqslant3/4)}$ are two indicator functions that equal 1 when the conditions in parentheses are satisfied and equal 0 otherwise. We generated $x$ from a uniform distribution on [0, 1]. The generated data for three univariate functions with $\\mathrm{SNR}=1$ and three true function values are shown in Fig. 1. The log-transformed relative efficacies of the proposed method and the order-based method for the three scenarios are shown in Fig. 2. The skip method reduces to the generalized cross-validation method in the single smoothing parameter selection. The performance of the proposed method is comparable to that of the generalized cross-validation method when the signal-to-noise ratio is low, with logtransformed relative efficacies close to zero. The performance of our method is better than that of the generalized cross-validation method as the signal-to-noise ratio increases. Such behaviour may result from unstably estimated smoothing parameters based on subsamples when the signalto-noise ratio is low. Even though the order-based method performs well in some scenarios, such as univariate scenario 3, it is not reliable because of the large variability in most scenarios."
  },
  {
    "qid": "statistic-posneg-1139-0-2",
    "gold_answer": "FALSE. The XEC method is accurate only for very smooth random fields relative to the mesh size. For non-smooth fields, the Bonferroni bound is more accurate. The DLM method is designed to bridge this gap, providing accurate approximations across a wider range of smoothness levels.",
    "question": "Is the expected Euler characteristic (XEC) method accurate for all levels of smoothness in the random field?",
    "question_context": "Discrete Local Maxima Method for Gaussian Random Fields\n\nThe paper discusses methods for approximating the probability that the maximum of a D-dimensional stationary Gaussian random field sampled on a uniform rectilinear lattice exceeds a threshold t. The Bonferroni bound is accurate for non-smooth fields, while the expected Euler characteristic method is accurate for very smooth fields. The proposed Discrete Local Maxima (DLM) method bridges the gap between these two extremes by considering the expected number of discrete local maxima above the threshold."
  },
  {
    "qid": "statistic-posneg-926-1-8",
    "gold_answer": "TRUE. In this design, $\\tau_{\\mathrm{AIE}}$ simplifies to $(m-1)\\left\\{\\frac{\\rho}{m}\\tau_{\\mathrm{SPIL,1}}+\\left(1-\\rho+\\frac{\\rho}{m}\\right)\\tau_{\\mathrm{SPIL,0}}\\right\\}$. For large $m$, this approximates $(m-1)\\tau_{\\mathrm{SPILL,0}}$ times the probability of being in the control group, as the term involving $\\tau_{\\mathrm{SPIL,1}}$ becomes negligible.",
    "question": "In the multi-stage completely randomized design with clusters, is the AIE $\\tau_{\\mathrm{AIE}}$ approximately equal to the spillover effect $\\tau_{\\mathrm{SPILL,0}}$ times the probability of being in the control group, scaled by the cluster size $m-1$?",
    "question_context": "Average Direct and Indirect Causal Effects Under Interference\n\nWe study different experimental designs using the potential outcomes model. The main difference between a setting with interference and the standard Neyman–Rubin model is that potential outcomes for the ith unit may also depend on the intervention given to the jth unit with $j\\neq i$ (e.g., Hudgens & Halloran, 2008; Aronow & Samii, 2017). For convenience, we use the shorthand $Y_{i}(w_{j}=x;W_{-j})$ to denote the potential outcome we would observe for the ith unit if we were to assign the jth unit to treatment status $x\\in\\{0,1\\}$ and maintain all units, but the $j$ th at their realized treatments $W_{-j}\\in\\{0,1\\}^{n-1}$ . Expectations $E$ are over the treatment assignment only; potential outcomes are held fixed."
  },
  {
    "qid": "statistic-posneg-1573-0-0",
    "gold_answer": "FALSE. While the conditions b+<0 (negative drift for x≥0) and b−>0 (positive drift for x<0) suggest a 'pull' toward the threshold x=0 from both sides, mean-reversion in the strict sense requires additional stability conditions. The process must also ensure that the threshold crossing dynamics and the relative magnitudes of σ± and b± do not lead to divergent behavior or transience. The paper's empirical evidence supports mean-reversion in specific financial data applications, but the mathematical guarantee depends on deeper ergodic properties not fully specified by the drift signs alone.",
    "question": "In the DOBM model, the condition b+<0 and b−>0 is sufficient to guarantee mean-reverting behavior of the process ξ_t across the threshold x=0, as claimed in the financial application context. True or False?",
    "question_context": "Drifted Oscillating Brownian Motion: Threshold Diffusion Model\n\nWe consider the process, called a drifted oscillating Brownian motion (DOBM), which is the solution to the stochastic differential equation (SDE) with discontinuous coefficients for volatility (σ) and drift (b) depending on the sign of the process value. The model exhibits regime-switching behavior at the threshold x=0, with applications in finance for modeling leverage effects and mean-reversion when b+<0 and b−>0."
  },
  {
    "qid": "statistic-posneg-1773-0-2",
    "gold_answer": "TRUE. The paper provides this explicit formula for the density $c_{\\delta}(u,v)$ in the case of the diagonal section $\\delta(t) = t^{\\alpha}$, derived from the conditions and constraints outlined in the context. This formula is valid for $(u,v) \\in \\triangle$ and is part of the optimal solution for the maximum entropy copula under the given diagonal section.",
    "question": "Is the density of the maximum entropy copula for the diagonal section $\\delta(t) = t^{\\alpha}$ given by $c_{\\delta}(u,v) = \\frac{\\alpha}{4}\\frac{2-\\alpha u^{\\alpha-1}}{(1-u^{\\alpha-1})^{\\alpha/(2\\alpha-2)}}v^{\\alpha-2}(1-v^{\\alpha-1})^{(2-\\alpha)/(2\\alpha-2)}$ for $(u,v) \\in \\triangle$?",
    "question_context": "Maximum Entropy Copula with Given Diagonal Section\n\nThe paper discusses the construction of maximum entropy copulas with given diagonal sections. It presents various formulas and conditions for determining the optimal copula density that maximizes entropy under given constraints. The context includes detailed mathematical derivations and examples of copula families with specific diagonal sections."
  },
  {
    "qid": "statistic-posneg-1238-0-0",
    "gold_answer": "TRUE. The context explicitly states that the bivariate M-estimator proposed by Maronna (1976), defined by these equations, has a breakdown point of 1/3 in two dimensions. This is a known property of this estimator, as referenced in the text.",
    "question": "Is the statement 'The bivariate M-estimator defined by the equations $$ \\frac{1}{n}\\sum_{i}u_{1}(d_{i})(\\pmb{x}_{i}-\\pmb{t})=\\mathbf{0} $$ and $$ \\frac{1}{n}\\sum_{i}u_{2}(d_{i}^{2})(\\pmb{x}_{i}-\\pmb{t})(\\pmb{x}_{i}-\\pmb{t})^{\\prime}=\\pmb{V} $$ has a breakdown point of 1/3 in two dimensions' true based on the context provided?",
    "question_context": "Robustification of Forward Selection and Stepwise Algorithms via Bivariate M-Estimators\n\nIn the last section we expressed the FS and SW algorithms in terms of sample means, variances and correlations. Because of these non-robust building blocks, these algorithms are sensitive to contamination in the data. A simple robustification of these algorithms can be achieved by replacing the non-robust ingredients of the algorithms by their robust counterparts. For the initial standardization of the variables, the choices of fast computable robust center and scale measures are straightforward: median (med) and median absolute deviation (mad). As mentioned earlier, most available robust correlation estimators are computed from the $d$ -dimensional data and therefore are very time consuming (see, e.g., Rousseeuw and Leroy, 1987). On the other hand, robust univariate approaches (see, e.g., Huber, 1981) are very sensitive to correlation outliers (outliers that are not detected by univariate analyses but affect the classical correlation). <PARAGRAPH_SEPARATOR> One solution is to derive correlations among pairs of variables from an affine-equivariant bivariate covariance estimator. A computationally efficient choice is a bivariate M-estimator proposed by Maronna (1976). Maronna’s bivariate M-estimator of the location vector $t$ and scatter matrix $V$ is defined as the solution of the system of equations <PARAGRAPH_SEPARATOR> $$ \\frac{1}{n}\\sum_{i}u_{1}(d_{i})(\\pmb{x}_{i}-\\pmb{t})=\\mathbf{0} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\frac{1}{n}\\sum_{i}u_{2}(d_{i}^{2})(\\pmb{x}_{i}-\\pmb{t})(\\pmb{x}_{i}-\\pmb{t})^{\\prime}=\\pmb{V}, $$ <PARAGRAPH_SEPARATOR> where $d_{i}^{2}=({\\pmb x}_{i}-t)^{\\prime}{\\pmb V}^{-1}({\\pmb x}_{i}-t)$ , and $u_{1}$ and $u_{2}$ are functions satisfying a set of general assumptions. The estimator is affine equivariant and has breakdown point $\\frac13$ in two dimensions (Maronna, 1976). To further simplify computations, we use the coordinatewise median as the bivariate location estimate and only use the second equation to estimate the scatter matrix and hence the correlation. In this equation we used the function $u_{2}(t)=\\operatorname*{min}(c/t,1)$ with $c=9.21$ , the $99\\%$ quantile of a $\\chi_{2}^{2}$ distribution. Finally, FS and SW algorithms are implemented using these robust pairwise correlations. <PARAGRAPH_SEPARATOR> Robust stopping rule: We replace the classical correlations in the partial $F$ statistic by their robust counterparts to form a robust partial $F$ statistic. For the stopping rule we use the standard $F$ -distribution as in Section 2. Since our robust pairwise correlation estimator (due to the choice of the constant $c$ ) behaves very similar to the classical correlation estimator in the absence of outliers, the standard $F$ -distribution seems appropriate. We also verified this empirically in a small simulation study."
  },
  {
    "qid": "statistic-posneg-534-0-0",
    "gold_answer": "TRUE. The local AIC criterion balances the trade-off between goodness-of-fit (measured by $\\log(\\hat{\\sigma}^2(x))$) and model complexity (measured by $2\\operatorname{tr}(H(x))/m_x$) for the local additive estimator within each region $R_k$. The term $\\operatorname{tr}(H(x))$ approximates the effective number of parameters, penalizing overfitting. This localized approach allows adaptive choices of $h_x$ and $\\eta(x)$ that vary across regions.",
    "question": "The local AIC-type criterion $AIC(h_x, \\eta(x)) = \\log(\\hat{\\sigma}^2(x)) + 2\\operatorname{tr}(H(x))/m_x$ is used to select $h_x$ and $\\eta(x)$ by minimizing it. Is this criterion designed to balance model fit and complexity specifically for the local additive estimator within each region $R_k$?",
    "question_context": "Local Linear-Additive Estimation and Bandwidth Selection via AIC\n\nThe paper discusses a method for selecting local regularization parameters and bandwidths in nonparametric regression using an AIC-type criterion. The approach involves splitting the support of covariates into rectangular regions, fitting local additive estimators within each region, and smoothing the selected parameters. The AIC criterion is used to choose optimal bandwidths and regularization parameters both locally and globally."
  },
  {
    "qid": "statistic-posneg-203-0-2",
    "gold_answer": "TRUE. The context cites a result from [17] showing that the distribution function of Z for an EV distribution G with arbitrary dependence function D is indeed given by P(Z ≤ z) = z + z(1 - z)(D'(z)/D(z)), where D'(z) is the right derivative of D(z).",
    "question": "Does the paper claim that for a general extreme value distribution G with dependence function D, the Pickands coordinate Z = V / (U + V) has a distribution function given by P(Z ≤ z) = z + z(1 - z)(D'(z)/D(z))?",
    "question_context": "Pickands Coordinates and Bivariate Extreme Value Distributions\n\nThe paper discusses Pickands coordinates, which are used to investigate the tail behavior in bivariate extreme value models. The coordinates are defined as c := x + y < 0 and z := y / (x + y) ∈ [0,1], transforming the vector (x, y) into (c(1 - z), cz). The distribution function G of a random variable (U, V) with reverse exponential margins is max-stable and represented as G(x, y) = exp((x + y)D(y / (x + y))), where D is a dependence function. The paper explores properties of the generalized Pareto (GP) distribution W, tail equivalence between G and W, and the distribution of Pickands coordinates (Z, C) for (U, V)."
  },
  {
    "qid": "statistic-posneg-520-0-2",
    "gold_answer": "TRUE. This condition ensures that the Hamiltonian error does not grow beyond a prespecified threshold $\\Delta$, preventing numerical instability and wasted computational effort by automatically rejecting unstable paths.",
    "question": "Is the condition $\\operatorname*{max}_{z\\in S_{a:b}(z^{\\mathrm{curr}})}H(z)-\\operatorname*{min}_{z\\in S_{a:b}(z^{\\mathrm{curr}})}H(z)<\\Delta$ used to ensure numerical stability in the Hamiltonian dynamics?",
    "question_context": "Detailed Balance and Efficiency in the AAPS Algorithm\n\nProposition 1. The AAPS algorithm satisfies detailed balance with respect to the extended posterior $\\tilde{\\pi}(x,p)$ . <PARAGRAPH_SEPARATOR> Proof. Step 1 preserves $\\tilde{\\pi}$ because $p$ is independent of $x$ and is sampled from its marginal. <PARAGRAPH_SEPARATOR> It will be helpful to define the system from the point of view of starting at $z^{\\mathrm{prop}}$ . Let $a^{\\prime},b^{\\prime}$ and $c^{\\prime}$ be as defined in (5), but with $z^{\\prime}=z^{\\mathrm{prop}}$ . Then, since $S_{\\#}(z^{\\mathrm{prop}};z^{\\mathrm{curr}})=-S_{\\#}(z^{\\mathrm{curr}};z^{\\mathrm{prop}})$ , <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{-c\\leq0\\leq K-c,-c\\leq S_{\\#}(z^{\\mathrm{prop}};z^{\\mathrm{curr}})\\leq K-c\\Leftrightarrow-c^{\\prime}}\\ &{\\quad\\leq S_{\\#}(z^{\\mathrm{curr}};z^{\\mathrm{prop}})\\leq K-c^{\\prime},-c^{\\prime}\\leq0\\leq K-c^{\\prime}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Equivalently, $1(c\\in\\{0,\\ldots,K\\})1(z^{\\mathrm{prop}}\\in S_{a:b}(z^{\\mathrm{curr}}))\\equiv1(c^{\\prime}\\in$ $\\{0,\\ldots,K\\})1(z^{\\mathrm{curr}}\\in S_{a^{\\prime}:b^{\\prime}}(z^{\\mathrm{prop}}))$ . Moreover, segment invariance (5) is equivalent to $z\\in S_{a:b}(z^{\\mathrm{curr}})\\iff z\\in S_{a^{\\prime}:b^{\\prime}}(z^{\\mathrm{prop}})$ . <PARAGRAPH_SEPARATOR> The resulting chain satisfies detailed balance with respect to $\\tilde{\\pi}$ because <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\tilde{\\pi}\\left(z^{\\mathrm{curr}}\\right)\\times\\mathbb{P}\\left(c\\right)}\\ &{\\quad\\quad\\times\\mathbb{P}\\left(\\mathrm{propose}z^{\\mathrm{prop}}|c\\right)\\times\\mathbb{P}\\left(\\mathrm{accept}z^{\\mathrm{prop}}|\\mathrm{proposed},c\\right)}\\end{array} $$ <PARAGRAPH_SEPARATOR> is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\tilde{\\pi}\\left(z^{\\mathrm{curr}}\\right)\\times\\frac{1\\left(c\\in\\{0,\\ldots,K\\}\\right)}{K+1}}\\ &{\\qquad\\times\\frac{w\\left(z^{\\mathrm{curr}},z^{\\mathrm{prop}}\\right)1\\left(z^{\\mathrm{prop}}\\in S_{a:b}\\left(z^{\\mathrm{curr}}\\right)\\right)}{\\sum_{z\\in S_{a:b}\\left(z^{\\mathrm{curr}}\\right)}w\\left(z^{\\mathrm{curr}},z\\right)}}\\ &{\\qquad\\times1\\wedge\\frac{\\tilde{\\pi}\\left(z^{\\mathrm{prop}}\\right)w\\left(z^{\\mathrm{prop}},z^{\\mathrm{curr}}\\right)\\sum_{z\\in S_{a:b}\\left(z^{\\mathrm{curr}}\\right)}w\\left(z^{\\mathrm{curr}},z\\right)}{\\tilde{\\pi}\\left(z^{\\mathrm{curr}}\\right)w\\left(z^{\\mathrm{curr}},z^{\\mathrm{prop}}\\right)\\sum_{z\\in S_{a:b}\\left(z^{\\mathrm{prop}}\\right)}w\\left(z^{\\mathrm{prop}},z\\right)},}\\end{array} $$ <PARAGRAPH_SEPARATOR> which is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\frac{1(c\\in\\{0,\\ldots,K\\})1(z^{\\mathrm{prop}}\\in{\\mathcal S}_{a:b}(z^{\\mathrm{curr}}))}{K+1}}\\ &{\\qquad\\times\\left\\{\\frac{\\tilde{\\pi}(z^{\\mathrm{curr}})w(z^{\\mathrm{curr}},z^{\\mathrm{prop}})}{\\sum_{z\\in{\\mathcal S}_{a:b}(z^{\\mathrm{curr}})}w(z^{\\mathrm{curr}},z)}\\wedge\\frac{\\tilde{\\pi}(z^{\\mathrm{prop}})w(z^{\\mathrm{prop}},z^{\\mathrm{curr}})}{\\sum_{z\\in{\\mathcal S}_{a:b^{\\prime}}(z^{\\mathrm{prop}})}w(z^{\\mathrm{prop}},z)}\\right\\}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> This expression is invariant to $(z^{\\mathrm{curr}},a,b,c)\\leftrightarrow(z^{\\mathrm{prop}},a^{\\prime},b^{\\prime},c^{\\prime})$ . 口 <PARAGRAPH_SEPARATOR> Remark 1. The AAPS could be applied with a numerical integration scheme that does not have a Jacobian of 1. In such a scheme, however, the Jacobian would need to be included in the acceptance probability; moreover, a Jacobian other than 1 would lead to greater variability in $\\tilde{\\pi}$ along a path and, we conjecture, to a reduced efficiency. <PARAGRAPH_SEPARATOR> The path $S_{a:b}$ may visit parts of the statespace where the numerical solution to (2) is unstable and the error in the Hamiltonian may increase without bound, leading to wasted computational effort as large chunks of the path may be very unlikely to be proposed and accepted. Indeed it is even possible for the error to increase beyond machine precision. The no Uturn sampler suffers from a similar problem and we introduce a similar stability condition to that in Hoffman and Gelman (2014). We require that <PARAGRAPH_SEPARATOR> $$ \\operatorname*{max}_{z\\in S_{a:b}(z^{\\mathrm{curr}})}H(z)-\\operatorname*{min}_{z\\in S_{a:b}(z^{\\mathrm{curr}})}H(z)<\\Delta, $$ <PARAGRAPH_SEPARATOR> for some prespecified parameter $\\Delta$ . This criterion can be monitored as the forward and backward integrations proceed, and if at any point the criterion is breached, the path is thrown away: the proposal is automatically rejected. Segment invariance means that $z~\\in~S_{a:b}(z^{\\operatorname{curr}})\\quad\\Longleftrightarrow\\quad z~\\in~S_{a^{\\prime}:b^{\\prime}}(z^{\\operatorname{prop}})$ , so the same rejection would have occured if we created the path from any proposal in $S_{a:b}(z^{\\mathrm{curr}})$ , and detailed balance is still satisfied. For the experiments in Section 4 we found that a value of $\\Delta=1000$ was sufficiently large that the criterion only interfered when something was seriously wrong with the integration. Step 3 of the algorithm then becomes: <PARAGRAPH_SEPARATOR> 3. Create $S_{a:b}$ by leapfrog stepping forward from $(x_{0},p_{0})$ and then backward from $(x_{0},p_{0})$ . If condition (7) fails then go to 6. <PARAGRAPH_SEPARATOR> For a $d$ -dimensional Markov chain, $\\{X_{t}\\}_{t=0}^{\\infty}$ , with a stationary distribution of $\\pi$ , the asymptotic variance is $\\begin{array}{r}{V_{f}:=\\operatorname*{lim}_{n\\uparrow\\infty}}\\end{array}$ nvar $\\textstyle\\left[{\\frac{1}{n}}\\sum_{i=1}^{n}f(X_{i})\\right]$ . Thus, after burn in, var $\\begin{array}{r l}{\\left[{\\frac{1}{n}}\\sum_{i=1}^{n}f(X_{i})\\right]}&{{}\\approx}\\end{array}$ $\\mathrm{{\\itV}}_{f}/n$ . The effective sample size is the number of iid samples from $\\pi$ that would lead to the same variance: $\\mathrm{ESS}_{f}=n\\mathrm{var}_{\\pi}\\left[f\\right]/V_{f}$ . Let ${\\mathrm{ESS}}_{i}$ be an empirical estimate of the ESS for the ith component of $X$ (i.e., $f$ gives the ith component) and let $n_{\\mathrm{leap}}$ be the total number of leapfrog steps taken during the run of the algorithm. We measure the efficiency of an algorithm as <PARAGRAPH_SEPARATOR> $$ \\mathrm{Eff}=\\frac{1}{n_{\\mathrm{leap}}}\\operatorname*{min}_{i=1,\\ldots,d}\\mathrm{ESS}_{i}. $$ <PARAGRAPH_SEPARATOR> Since the leapfrog step is by far the most computationally intensive part of the algorithm, Eff is proportional to the number of effective iid samples generated per second in the worst mixing component. <PARAGRAPH_SEPARATOR> # 3.3. Choice of Weight Function <PARAGRAPH_SEPARATOR> The weight function $\\begin{array}{r}{\\ w(z,z^{\\prime})=\\tilde{\\pi}(z^{\\prime})}\\end{array}$ mentioned previously is a natural choice, and substitution into (6) shows that this leads to an acceptance probability of 1; however, it turns out not to be the most efficient choice. For example, intuitively the algorithm might be more efficient if points on the path that are further away from the current point are more likely to be proposed. To investigate the effects of the choice of $w$ we examine six possibilities:"
  },
  {
    "qid": "statistic-posneg-1488-1-0",
    "gold_answer": "FALSE. The decomposition does not aim to minimize the number of 1's in $E$ directly. Instead, it seeks to minimize the description length, which is a weighted sum of the number of types ($t$) and the number of unit entries in $R$, $C$, and $E$ ($r$, $c$, $e$). The weights are derived from the log probabilities of the parameters $\\tau,\\rho,\\gamma,\\varepsilon$. The best approximation is the one that maximizes the posterior probability, which corresponds to minimizing this weighted sum, not merely the number of 1's in $E$.",
    "question": "In the binary matrix decomposition $A=R C^{\\prime}+E$, the error matrix $E$ is used to account for discrepancies in the approximation of the original matrix $A$ by the product of $R$ and $C^{\\prime}$. Given that all arithmetic is modulo 2, is it correct to say that the decomposition aims to minimize the number of 1's in $E$ to achieve the best approximation?",
    "question_context": "Binary Matrix Decomposition for Classification Approximation\n\nThe data are represented by an $m\\times n0{-}1$ matrix $A$, where $A_{i j}=1$ means the ith individual has the jth character, and $A_{i j}=0$ means the ith individual does not have the jth character. Some entries might be missing. We seek a decomposition $A=R C^{\\prime}+E$, similar to principal component analysis. The $R$ matrix is an $m\\times t~0{-}1$ matrix specifying underlying types on individuals, $C$ is an $n\\times t0{-}1$ matrix specifying which type approximates each given class, and $E$ is a 0-1 error matrix. All arithmetic is modulo 2. A probability model guides the choice among decompositions, with parameters $\\tau,\\rho,\\gamma,\\varepsilon$ for the number of types, and Bernoulli expectations for $R$, $C$, and $E$. The log probability of $A=R C^{\\prime}+E$ is given by a weighted sum of the number of types and unit entries in $R,C,E$, which is taken as the description length. The maximum posterior probability solution minimizes this weighted sum. Markov chain Monte Carlo can explore decompositions with comparable probability, and the model can predict unknown entries in $A$."
  },
  {
    "qid": "statistic-posneg-1148-0-2",
    "gold_answer": "TRUE. The paper explicitly states that the speedup is relative to the CPU implementation with one million samples, using a single CPU core. The GPU implementation on System 2, which has a more powerful GPU, achieves up to a 70-fold speedup for the Fine-Gray model compared to this single-threaded CPU implementation.",
    "question": "The paper claims that GPU parallelization achieves a 70-fold speedup for the Fine-Gray model on System 2. Is this speedup relative to a single-threaded CPU implementation?",
    "question_context": "Sparse Survival Analysis with Competing Risks and GPU Parallelization\n\nThe paper discusses a simulation study for survival analysis with competing risks, using sparse high-dimensional data. The data generation involves sparse indicator matrices and survival times drawn from exponential distributions with regression parameters following a normal distribution multiplied by a Bernoulli sparsity indicator. The cumulative incidence function for the primary event is modeled using a unit exponential mixture, and competing event times are drawn from an exponential distribution with a rate based on the negative of the primary event's regression parameters. The study compares the performance of model fitting on CPU versus GPU implementations, demonstrating significant speedups with GPU parallelization."
  },
  {
    "qid": "statistic-posneg-1430-2-0",
    "gold_answer": "FALSE. The condition $\\int_{\\mathbb{R}^{d}}x^{\\alpha}K(x)d x=0$ for $1\\leqslant|\\alpha|\\leqslant r-1$ is related to the moment properties of the kernel, ensuring it has a certain order of vanishing moments. Symmetry, $K(-x)=K(x)$, is an additional property that is not implied by the vanishing moments condition. A kernel can satisfy the vanishing moments condition without being symmetric, and vice versa. The symmetry condition is explicitly stated as a separate requirement (i) in the context.",
    "question": "Is it true that for an order $r$ kernel $K$, the condition $\\int_{\\mathbb{R}^{d}}x^{\\alpha}K(x)d x=0$ for $1\\leqslant|\\alpha|\\leqslant r-1$ implies that the kernel is symmetric, i.e., $K(-x)=K(x)$?",
    "question_context": "Asymptotic Error Analysis of Binned Kernel Density Estimation\n\nWe develop asymptotic error formulas assuming that a kernel of some fixed order is used. Let $r\\geqslant2$ be an even integer. By definition, an order $r$ kernel $K$ satisfies the conditions (i) $K$ is symmetric, i.e., $K(-x)=K(x)$ , $x\\in\\mathbb{R}^{d}.$ , (ii) $\\int_{\\mathbb{R}^{d}}K=1,$ and (iii) $\\int_{\\mathbb{R}^{d}}|x^{\\alpha}||K(x)|d x<\\infty\\mathrm{~for~}|\\alpha|\\leqslant r.$ $\\int_{\\mathbb{R}^{d}}x^{\\alpha}K(x)d x=0\\mathrm{~for~}1\\leqslant|\\alpha|\\leqslant r-1,$ $\\int_{\\mathbb{R}^{d}}x^{\\alpha}K(x)d x\\neq0\\mathrm{~for~some~}|\\alpha|=r.$ Lemma 2.4. Suppose $p\\in\\{1,2\\}$ , $s>0$ is an integer, and $g\\in C^{s,p}(\\mathbb{R}^{d})$ . Let $K$ be a kernel of order $r$ and denote $\\mu^{\\alpha}(K)=[(-1)^{|\\alpha|}/\\alpha!]\\int_{\\mathbb{R}^{d}}y^{\\alpha}K(y)d y$ . Then, for a.e. $x\\in\\mathbb{R}^{d}.$ , $(g*K_{h})(x)=g(x)+h^{r\\wedge s}\\sum_{|\\alpha|=r\\wedge s}\\mu^{\\alpha}(K)D^{\\alpha}g(x)+h^{r\\wedge s}R(x,h),$ where $r\\wedge s$ is the smaller of the integers $r$ and $s$ and $R(\\cdot,h)\\to0$ in $L^{p}(\\mathbb{R}^{d})$ as $h\\rightarrow0$ . Further, when $r\\leqslant s.$ , the second term on the right hand side vanishes identically only if $g$ is identically zero ( for $r>s$ it of course always vanishes $b y$ (iv) above). If $D^{\\alpha}g$ is bounded for $|{\\pmb{\\alpha}}|=r\\wedge s$ , then (2.33) holds for all $x$ and $R(\\cdot,h)\\to0$ pointwise."
  },
  {
    "qid": "statistic-posneg-1425-4-2",
    "gold_answer": "TRUE. These conditions collectively ensure that the spatial and temporal dependencies in the error process do not lead to explosive behavior, maintaining stationarity. The conditions balance the spatial ($\\rho, \\theta$) and temporal ($\\phi$) effects to prevent non-stationarity.",
    "question": "Is the stationarity condition $\\rho+\\theta\\geq0, \\phi+\\rho+\\theta<1, \\rho-\\theta\\geq0$, and $\\phi-\\rho+\\theta>-1$ sufficient to ensure the stability of the spatial-temporal error process in the model?",
    "question_context": "Bayesian Analysis of Constant Returns to Scale in Growth Models with Spatial and Temporal Dependence\n\nThe paper examines the hypothesis of constant returns to scale in a growth model incorporating spatial and temporal dependencies. The key hypotheses tested are $H_{1}:\\alpha+\\beta=1$ versus $H_{2}:\\alpha+\\beta\\neq1$, using the Bayes factor $B F_{12}=\\frac{P(\\alpha+\\beta=1|y,H_{2})}{P(\\alpha+\\beta=1|H_{2})}$. The model accounts for spatial and serial correlation in disturbances, with estimation results suggesting strong evidence against constant returns to scale (Bayes factor of 68.93). The paper also discusses stationarity conditions and the impact of spatial and temporal dependencies on growth rates."
  },
  {
    "qid": "statistic-posneg-2103-0-0",
    "gold_answer": "FALSE. The context explicitly states that the variance of the second-level model depends on the explanatory variable $z_t$ (i.e., $\\theta_t \\sim N(\\mu, \\lambda(1 + z_t^2))$), making it heteroscedastic. The paper criticizes Xie et al. [9] for assuming heteroscedasticity only in the first-level model, arguing that second-level variances also tend to vary across sub-populations in multi-level models.",
    "question": "In the hierarchical model presented, the variance of the second-level model is assumed to be homoscedastic (constant across sub-populations).",
    "question_context": "Heteroscedastic Hierarchical Model and Shrinkage Estimators\n\nParametric empirical Bayes estimators in comparison with nonparametric empirical Bayes estimators are commonly used to analyze real data sets. In spite of their interesting and flexibility features, they usually involve some hyperparameters which are initiated from prior distributions. Therefore, finding an efficient method to estimate these second-interest parameters has always been main interest in the literature. Two effective methods that are popular in this area are empirical Bayes maximum likelihood estimator (EBMLE) and empirical Bayes moment estimator (EBMOM). Evaluation of the performance of these estimators, through simulation study, has been one major focus by many authors among all one can refer to Brown [2]. Recently, Xie et al. [9] proposed a class of shrinkage estimators that can be readily applied in the heteroscedastic hierarchical normal models. Their motivation in that work was to know whether it is possible to formally compare these different shrinkage estimators and identify the ‘optimal’ shrinkage estimator. They called their shrinkage estimators SURE (Stein’s unbiased risk estimate). They also established the asymptotic optimality property for the SURE estimators. <PARAGRAPH_SEPARATOR> We believe that assuming heteroscedasticity only for the first-level model variance, which was considered by Xie et al. [9], is an exception to the rule. This is generally true. Since in a multi-level model, the variance of second-level model, the marginal of more than two levels, usually has a tendency to change through sub-populations. Negligence in considering this fundamental assumption can lead to substantial bias in the estimates of the parameters. <PARAGRAPH_SEPARATOR> As an example consider the following weighted simple regression hierarchical model: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{X_{t}|\\theta_{t}\\sim N(\\theta_{t},k(z_{t})),}\\ &{\\theta_{t}-\\mu=\\beta_{0}+\\beta_{1}z_{t},}\\ &{\\beta_{0}\\sim N(0,\\lambda),}\\ &{\\beta_{1}\\sim N(0,\\lambda),}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $z_{t}$ is an explanatory variable, $\\lambda>0$ and $\\mu$ are hyperparameters and finally $k:\\mathcal{U}\\subseteq\\mathbb{R}\\to\\mathbb{R}^{+}$ is either completely known or will be known by some plug-in robust estimators. The equivalent two-level marginal model is given by: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{X_{t}|\\theta_{t}\\sim N(\\theta_{t},k(z_{t})),}\\ &{\\theta_{t}\\sim N(\\mu,\\lambda(1+z_{t}^{2})).}\\end{array} $$ <PARAGRAPH_SEPARATOR> As one can see the variances of both levels of above model depend $\\mathrm{on}z_{t}$ and therefore they are unequal. This representation is very popular in many statistical models such as linear regression models, log-linear models, association models, logistic models and in general in GLMs, which involve several explanatory variables. Therefore, the class of shrinkage estimators introduced by Xie et al. [9] has limitations in real-world applications. <PARAGRAPH_SEPARATOR> In this paper, we solve the problem considering more general cases, of course with mild conditions, and propose a class of weighted shrinkage estimators, weighted by the variance of the response variable, so-called Mean General Sure (General Sure), MGS(GS)-estimators, based on Stein’s unbiased estimate of risk. The asymptotic properties of various MGS(GS)- estimators will be established. <PARAGRAPH_SEPARATOR> Although building a good estimation of the nuisance parameters $\\lambda$ and $\\mu$ is important in their own rights, here, our ultimate goal is to estimate the unknown parameters $\\theta_{1}$ $\\mathfrak{i}_{1},\\theta_{2},\\ldots,\\theta_{n}$ based on their ‘best’ estimates and study their performance by comparing to other shrinkage estimators."
  },
  {
    "qid": "statistic-posneg-825-0-1",
    "gold_answer": "TRUE. The prior on $\\gamma$ is specified as $p(\\gamma)=w^{q_{\\gamma}}(1-w)^{q-q_{\\gamma}}$, where $q_{\\gamma}$ is the number of predictors included in the model. This formulation explicitly assumes that each predictor is included independently with probability $w$, as the prior factorizes into a product of Bernoulli probabilities for each $\\gamma_j$.",
    "question": "Does the independence Bernoulli prior on $\\gamma$ imply that the inclusion of each predictor is independent of the others, with probability $w$?",
    "question_context": "Bayesian Variable Selection in Linear Regression with Zellner's g-Prior\n\nThe most commonly used setup for variable selection in Bayesian linear regression is described as follows. We have a response vector $Y=(Y_{1},\\dots,Y_{m})^{\\top}$ and a set of potential predictors $X_{1},\\dots,X_{q}$ , each a vector of length $m$ . Every subset of predictors is identified with a binary vector $\\gamma=(\\gamma_{1},\\dots,\\gamma_{q})^{\\top}\\in\\{0,1\\}^{q}$ , where $\\gamma_{j}=1$ if $X_{j}$ is included in the model and $\\gamma_{j}=0$ otherwise. For every $\\gamma$ , we have a model given by $Y=1_{m}\\beta_{0}+X_{\\gamma}\\beta_{\\gamma}+\\epsilon,$ where $1_{m}$ is the vector of m 1’s, $X_{\\gamma}$ is the design matrix whose columns consist of the predictor vectors corresponding to $\\gamma,\\beta_{\\gamma}$ is the vector of coefficients for that subset, and $\\epsilon\\sim\\mathcal{N}_{m}(0,\\sigma^{2}I)$ . For this setup, the unknown parameter is $\\theta=(\\gamma,\\sigma,\\beta_{0},\\beta_{\\gamma})$ , which includes the indicator of the subset of variables that go into the regression model. The prior on $\\theta$ is a hierarchy in which we first select the variables that go into the regression model, then a “noninformative prior” is given to $(\\sigma^{2},\\beta_{0})$ , and given $\\gamma$ and $\\sigma$ , we choose $\\beta_{\\gamma}$ from some proper distribution. The specific instance of this model that we will consider is indexed by two hyperparameters, $w\\in(0,1)$ and $g>0$ , and is given in detail as follows: $\\mathrm{given}\\gamma,\\sigma,\\beta_{0},\\beta_{\\gamma},\\quad Y\\sim\\mathcal{N}_{m}(1_{m}\\beta_{0}+X_{\\gamma}\\beta_{\\gamma},\\sigma^{2}I),$ $\\begin{array}{r}{\\operatorname{given}\\gamma,\\sigma,\\quad\\beta_{\\gamma}\\sim\\mathcal{N}_{q_{\\gamma}}\\big(0,g\\sigma^{2}(X_{\\gamma}^{\\top}X_{\\gamma})^{-1}\\big),}\\end{array}$ $(\\sigma^{2},\\beta_{0})\\sim p(\\beta_{0},\\sigma^{2})\\propto1/\\sigma^{2},$ $\\gamma\\sim p(\\gamma)=w^{q_{\\gamma}}(1-w)^{q-q_{\\gamma}}.$ The prior on $\\gamma$ given by (4.1d) is the so-called independence Bernoulli prior, in which every variable goes into the model with probability $w$ , independently of all the other variables. In (4.1b), $\\begin{array}{r}{q_{\\gamma}=\\sum_{j=1}^{q}\\gamma_{j}}\\end{array}$ is the number of predictors that go in the regression, and the prior on $\\beta_{\\gamma}$ is Zell ner’s $g$ -prior (Zellner 1986). Because $(\\sigma^{2},\\beta_{0})$ is given an improper prior (line (4.1c)), the prior on $\\theta$ is improper; however, it turns out that the posterior distribution of $\\theta$ is proper. Models of the type (4.1) were introduced by Mitchell and Beauchamp (1988) and have been studied in dozens of papers; see Liang et al. (2008) for a review and recent developments."
  },
  {
    "qid": "statistic-posneg-1792-1-2",
    "gold_answer": "FALSE. The bandwidth b is selected using a robust two-fold procedure, as indicated by the minimization of the weighted sum of absolute deviations in the validation set, which incorporates robust weights ω_2(Y_i).",
    "question": "Is the bandwidth b in the nonparametric estimation of selection probabilities selected using a non-robust procedure?",
    "question_context": "Robust Semiparametric Estimation with Missing Covariates and Outliers\n\nThe paper discusses robust semiparametric estimation methods for handling missing covariates and outliers in data. It introduces a weight function ω(X,Z) and a robust nonparametric estimator for selection probabilities. The context includes formulas for estimating parameters and handling contaminated data with outliers."
  },
  {
    "qid": "statistic-posneg-1474-0-0",
    "gold_answer": "FALSE. The formula for $P(F|K)$ is not correctly derived as presented. The correct derivation should be $P(F|K) = \\sum_{i} P(F|K \\cap S_{i}) P(S_{i}|K)$, where $P(S_{i}|K)$ is given by the recognition probability formula. The provided formula incorrectly uses $P(F \\cap K|S_{i})$ instead of $P(F|K \\cap S_{i})$. The correct form aligns with the law of total probability and the Bayesian framework described in the context.",
    "question": "Is the formula for $P(F|K)$ correctly derived from the given probabilities and Bayes' theorem, assuming the context's model setup?",
    "question_context": "Bayesian Recognition and Prediction in Statistical Modeling\n\n$\\bullet$ Let $F$ be some future prediction about the present object $o$ $\\bullet$ Let $K$ be present knowledge of the object. $\\bullet$ Let $S_{1},S_{2},\\ldots,S_{n}$ be the remembered species. <PARAGRAPH_SEPARATOR> The knowledge $K$ would be generated by an object in species $S_{i}$ with probability $P(K|S_{i})$ Prior to knowing $K$ , an object belongs to species $S_{i}$ with probability $P(O\\in S_{i}\\}$ <PARAGRAPH_SEPARATOR> With these probabilities, we use Bayes theorem to derive Recognition probabilities: <PARAGRAPH_SEPARATOR> $$ P\\{O\\in S_{i}|K\\}=P(K|S_{i})P\\{O\\in S_{i}\\}/{\\sum_{i}}P(K|S_{i})P\\{O\\in S_{i}\\}. $$ <PARAGRAPH_SEPARATOR> Prediction probabilities: <PARAGRAPH_SEPARATOR> $$ P(F|K)=\\sum_{i}{P(F\\cap K|S_{i})P\\{O\\in S_{i}\\}}/{\\sum_{i}{P(K|S_{i})P\\{O\\in S_{i}\\}}}. $$ <PARAGRAPH_SEPARATOR> Recognition leads to accurate prediction when there is enough data known about the present object to confidently identify its species (that is $P\\{O\\in S_{i}^{*}|K\\}$ is close to 1 for some recognised species $S_{i}^{*}$ ), and when we are relatively sure about the feature $F$ given the identified object (that is, $P\\{F\\vert K\\cap S_{i}^{*}\\}$ is close to zero or one). <PARAGRAPH_SEPARATOR> Recognition as described above corresponds to a familiar scheme for statistical modelling. Our experience is expressed as a set of probability models $S_{i}$ eachof which specifies a probability distribution of observable quantities. The present object is some set of available observations. We recognise the object as coming from one of the models with various probabilities. We predict other properties of the object using the probability distributions of those properties based on the likely models given the present data."
  },
  {
    "qid": "statistic-posneg-1038-3-0",
    "gold_answer": "TRUE. The function g1(xi) indeed represents the probability of observing a zero count in the zero-inflated portion of the model. The formula φ / (φ + exp(xi^Tβ)) is derived from the zero-inflation component, where φ is the dispersion parameter of the negative binomial distribution and exp(xi^Tβ) relates to the Poisson mean in the count component. This formulation ensures that g1(xi) is bounded between 0 and 1, as required for a probability.",
    "question": "In the zero-inflated negative binomial model, the function g1(xi) represents the probability of observing a zero count. Is it correct that g1(xi) = φ / (φ + exp(xi^Tβ)) where φ is the dispersion parameter and β is the coefficient vector?",
    "question_context": "Zero-Inflated Negative Binomial Regression: Influence Diagnostics\n\nThe authors derive formulas for obtaining the second-order partial derivatives of the log-likelihood function in a zero-inflated negative binomial (ZINB) regression model. The model includes functions for zero-inflation probabilities (g1, g2), a combined hazard function (h), and logistic components (v). The perturbation schemes (case-weight, explanatory variable, and simultaneous) are used to assess the influence of observations on parameter estimates."
  },
  {
    "qid": "statistic-posneg-647-0-0",
    "gold_answer": "TRUE. The paper explicitly states that Pearson & Chandrasekar found Thompson's criterion to be effective only when there is essentially one outlier, contradicting Thompson's belief that it could handle multiple outliers. This conclusion is based on their critical examination of Thompson's method.",
    "question": "Is the statement 'Thompson's criterion for detecting outliers is effective only when there is essentially one outlier and not many' supported by the findings of Pearson & Chandrasekar as mentioned in the paper?",
    "question_context": "Distribution of the Extreme Deviate and its Studentized Form\n\nThe paper discusses the distribution of the extreme deviate from the sample mean and its studentized form, focusing on the case where both population mean (μ) and standard deviation (σ) are unknown. The sample estimates are given by the sample mean (x̄) and sample standard deviation (ϑ). The paper explores various test criteria for outliers, including the modified McKay criterion and Thompson's criterion, and introduces a new method of studentization different from Thompson's. The distribution of the extreme deviate is derived using G-functions, and approximations for the probability integral Pₙ(u) are provided."
  },
  {
    "qid": "statistic-posneg-164-0-1",
    "gold_answer": "TRUE. The tangent space $\\dot{\\mathcal{P}}_{S}$ for $S$ is indeed represented as $\\{\\dot{l}_{S}b:b\\in L_{2}^{0}(S)\\}$, where the score operator $\\dot{l}_{S}$ is defined by the integral $\\int_{0}^{U^{c}(\\theta_{0})}R b(s)d M(s)$. This is derived from one-dimensional submodels for $S$ and is a key component in the efficient score calculation.",
    "question": "The tangent space for $S$ in the semiparametric model is given by $\\{\\dot{l}_{S}b:b\\in L_{2}^{0}(S)\\}$, where $\\dot{l}_{S}b=\\int_{0}^{U^{c}(\\theta_{0})}R b(s)d M(s)$. Is this representation correct according to the paper?",
    "question_context": "Efficient Score Estimation in Semiparametric Accelerated Failure Time Models\n\nThe paper discusses efficient score estimation in semiparametric accelerated failure time models, focusing on the derivation of efficient scores and tangent spaces. The model is defined by the distribution $P_{\theta,g,h}^{*}$ with density $\\frac{d P_{\theta,g,h}^{*}}{d\\nu}(t,z)=e^{(\\theta-\\theta_{0})^{\\prime}z}g\\{e^{(\\theta-\\theta_{0})^{\\prime}z}t\\}h(z)$, where $\\theta_{0}$ is the true parameter value. The efficient score for $\\theta$ is derived using orthogonal projections onto tangent spaces for $S$ and $h$."
  },
  {
    "qid": "statistic-posneg-1980-0-0",
    "gold_answer": "TRUE. The context states that at $\\tau=0.25,0.5$ and 0.75, $\\mathrm{GAL}_{\\gamma}$ identifies 47, 40 and 44 TFs, respectively. This shows that the model identifies the highest number of TFs at the 0.25 quantile level compared to the other two quantile levels.",
    "question": "Is the statement 'The quantile-varying coefficient model identifies more TFs at the 0.25 quantile level compared to the 0.5 and 0.75 quantile levels' supported by the results in the context?",
    "question_context": "Quantile-Varying Coefficient Model for Gene Expression Analysis\n\nIn this section, we apply the proposed method to a subset of the microarray time-course gene expression data set (Spellman et al., 1998). Luan and Li (2003) identified 297 cell cycle regulated genes using a model-based method. Wang et al. (2007a,b) used the ChIP data of Lee et al. (2002), and derived the binding probabilities for these 297 cell-cycle-regulated genes for a total of 96 transcription factors (TFs). By applying the gSCAD method, Wang et al. (2008) identified 71 TFs that might be related to the mean expression patterns of the 297 genes. <PARAGRAPH_SEPARATOR> To identify the TFs that have nonzero time-dependent effects on the quantiles of the gene expression, we consider the following model <PARAGRAPH_SEPARATOR> $$ y_{i t}=\beta_{0}(t,\tau)+\\sum_{k=1}^{96}\\beta_{k}(t,\tau)x_{i k}+\\epsilon_{i t}(\\tau), $$ <PARAGRAPH_SEPARATOR> where $y_{i t}$ denotes the log-expression level for gene $i$ at time t (in days) during the cell-cycle process for $i=1,\\ldots,297$ and $t=0$ , 7, 14, . . . , 119, and $\\beta_{k}(t,\\tau)$ is the transcription effect of the kth TF on the τ th quantile of $y_{i t}$ . We focus on three quantiles levels, $\\tau=0.25$ , 0.5 and 0.75. Using the SIC criterion in Section 2.3, the tuning parameter $k_{n}$ is selected as 2, and $\\lambda_{n}$ is selected as 12.5, 13.5 and 15 at the three quartiles, respectively. <PARAGRAPH_SEPARATOR> At $\\tau=0.25,0.5$ and 0.75, $\\mathrm{GAL}_{\\gamma}$ identifies 47, 40 and 44 TFs related to yeast cell-cycle processes, respectively, including 15, 15 and 16 of the 21 known and experimentally verified cell-cycle-related TFs. As an illustration, Fig. 2 shows the estimated time-dependent transcriptional effects of two known TFs that are selected by our method. The shaded areas correspond to the $90\\%$ point-wise bootstrap confidence bands for the median estimates obtained under the selected model with 200 bootstrap runs. The effects of MBP1 have similar profiles at three quartiles, and three quantile coefficient curves exhibit differences mainly for $t\\in[40,80]$ . The SWI5 shows very different effects at three quartiles across the main support of t except for $t\\geq100$ , and this suggests possible population heterogeneity. <PARAGRAPH_SEPARATOR> Table 5 summarizes the results of the proposed method at $\\tau=0.25$ , 0.5, 0.75 and the results of $g S C A D$ at the mean for identifying the 21 known TFs. The top 13 TFs are identified at all three quartiles and at the mean, the next four TFs are identified at mean and at least one of the quartiles, GCR1 and LEU3 are identified at mean but missed by the quantile regressions, GCN4 is identified at $\\tau=0.75$ but missed by the mean, and CBF1 is missed by all methods. In the following, we will focus on the cases on which the mean and the quantile methods have discrepancy."
  },
  {
    "qid": "statistic-posneg-1111-0-1",
    "gold_answer": "FALSE. The condition $A V \\geqslant A V A'$ is related to the Loewner partial ordering and does not inherently require $A$ to be symmetric. It ensures that the estimator $A Y$ is admissible under the Gauss-Markov model, but symmetry of $A$ is not a necessary condition for this inequality to hold.",
    "question": "Does the condition $A V \\geqslant A V A'$ imply that the matrix $A$ is symmetric?",
    "question_context": "Admissible Linear Estimation in General Gauss-Markov Models\n\nThe paper investigates the validity of admissible linear estimators in general Gauss-Markov models, focusing on conditions under which estimators remain valid when the dispersion matrix is incorrectly specified. Key formulas include conditions for admissibility and representations of estimators."
  },
  {
    "qid": "statistic-posneg-803-0-1",
    "gold_answer": "FALSE. While the mean population size does follow an exponential form e^{(λ-μ)t} when λ ≠ μ, this is not equivalent to the deterministic model in all cases. Specifically, when λ = μ, the mean population size remains constant (equal to the initial size N₀), but the variance grows linearly with time, indicating significant stochastic fluctuations not captured by a deterministic model. The deterministic and stochastic models only align in mean behavior when λ ≠ μ.",
    "question": "The mean population size in the birth-and-death process always follows the deterministic exponential growth model, regardless of the values of λ and μ. Is this statement correct?",
    "question_context": "Stochastic Birth-and-Death Process in Population Growth\n\nThe context discusses a stochastic model for population growth, specifically a birth-and-death process where individuals can reproduce or die with certain probabilities. The model is characterized by birth rate λ and death rate μ, and it explores the probability distribution of population size over time, the mean and variance of the population, and conditions for population extinction."
  },
  {
    "qid": "statistic-posneg-551-0-1",
    "gold_answer": "TRUE. The skew-Gaussian target (π_SG) is defined as a product of one-dimensional skew-Gaussian distributions, each of which includes a standard Gaussian component (exp(-x_i^2 / (2σ_i^2))) multiplied by the cumulative distribution function Φ(αx_i / σ_i), where α = 3. This introduces skewness into the distribution.",
    "question": "Does the skew-Gaussian target (π_SG) include a standard Gaussian component multiplied by the cumulative distribution function (Φ) of a standard normal variable?",
    "question_context": "Comparison of Probability Density Functions in MCMC Sampling\n\nThe paper investigates performance across various target densities with simple functional forms and different dimensions. The targets include Gaussian (π_G), logistic (π_L), skew-Gaussian (π_SG), and a modified Rosenbrock (π_MR) density. The paper explores different scale parameters (σ_i) and their sequences, with a focus on the largest ratio of length scales (ξ). The targets are designed to benchmark MCMC algorithms, with special attention to the stability and performance of HMC and other samplers."
  },
  {
    "qid": "statistic-posneg-1543-0-2",
    "gold_answer": "FALSE. While the paper acknowledges that finding the shortest Hamiltonian path (required for $T_{m,n}$) is NP-complete, it proposes heuristic algorithms like Kruskal's method as a practical solution. However, the computational feasibility for high-dimensional data depends on the efficiency of these heuristics, which may not always guarantee optimal solutions. The paper discusses the success of Kruskal's algorithm in this context, but it does not universally guarantee computational feasibility for all high-dimensional scenarios, especially as $N$ grows large.",
    "question": "The computation of $T_{m,n}$ for multivariate data requires solving an NP-complete problem, but the paper suggests that heuristic algorithms like Kruskal's method are sufficient for practical purposes. Is this assertion accurate, and does it imply that the test is computationally feasible for high-dimensional data?",
    "question_context": "Distribution-Free Two-Sample Run Test for High-Dimensional Data\n\nThe paper discusses a distribution-free two-sample run test applicable to high-dimensional data, where the test statistic $T_{m,n}$ generalizes the univariate run test to higher dimensions while retaining the distribution-free property. Under the null hypothesis $H_0$, the exact null distribution of $T_{m,n}$ matches that of the univariate run statistic, given by specific combinatorial probabilities. For large samples, the asymptotic distribution of $T_{m,n}$ is derived, showing convergence to a normal distribution under certain conditions. The computation of $T_{m,n}$ involves solving a shortest Hamiltonian path problem, which is addressed using heuristic algorithms like Kruskal's method."
  },
  {
    "qid": "statistic-posneg-1653-0-1",
    "gold_answer": "TRUE. The BETEL approach involves specifying a prior distribution for the target parameter $\\mu$ as part of the Bayesian framework. In the paper, two versions of the BETEL estimator are considered: $\\hat{\\mu}_{\\mathrm{BETEL,1}}$ with a weakly informative prior $\\mu\\sim t_{3}(210,1)$ and $\\hat{\\mu}_{\\mathrm{BETEL},2}$ with a more informative prior $\\mu\\sim N(210,1)$. The prior specification is a key component of the Bayesian approach and influences the posterior inference.",
    "question": "Does the Bayesian exponentially tilted empirical likelihood (BETEL) approach require specifying a prior distribution for the target parameter $\\mu$?",
    "question_context": "Bayesian Doubly Robust Estimation with Exponentially Tilted Empirical Likelihood\n\nThe paper discusses a Bayesian approach to doubly robust estimation using exponentially tilted empirical likelihood. The models involve propensity score estimation via logistic regression and outcome regression via linear models. The target parameter is the population mean of the outcome variable, estimated using a doubly robust estimator that combines inverse probability weighting and regression adjustment. The paper compares different Bayesian methods, including the Bayesian exponentially tilted empirical likelihood (BETEL) approach and the method proposed by Saarela et al. (2016)."
  },
  {
    "qid": "statistic-posneg-775-0-0",
    "gold_answer": "FALSE. The formula and calculation are not clearly justified in the context. The context mentions the mean of the y's as $(500+800)$ and the mean of the $\\yen8$ as $(691\\div900)$, but the exact derivation of $691\\div200=3\\cdot455$ is not explained or linked to any specific data in the tables. The context lacks clarity on how this mean is derived from the given data.",
    "question": "Is the formula for the mean given as $\\mathtt{=m e a n=691\\div200=3\\cdot455}$ correctly calculated based on the provided context?",
    "question_context": "Statistical Moments and Correlation Table Analysis\n\nThe context discusses the calculation of statistical moments and the use of correlation tables. It includes various formulas and tables to compute means, standard deviations, and moments. The text mentions the formation of Table V from Table II and the calculation of xy-moments and standard deviations. The example provided does not adjust the moments due to insufficient evidence of high contact in the row totals."
  },
  {
    "qid": "statistic-posneg-699-0-0",
    "gold_answer": "FALSE. The model is specified with five distinct regimes, but the question incorrectly assumes that each regime has different autoregressive coefficients and error variances. The formula shows that each regime has its own set of autoregressive coefficients (φ_k1 to φ_k5) and error variances (σ1 to σ5), which are indeed different across regimes. However, the question's phrasing suggests doubt about this specification, which is actually correct as per the given formula.",
    "question": "Is the piecewise autoregressive model described in the context correctly specified to have five distinct regimes with different autoregressive coefficients and error variances?",
    "question_context": "Bayesian Mixtures of Autoregressive Models for Time Series Segmentation\n\nThis section analyzes 50 time series, each containing 2000 observations, generated from the piecewise autoregressive model described by $$y_{t}=\\left\\{\\begin{array}{l l}{\\sum_{k=1}^{6}\\phi_{k1}y_{t-k}+\\sigma_{1}\\epsilon_{t}^{(1)}}&{\\mathrm{for~}1\\leq t\\leq200}\\ {\\sum_{k=1}^{6}\\phi_{k2}y_{t-k}+\\sigma_{2}\\epsilon_{t}^{(2)}}&{\\mathrm{for~}201\\leq t\\leq1000}\\ {\\sum_{k=1}^{6}\\phi_{k3}y_{t-k}+\\sigma_{3}\\epsilon_{t}^{(3)}}&{\\mathrm{for~}1001\\leq t\\leq1300}\\ {\\sum_{k=1}^{6}\\phi_{k4}y_{t-k}+\\sigma_{4}\\epsilon_{t}^{(4)}}&{\\mathrm{for~}1301\\leq t\\leq1600}\\ {\\sum_{k=1}^{6}\\phi_{k5}y_{t-k}+\\sigma_{5}\\epsilon_{t}^{(5)}}&{\\mathrm{for~}1601\\leq t\\leq2000.}\\end{array}\\right.$$"
  },
  {
    "qid": "statistic-posneg-569-0-0",
    "gold_answer": "TRUE. The conditions of Theorem 4, namely $\\delta_{n}^{\\infty}=o_{\\mathrm{p}}(r_{n})$, $\\delta_{n}^{\\infty}\\{B_{t}(\theta_{0})\\}=o_{\\mathrm{p}}(r_{n}^{1/2})$, and $\\gamma_{n}^{\\infty}\\{B_{t}(\theta_{0})\\}=o_{\\mathfrak{p}}(r_{n})$, ensure that the total variation distance between the approximate and exact posteriors converges to zero in probability as $n\rightarrow\\infty$. This implies that the approximate posterior $\tilde{\\pi}(\theta\\mid y)$ becomes indistinguishable from the exact posterior $\\pi(\theta\\mid y)$ in the limit, validating the use of the approximate posterior for large samples.",
    "question": "Is it true that the total variation distance between the approximate and exact posteriors, $d_{\\mathrm{TV}}\\{\tilde{\\pi}(\theta\\mid y),\\pi(\theta\\mid y)\\}$, tends to zero as $n\to\\infty$ under the conditions of Theorem 4?",
    "question_context": "Asymptotic Validity of Approximate Bayesian Inference\n\nThe Wald and score test statistics, $W_{n}$ and $S_{n}$ , are asymptotically equivalent to the likelihood ratio test, so under H0, all three statistics have limiting distribution χp2−q. Under the conditions of Theorem 3, $I(\theta_{0})$ is consistently estimated by $r_{n}^{-1}\tilde{J}_{n}(\tilde{\theta}_{n})$ , so the approximate Wald and score test statistics $\tilde{W}_{n}$ and $\boldsymbol{\tilde{S}_{n}}$ are also asymptotically equivalent to $\\Lambda_{n}$ . <PARAGRAPH_SEPARATOR> # 2·3. Approximate Bayesian inference <PARAGRAPH_SEPARATOR> We now consider the approximate posterior $\tilde{\\pi}(\theta\\mid y)\\propto\tilde{L}_{n}(\theta;y)\\pi(\theta)$ , where we suppose that the prior is such that log $\\pi(\\cdot)$ is three times differentiable. Under the same conditions used to show asymptotic correctness of maximum likelihood inference, the total variation distance between the approximate and exact posteriors, <PARAGRAPH_SEPARATOR> $$ d_{\\mathrm{TV}}\\{\tilde{\\pi}(\theta\\mid y),\\pi(\theta\\mid y)\\}=\\frac{1}{2}\\int_{\\Theta}\\left|\tilde{\\pi}(\theta\\mid y)-\\pi(\theta\\mid y)\right|\\mathrm{d}\theta, $$ <PARAGRAPH_SEPARATOR> tends to zero as $n\to\\infty$ . <PARAGRAPH_SEPARATOR> THEOREM 4. Suppose that $\\delta_{n}^{\\infty}=o_{\\mathrm{p}}(r_{n})$ and there exists $t>0$ such that $\\delta_{n}^{\\infty}\\{B_{t}(\theta_{0})\\}=o_{\\mathrm{p}}(r_{n}^{1/2})$ and $\\gamma_{n}^{\\infty}\\{B_{t}(\theta_{0})\\}=o_{\\mathfrak{p}}(r_{n})$ . Then $d_{\\mathrm{TV}}\\{\tilde{\\pi}(\theta\\mid y),\\pi(\theta\\mid y)\\}=o_{\\mathsf{p}}(1)$ as $n\rightarrow\\infty$ . <PARAGRAPH_SEPARATOR> If the Bernstein–von Mises theorem (van der Vaart, 1998, Ch. 10) holds for the exact posterior distribution, under the conditions of Theorem 4 it will also hold for the approximate posterior distribution $\tilde{\\pi}(\theta\\mid y)$ . In that case, credible regions formed from the approximate posterior distribution will also be valid confidence sets."
  },
  {
    "qid": "statistic-posneg-1995-1-3",
    "gold_answer": "FALSE. While the formula includes a term for the correlation $\\rho_{A}$ between AUCs, it does not fully account for the potential misspecification of this parameter, which can lead to significant power loss or unnecessary sample size increases. The paper recommends assuming $\\rho_{A}=0$ for conservative sample sizes, but this does not address the potential variability in the true correlation, which can adversely affect the study's power.",
    "question": "Does the initial sample size formula $M_{K}=\\lambda N_{K}=\\frac{\\delta_{\\alpha\\beta,\\underline{{{\\sf g}}}}^{2}}{\\delta_{\\alpha\\beta,\\mathrm{f}}^{2}}\\frac{\\{\\tau_{1-\\alpha/2}\\sqrt{2(1-\\rho_{A})\\tilde{V}_{0}}+z_{\\beta}\\sqrt{\\tilde{V}_{1}+\\tilde{V}_{2}-2\\rho_{A}\\sqrt{\\tilde{V}_{1}\\tilde{V}_{2}}}\\}^{2}}{(\\Omega_{1}^{A}-\\Omega_{2}^{A})^{2}}$ correctly account for the correlation between AUCs?",
    "question_context": "Sample Size Recalculation in Sequential Diagnostic Testing\n\nThe paper discusses the recalculation of sample sizes in sequential diagnostic testing, focusing on reducing dependence on prespecified correlation parameters by updating sample sizes at interim analyses. It provides formulas for initial and recalculated sample sizes, stopping rules, and discusses the large sample properties of the adaptive procedure."
  },
  {
    "qid": "statistic-posneg-1722-3-3",
    "gold_answer": "TRUE. The paper states that in the asymptotic case, A = 0, and the conditions (4.1) reduce to those of the asymptotic case (2.2) and (3.1). This shows the consistency between the finite sample and asymptotic formulations when sampling variability is ignored.",
    "question": "Is it correct that in the finite sample case, the confidence region Gγ(Y) reduces to the asymptotic case when A = 0?",
    "question_context": "Disconnected Confidence Regions in Nonlinear Calibration\n\nThe paper discusses the formation of disconnected confidence regions in nonlinear calibration, focusing on both asymptotic and finite sample cases. In the asymptotic case, the confidence region for ξ is defined as Cγ(Y) = {ξ: fY(ξ) ≤ λ}, where fY is given and λ is the γ-fractile of the χ²(2) distribution. For the region to be disconnected, there must be two local minima ξi for fY with fY(ξi) < λ. In the finite sample case, the model Y = BX + ε is considered, with an exact γ-level confidence region Gγ(Y) defined similarly but incorporating sampling variability. The paper also provides geometric interpretations and necessary conditions for disconnected regions, including constraints on coefficients ci."
  },
  {
    "qid": "statistic-posneg-38-0-2",
    "gold_answer": "FALSE. The first moment μ₁(d) is actually (1/4) * C(n,3), not (3/16). The value (3/16) * C(n,3) corresponds to the second central moment (variance). The expectation arises because each triad has a 1/4 probability of being circular under random preferences, and there are C(n,3) triads.",
    "question": "Is the first moment μ₁(d) correctly given as (3/16) * C(n,3), where C(n,3) is the binomial coefficient?",
    "question_context": "Distribution of Circular Triads in Paired Comparisons\n\nM. G. Kendal & B. Babington Smith (l940) have discussed the 'method of paired comparisons' for investigating preferences. Suppose we are given n objeots $\\pmb{A},...,\\pmb{K}$ , and an observer ia asked to choose betwean every pair. If A is preferred to $\\pmb{B}$ wewrite $\\pmb{A}\\rightarrow\\pmb{B}$ . If the observer is not completely consistent, either because of his own ineffciency or because the objects are not really capable of being ranked in reapect of the quality under consideration, he may make preferences of the type $A\\to B\\to C\\to A$ ,andwe call this an inconsistent or circular triad. Write d for the number of circular triads in a given erperiment. Then Kendall & Babington Smith show that\n\n$$\n\\begin{array}{l}{\\displaystyle\\xi=1-\\frac{24d}{n^{3}-n}\\quad(n~\\mathrm{odd})}\\ {\\displaystyle=1-\\frac{24d}{n^{3}-4n}\\quad(n~\\mathrm{even})}\\end{array}\n$$\n\nmay be regarded as & ‘coefficient of consistence' and lies between 0 and 1, being capable of attaining boththeselimite.\n\nNow suppose that each comparison is made at random so that there are equal chances that $\\pmb{A}\\rightarrow\\pmb{B}$ and $\\pmb{B}\\rightarrow\\pmb{A}$ .The distribution of $\\pmb{d}$ in then of interest. They calculate this distribution exactly for $\\pmb{n=2},...,7$ and conjeoture that ite moments are given by\n\n$$\n\\begin{array}{l}{{\\mu_{1}^{\\prime}=\\displaystyle\\frac{1}{4}\\left(\\displaystyle\\frac{n}{3}\\right),}}\\ {{\\mu_{1}=\\displaystyle\\frac{3}{16}\\left(\\displaystyle\\frac{n}{3}\\right),}}\\ {{\\mu_{1}=\\displaystyle-\\frac{3}{32}\\left(\\displaystyle\\frac{n}{3}\\right)(n-4),}}\\ {{\\mu_{4}=\\displaystyle\\frac{3}{256}\\left(\\displaystyle\\frac{n}{3}\\right)\\left\\langle9\\left(\\displaystyle^{n-3}_{3}\\right)+39\\left(\\displaystyle^{n-3}_{2}\\right)+9\\left(\\displaystyle^{n-3}_{1}\\right)+7\\right\\rangle,}}\\end{array}\n$$\n\nthese being polynomials in n which agree with their numerical calculations for ${\\mathfrak{n}}=\\mathbf{2},...,\\mathbf{7}$ They albo conjecture that the distribution tends to normality when $\\pmb{\\mathscr{n}}$ increases. In the present note we prove these statemente."
  },
  {
    "qid": "statistic-posneg-1439-0-1",
    "gold_answer": "FALSE. While Lemma 1 shows that ½L_T,N converges in probability to ½L_T(ξ) as N→∞ for fixed T, the convergence rate isn't necessarily √N. The paper mentions this involves technical results from Jacod (1998) and the convergence depends on the behavior of the Skew Brownian motion transform. The expression with √N terms appears in the intermediate calculations, but the actual convergence of the local time estimator is more complex and not uniformly characterized in T.",
    "question": "Does the discretized local time estimator L_T,N converge to the true local time L_T(ξ) at rate √N for fixed T, as suggested by the expression involving the sum of indicator functions and jumps?",
    "question_context": "Maximum Likelihood Estimation for Discontinuous Diffusion Processes\n\nThe paper discusses the construction of maximum likelihood estimators for the drift parameters of a discontinuous diffusion process observed either continuously or discretely over a time interval. The process is characterized by different drift and volatility parameters on the positive and negative parts of its domain, leading to a piecewise-defined stochastic differential equation. The estimators are derived based on occupation times, local time, and discretized versions of these quantities."
  },
  {
    "qid": "statistic-posneg-1826-0-0",
    "gold_answer": "TRUE. The condition $\\rho = 0$ implies independence between populations, simplifying the coverage probability to the minimum of two terms: the standard normal CDF difference $\\Phi(a_{1}) - \\Phi(-a_{2})$ and its $k$-th power version $\\Phi^{k}(a_{1}) - \\Phi^{k}(-a_{2})$. This matches the result derived by Dudewicz & Tong (1971) for independent normal populations, as the correlation structure vanishes, and the multivariate normal CDF decomposes into a product of univariate CDFs.",
    "question": "Is it true that for independent populations ($\\rho = 0$), the optimal confidence interval for the largest mean $\\mu^{*}$ reduces to the case considered by Dudewicz & Tong (1971), where $\\beta(a_{1},a_{2})=\\operatorname*{min}\\{\\Phi(a_{1})-\\Phi(-a_{2}),\\Phi^{k}(a_{1})-\\Phi^{k}(-a_{2})\\}$?",
    "question_context": "Optimal Confidence Intervals for the Largest Mean in Repeated Measurements Design\n\nIn this section we consider an optimal confidence interval for the largest population mean $\\mu^{*}$ based on the largest sample mean $\\overline{{X}}^{*}$ . For given $\\gamma$ the distance $\\pmb{a_{1}+a_{2}}$ or $\\ensuremath{b_{1}}+\\ensuremath{b_{2}}$ is minimized for the case where the correlation coeficients are assumed to be a given equal nonnegative value, with the variance ${\\pmb\\sigma}^{\\mathbf{2}}$ being known or unknown. It is known that the largest sample mean overestimates the largest population mean (Dudewicz $\\&$ Tong, 1971). This suggests that an unsymmetric confidence interval should be considered. That is, ${\\pmb a}_{1}>{\\pmb a}_{2}$ and $\\pmb{b_{1}>b_{2}}$ in the intervals (2·1) and (2:5) respectively. In addition, by knowing the correlation coefficient one can increase the precision of estimation."
  },
  {
    "qid": "statistic-posneg-817-0-0",
    "gold_answer": "FALSE. The formula provided is incomplete and lacks the necessary terms to fully adjust the statistical moments. The correct application requires including all relevant ordinates and their corresponding weights, as well as ensuring the integral bounds and normalization are properly accounted for in the context of the quadrature formulae.",
    "question": "Is the formula $\\int_{-1}^{a-1}y_{x}d x={\\frac{1}{5760}}\\left\\{6463y_{0}+4371y_{1}+6869y_{3}+5537y_{3}\\right.$ correctly used to adjust statistical moments by weighting the ordinates?",
    "question_context": "Adjustment of Statistical Moments Using Quadrature Formulae\n\nThe quadrature formulae given above can be used very conveniently for adjusting the statistical momenta. We may first take the calculation of momenta when the ordinates at equidistant points are known. Let $\\pmb{y}_{0},\\pmb{y}_{1},\\pmb{y}_{3}\\dots\\pmb{y}_{n-1}$ be given and we require $\\int\\limits_{-t}^{\\infty-t}y_{s}d x.$ ydx.Now. If we now apply Formula $\\texttt{L}$ We see that we can use it for all the integrals on the righthand side of this equation except the first two and the last two and the value of these can be taken from Formula IV. Summing the values we have $\\int_{-1}^{a-1}y_{x}d x={\\frac{1}{5760}}\\left\\{6463y_{0}+4371y_{1}+6869y_{3}+5537y_{3}\\right.$ which means that we can multiply the first and last ordinates by $\\frac{6463}{5760}(0\\times1\\cdot1882205)_{\\cdot}$ the second and last but one by $\\frac{4371}{5760}(05\\cdot7588854)_{}$ the third and last but two by $\\frac{6869}{8760}(\\infty1{\\cdot}157813)_{b}$ the fourth and last but three by $\\frac{5537}{5760}(05981285)_{\\mathrm{b}}$ leave all the other ordinates unaltered and work out the momenta in the usual way from the modified series of ordinates which will now give the proper values for equating to the moments from the formula for the curve."
  },
  {
    "qid": "statistic-posneg-1672-1-0",
    "gold_answer": "FALSE. While the bias-corrected method generally reduces bias compared to standard ML, its performance is context-dependent. In scenarios with extreme probabilities (e.g., one category dominating as in the second and third simulation sets), standard ML may fail to converge or produce large biases. However, the paper's results show that even the bias-corrected method can struggle in these extreme cases, though it typically performs better than standard ML. The effectiveness of bias correction is influenced by the distribution of response probabilities and the magnitude of regression parameters.",
    "question": "Is it true that the bias-corrected method proposed in the paper consistently outperforms the standard ML approach in terms of bias reduction across all simulation scenarios, including those with small probabilities in most response categories?",
    "question_context": "Bias Correction in Proportional Odds Logistic Regression\n\nThe paper examines finite sample bias in estimating parameters for the proportional odds logistic regression model using maximum likelihood (ML) and compares it with a proposed bias-corrected method and an alternative approach by Clogg et al. (1991). The model is specified as logit(π_ij) = β_0j + β_1x_i1 + β_2x_i2 + β_3x_i3, with J=5 response categories. Three simulation sets are conducted with different parameter values and covariate distributions to evaluate bias under varying conditions."
  },
  {
    "qid": "statistic-posneg-864-1-1",
    "gold_answer": "TRUE. When I_10 ≤ I_01, it follows that I_01 I_10 = I_10, which implies ν = E(I_01 I_10) = E(I_10) = π_10. Substituting ν = π_10 into the expressions for the class probabilities ρ_A, ρ_B, ρ_S, and ρ_P yields the given formulas. This scenario represents a case of spurious synergism where the interaction between A and B is entirely due to the differing amounts of a single harmful agent, leading to the simplified expressions for the ρ's.",
    "question": "Does the condition I_10 ≤ I_01 imply that ν = π_10 and thus ρ_A = 0, ρ_B = π_01 - π_10, ρ_S = π_11 - π_01, ρ_P = π_10 - π_00?",
    "question_context": "Synergistic Interaction Analysis in Binary Exposure Factors\n\nThe paper discusses the analysis of synergistic interactions between two binary exposure factors, A and B, in causing an outcome D. It introduces six classes of indicator matrices (I_jk) based on Assumption 3, labeled as ϕ, Ω, A, B, S, P. The probabilities of these classes (ρ_ϕ, ..., ρ_P) are functions of the π_jk parameters and a fifth parameter ν = E(I_01 I_10). The maximum-entropy value ν* is proposed to estimate the unobservable ν, leading to the calculation of class probabilities under constraints. The paper also discusses the distinction between genuine and spurious synergism and provides formulas for conditional expectations and inequalities for ν."
  },
  {
    "qid": "statistic-posneg-293-0-0",
    "gold_answer": "TRUE. Sklar's theorem states that if F is a continuous d-variate distribution function, then the copula C is unique. However, if F is not continuous, C is only unique on the set Range(F1) × ... × Range(Fd).",
    "question": "According to Sklar's theorem, the copula C associated with a d-dimensional distribution F is unique only if F is a continuous d-variate distribution function. Is this statement true?",
    "question_context": "Vine Copulas and Conditional Distributions\n\nThe paper discusses vine copulas, which use bivariate copulas as building blocks to specify dependence structures in multivariate distributions. Sklar's theorem is used to decompose a d-dimensional distribution into marginal distributions and an associated copula. The paper also presents various formulas for joint density functions and conditional distributions, considering different combinations of continuous and discrete random variables."
  },
  {
    "qid": "statistic-posneg-705-0-1",
    "gold_answer": "TRUE. The CDF of Λ₃ is expressed using a series expansion involving terms θᵩᵏᵗ and generalized hypergeometric functions H. This form is derived from the density function and involves integrating the density over the range of Λ₃. The series expansion ensures that the CDF accounts for the noncentrality parameter Θ and the other distribution parameters, providing an exact expression for the cumulative probability.",
    "question": "Does the CDF of Λ₃ involve a series expansion with terms θᵩᵏᵗ and generalized hypergeometric functions H?",
    "question_context": "Noncentral Bimatrix Variate Beta Type V Distribution and Density Function of Λ₃\n\nThe paper introduces the noncentral bimatrix variate beta type V distribution, which is useful for test statistics requiring constant factors in the covariance matrices of Wishart matrix variates. The density function and CDF of Λ₃, defined as the product of determinants of matrices Q₁ and Q₂, are derived. The distribution is generated from ratios of Wishart matrices, and the paper provides detailed expressions for the density function, product moments, and CDF of Λ₃."
  },
  {
    "qid": "statistic-posneg-1510-4-1",
    "gold_answer": "FALSE. The context clearly states that Marshall-Olkin copulas have both an absolutely continuous component and a singular component. The singular component is determined by points satisfying $u_{1}^{\\alpha_{1}} = u_{2}^{\\alpha_{2}}$. This is a notable feature of Marshall-Olkin copulas, making them more complex than copulas with only an absolutely continuous component.",
    "question": "Does the Marshall-Olkin copula formula $$C(u_{1},u_{2})=\\operatorname*{min}\\{u_{1}^{1-\\alpha_{1}}u_{2},u_{1}u_{2}^{1-\\alpha_{2}}\\}$$ imply that the copula has only an absolutely continuous component?",
    "question_context": "Nested Archimedean Copulas and Marshall-Olkin Copulas in Multivariate Distributions\n\nNested Archimedean copulas (NACs) are Archimedean copulas with arguments possibly replaced by other NACs; see McNeil (2008) or Hofert (2012). In particular, this class of copulas allows us to construct asymmetric extensions of Archimedean copulas. Important to note here is that NACs are copulas for which there is no known (tractable) CDM. To demonstrate the ability of GMMNs to capture such dependence structures, we consider the simplest three-dimensional copula for visualization and investigate higher-dimensional NACs in Sections 3.3 and 4. The three-dimensional NAC we consider here is $$C(\\pmb{u})=C_{0}(C_{1}(u_{1},u_{2}),u_{3}),\\quad\\pmb{u}\\in[0,1]^{3},$$ where $C_{0}$ is a Clayton copula with Kendall’s tau $\\tau_{0}=0.25$ and $C_{1}$ is a Clayton copula with Kendall’s tau $\\tau_{1}=0.50$ . In Sections 3.3 and 4, we will present examples of 5- and 10-dimensional NACs. Bivariate Marshall–Olkin copulas are of the form $$C(u_{1},u_{2})=\\operatorname*{min}\\{u_{1}^{1-\\alpha_{1}}u_{2},u_{1}u_{2}^{1-\\alpha_{2}}\\},\\quad u_{1},u_{2}\\in[0,1],$$ where $\\alpha_{1},\\alpha_{2}~\\in~[0,1]$ . A notable feature of Marshall–Olkin copulas is that they have both an absolutely continuous component and a singular component. In particular, the singular component is determined by all points which satisfy $u_{1}^{\\alpha_{1}}~=$ $u_{2}^{\\alpha_{2}}$ . Accurately capturing this singular component may present a different challenge for GMMNs, which is why we included this copula despite the fact that there also exists a CDM for this copula. As an example for visual assessment, we consider a Marshall–Olkin copula with $\\alpha_{1}=0.75$ and $\\alpha_{2}=0.60$ ."
  },
  {
    "qid": "statistic-posneg-1246-0-0",
    "gold_answer": "TRUE. The misclassification measure M(C,T) is indeed defined as the sum of absolute differences between the indicators I_C(i,i') and I_T(i,i') for all pairs of points (i, i'), normalized by the total number of pairs, which is given by the binomial coefficient C(n, 2). This measure is proportional to the Hamming distance between the two indicators and scales from 0 (perfect agreement) to 1 (total disagreement).",
    "question": "In the context of the paper, is it true that the misclassification measure M(C,T) is defined as the sum of absolute differences between the indicators I_C(i,i') and I_T(i,i') for all pairs of points, normalized by the total number of pairs?",
    "question_context": "Hybrid Hierarchical Clustering Performance Metrics\n\nThe paper evaluates clustering methods using three performance measures: the number of 'broken' mutual clusters, the percentage of misclassified points, and the relative within-cluster sum of squares (WSS). The misclassification measure M(C,T) is defined as the normalized Hamming distance between the true and estimated clusterings. The WSS is calculated based on partitioning the dendrogram into the true number of clusters, and relative WSS is reported as the ratio of WSS for the estimated clustering to WSS for the true clustering."
  },
  {
    "qid": "statistic-posneg-1442-0-1",
    "gold_answer": "FALSE. The roles are reversed: ψ₁(Lᵐ|L⁽⁻ᵐ⁾) measures the maximum similarity (D₁) between the m-th estimation path and all other paths (F1: repulsion between paths), while ψ₂(L₁ᵐ,...,L_Kᵐᵐ) measures the maximum similarity (D₂) between trees within the m-th path (F2: repulsion within a path). The constraints ψ₁ ≥ t₁ and ψ₂ ≥ t₂ enforce dissimilarity at both levels.",
    "question": "In the PRLR algorithm, the repulsive force ψ₁(Lᵐ|L⁽⁻ᵐ⁾) ensures that trees within the same estimation path are dissimilar, while ψ₂(L₁ᵐ,...,L_Kᵐᵐ) ensures dissimilarity between different estimation paths. Is this interpretation accurate?",
    "question_context": "Parallel Repulsive Logic Regression with Biological Adjacency Constraints\n\nThe paper introduces a Parallel Repulsive Logic Regression (PRLR) algorithm that incorporates biological adjacency constraints to enhance the search for optimal logic regression models. The biological adjacency is defined by physical SNP locations and gene pathways, represented through a weighted adjacency matrix Σ. The PRLR algorithm uses repulsive forces to ensure diverse estimation paths and trees, minimizing overlap and encouraging exploration of dissimilar regions in the search space. The optimization problem includes penalties for path similarity, tree similarity, and tree complexity, solved using a simulated annealing approach."
  },
  {
    "qid": "statistic-posneg-1016-2-1",
    "gold_answer": "TRUE. The refinement score $r(\\nu)$ is maximized when $\\nu$ is the sole parent for all its descendants in subsequent levels. This condition ensures that $w(\\nu, \\nu_{m^{\\prime}}^{\\prime}) > 0$ and $w(u, \\nu_{m^{\\prime}}^{\\prime}) = 0$ for any other node $u$ in level $m$, reflecting a strong and unique hierarchical relationship.",
    "question": "Does the refinement score $r(\\nu)$ reach its maximum value $|V_{m}|$ only if every descendant of $\\nu$ has $\\nu$ as its sole parent in level $m$?",
    "question_context": "Topic Coherence and Refinement in Multiscale Analysis of Count Data\n\nIn simulations below, we find that, when a topic model is appropriate, the true value $K$ is captured by a plateau in the number of paths (Figure 3a). Hence, this metric can be used analogously to the identification of an “elbow” from a scree plot. Further, consistently slow growth in the number of paths identified may indicate departures from the assumed LDA model. Examples of both phenomena are provided in Section 4. The number of paths is a property of a model within the alignment. In contrast, the scores introduced below focus on individual topics.<PARAGRAPH_SEPARATOR>3.3.2. Topic coherence We call a topic coherent if it is found in models fitted across a range of values of $K$ . When coherent topics are recovered across multiple levels of an alignment, there is more evidence that the discovered structure is real, because it is not sensitive to the particular $K$ of the model used.<PARAGRAPH_SEPARATOR>Topic coherence is defined in the context of paths. It measures the similarity between a given topic $\\nu$ and the other topics on the same path $\\mathcal{P}\\left(\\nu\\right)=\\left\\{\\nu^{\\prime}:\\mathrm{Path}\\left(\\nu^{\\prime}\\right)=\\mathrm{Path}\\left(\\nu\\right)\\right\\}$ . It is defined as<PARAGRAPH_SEPARATOR>$$c(\\nu)=\\frac{1}{\\left|\\mathcal{P}\\left(\\nu\\right)\\right|}\\sum_{\\nu^{\\prime}\\in\\mathcal{P}(\\nu)}\\operatorname*{min}\\big(w_{\\mathrm{in}}\\left(\\nu,\\nu^{\\prime}\\right),w_{\\mathrm{out}}\\left(\\nu,\\nu^{\\prime}\\right)\\big).$$<PARAGRAPH_SEPARATOR>Our simulations illustrate how this score can be used to identify “good” values of $K$ in LDA as well as to detect departures from the assumed LDA structure. Note that coherence focuses solely on the path containing a topic. We introduce another measure, topic refinement, to reflect the richer branching pattern downstream of a topic.<PARAGRAPH_SEPARATOR>3.3.3. Topic refinement A topic identified at a small value of $K$ may have low coherence but still be a useful topic if it is the sole ancestor of topics in subsequent models. We expect true topics and compromises between true topics to have this property. We introduce the refinement score to identify such topics.<PARAGRAPH_SEPARATOR>Recall that for a node $\\nu^{\\prime}$ , $w_{\\mathrm{in}}(\\nu,\\nu^{\\prime})$ measures the extent to which mass at $\\nu^{\\prime}$ flows from parent node $\\nu$ . For each $\\nu$ , the refinement score is a weighted average of $w_{\\mathrm{in}}(\\nu,\\nu^{\\prime})$ over all its children $\\nu^{\\prime}$ . More formally, collect topics into levels $V_{1},\\dots,V_{M}$ . We define the refinement score of node $\\nu$ in level $m$ as<PARAGRAPH_SEPARATOR>$$r\\left(\\nu\\right)=\\frac{\\left|V_{m}\\right|}{M-m}\\sum_{m^{\\prime}=m+1}^{M}\\sum_{\\nu_{m^{\\prime}}^{\\prime}\\in V_{m^{\\prime}}}w_{\\mathrm{out}}\\left(\\nu,\\nu_{m^{\\prime}}^{\\prime}\\right)w_{\\mathrm{in}}\\left(\\nu,\\nu_{m^{\\prime}}^{\\prime}\\right).$$<PARAGRAPH_SEPARATOR>To better understand this score, we can establish its properties in some simple cases (proofs given in the Supplementary material available at Biostatistics online). Continuing to assume that node $\\nu$ is in level $m$ ,<PARAGRAPH_SEPARATOR>The refinement score is maximized $\\left(r(\\nu)=|V_{m}|\\right)$ if and only if $w(\\nu,\\nu_{m^{\\prime}}^{\\prime})>0$ implies $w(u,\\nu_{m^{\\prime}}^{\\prime})=0$   for any $u\\in V_{m}\\setminus\\{\\nu\\}$ . This condition means that every descendant of $\\nu$ has $\\nu$ as its sole parent in level $m$ .   The refinement score is minimized $(r(\\nu)\\to0$ ) when all of the descendants of $\\nu$ descend primarily from other nodes in level $m$ (i.e., the score is smallest for nodes that don’t have any descendants that recognize them as parents at all). Indeed, suppose that for every $\\nu^{\\prime}\\in V_{m^{\\prime}}$ , we have fixed weights $w(\\nu,\\nu^{\\prime})$ . Then $r(\\nu)\\to0$ when for each $w$ s.t. $w(\\nu,\\nu^{\\prime})>0$ , $w(u,\\nu^{\\prime})\\to\\infty$ for some $u\\in V_{m}\\setminus\\{\\nu\\}$ .   The refinement score is defined such that $r(\\nu)=1$ if all the weights in the graph are equal, which indicates an absence of topic structure in the data.<PARAGRAPH_SEPARATOR>3.3.4. Comparing diagnostics The diagnostics measure different properties of an alignment. Both low coherence/high refinement and low refinement/high coherence combinations are possible, although in the examples below the diagnostics tend to track each other. We would expect the refinement score to be high but the coherence score to be low in the case that the alignment plot has a branching structure. On the other hand, the refinement score can be small for a topic with high coherence if that topic does not have many descendants. We discuss this further and provide examples in Section S4 of the Supplementary material available at Biostatistics online."
  },
  {
    "qid": "statistic-posneg-577-4-2",
    "gold_answer": "FALSE. Adding discrete variables complicates the problem because the copula must now satisfy constraints for every combination of the discrete conditioning variables. The paper notes that the non-constant conditional copula-vine approach becomes necessary, requiring more complex modeling (e.g., via (22)) to handle the increased dimensionality and discrete-continuous interactions.",
    "question": "In the multivariate case, does the inclusion of additional discrete variables in the conditioning set 𝐕\\ᵢ simplify the task of finding a suitable copula for mixed discrete-continuous models?",
    "question_context": "Copula Constraints in Mixed Discrete-Continuous Bivariate Models\n\nThe paper discusses the challenges of using copulas to model bivariate distributions with mixed discrete and continuous variables. Specifically, it examines the case where one variable is binary (X₁ ∈ {0,1}) and the other is either discrete with multiple states (X₂ ∈ {0,1,2}) or continuous. The copula C must satisfy specific conditions to accurately represent the joint distribution, such as ℙ(X₁≤0,X₂≤x₂) = C(ℙ(X₁≤0), ℙ(X₂≤x₂)). The paper highlights that normal copulas often fail to meet these conditions due to over-determined systems, especially as the number of states increases. Graphical illustrations and conditional distributions are used to demonstrate these constraints."
  },
  {
    "qid": "statistic-posneg-177-0-0",
    "gold_answer": "TRUE. The indicator function $I_{c}(Y,X,U,\\theta)$ explicitly enforces the condition that $Y_{i}=g(h^{\\operatorname{T}}X_{i}+s_{i})$ for all $i\\in\\{1,2,\\dots,n\\}$. This is a hard constraint in the model, meaning the likelihood is zero (and thus the posterior is zero) unless this condition is satisfied for every observation. This reflects a deterministic observation equation in the state-space model, where $Y_{i}$ is a strict function of the hidden state $X_{i}$ and seasonal component $s_{i}$.",
    "question": "Is the statement 'The indicator function $I_{c}(Y,X,U,\\theta)$ equals 1 only if the observation $Y_{i}$ is exactly equal to the deterministic transformation $g(h^{\\operatorname{T}}X_{i}+s_{i})$ for all $i$' true, given the model's likelihood specification?",
    "question_context": "Bayesian State-Space Modeling for Traffic Volume Analysis\n\nLet $Y=\\{Y_{1},\\ldots,Y_{n}\\}$ denote the entire observation process, $X=\\{X_{1},X_{2},\\ldots,X_{n}\\}$ the (hidden) state process and $U=(u_{1}^{1},\\dots,u_{48}^{1})$ the 48-period (daily) seasonal subcomponent of $\\left\\{s_{t}\\right\\}$ . Let $\\theta=(\\phi,\\sigma,\\beta_{w})$ denote the vector of remaining parameters to be estimated. Strictly speaking, the seasonal component (see equation (5)) has a period of 336, but we need only 49 parameters to specify it. Independent prior distributions to both $U$ and $\\theta$ are assigned, and they are denoted by $p(U)$ and $p(\\theta)$ respectively. <PARAGRAPH_SEPARATOR> Our goal is to construct a Markov chain $\\{M_{k}=(X^{(k)},U^{(k)},\\theta^{(k)})$ , $k=1,2,\\hdots\\}$ whose limiting distribution is the posterior distribution $p(X,U,\\theta|Y)~\\propto~p(Y,X,U,\\theta)$ . Here $X^{(k)}$ is a set $\\{\\bar{X^{(k)}},t=1,\\ldots,n\\}$ representing the $k$ th approximate sample from the marginal posterior distribution of the state process, with $U^{(k)}$ and ${\\boldsymbol{\\theta}}^{(k)}$ also representing $k$ th approximate draws from the marginal posteriors of the respective distributions. After an initial burn-in period, elements $(X^{(k)},U^{(k)},\\theta^{(k)})$ of the chain can be regarded as approximate (non-independent) draws from the desired posterior distribution. <PARAGRAPH_SEPARATOR> Before formally stating the sampling algorithm, we point out that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{p(Y,X,U,\\theta)=I_{c}(Y,X,U,\\theta)p(X,U,\\theta)=I_{c}(Y,X,U,\\theta)p(X|U,\\theta)p(U,\\theta)}}\\ {{\\mathrm{}}}\\ {{\\mathrm{}=I_{c}(Y,X,U,\\theta)p(X|\\theta)p(U)p(\\theta),}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $I_{c}(Y,X,U,\\theta)$ is equal to 1 if $Y_{i}=g(h^{\\operatorname{T}}X_{i}+s_{i})$ for $i\\in\\{1,2,\\dots,n\\}$ , and 0 otherwise. Furthermore, <PARAGRAPH_SEPARATOR> $$ p(X|\\theta)=p(X_{1}|\\theta)\\prod_{i=2}^{n}p(X_{i}|X_{i-1}), $$ <PARAGRAPH_SEPARATOR> where $X_{1}|\\theta\\sim N(0,\\Lambda)$ , $\\Lambda$ being the solution to $F\\Lambda F^{\\mathrm{T}}+\\Sigma=\\Lambda$ , and, for $i>1$ , $X_{i}|X_{i-1}\\sim$ $N(F X_{i-1},\\Sigma)$ . Therefore it is straightforward to compute the density $p(Y,X,U,\\theta)$ . <PARAGRAPH_SEPARATOR> Our MCMC sampler is as follows. <PARAGRAPH_SEPARATOR> Step 1: set $k=1$ . Choose some initial state $(X^{(1)},U^{(1)},\\theta^{(1)})$ satisfying $I_{c}(Y,X^{(1)},U^{(1)},\\theta^{(1)})=1$ : Step 2: replace $k$ by $k+1$ . Set $X^{(k)}=X^{(k-1)},U^{(k)}=U^{(k-1)}$ and $\\theta^{(k)}=\\theta^{(k-1)}$ . Step 3: for $i=1,2,\\hdots,n$ , replace the vector $X_{i}^{(k)}$ by a draw from its conditional distribution, given $Y$ , $\\{X_{j}^{(k)},j\\neq i\\}$ , $U^{(k)}$ and ${\\theta}^{(k)}$ ."
  },
  {
    "qid": "statistic-posneg-475-0-0",
    "gold_answer": "FALSE. The spatial dispersion function gϕ(·) does not need to be negative semi-definite as a variogram would be. It only needs to be a smooth, non-negative function of distance that captures the spatial covariance structure. This flexibility allows for the use of various parametric or non-parametric forms, such as non-negative splines, without the constraints of traditional variogram models.",
    "question": "Is the spatial dispersion function gϕ(·) required to be negative semi-definite like a variogram in order to be used effectively in the ASDC method?",
    "question_context": "Spatial Data Compression via Adaptive Dispersion Clustering\n\nThe paper introduces a method for spatial data compression using adaptive dispersion clustering (ASDC). The framework involves a hidden spatial process Y(v) observed with noise, resulting in observations Z(s) = Y(s) + ε(s). The goal is to compress spatial data by clustering locations in an exterior subdomain De while preserving the covariance structure relative to a central prediction region D0. The method uses spatial dispersion functions to quantify spatial covariance and performs spectral clustering on a similarity matrix derived from these functions."
  },
  {
    "qid": "statistic-posneg-870-0-1",
    "gold_answer": "FALSE. While a three-step approximation can provide a better mathematical fit to the time-varying coefficient, it may not always yield more precise estimates. The precision of estimates depends on the number of failures observed in each time interval. For intervals with fewer failures (e.g., the middle interval for $\\gamma_j$), estimates may be less precise. Additionally, practical estimation problems, such as infinite estimates for binary covariates, can arise, and the added complexity of estimating additional breakpoints ($B$ and $B_1$) may not justify the marginal gain in fit. The paper suggests using as few steps as possible to avoid these issues.",
    "question": "Does the step-function approximation with three steps always provide more precise estimates of the time-varying coefficients compared to a two-step approximation?",
    "question_context": "Step-Function Approximation for Time-Varying Coefficients in Cox Proportional Hazards Model\n\nComparing Figs 1 and 3 we see that although they have the same general shape, the limitation of the covariate effects to 2 years in Fig. 3 gives a steeper drop in the first 2 years followed by much flatter curves. Hence as compared with Fig. 1, Fig. 3 indicates lower values of the survival function for the first 2 years but higher values after 5 years.<PARAGRAPH_SEPARATOR>We have presented an approximate approach to time-varying covariate efects in Cox's (1972) proportional hazards model. This has proved useful and effective in the example given in deriving a model which did give a reasonabie fit to the data, when Cox's basic model (1) did not. Our approximation replaces a time-varying coeficient by the step-function given in (2), representing short-term and long-term effects. A better mathematical approximation to the time-varying coefficient wouid be given by taking three steps in the function:<PARAGRAPH_SEPARATOR>$$\\beta_{j}(t)=\\alpha_{j}\\quad\\mathrm{for}t\\leqslant B;\\quad\\beta_{j}(t)=\\gamma_{j}$$<PARAGRAPH_SEPARATOR>$$B<t<B_{1};\\quad\\beta_{j}(t)=\\delta_{j}\\quad\\mathrm{for}~B_{1}\\geqslant t.$$<PARAGRAPH_SEPARATOR>Equally, the estimation technique given in Section 3 extends simply to the estimation of $\\alpha,\\gamma$ sand $\\delta$ from the conditional log-likelihood. Given $\\pmb{B}$ and $B_{1}$ one can express the logconditional likelihood as the sum of three functions, $l(\\alpha,\\gamma,\\delta)=g_{1}(\\alpha)+g_{2}(\\gamma)+g_{3}(\\delta)$ Hence the maximum likelihood estimates of $\\alpha,\\gamma,\\delta$ are found by maximizing the functions $\\{\\boldsymbol{g}_{i}(.)\\}$ separately. This is all straightforward, but note that $\\alpha,\\gamma$ and $\\delta$ are estimated only from the failures in the appropriate time interval. In many cases this would give relatively fewer failures for $\\gamma$ and hence relatively less precise estimation. Further, if there are binary covariates, the difficulties of infinite estimates discussed in Section 3 will become acute. There is also the extra difficulty of estimating $\\pmb{B}$ and $B_{1}$<PARAGRAPH_SEPARATOR>The approximation of $\\beta(t)$ by step-functions with arbitrarily many steps can be set up as in (10) and the estimation problems approached as above. In principle there is no dificulty but even with quite large data sets, practical estimation problems do arise and it is sensible practice to use as few steps as possible. In the medical context, once an effect has decayed, its coefficient will probably be zero (as in Section 4) and in most cases there will probably be little gained by taking more than two steps.<PARAGRAPH_SEPARATOR>Copies of programs which carry out the estimation procedures are available from the authors. Note that $\\alpha,\\gamma$ and $\\pmb{B}$ can be estimated using programs for Cox's (1972) proportional hazards model (1), which allow for time-dependent covariates."
  },
  {
    "qid": "statistic-posneg-2112-0-1",
    "gold_answer": "TRUE. The CPD is defined as the absolute difference between the nominal coverage probability (the desired coverage level, e.g., 95%) and the empirical coverage probability (the actual proportion of observations that fall within the forecasted interval). A lower CPD indicates better performance, as it means the empirical coverage is closer to the nominal coverage.",
    "question": "Does the coverage probability difference (CPD) measure the absolute difference between the nominal and empirical coverage probabilities?",
    "question_context": "Evaluation of Interval Forecast Accuracy in Functional Time Series\n\nTo evaluate pointwise interval forecast accuracy, we consider the coverage probability difference (CPD) between the nominal and empirical coverage probabilities. The empirical coverage probability is defined as follows where $H$ denotes the number of curves in the forecasting period, $p$ $\\widehat{\\mathcal{V}}_{T+\\zeta|T,s}^{g,\\mathrm{ub}}$ and YTg,lbζ $\\widehat{\\mathcal{V}}_{T+\\zeta|T,s}^{g,\\mathrm{lb}}$ denote the upper and lower bounds of the corresponding forecasted interval, and $\\mathbb{1}\\{\\cdot\\}$ the binary indicator function. Pointwise CPD is defined as The lower the $\\mathrm{CPD}_{s}^{g}$ value, the better the forecasting method’s performance. Additionally, we use the interval score of Gneiting and Raftery (2007) (see also Gneiting and Katzfuss 2014). For each year in the forecasting period, the $h$ -step-ahead prediction intervals were calculated at the $100(1~-~\\alpha)\\%$ nominal coverage probability. We consider the common case of the symmetric $100(1-\\alpha)\\%$ prediction interval, with lower and upper bounds that are predictive quantiles at $\\alpha/2$ and $1-\\alpha/2$ , denoted by YT,l+bζ |T,s(ui) and YT,+ubζ | T,s(ui). The scoring rule for the interval forecast at discretized point $u_{i}$ is where $\\mathbb{1}\\{\\cdot\\}$ represents the binary indicator function, and $\\alpha$ denotes a level of significance. Finally, we compute the mean interval score for the total of $T$ series as The optimal interval score is achieved when $\\mathcal{V}_{T+\\zeta|T,s}^{g}(u_{i})$ lies between YTg,lbζ T,s(ui) and YTg,ubζ T,s(ui), with the distance between the upper bound and the lower bound being minimal."
  },
  {
    "qid": "statistic-posneg-1027-0-0",
    "gold_answer": "TRUE. The confidence ellipse is a circle because the covariance matrix for $(\\hat{\\xi}, \\hat{\\eta})$ is diagonal with equal variances (i.e., $\\hat{V}$ is the same for both $\\hat{\\xi}$ and $\\hat{\\eta}$). This equality of variances ensures that the confidence region is symmetric in both dimensions, resulting in a circular shape. The equation $(\\hat{\\xi}-\\xi)^{2}+(\\hat{\\eta}-\\eta)^{2}=2\\hat{V}F_{\\gamma,2,2N-4}$ describes a circle centered at $(\\hat{\\xi}, \\hat{\\eta})$ with radius $\\sqrt{2\\hat{V}F_{\\gamma,2,2N-4}}$.",
    "question": "Is it true that the confidence ellipse for $(\\xi, \\eta)$ is always a circle, as given by the equation $(\\hat{\\xi}-\\xi)^{2}+(\\hat{\\eta}-\\eta)^{2}=2\\hat{V}F_{\\gamma,2,2N-4}$? Explain the reasoning behind this result.",
    "question_context": "Parameter Estimation for Circular Data with Known Angular Differences\n\nThe assumption that the angular differences $\\theta_{i}=\\tau_{i}-\\tau_{i-1}$ are known is equivalent to the representation $\\tau_{t}=\\theta_{0}+\\theta_{t},i=1,...,N,$ where $\\theta_{i}, i = 1, .. .,N,$ are known (with typically $\\theta_{1} = 0$), and $\\theta_{0}$ is unknown. Substituting into the model, one obtains $X_{i}=\\xi+\\alpha\\cos\\theta_{i}-\\beta\\sin\\theta_{i}+\\epsilon_{i},\\quad Y_{i}=\\eta+\\alpha\\sin\\theta_{i}+\\beta\\cos\\theta_{i}+\\delta_{i},i=1,...,N,$ where $\\alpha=\\rho\\cos\\theta_{0},\\quad\\beta=\\rho\\sin\\theta_{0}.$ The model is linear in the four parameters $\\xi,\\eta,\\alpha,\\beta$ and so one can obtain explicit maximum likelihood estimates for all parameters and an exact covariance matrix. The covariance matrix for $\\hat{\\xi},\\hat{\\eta},\\hat{\\alpha},\\hat{\\beta}$ is given by $V=\\sigma^{2}/N D$, where $D=1-\\bar{c}^{2}-\\overline{s}^{2}$. The normality of $(\\hat{\\xi},\\hat{\\eta})$ implies that the usual exact $100(1-\\gamma)\\%$ confidence ellipse for $(\\xi,\\eta)$ is always a circle, specifically $(\\hat{\\xi}-\\xi)^{2}+(\\hat{\\eta}-\\eta)^{2}=2\\hat{V}F_{\\gamma,2,2N-4},$ where $\\hat{V}=\\hat{\\sigma}^{2}/N D$ and $F_{\\gamma,2,2N-4}$ is the upper $100\\%$ point of the $F$ distribution with degrees of freedom 2 and $2N - 4$."
  },
  {
    "qid": "statistic-posneg-35-0-0",
    "gold_answer": "TRUE. The model correctly captures the observed response by considering all possible treatment combinations $(Z_{s i1}, Z_{s i2})$ and their corresponding potential responses $(r_{11s i}, r_{10s i}, r_{01s i}, r_{00s i})$. This formulation ensures that the observed response $R_{s i}$ is a weighted sum of potential responses, where the weights are determined by the actual treatment assignment. This is consistent with the potential outcomes framework for causal inference in observational studies.",
    "question": "Is the observed response model $R_{s i}=Z_{s i1}Z_{s i2}r_{11s i}+Z_{s i1}(1-Z_{s i2})r_{10s i}+(1-Z_{s i1})Z_{s i2}r_{01s i}+(1-Z_{s i1})(1-Z_{s i2})r_{00s i}$ correctly specified for capturing the differential effects of treatment combinations?",
    "question_context": "Differential Effects and Generic Biases in Observational Studies\n\nThe paper discusses the differential effects of treatment combinations in observational studies, focusing on the potential biases introduced by unobserved covariates. It presents models for treatment assignment under various scenarios of unobserved bias, including generic and differential biases. The key formulas include the observed response model, conditional independence conditions, and sensitivity analysis bounds for differential biases."
  },
  {
    "qid": "statistic-posneg-1789-0-0",
    "gold_answer": "TRUE. According to the paper, for $\\rho>0$ and $\\alpha>2/\\rho-1$ or $\\rho<0$, we have $\\varDelta_{\\rho,\\alpha}>0$, which implies $\\mathbb{P}(X_{1}\\geq\\alpha t|X_{2}=t)\\ll\\mathbb{P}(Y_{1}\\geq\\alpha t|Y_{2}=t)$ for large t. This is due to the difference in the copula structures affecting the tail behavior of the conditional probabilities.",
    "question": "Is it true that for $\\rho>0$ and $\\alpha>2/\\rho-1$ or $\\rho<0$, the conditional probability $\\mathbb{P}(X_{1}\\geq\\alpha t|X_{2}=t)$ is significantly less than $\\mathbb{P}(Y_{1}\\geq\\alpha t|Y_{2}=t)$ for large t?",
    "question_context": "Comparison of Conditional Extreme Event Probabilities in Gaussian Copulas\n\nWe compare the conditional probabilities of extreme values of a pair of random variables $(X_{1},X_{2})$ which has bivariate normal distribution with standard normal marginals and correlation coefficient $\\rho$ , with a pair of random variables $(Y_{1},Y_{2})$ whose marginals are also standard normal, but has copula $c_{\\delta}$ , where $\\delta$ is the diagonal of the copula of $(X_{1},X_{2})$ . We compute the conditional probabilities $\\mathbb{P}(X_{1}\\geq\\alpha t|X_{2}=t)$ and $\\mathbb{P}(Y_{1}\\geq\\alpha t|Y_{2}=t)$ with $\\alpha\\geq1$ and consider their asymptotic behaviour when t goes to infinity. This comparison is motivated by consideration of correlated defaults in mathematical finance, see Section 10.8 in [24]. (Notice however the parameters of upper tail dependence of the two copulas are the same since they have the same diagonal.)"
  },
  {
    "qid": "statistic-posneg-1285-1-0",
    "gold_answer": "TRUE. The condition λ ≥ (∑_{i=1}^m p_i λ_i^k)^{1/k} ensures that the hazard rate of the mixture V is less than or equal to the hazard rate of U for all t > 0, which implies V ≥_{st} U. This is derived from the properties of the hazard rate function and the stochastic order under the given conditions.",
    "question": "Is the condition λ ≥ (∑_{i=1}^m p_i λ_i^k)^{1/k} sufficient to ensure that the mixture V is stochastically greater than the single distribution U (V ≥_{st} U)?",
    "question_context": "Stochastic Ordering and Hazard Rate Bounds for Mixtures of Exponential Order Statistics\n\nThe paper discusses stochastic ordering and hazard rate bounds for mixtures of exponential order statistics. It presents various formulas and theorems to compare the survival and hazard rate functions of order statistics from heterogeneous independent exponential random variables with those from i.i.d. exponential random variables. Key formulas include the survival function of the kth order statistic, the hazard rate function, and conditions for stochastic and hazard rate ordering."
  },
  {
    "qid": "statistic-posneg-1197-0-0",
    "gold_answer": "TRUE. The paper shows that if $\\delta_n/h_n \not\to 0$, the estimator $\\hat{f}$ can only be consistent under very specific conditions (e.g., when $\\sum_{\\mu\\in\\mathbb{Z}}\\alpha K(\\alpha(x-\\mu))=1$ for all $x$). For general densities, $\\delta_n/h_n \to 0$ is required to ensure asymptotic unbiasedness, as otherwise, the bias term $(\\delta_n/h_n)^2G(x,\\delta_n,h_n)$ may not vanish.",
    "question": "The condition $\\delta_n/h_n \to 0$ is necessary for the binned kernel density estimator $\\hat{f}$ to be asymptotically unbiased for any density $f$. Is this statement true based on the paper's analysis?",
    "question_context": "Bias and Variance in Binned Kernel Density Estimation\n\nThe paper discusses the estimation error of the binned kernel density estimator $\\hat{f}$ for a density function $f$. It provides expansions for the expected value $E[\\hat{f}(x)]$ and analyzes the bias and variance components of the mean squared error. Key conditions include $\\delta_n, h_n, \\delta_n/h_n \to 0$ as $n \to \\infty$ for asymptotic unbiasedness. The paper also examines special cases where $\\delta_n/h_n \to \\alpha > 0$ and derives conditions under which consistency can still be achieved."
  },
  {
    "qid": "statistic-posneg-919-0-1",
    "gold_answer": "FALSE. The context explicitly states that the table is formed from a general mean and additive main effects only, as given by the array $(m,a_{1},a_{2},a_{3},b_{1},b_{2},b_{3},b_{4},c_{1},c_{2})$. There is no mention or inclusion of interaction effects in the provided formula or context.",
    "question": "Does the formula $y_{i j k}$ represent a three-way table with interaction effects included in addition to the main effects?",
    "question_context": "Multi-Way Table Construction from Additive Main Effects\n\nThe algorithm produces a multi-way table from a general mean and set of additive main effects supplied as a single array. Thus suppose $y_{i j k}$ represents a $3\\times4\\times2$ three-way table formed from effects given by $$(m,a_{1},a_{2},a_{3},b_{1},b_{2},b_{3},b_{4},c_{1},c_{2})$$ and stored in a single array of length $1+3+4+2$."
  },
  {
    "qid": "statistic-posneg-1086-0-0",
    "gold_answer": "TRUE. The context explicitly states that the GPD distribution function is highly sensitive to the shape parameter $\\xi$, making direct fitting challenging if initial values are not close to the true parameter values. This is illustrated by the simulation results in Table 1, where the theoretical GPD values are almost identical (close to 1) when initial $\\xi$ values are far from the true value ($\\xi=2$), rendering the NLS method ineffective. The proposed two-step procedure addresses this by first fitting the log-transformed model: $$\\log(1-F_{n}(x))=-\\frac{1}{\\xi}\\log\\left(1+\\frac{\\xi x}{\\sigma}\\right)+\\text{error},$$ which stabilizes the range of explanatory variables (as shown in Table 1's 'Log GPD' columns). The initial estimates from this step ($\\hat{\\sigma}^1, \\hat{\\xi}^1$) are then used to fit the original EDF to the GPD function, ensuring convergence. The economic/statistical intuition is that the log transformation mitigates the sensitivity issue by compressing the scale of extreme values, allowing for more stable parameter estimation before refining with the original model.",
    "question": "Is the claim that the GPD distribution function is very sensitive to the shape parameter $\\xi$ and difficult to fit without close initial values supported by the provided context and formulas? Explain the reasoning behind the proposed two-step fitting procedure.",
    "question_context": "Generalized Pareto Distribution (GPD) Parameter Estimation for High Quantiles\n\nMany applications need estimation of high quantiles for heavy-tailed distributions such as in the case of extreme events with low occurrence but great impact, especially in finance and insurance applications. Statisticians have been using extreme value theory to estimate high quantiles and the POT method is one of the most widely used methods. Pickands (1975) and Balkema and de Haan (2004) showed that the distribution of excesses can be approximated by the GPD if the distribution is in the maximum domain of attraction. In fact, most of the common continuous distributions are in the maximum domain of attraction as seen in Embrechts et al. (1997). Therefore, we may apply the POT method even if we do not know the underlying distribution in many applications. The distribution function of GPD is defined as follows: $$F_{\\xi,\\sigma}(x)=\\left\\{\\begin{array}{l l}{1-(1+\\xi x/\\sigma)^{-1/\\xi}}&{\\mathrm{~if~}\\xi\\neq0,}\\ {1-\\exp(-x/\\sigma)}&{\\mathrm{~if~}\\xi=0,}\\end{array}\\right.$$ where $\\sigma>0.x\\ge0$ when $\\xi\\ge0$ and $0\\leq x\\leq-\\sigma/\\xi$ when $\\xi<0$. This GPD family can be also extended to define a GPD with a location parameter, $\\mu$, as $F_{\\xi,\\mu,\\sigma}(x)=F_{\\xi,\\sigma}(x-\\mu)$. We define $\\sigma$ as the scale parameter and $\\xi$ as the shape parameter of the GPD. In general, we consider that the distribution is heavy-tailed when the shape parameter $\\xi>0$, medium-tailed when $\\xi=0$, and short-tailed when $\\xi<0$. According to this rule, distributions such as Pareto, Burr, log-gamma, and Cauchy are in the class of heavy-tailed distributions, distributions such as normal, exponential, gamma, and lognormal are in the class of medium-tailed distributions, and distributions such as uniform and beta are in the class of short-tailed distributions. For details of GPD, one can refer to McNeil and Saladin (1997). Our interest in this paper is in medium- to heavy-tailed distributions, so we focus on the cases when $\\xi\\ge0$ in the following sections. Moreover, when we apply the POT method, the location parameter $\\mu$ is not relevant because we consider observations that are larger than a certain threshold value. Therefore, we are only interested in estimating $\\sigma$ and $\\xi$."
  },
  {
    "qid": "statistic-posneg-2067-0-0",
    "gold_answer": "TRUE. The separation measure T(x,y;h_n,s) is defined as the ratio of between-group variability (numerator) to within-group variability (denominator). A higher value of T(x,y;h_n,s) indicates that the between-group variability is large relative to the within-group variability, which implies better separation between the two clusters. This is consistent with the paper's approach of selecting the threshold s that maximizes T(x,y;h_n,s) to achieve optimal clustering.",
    "question": "The separation measure T(x,y;h_n,s) used for clustering is defined as the ratio of between-group variability to within-group variability. Is it correct to say that a higher value of T(x,y;h_n,s) indicates a better separation between the two clusters?",
    "question_context": "Image Denoising via Local Clustering and Adaptive Smoothing\n\nThe paper presents a method for image denoising using a local clustering framework. The observed image is modeled as a 2D regression surface with discontinuities at object boundaries. The method involves clustering pixels into two groups based on intensity values within a neighborhood, using a separation measure to determine the optimal threshold. The denoised image is then estimated by weighted averaging within the identified clusters, with weights based on local similarity measures."
  },
  {
    "qid": "statistic-posneg-75-0-1",
    "gold_answer": "FALSE. The optimal subsampling probability in the first stage p_{n₁,N}^{opt}(·) is derived to minimize the asymptotic variance V_{p,q} and depends only on the first-stage subsample size n₁ and the data characteristics through ν(Z; ᾱ, γ̄). It does not depend on the second-stage subsample size n₂, as the optimization is performed separately for each stage.",
    "question": "Does the optimal subsampling probability in the first stage p_{n₁,N}^{opt}(·) depend on the second-stage subsample size n₂?",
    "question_context": "Two-Stage Optimal Subsampling for Missing Data Analysis\n\nThe paper discusses a two-stage subsampling method to estimate the parameter of interest μ=E[Y] in the presence of missing data. The method involves estimating model parameters in the first stage using a subsample and then estimating μ in the second stage using another subsample. The optimal subsampling probabilities are derived to minimize the asymptotic variance of the estimator."
  },
  {
    "qid": "statistic-posneg-1726-1-0",
    "gold_answer": "FALSE. While $E[\\log(\\psi_{1})]<0$ is a necessary condition for stationarity, it is not sufficient on its own. The proposition also requires additional conditions such as $\\phi_{1}+\\phi_{2}<1$, $\\L_{2}<1$, $\\phi_{2}-\\phi_{1}<1$, and $|\\phi_{2}|<1$ for the AR(2) process, as well as $E[\\log^{+}(\\epsilon_{1})]<\\infty$ for the i.i.d. process $\\epsilon_{t}$. These conditions collectively ensure the convergence of the macroeconomic sequences to a stationary distribution.",
    "question": "Is the condition $E[\\log(\\psi_{1})]<0$ sufficient to ensure the stationarity of the macroeconomic sequences in the model?",
    "question_context": "Stationarity and Fat-Tail Behavior in Macroeconomic Sequences\n\nIn our model, the asymptotic behavior of macroeconomic sequences is ultimately driven by the asymptotic behavior of the capital stock. We first establish a sufficient condition for the convergence of relevant aggregate variables towards a stationary distribution. We then show that this distribution may display fat-tail behavior. In particular, we show that arbitrarily small variations of the externality may lead to fat tails while being compatible with the existence of a stationary distribution in the long term. <PARAGRAPH_SEPARATOR> As it is usual in the macro-literature, we assume that the TFP process $a_{t}$ converges to a stationary sequence. More precisely we suppose that the TFP process $a_{t}$ admits a strong, stationary AR(2) representation. As a careful reading of the proofs reveals, the results of this section may also be established – at the cost of much heavier notation – when $a_{t}$ follows strong strictly stationary $\\mathsf{A R}(p)$ processes for any given value of $p$ . As the AR(2) assumption is supported by our data, we restrict our attention to this case. Also remark that the AR(2) case may cope for the possible presence of – exogenous – cycles in the TFP process. Define $a_{t}^{\\prime}=a_{k}+(1-\\delta)(a_{t}+(1-\\alpha)n+s)$ . The processes $a_{t}^{\\prime}$ and $a_{t}$ share the same strong AR(2) representation: <PARAGRAPH_SEPARATOR> $$ a_{t}^{\\prime}=\\phi_{1}a_{t-1}^{\\prime}+\\phi_{2}a_{t-2}^{\\prime}+\\epsilon_{t}, $$ <PARAGRAPH_SEPARATOR> where $\\epsilon_{t}$ is an i.i.d. process. <PARAGRAPH_SEPARATOR> Proposition 1. Assume the following conditions hold <PARAGRAPH_SEPARATOR> 1. $\\psi_{t}=\\delta+(1-\\delta)(\\alpha+\\gamma_{t})$ is an i.i.d. process such that $P(\\psi_{1}>0)=1$ and $E[\\log(\\psi_{1})]<0.$   \n2. $k_{0}$ is a given quantity.   \n3. $E[\\log^{+}(\\epsilon_{1})]<\\infty$ .   \n4. $\\phi_{1}+\\phi_{2}<1$ $\\L_{2}<1,\\phi_{2}-\\phi_{1}<1,|\\phi_{2}|<1.$"
  },
  {
    "qid": "statistic-posneg-918-0-2",
    "gold_answer": "TRUE. The proportional odds model for Pr(b_ij ≤ k) assumes that the coefficients θ_1 and β are the same for all k = 1,...,K-1, meaning the effect of the covariates on the log-odds of being in a higher-risk class is constant across all thresholds. Only the intercepts γ_k vary, allowing the baseline odds to differ while maintaining proportional effects of the predictors.",
    "question": "For Model 2 (ordinal latent variable), the proportional odds assumption is used for the latent class probabilities Pr(b_ij ≤ k). Does this imply that the effect of the covariates (y_i,j-1 and x_i,j-1) on the log-odds of being in a higher-risk class is the same across all class boundaries?",
    "question_context": "Latent Variable Models for Risky Driving Behavior Analysis\n\nThe paper discusses longitudinal data analysis where binary and count outcomes are influenced by an unobservable 'risky driving' state. Two models are presented: a binary latent variable model (Model I) and an ordinal latent variable model (Model 2), both extended with random effects (Models IR and 2R). The models incorporate previous outcomes and kinematic measures to predict the latent state and observed outcomes, with an offset for miles driven. Estimation is performed using EM and Monte Carlo EM algorithms."
  },
  {
    "qid": "statistic-posneg-140-0-1",
    "gold_answer": "TRUE. For $\\lambda=0$, $\\phi_{(\\lambda)}(x)$ reduces to $x\\log x - x + 1$, which corresponds to the Kullback-Leibler divergence, and the minimum $\\phi$-divergence estimator coincides with the maximum likelihood estimator.",
    "question": "Does the power divergence family, defined by $\\phi_{(\\lambda)}(x)$, include the maximum likelihood estimator when $\\lambda=0$?",
    "question_context": "Power Divergence Measures in Three-Way Contingency Tables\n\nThe paper discusses the use of power divergence measures in three-way contingency tables, focusing on hypotheses of independence, partial independence, and conditional independence. It introduces the $(h,\\phi)$-divergence measures and examines their properties and applications in statistical testing."
  },
  {
    "qid": "statistic-posneg-1783-0-0",
    "gold_answer": "TRUE. The similarity index $\\rho(\\mathbf{c}_{1},\\mathbf{c}_{2})$ reaches its maximal value of 1 when the two curves are identical except for shifts and dilations of the components, as stated in the context. This is because the cosine of the angle between the derivatives of homologous components will be 1 if the curves are identical up to affine transformations, leading to an average of 1 across all components.",
    "question": "Is the similarity index $\\rho(\\mathbf{c}_{1},\\mathbf{c}_{2})$ defined as the average of the cosines of the angles between the derivatives of homologous components of $\\mathbf{c}_{1}$ and $\\mathbf{c}_{2}$ always equal to 1 if the two curves are identical except for shifts and dilations of the components?",
    "question_context": "Curve Clustering and Alignment via Similarity Index and Warping Functions\n\nThe variability among two or more curves can be thought of as having two components: phase variability and amplitude variability. Heuristically, phase variability is the one that can be eliminated by suitably aligning the curves, and amplitude variability is the one that remains among the curves once they have been aligned. Consider a set $\\mathcal{C}$ of (possibly multidimensional) curves $\\mathbf{c}(s):\\mathbb{R}\\rightarrow\\mathbb{R}^{d}$ . Aligning $\\mathbf{c}_{1}\\in\\mathcal{C}$ to $\\mathbf{c}_{2}\\in\\mathcal{C}$ means finding a warping function $h(s):\\mathbb{R}\\rightarrow\\mathbb{R}$ of the abscissa parameter s, such that the two curves $\\mathbf{c}_{1}\\circ h$ and ${\\bf c}_{2}$ are the most similar (with $(\\mathbf{c}\\circ h)(s):=\\mathbf{c}(h(s)))$ . It is thus necessary to specify a similarity index $\\rho(\\cdot,\\cdot):\\mathcal{C}\\times\\mathcal{C}\\rightarrow\\mathbb{R}$ that measures the similarity between two curves, and a class W of warping functions $h$ (such that $\\mathbf{c}\\circ h\\in{\\mathcal{C}}$ , for all $\\mathbf{c}\\in\\mathcal{C}$ and $h\\in W.$ indicating the allowed transformations for the abscissa. Aligning ${\\mathfrak{c}}_{1}$ to ${\\bf c}_{2}$ , according to $(\\rho,W)$ , means finding $h^{*}\\in W$ that maximizes $\\rho(\\mathbf{c}_{1}\\circ h,\\mathbf{c}_{2})$ . This procedure decouples phase and amplitude variability without loss of information: phase variability is captured by the optimal warping function $h^{*}$ , whilst amplitude variability is the remaining variability between $\\mathbf{c}_{1}\\circ h^{*}$ and $\\mathbf{c}_{2}$ . Note that the choice of the couple $(\\rho,W)$ defines what is meant by phase variability and by amplitude variability."
  },
  {
    "qid": "statistic-posneg-683-1-0",
    "gold_answer": "TRUE. Explanation: From the equation $P_{0}=1-P_{i}-P_{j}=(1-\\slash p_{g})(1-\\slash p_{s})$, if $p_g = 1$, then $(1 - p_g) = 0$, making $P_0 = 0$. This aligns with the scenario where guessing is obligatory, leaving no room for ties, as subjects are forced to choose one option over the other even when they cannot discriminate.",
    "question": "In the context of discrimination testing, if ties are allowed and the probability of guessing ($p_g$) is 1, does the probability of a tie ($P_0$) become zero?",
    "question_context": "Discrimination and Preference Testing in Hedonic Scaling\n\nThe question of a central zero point in hedonic scaling is part of the wider question of whether or not, in straight pair comparison, ties should be allowed. The answer is: it all depends. If we are concerned with testing discriminability, or if the null hypothesis that items A and B are indistinguishable' is being checked, ties are best avoided. The question put to the subject should be 'Which. .. .? Answer one or the other even if you feel you are guessing', rather than, \\*Which, if either. .. .?' On the other hand, when preferences in a population are sought, the answer 'Neither’ should be allowed. Some further remarks on this topic follow; they mostly stem from a recent investigation (Gridge$\\mathtt{m a n^{12}}.$ 一 <PARAGRAPH_SEPARATOR> If the subject is judging the relative intensity of two taste stimuli, the test is purely of discrimination, for we may assume that, having detected the difference, he will not mistake its direction. When, on occasion, he does not detect the difference, he has to guess. Assume an empirical probability $\\pmb{\\phi}_{s}$ of true discriminability. Then the left-hand side equation (l) can be written: <PARAGRAPH_SEPARATOR> $$ \\displaystyle{\\mathbf{P}(i>j)=(1\\pm\\mathbf{\\hat{p}_{s}})/2} $$ <PARAGRAPH_SEPARATOR> For a specific pair in which $C_{i}{>}C_{j},$ we have <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{P_{i}=\\P(i{>}j)=(1+\\pmb{\\rho}_{s})/2\\biggr\\}}\\ {P_{j}=\\P(j{>}i)=(1-\\pmb{\\rho}_{s})/2\\biggr\\}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Now if ties are allowed, the subject will not invariably record ‘no apparent difference' when he fails to discriminate; sometimes (and not necessarily consciously) he will guess. We therefore. postulate a probability $\\pmb{\\phi}_{\\pmb{g}}$ of his guessing (and, of course, about half of his guesses will be correct). Then, writing $P_{\\mathbf{0}}$ for the probability of a tie, we have: <PARAGRAPH_SEPARATOR> $$ \\left.\\begin{array}{l}{P_{i}=\\phi_{s}+\\slash p_{s}(1-\\slash p_{s})/2}\\ {P_{j}=\\slash p_{g}(1-\\slash p_{s})/2}\\ {P_{0}=1-P_{i}-P_{j}=(1-\\slash p_{g})(1-\\slash p_{s})}\\end{array}\\right\\} $$ <PARAGRAPH_SEPARATOR> Equations (8) have as a special case ${p}_{g}=1$ , which means that guessing is obligatory, and which is represented by equations (7). In general, theory shows that ties are best banned in discrimination work provided that $\\phi_{s}$ itself (when non-zero) is independent of the design. But the design is rarely without effect on $\\phi_{s}$ .This is not surprising. Normally, the simpler the design, and the more straightforward the subject's task, the larger $\\pmb{\\mathscr{p}}_{s}$ will be—-a circumstance that is apt to offset the theoretical advantages of the no-tie system. <PARAGRAPH_SEPARATOR> # Ties in Preference Tasting <PARAGRAPH_SEPARATOR> When preference is at issue, three classes of subject can be distinguished: <PARAGRAPH_SEPARATOR> Those (a proportion $\\pmb{\\mathscr{p}}_{i,\\pmb{\\mathscr{N}}}$ who prefer $\\textit{i}$ Those (a proportion $\\scriptstyle{p_{j}}.$ who prefer $j$ The rest (who have no preference) <PARAGRAPH_SEPARATOR> But not all of the third (the “indifferent') class will say they have no preference; some of them, some of the time, will guess, and we assume that there is a probability $\\scriptstyle{p_{g}}$ that any member of this class, on a particular occasion, will in fact state a pseudopreference. We must further suppose that such decisions will not be influenced by extraneous circumstances or characteristics of the compared items, so that there is an equal chance of choosing $\\textit{i}$ or $j$ in the absence of preference. To take an exaggerated example: it would be methodologically wrong to put chocolates $\\mathbf{\\nabla}_{i}\\mathbf{\\cdot}_{i}$ in a handsome white pack embellished with a pretty girl, and to put $\\mathfrak{f}^{}$ in an ill-made red pack covered with a hammer-and-sickle motif. This does not however imply that all extraneous differences should be equalised; it may be best to.retain them and “balance them out'. Thus we might put half the $\\mathbf{\\nabla}_{i}^{\\star}$ chocolates in white packs and the other half in red packs, and ditto with the $\\mathfrak{f}^{}$ chocolates; then the two main pairings (white: red and red: white) or perhaps all four possible pairings (white: red and red: white, red: red, and white: white) should be equally distributed among trials. Then, ties being permitted, the composition of the three kinds of answer will be: <PARAGRAPH_SEPARATOR> And if ties are prohibited (which means that $\\scriptstyle{p_{g}}$ is constrained to unity), wehave <PARAGRAPH_SEPARATOR> $$ \\left.\\begin{array}{r l}&{P_{i}=\\rlap/p_{i}+(1-p_{i}-p_{j})p_{g}/2}\\ &{P_{j}=p_{j}+(1-p_{i}-p_{j})p_{g}/2}\\ &{P_{\\mathbf{0}}=1-P_{i}-P_{j}=(1-p_{i}-p_{j})(1-p_{g})}\\end{array}\\right\\} $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{P_{i}=(1+{p_{i}}-{p_{j}})/2}\\ {P_{j}=1-P_{i}=(1-{p_{i}}+{p_{j}})/2}\\end{array} $$ <PARAGRAPH_SEPARATOR> Now, often and perhaps mostly, it is helpful not only to test $\\mathrm{H}_{0}\\left({\\pmb{\\mathit{p}}}_{i}={\\pmb{\\mathit{p}}}_{j}\\right)$ but to be able to estimate $\\scriptstyle{\\boldsymbol{p}}_{i}$ and $\\phi_{j}$ .In other words the size of the 'indifferent group, a proportion $1-\\phi_{i}-\\phi_{j}$ of the whole, is a matter of consequence. (If $\\mathbf{\\nabla}_{i}\\mathbf{\\cdot}_{}$ and $\\mathfrak{f}^{}$ are experimental products a decision on whether either or both should be marketed would turn on such information.) Therefore ties should be allowed in answers. Even so, it does not necessarily follow how estimates of the class sizes should be arrived at. We present the choice of chocolates to a sample of $\\mathcal{N}$ subjects, and we observe that a number $r_{i}$ say that they prefer $\\mathbf{\\epsilon}_{i}\\mathbf{\\cdot}\\mathbf{\\sigma} $ , and a number $r_{j}$ that they prefer $\\mathfrak{f}^{}$ , so that $\\dot{r}_{i}/\\dot{\\mathcal{N}}=\\hat{P}_{i},$ and $r_{j}^{\\mathrm{~\\scriptsize~1~}}/N=\\hat{P}_{j}$ . But all we have here are two independent equations for thrce parameters, so that no solution is possible. It is to the credit of G. E. Ferris8 to have couched this problem in clear statistical terms, to have seen that when replication is introduced a solution is possible, and so to have put forward the $^{\\bullet}k$ visit method'. <PARAGRAPH_SEPARATOR> # The $k$ -visit Method <PARAGRAPH_SEPARATOR> Let each subject state his preference, in each pair comparison, on $k$ $(k>1)$ different occasions. (Incidentally, if there are extraneous differences, such as varied packs, they can now be balanced out within each subject's $k$ trials.) Any complete ( $k\\cdot$ -fold) result falls into one of $3^{k}$ categories, whose expectations can readily be found from the fundamental assumptions and equations. Estimation of the three parameters then becomes possible (although $\\scriptstyle{\\pmb{\\mathscr{p}}}_{\\pmb{\\mathscr{g}}}$ is of no practical interest). For the simplest case, $k=2$ , Ferris gives the maximum-likelihood solutions and their standard errors."
  },
  {
    "qid": "statistic-posneg-2044-2-0",
    "gold_answer": "FALSE. Explanation: The parameter $\\phi_i$ is not the probability of exceeding the threshold $u_i$. Instead, $\\phi_i$ is the scale parameter in the generalized Pareto distribution (GPD) used to model the exceedances over the threshold. The probability of exceeding the threshold is determined by the rate at which exceedances occur, which is a separate parameter in the PoT model. The conditional probability formula describes the distribution of exceedances given that the threshold has been exceeded, not the probability of exceeding the threshold itself.",
    "question": "In the Peaks-over-Threshold random effects model, the conditional probability $\\operatorname*{Pr}[Y_{i j}\\leq y\\mid Y_{i j}>u_{i}]$ is given by $1-\\phi_{i}\\biggl[1+\\frac{\\xi_{i}}{\\psi_{i}}(y-u_{i})\\biggr]_{+}^{-1/\\xi_{i}}$. Is it correct to interpret $\\phi_i$ as the probability that an observation in year $i$ exceeds the threshold $u_i$?",
    "question_context": "Peaks-over-Threshold Random Effects Model for Extreme Value Analysis\n\nThe most common extension of the vanilla PoT model is to model one or more of the distribution parameters $(\\phi,\\psi,\\xi)$ as a function of covariates. The motivation for this is that most physical processes display changes in their behaviour over time. Such changes may be cyclic, smooth or sudden and often arise in response to changes in one or more of climate, weather conditions or other geophysical processes. While many commonly used regression modelling frameworks, both parametric (Davison & Smith, 1990; Eastoe & Tawn, 2009; Smith, 1989) and semi-parametric (Hall & Tajvidi, 2000; Jones et al., 2016; Yee & Stephenson, 2007; Youngman, 2019), have been incorporated into the PoT framework, there are several barriers that prevent their successful implementation. Firstly, since most environmental processes are driven by multiple interacting factors, appropriate drivers will often be either unknown or unobserved, or both. Secondly, PoT data sets are small by definition and there is frequently insufficient information to identify the often subtle covariate relationships. Lastly, while the motivating extreme value theory provides a justification to extrapolate into the underlying tail distribution given values of the covariates, there is no similar justification to extrapolate the covariate-response relationship beyond the measurement period. This is a serious limitation for climate change forecasts.<PARAGRAPH_SEPARATOR>A plausible alternative is the PoT random effects model which overcomes the above limitations for cases where changes in extremal behaviour occur between, but not within, years. Previously applied to hydrological data by Eastoe (2019) and Towe et al. (2020), inter-year variability in both the magnitudes and frequency of the extremes is modelled by latent variables (or ‘random effects’) which vary between years. The advantage of this framework is that, unlike a parametric regression, it requires no prior specification of the functional form for the inter-year variability. In this sense, the model is a compromise between the rigid fully parametric regression model and the flexible nonparametric regression. It can be used to identify long-term trends, unusually extreme years, and groupings of years in which the extremal behaviour is similar. The model also has the capability to make predictions about extreme episodes over a period of time (e.g., a number of decades) but not to do so for a specific out-of-sample year. Although it is beyond the scope of the current paper, such predictions could be achieved by incorporating serial dependence in the random effects.<PARAGRAPH_SEPARATOR>For generality, we define a threshold $u_{i}$ for year $i$ . This allows for cases in which it is appropriate for the threshold, and hence the definition of an extreme observation, to vary between years. Let $Y_{i j}$ be the jth observation in year $i$ then, for $y>u_{i}$ ,<PARAGRAPH_SEPARATOR>$$\\operatorname*{Pr}[Y_{i j}\\leq y\\mid Y_{i j}>u_{i}]=1-\\phi_{i}\\biggl[1+\\frac{\\xi_{i}}{\\psi_{i}}(y-u_{i})\\biggr]_{+}^{-1/\\xi_{i}}$$<PARAGRAPH_SEPARATOR>where<PARAGRAPH_SEPARATOR>$$\\psi_{i}\\sim F_{\\psi}(\\psi_{i}\\mid\\theta_{\\psi}),\\quad\\xi_{i}\\sim F_{\\xi}(\\xi_{i}\\mid\\theta_{\\xi}),\\quad\\phi_{i}\\sim F_{\\phi}(\\phi_{i}\\mid\\theta_{\\phi})$$<PARAGRAPH_SEPARATOR>and $F_{\\psi},F_{\\xi}$ , and $F_{\\phi}$ are prespecified distributions with parameters $\\theta_{\\psi},\\theta_{\\xi}$ , and $\\theta_{\\phi}$ , respectively. In hierarchical modelling terminology, equations (2) and (3) are data and latent process models, respectively, and $(\\psi_{i},\\xi_{i},\\phi_{i})$ are random effects for year $i$ . We assume throughout that the responses $Y_{i j}$ are independent given the random effects and that the random effects are mutually independent and serially uncorrelated.<PARAGRAPH_SEPARATOR>Let $y$ denote the full vector of data, $N$ be the number of years in the observation period and $n_{i}$ be the number of observations in each year. Denote the vectors of random effects by $\\psi=(\\psi_{1},...,\\psi_{N})$ , $\\xi=(\\xi_{1},...,\\xi_{N})$ , $\\boldsymbol{\\phi}=(\\phi_{1},\\dots,\\phi_{N})$ . The starting point for inference is the joint likelihood function<PARAGRAPH_SEPARATOR>$$L(y\\mid\\psi,\\xi,\\phi,\\theta_{\\psi},\\theta_{\\xi},\\theta_{\\phi})=L(y\\mid\\psi,\\xi,\\phi)f(\\psi\\mid\\theta_{\\psi})f(\\xi\\mid\\theta_{\\xi})f(\\phi\\mid\\theta_{\\phi}),$$<PARAGRAPH_SEPARATOR>where<PARAGRAPH_SEPARATOR>$$L(\\boldsymbol{y}\\mid\\boldsymbol{\\psi},\\boldsymbol{\\xi},\\boldsymbol{\\phi})=\\prod_{i=1}^{N}\\prod_{j=1}^{n_{i}}\\left\\{\\phi_{i}\\psi_{i}^{-1}\\biggl[1+\\frac{\\xi_{i}}{\\psi_{i}}(y_{i j}-u_{i})\\biggl]_{+}^{-1/\\xi_{i}-1}\\right\\}^{\\parallel[y_{i j}>u_{i}]}(1-\\phi_{i})^{\\parallel[y_{i j}\\leq u_{i}]}$$<PARAGRAPH_SEPARATOR>and $f(\\psi|\\theta_{\\psi}),f(\\xi|\\theta_{\\xi})$ , and $f(\\phi|\\theta_{\\phi})$ are each the product of the relevant distribution taken from equation (3), for example $\\begin{array}{r}{f(\\psi|\\theta_{\\psi})=\\prod_{i=1}^{N}f_{\\psi}(\\psi_{i}|\\theta_{\\psi})}\\end{array}$ where $f_{\\psi}$ is the density associated with $F_{\\psi}$ . Note that the dimension of the full parameter vector is $3N+|\\theta_{\\psi}|+|\\theta_{\\xi}|+|\\theta_{\\phi}|$ .<PARAGRAPH_SEPARATOR>Since the random effects cannot be integrated out of the joint likelihood function, maximum likelihood parameter estimates must be obtained through numerical optimization of the very highdimensional log-likelihood function. A more pragmatic approach is to use Bayesian inference; to do so, prior distributions $\\pi_{\\psi}(\\psi)$ , $\\pi_{\\xi}(\\xi)$ , and $\\pi_{\\phi}(\\phi)$ must now be specified for $\\theta_{\\psi},\\theta_{\\xi}$ , and $\\theta_{\\phi}$ , respectively. We recommend that these are chosen to be as uninformative as possible, e.g., flat Gaussian distributions, in order to avoid undue impact on the analysis. Combining the prior distributions with the likelihood function (4), the joint posterior is<PARAGRAPH_SEPARATOR>$$\\pi(\\theta_{\\psi},\\theta_{\\xi},\\theta_{\\phi},\\psi,\\xi,\\phi)=L(y\\mid\\psi,\\xi,\\phi)f(\\psi\\mid\\theta_{\\psi})f(\\xi\\mid\\theta_{\\xi})f(\\phi\\mid\\theta_{\\phi})\\pi_{\\psi}(\\psi)\\pi_{\\xi}(\\xi)\\pi_{\\phi}(\\phi).$$<PARAGRAPH_SEPARATOR>Since closed forms cannot be found for the joint and marginal posterior distributions, the posterior distribution is estimated using a sampling algorithm. We use an adaptive Markov chain Monte Carlo (MCMC) algorithm to enable automatic tuning of the proposal step-sizes and achieve fast convergence. Our algorithm is based on the adaptive Metropolis–Within–Gibbs scheme proposed by Roberts and Rosenthal (2009). Each parameter is updated separately via a Metropolis-Hastings random walk (MHRW) step, allowing separate evolution of the MHRW scaling parameter associated with each of the model parameters. Earlier work by Eastoe (2019) showed that of a number of possible alternatives, this algorithm worked the best for this particular model.<PARAGRAPH_SEPARATOR># 3 Case study 1: US air temperatures<PARAGRAPH_SEPARATOR>This case study is motivated by the 2021 heatwave which affected three western states in the United States. An analysis of climate model annual maxima by Philip et al. (2021) showed that the observed 2021 annual maximum were inconsistent with historical extremes. Further, this finding recurred across a number of stations and held despite adjustments made to the data to account for long-term trends in global temperature. Our objectives are to examine whether (a) an analysis of observational annual maxima is consistent with the ERA5 analysis of Philip et al. (2021), (b) an analysis of PoT data points to the same conclusions as an analysis of annual maxima, and (c) a more flexible model for long-term trends captures better the 2021 event."
  },
  {
    "qid": "statistic-posneg-1942-0-1",
    "gold_answer": "FALSE. While the paper claims $L_2$-consistency and provides bounds like $E(\\widetilde{v}_c^2 - v_c^2)^2 = O\\left(\\frac{1}{m_1} + \\frac{1}{m_2}\\right)$, it does not explicitly verify Assumption 3.1's conditions (e.g., moment requirements or dependence structures) in the proof. The bounds alone are insufficient to guarantee $L_2$-consistency without linking them to the assumption's specific constraints.",
    "question": "Does the paper correctly assert that the variance estimator $\\widetilde{\\sigma}_{\\theta}^2$ is $L_2$-consistent for $\\sigma_{\\theta}^2$ under Assumption 3.1?",
    "question_context": "Asymptotic Unbiasedness and Consistency in Matched Pairs with Missing Data\n\nThe paper examines the asymptotic properties of estimators in matched pairs with missing data. It focuses on proving that the estimator $\\widehat{p}_{\\theta}$ is asymptotically unbiased and consistent. The proofs involve decomposing the estimator into components based on complete and incomplete pairs, and analyzing their expectations and variances. The Glivenko-Cantelli theorem is used to establish strong consistency, and the paper also discusses the $L_2$-consistency of variance estimators under certain assumptions."
  },
  {
    "qid": "statistic-posneg-86-1-0",
    "gold_answer": "FALSE. The bootstrap .632 estimator is designed to balance the optimism of the resubstitution estimate ($R^{\\mathrm{LS}}$) with the pessimism of the out-of-bootstrap estimate ($R^{\\mathrm{TS}}$). However, its performance relative to cross-validation depends on the specific dataset and the nature of the classification problem. There is no universal guarantee that it will always outperform cross-validation, as the effectiveness of the estimator depends on factors like the sample size, the complexity of the model, and the distribution of the data.",
    "question": "Is the bootstrap .632 risk estimator $(.632)(R^{\\mathrm{TS}})+(.368)(R^{\\mathrm{LS}})$ guaranteed to outperform cross-validation in all scenarios?",
    "question_context": "Bootstrap .632 Risk Estimation for Classification Trees\n\nThe paper describes the bootstrap .632 approach for risk estimation in classification trees, which involves creating separate learning and test samples from the original learning sample. The method computes a global estimate of risk for each bootstrap sample and averages these to obtain the final estimate. The formula for the global estimate of risk is given by $(.632)(R^{\\mathrm{TS}})+(.368)(R^{\\mathrm{LS}})$, where $R^{\\mathrm{TS}}$ and $R^{\\mathrm{LS}}$ are the test and learning sample risks, respectively. The paper also discusses Breiman's method for improving the resubstitution estimate of misclassification cost by adjusting the empirical probabilities at terminal nodes."
  },
  {
    "qid": "statistic-posneg-166-0-3",
    "gold_answer": "FALSE. The context states that for G(x, y) = exp(min(x, y)), there is complete dependence (U = V), and thus Z is the constant function 1/2, not uniformly distributed on (0,1). The uniform distribution applies only in the independent case (G(x, y) = exp(x + y)).",
    "question": "Is the assertion 'For the case where (U, V) has df G(x, y) = exp(min(x, y)), the Pickands coordinate Z is uniformly distributed on (0,1)' correct?",
    "question_context": "Pickands Coordinates and Bivariate Extreme Value Distributions\n\nThe paper discusses Pickands coordinates, which are used to investigate the tail behavior in bivariate extreme value models. The coordinates are defined as c := x + y < 0 and z := y / (x + y) ∈ [0,1], transforming the vector (x, y) into (c(1 - z), cz). The distribution function G of a random variable (U, V) with reverse exponential margins is max-stable and represented as G(x, y) = exp((x + y)D(y / (x + y))), where D is a dependence function. The paper explores properties of the generalized Pareto (GP) distribution W, tail equivalence between G and W, and the distribution of Pickands coordinates (Z, C) for (U, V)."
  },
  {
    "qid": "statistic-posneg-61-0-1",
    "gold_answer": "TRUE. For symmetrical $s_{v}$ curves, $\\beta_{\\mathbf{1}}=0$ implies no skewness, leading to $\\gamma=0$. The formula $\\omega=\\{(2\\beta_{2}-2)^{\\frac{1}{2}}-1\\}^{\\frac{1}{2}}$ is used to determine $\\omega$ based on $\\beta_{2}$, which is consistent with the symmetrical case.",
    "question": "For symmetrical $s_{v}$ curves, is it correct that $\\gamma=0$ and $\\omega$ is determined by $\\omega=\\{(2\\beta_{2}-2)^{\\frac{1}{2}}-1\\}^{\\frac{1}{2}}$?",
    "question_context": "Fitting Johnson Curves by Moments\n\nThis algorithm supplements Tables 34, 35 and 36 of Pearson and Hartley (1972), for $s_{v}$ and $S_{B}$ curves. These tables are perfectly adequate for many purposes, but interpolation or extrapolation may be hazardous when the required curve is near one of the boundaries. <PARAGRAPH_SEPARATOR> # NUMERICAL METHOD <PARAGRAPH_SEPARATOR> Defining, as is customary, $\\sqrt{\\beta_{1}}$ as $\\mu_{3}/\\sigma^{3}$ and $\\beta_{\\mathbf{2}}$ as $\\mu_{4}/\\sigma^{4}$ the $s_{L}$ curves lie on a line in the $\\beta_{\\mathbf{1}}\\beta_{\\mathbf{2}}$ plane—thus for these curves $\\beta_{\\mathbf{1}}$ determines $\\beta_{\\mathbf{2}}$ Using $\\omega$ to denote exp $(\\delta^{-2})$ the $S_{L}\\beta_{\\mathbf{2}}$ value is found by solving <PARAGRAPH_SEPARATOR> $$ (\\omega-1)(\\omega+2)^{2}=\\beta_{1} $$ <PARAGRAPH_SEPARATOR> for $\\pmb{\\omega}$ , and then evaluating <PARAGRAPH_SEPARATOR> $$ \\beta_{2}=\\omega^{4}+2\\omega^{3}+3\\omega^{2}-3. $$ <PARAGRAPH_SEPARATOR> If the required $\\beta_{\\mathbf{2}}$ is less than this value, $\\pmb{S_{B}}$ (or $s_{x}$ is appropriate; if greater, $s_{v}$ is appropriate. <PARAGRAPH_SEPARATOR> (1) $s_{L}$ curves $\\pmb{\\omega}$ having been evaluated as above, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l l}{{\\delta=(\\ln\\omega)^{-\\frac{1}{4}},}}&{{\\gamma=\\frac{1}{2}\\delta\\ln\\left\\{\\omega(\\omega-1)/\\mu_{2}\\right\\},}}\\ {{\\xi=\\pm\\mu_{1}^{\\prime}-\\exp\\left\\{(1/2\\delta-\\gamma)/\\delta\\right\\},}}&{{\\lambda=\\pm1,}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where the $\\pm$ is determined in each case to be the sign of $\\pmb{\\mu_{3}}$ .As Johnson (1949) points out, only three parameters are necessary for an $s_{L}$ curve, but we have found it convenient to include $\\lambda$ as above. <PARAGRAPH_SEPARATOR> # (2) $s_{v}$ curves <PARAGRAPH_SEPARATOR> When $\\beta_{\\mathbf{1}}=0$ , the required curve is symmetrical, and <PARAGRAPH_SEPARATOR> $$ \\omega=\\{(2\\beta_{2}-2)^{\\frac{1}{2}}-1\\}^{\\frac{1}{2}};~\\hat{\\delta}=(\\ln\\omega)^{-\\frac{1}{2}};~\\gamma=0. $$ <PARAGRAPH_SEPARATOR> For an asymmetrical curve <PARAGRAPH_SEPARATOR> $$ \\omega_{1}=\\{(2\\beta_{2}-2{\\cdot}8\\beta_{1}-2)^{\\sharp}-1\\}^{\\sharp} $$ <PARAGRAPH_SEPARATOR> is taken as a first estimate, and $\\omega,\\delta$ and $\\gamma$ found by Johnson's iterative method (Elderton and Johnson, 1969, p. 127). The sign of $\\gamma$ is set to be the opposite of that of $\\pmb{\\mu_{3}}$ <PARAGRAPH_SEPARATOR> In either case $\\pmb{\\xi}$ and $\\lambda$ are then found from <PARAGRAPH_SEPARATOR> $$ \\mu_{2}={\\textstyle\\frac{1}{2}}\\lambda^{2}(\\omega-1)\\{\\omega\\cosh{(2\\gamma/\\delta)}+1\\};\\mu_{1}^{\\prime}=\\xi-\\lambda\\omega^{\\frac{1}{2}}\\sinh{(\\gamma/\\delta)}. $$ <PARAGRAPH_SEPARATOR> (3) $\\pmb{S_{B}}$ curves <PARAGRAPH_SEPARATOR> Approaching the $s_{\\tau}$ boundary, $\\mathfrak{d}{\\to}0$ ; approaching the $s_{L}$ boundary 8 tends to the same value as for an $s_{L}$ curve. A first approximation to 8 can be found by interpolating between these two values. The interpolation is made by assuming the shape of the function to be the same at the required $\\beta_{\\mathbf{1}}$ value as it is between the same two 8 values when $\\beta_{1}=0$ This is well approximated by <PARAGRAPH_SEPARATOR> $$ \\hat{\\delta}=(0\\cdot626\\beta_{2}-0\\cdot408)/(3\\cdot0-\\beta_{2})^{0\\cdot479}\\quad\\mathrm{if~}\\beta_{2}\\geqslant1\\cdot8, $$ <PARAGRAPH_SEPARATOR> and by <PARAGRAPH_SEPARATOR> $$ \\updelta=0{\\cdot}8(\\beta_{2}{-}1)\\quad\\mathrm{otherwise}. $$ <PARAGRAPH_SEPARATOR> For a given $\\beta_{\\pm}$ and frst approximation to 8, a first approximation to $\\gamma$ is found using formulae due to Draper (1951). <PARAGRAPH_SEPARATOR> # APPLIEDSTATISTICS <PARAGRAPH_SEPARATOR> Evaluation of the first six moments at the given 8 and $\\gamma$ values, using Draper's (1952) form of Goodwin's (1949) integral, then enables a two-dimensional Newton-Raphson process to converge on the required values."
  },
  {
    "qid": "statistic-posneg-670-0-1",
    "gold_answer": "FALSE. The bound is derived under the assumption that the random variables are identically distributed with a common distribution satisfying the given moment conditions, but it specifically assumes maximal dependence in the Lai and Robbins sense. The paper explicitly states that the inequality is sharp under this maximal dependence condition, and it would not necessarily hold for arbitrary dependence structures.",
    "question": "Does the bound E(X_{n:n}) ≤ [(n(n-1)^{p-1})/(1+(n-1)^{p-1})]^{1/p} hold without any assumptions on the dependence structure of the random variables X₁, X₂, ..., Xₙ?",
    "question_context": "Bounds on the Expectation of the Maximum of Dependent Random Variables\n\nThe paper discusses bounds on the expectation of the maximum of dependent random variables with a common distribution function F, assuming mean 0 and finite p-th absolute moment. Various inequalities and theorems are presented to bound E(X_{n:n}), the expectation of the maximum order statistic, under different dependence and symmetry conditions."
  },
  {
    "qid": "statistic-posneg-1460-1-2",
    "gold_answer": "FALSE. While the paper does not explicitly state the asymptotic distribution of $\\mathcal{D}(\\pmb{\\beta}_{o},\\widehat{\\pmb{\\Sigma}})$, it is implied by the asymptotic normality of $\\widehat{\\beta}$ (Theorem 3.3) and the consistency of $\\widehat{\\pmb{\\Sigma}}$. However, the paper does not provide a formal proof that the test statistic follows a chi-squared distribution under $H_{0}$. The claim is plausible but not directly verified in the given context.",
    "question": "The test statistic $\\mathcal{D}(\\pmb{\\beta}_{o},\\widehat{\\pmb{\\Sigma}})=n(\\widehat{\\pmb{\\beta}}-\\pmb{\\beta}_{o})^{\\prime}\\widehat{\\pmb{\\Sigma}}^{-1}(\\widehat{\\pmb{\\beta}}-\\pmb{\\beta}_{o})$ has an asymptotic chi-squared distribution under the null hypothesis $H_{0}:{\\boldsymbol{\\beta}}=\\pmb{\\beta}_{o}$, even when the weight function $w(\\cdot)$ is a hard rejection weight. Is this claim supported by the paper's results?",
    "question_context": "Robust Testing in Logistic Regression with Weighted M-Estimators\n\nThis paper discusses robust estimators for logistic regression, focusing on weighted M-estimators that downweight outliers in the explanatory variables. The estimators are defined by minimizing a robust loss function with a bias correction term. The asymptotic properties of these estimators are derived, including consistency and normality, and a Wald-type test statistic is proposed for hypothesis testing."
  },
  {
    "qid": "statistic-posneg-806-2-1",
    "gold_answer": "FALSE. The context clarifies that the closed-form Hjort and Jones estimators, including $\\hat{f}_{\\mathrm{quad}}$, are specific to the normal kernel and locally parametric forms. Other kernel choices or local functions would require substantial numerical optimization and could face difficulties, as noted in the paper.",
    "question": "Is the local log-quadratic estimator $\\hat{f}_{\\mathrm{quad}}$ generally applicable to any kernel choice without requiring numerical optimization?",
    "question_context": "Comparison of Density Estimation Methods: Data Sharpening vs. Higher Order Kernels\n\nIn the simulation study that occupies the remainder of this section we compare data sharpening with local (log-)polynomial methods and techniques based on high order kernels. In company with the latter approach, data sharpening can be implemented virtually by hand for moderate sample sizes. However, unlike that approach the data sharpening density estimator is a bona fide density—it is non-negative and integrates to 1. Local polynomial methods produce a non-negative estimator that may fail to integrate to 1. However, Simonoff (1998) has provided an ingenious device, based on Poisson regression estimation on a fine grid, for guaranteeing that the integral equals 1 as accurately as desired. <PARAGRAPH_SEPARATOR> The main potential advantage of local polynomial methods is their theoretical immunity to edge effects. In practice, however, this can be more of an asymptotic advantage than a practical one, since the performance of local polynomial methods for density estimation at boundaries depends in unexpected ways on the slope at the boundary and the oddness or evenness of the degree of the fitted polynomial. For instance, the performance at the boundary is substantially reduced if one repeats for a local linear estimator the experiment reported in Fig. 1 of Loader (1996) for the local constant case. It may be shown that in this example a local linear estimator has an order of magnitude more boundary bias than a local constant estimator has. <PARAGRAPH_SEPARATOR> To investigate finite sample properties of sharpened estimators a simulation study was conducted. The results for three densities are reported here; the densities were the standard normal and Marron and Wand's (1992) strongly skewed and claw densities, for which the distributions are respectively those of <PARAGRAPH_SEPARATOR> $$ \\sum_{l=0}^{7}\\frac{1}{8}N\\left[3\\left\\{\\left(\\frac{2}{3}\\right)^{l}-1\\right\\},\\left(\\frac{2}{3}\\right)^{2l}\\right] $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ {\\frac{1}{2}}N(0,1)+\\sum_{l=0}^{4}{\\frac{1}{10}}N\\left({\\frac{1}{2}}l-1,{\\frac{1}{100}}\\right). $$ <PARAGRAPH_SEPARATOR> For brevity we shall give results only in the case of explicitly sharpened estimators $\\tilde{f_{r}}$ , not the implicitly sharpened form $\\hat{f}_{r}$ . Properties of the latter are very similar but depend a little on the protocol that is used to define $\\hat{\\gamma}_{r}(x)$ when $\\hat{H}_{r}$ is not monotone increasing in the neighbourhood of $x$ . <PARAGRAPH_SEPARATOR> For each density, 1000 samples were drawn for sample sizes ranging from $10^{2}$ to $10^{5}$ Estimates were generated for each sample at 301 points from $^{-3}$ to 3, over a range of 101 bandwidths (0.1±2.0 for the normal; 0.02±2.0 for the skewed and claw), using seven different estimation methods. In addition to the second-order kernel estimator $\\hat{f}$ and the fourth- and sixth-order sharpened estimators $\\tilde{f_{4}}$ and $\\tilde{f_{6}}$ (all using the normal kernel $\\phi$ ), we also computed two additional pairs of estimators, as follows. More traditional higher order estimators were represented by higher order kernel estimators $\\check{f}_{4}$ and $\\check{f}_{6}$ using the normal-based fourth- and sixth-order kernels, $K_{4}(t)={\\textstyle{\\frac{1}{2}}}(3-t^{2})\\phi(t)$ and $K_{6}(t)={\\textstyle{\\frac{1}{8}}}(t^{4}-10t^{2}+15)\\phi(t)$ . And as examples of alternative approaches to density estimator adjustment we computed the normal-kernel-based local log-linear and local log-quadratic estimators of Hjort and Jones (1996), <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\hat{f}_{\\mathrm{lin}}=\\hat{f}\\exp\\big\\{-\\frac{1}{2}h^{2}(\\hat{f}^{'}/\\hat{f})^{2}\\big\\},~}\\\\ {\\hat{f}_{\\mathrm{quad}}=\\hat{f}\\hat{R}\\exp\\big\\{-\\frac{1}{2}h^{2}\\hat{R}^{2}(\\hat{f}^{'}/\\hat{f})^{2}\\big\\},~}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\hat{D}=\\hat{f}^{\\prime\\prime}/\\hat{f}-(\\hat{f}^{\\prime}/\\hat{f})^{2}$ and $\\hat{R}=(1+h^{2}\\hat{D})^{-1/2}$ if $1+h^{2}\\hat{D}>0$ , and $\\hat{R}=0$ otherwise. Note that, although both the higher order kernels and our sharpened estimators are quite general, the calculations leading to the closed form Hjort and Jones estimators are specific to the normal kernel and these particular locally parametric forms. Other choices of kernel or local function require substantial numerical optimization and can suffer from the sorts of difficulties noted in Section 1. <PARAGRAPH_SEPARATOR> In the interest of simplicity and clarity we focus on the original second-order estimator ${\\hat{f}},$ the sixth-order estimators $\\tilde{f_{6}}$ and $\\check{f}_{6}$ , and the local log-quadratic estimator $\\hat{f}_{\\mathrm{quad}}$ . In general, the fourth-order estimators performed, as expected, between the second-order estimator and their respective sixth-order companions. The local log-linear estimator performed similarly to the second-order estimator. In the case of normal data a comparison with $\\hat{f}_{\\mathrm{quad}}$ is not informative, since the normal distribution is globally log-quadratic and, consequently, for a suitably chosen fixed bandwidth (not converging to zero as the sample size increases) $\\hat{f}_{\\mathrm{quad}}$ is root $n$ consistent for $f.$ Therefore in the normal data case we shall omit $\\hat{f}_{\\mathrm{quad}}$ from our comparative study. <PARAGRAPH_SEPARATOR> For each sample, each estimator and each bandwidth, integrated squared errors were calculated using the true underlying density. These were then averaged over all the samples for each estimator and bandwidth. Fig. 3 shows results when $f$ is the normal density. Several points are worth making here. First, the sixth-order methods substantially outperform the traditional second-order kernel estimator (and the fourth-order methods, not shown), if all are allowed to use optimal, or nearly optimal, bandwidths. This remains true even for the $n=100$ case, contrary to the conventional wisdom that particularly large sample sizes are required for sixth-order, or even fourth-order, methods to be beneficial. It is true, however, that the relative gain is greater in moving from a secondorder to a fourth-order estimator than when moving on to the sixth order. The sixthorder sharpened estimator has a smaller minimal average integrated squared error AISE than its sixth-order kernel counterpart, by a factor which varies with the sample size from $21\\%$ to $30\\%$ ."
  },
  {
    "qid": "statistic-posneg-560-2-0",
    "gold_answer": "TRUE. The formula correctly accounts for the covariance term (-2σχσχ′γ′x³,χ′³) between the two correlated samples, which is essential for calculating the variance of their difference. This aligns with standard statistical theory where the variance of the difference between two correlated variables includes their individual variances minus twice their covariance.",
    "question": "Is the formula for the variance of the difference between two samples, $$\\sigma^{2}x^{3}-\\chi^{\\prime}{}^{3}=\\sigma^{2}x^{3}+\\sigma^{2}\\chi^{\\prime}{}^{3}-2\\sigma\\chi\\sigma\\chi^{\\prime}\\gamma^{\\prime}x^{3},\\chi^{\\prime}{}^{3}$$, correctly derived assuming the samples are correlated?",
    "question_context": "Experimental Analysis of Goodness-of-Fit Using Chi-Square Test\n\nThe study involves testing the goodness-of-fit between raw basic samples, graduated basic samples, and the parent population using the chi-square test. The main experimental work tests 100 samples of 30 drawn from the parent population against the graduated basic sample, treated as a step-parent population. The analysis includes the mean difference, standard deviation, and correlation between the samples. The formulas provided relate to the variance and standard deviation calculations under specific conditions."
  },
  {
    "qid": "statistic-posneg-2107-0-1",
    "gold_answer": "TRUE. The context clarifies that directed cycles (feedback loops) in the path diagram inherently make the system non-recursive, as they violate the acyclic dependency structure required for recursiveness. This holds even if ψ is diagonal (uncorrelated errors). The example in the text explicitly contrasts system (I) (recursive when ignoring correlated errors E1 and E2) with system (II) (non-recursive due to its directed cycle Y4 → Y3 → Y2 → Y4, irrespective of ψ12 ≠ 0).",
    "question": "The presence of a directed cycle in a path diagram (e.g., Y4 → Y3 → Y2 → Y4) implies that the associated LSEM is non-recursive, regardless of the structure of the error covariance matrix ψ. Is this claim correct?",
    "question_context": "Linear Structural Equation Models and Path Diagrams\n\nThe paper discusses linear structural equation systems for Gaussian variables, their associated path diagrams, and the set of probability distributions defined by such systems. It introduces the LISREL-type specification of a linear structural equations system and explains the relationships between observed and unobserved variables, error terms, and the covariance structure. The paper also covers the concept of recursive and non-recursive systems, path diagrams, and the implications of directed cycles in such diagrams."
  },
  {
    "qid": "statistic-posneg-916-0-0",
    "gold_answer": "FALSE. The correct F1 score formula should include both false positives and false negatives in the denominator: $F_{1}=\\frac{2\\times\\#\\mathrm{oftrue}\\mathrm{positives}}{2\\times\\#\\mathrm{oftrue}\\mathrm{positives}+\\#\\mathrm{offalse}\\mathrm{positives}+\\#\\mathrm{offalse}\\mathrm{negatives}}$. The provided formula omits false negatives, which are crucial for assessing the balance between precision and recall in variable selection.",
    "question": "Is the F1 score formula correctly specified as $F_{1}=\\frac{2\\times\\#\\mathrm{oftrue}\\mathrm{positives}}{2\\times\\#\\mathrm{oftrue}\\mathrm{positives}+\\#\\mathrm{offalse}\\mathrm{positives}}$ for evaluating variable selection performance in spatial GLLVMs?",
    "question_context": "Evaluation of GEE-Assisted Forward Regression for Spatial Latent Variable Models\n\nWe performed a numerical study to evaluate the proposed GEEassisted forward regression procedure. That is, we generated multivariate spatial data from a sparse spatial GLLVM, and then applied Algorithm 1 in conjunction with an early stopping using SIC (setting $\\tau=\\log(N)\\log\\log(p(p))$ to select a relatively small model containing all the truly important covariates. We considered two simulation designs: one with $p$ fixed and data generated with similar attributes to those from the application in Section 6, and one with $p$ growing with $n$ . In both cases, we applied GEE-assisted forward regression with the working correlation matrix $\\pmb R$ set either to the identity matrix (IEE), or a separable nonindependence structure as detailed in Section 3 where the spatial smoothness parameter was fixed a-priori at one of three possible values, $\\nu=\\{0.5,1,1.5\\}$ .<PARAGRAPH_SEPARATOR>To our knowledge, there is no publicly available software which can perform variable selection on spatial GLLVMs. Thus, as baseline but nevertheless useful comparisons to our approach, we considered the following alternative methods for variable selection: (a) LASSO-penalized GLMs fitted separately to each response, implemented via the R package glmnet (Friedman, Hastie, and Tibshirani 2010), with the tuning parameter chosen based on the 10-fold cross-validation. Two versions of this approach were considered, based on choosing the tuning parameter which either minimizes the crossvalidation deviance, or the largest tuning lambda such that the error is within one standard error of the minimum; (b) backward elimination from the saturated GLM, performed separately for each response. That is, we fitted a saturated GLM to each response, and then applied backward selection using BIC via the stepAIC function in the MASS package (Venables and Ripley 2002). For both methods 1 and 2, which are based around GLMs, we used the same distribution as the one used in the underlying sparse spatial GLLVM for example, if the true spatial GLLVM generated counts from a negative binomial distribution, then we considered LASSOpenalized negative binomial GLMs, and performed backward elimination from a saturated negative binomial GLM (c) fitting the saturated spatial GLLVM, calculating $95\\%$ Wald intervals for each estimated regression coefficient, and then selecting only the predictors whose Wald interval excluded zero. The intervals were not adjusted for multiple comparison, and in additional exploration we found that making the adjustment for example, using the Bonferonni approach, led to worse performance. Fitting the saturated spatial GLLVM also serves as a useful basis for assessing computational burden of any potential selection approach based directly on spatial GLLVMs. Note that we purposefully choose not to consider a variable selection method based on spatially independent GLLVMs, in light of the recent results in Hui, Hill, and Welsh (2021a) regarding the potentially poor performance under misspecification of the spatial correlation in GLLVMs; see also the discussion above Equation (1). Finally, note all three methods above make explicit distributional assumptions, in contrast to our proposed GEE-assisted forward regression approach which relies only on moment assumptions.<PARAGRAPH_SEPARATOR>In both simulation settings, we considered sample sizes $N=\\{100,225,400,625\\}$ and for each value of $N$ simulated 400 datasets. We assessed selection performance of each method using four metrics: (a) false positive rate or FPR; (b) false negative rate or FNR; (c) the $\\mathrm{F}_{1}$ score, and (d) Accuracy, where<PARAGRAPH_SEPARATOR>$$F_{1}=\\frac{2\\times\\#\\mathrm{oftrue}\\mathrm{positives}}{2\\times\\#\\mathrm{oftrue}\\mathrm{positives}+\\#\\mathrm{offalse}\\mathrm{positives}}$$<PARAGRAPH_SEPARATOR>Both an $F_{1}$ score and Accuracy closer to one indicates a selection method that more effectively distinguishes between truly important versus unimportant coefficients.<PARAGRAPH_SEPARATOR># 5.1. Setting 1<PARAGRAPH_SEPARATOR>We used the application to the SO-CPR survey, detailed in Section 6, as the basis for generating multivariate spatial data. Specifically, the true model was set to a spatial GLLVM with $m=15$ responses, $p=17$ covariates including the intercept, and $d=3$ latent variables. We generated $N$ spatial units by randomly sampling spatial locations from a rectangular region whose boundaries were defined by the original sampling locations in the SO-CPR survey. Note these boundaries are fixed with increasing $N$ , meaning the simulation reflected an spatial infill setting. For unit $s=1,\\ldots,N$ we generated the $p$ - vector of covariates $\\pmb{x}_{s}=(x_{s1},\\ldots,x_{s p})^{\\top}$ by randomly sampling with replacement a row from the model matrix used in the application to the SO-CPR survey, noting the original dataset contained 579 spatial locations. As the true spatial GLLVM regression coefficients, we used the estimated coefficients values from the left panel of Figure 2 that is, based on applying GEEassisted forward regression to the SO-CPR survey. To complete the formulation, we each of the vectors of latent variables $z_{*k};k=1,2,3$ from a multivariate normal distribution with mean vector zero and correlation matrix characterized by a Matérn correlation function with $\\nu=1$ and $h_{k}=C_{k}\\bar{\\Delta}$ , where $\\bar{\\Delta}$ is that average pairwise distance between all $N$ simulated spatial locations and $C_{k}=0.1,0.5,1,$ , respectively, for $k=1,2,3$ . The corresponding loadings $\\lambda_{j};j=1,...,p$ were simulated independently from a normal distribution with mean zero and variance equal to 0.5. Finally, based on the above set up, we simulated multivariate spatial data with three possible response types: (a) Poisson counts; (b) negative binomial counts; (c) nonnegative continuous data from the Tweedie distribution. In the latter two cases, for the true dispersion $\\phi_{j}=1,\\ldots,p$ and power parameter $\\rho$ we used the values estimated from the GEE-assisted forward regression applied to the SO-CPR survey data. For reasons of brevity, we focus on the case of negative binomial counts below; results for the Poisson and Tweedie response cases are provided in supplementary material D, and present similar trends to those seen below. Also, for the LASSOpenalized GLMs, we only present results based on selecting the tuning parameter that minimizes the cross-validation deviance; results from the other version are also provided in supplementary material D."
  },
  {
    "qid": "statistic-posneg-1448-0-0",
    "gold_answer": "TRUE. The context states that if $\\mu$ is $(B,b,\\beta)$-decomposable, the same is true for all $\\mu_{t}, t>0$ because $\\pmb{\\mu}$ uniquely determines its semigroup $(\\mu_{t})_{t>0}$. This implies the decomposability property is preserved across the semigroup.",
    "question": "Is it true that if $\\mu$ is $(B,b,\\beta)$-decomposable, then all $\\mu_{t}, t>0$ are also $(B,b,\\beta)$-decomposable?",
    "question_context": "Operator-Semistable Measures and Decomposability\n\nDEFINITION. $\\mu$ is said to be $\\left(B,\\boldsymbol{b},\\beta\\right)$ -decomposable if the equation $\\mu_{\\beta}=B\\mu*\\varepsilon_{b}$ is valid. <PARAGRAPH_SEPARATOR> Trivially every $\\mu$ is $(I,0,1)$ -decomposable. Moreover if $\\mu$ is $(B,b,\\beta)$ decomposable the same is true for all $\\mu_{t},t>0$ (since $\\pmb{\\mu}$ uniquely determines its semigroup $\\left(\\mu_{t}\\right)_{t>0})$ <PARAGRAPH_SEPARATOR> Now let us denote by $L=L(\\mu)$ the collection of all triples $\\left(B,\\dot{b},\\beta\\right)$ such that $\\mu$ is $(B,b,\\beta)$ -decomposable. Let us identify $(B,b,\\beta)$ with the matrix <PARAGRAPH_SEPARATOR> $$ {\\binom{B}{0}}\\quad{\\binom{b}{\\beta}}\\in G L(\\mathbb{R}^{d+1}). $$ <PARAGRAPH_SEPARATOR> Then it can be easily seen that $\\pmb{L}$ is a closed subgroup of $G L(\\mathbb{R}^{d+1})$ . Hence $L$ is a Lie group. By $f((B,b,\\beta)):=\\beta$ there is defined a continuous homomorphism of $L$ into the multiplicative group $\\mathbb{R}_{+}^{*}$ <PARAGRAPH_SEPARATOR> Let us assume now that the measure $\\mu$ is full, i.e.,. $\\mu$ is not supported by a proper affine  subspace  of $V$ Then the compactness lemma of Sharpe, [12, Proposition 4] can be applied to the effect that the kernel $\\left\\{(B,b,1)\\in L\\colon\\mu=B\\mu\\ast\\varepsilon_{b}\\right\\}=:\\operatorname{Inv}(\\mu)$ of $f$ is compact and that the mapping $f$ is closed. Hence exactly one of the following three possibilities can occur: $f(L)=\\left\\{1\\right\\}$ ; or $f(L)=\\left\\{\\beta^{n}\\colon n\\in\\mathbb{Z}\\right\\}$ with some $\\beta\\in]0,1[$ ; or $f(L)=\\mathbb{R}_{+}^{*}$ . The case $f(L)=\\left\\{1\\right\\}$ being uninteresting let us discuss the other ones. <PARAGRAPH_SEPARATOR> 1. Let $f(L)=\\left\\{\\beta^{n}\\colon n\\in\\mathbb{Z}\\right\\}$ with some $\\beta\\in\\]0$ , 1[. We put $k_{n}:=[\\beta^{-n}]$ for all $n\\in\\mathbb{N}$ . Then  we  have lim $k_{n}/k_{n+1}=\\beta$ and lim $B^{n}\\mu^{*k_{n}}*\\varepsilon_{b_{n}}=\\mu$ for appropriate $b_{n}\\in V,n\\in\\mathbb{N}$ Hence $\\mu$ is operator-semistable [5, Definition 1]."
  },
  {
    "qid": "statistic-posneg-1318-0-3",
    "gold_answer": "FALSE. The error bound for the TC case in Theorem 4 is not independent of $\\Delta$. The bound includes the term $\\frac{4h^{2}}{\\left\\{\\Phi(-\\sqrt{2}M)\\right\\}^{2}}$, which becomes very large as $M$ increases (since $\\Phi(-\\sqrt{2}M)$ approaches 0). This demonstrates that the approximation error is strongly influenced by the size of $\\Delta$, similar to the BC case. The bound explicitly shows that the error increases as $\\Delta$ becomes larger (i.e., as the proportion of zeros increases).",
    "question": "Is the error bound for the TC case in Theorem 4 independent of the size of $\\Delta$?",
    "question_context": "Latent Correlation Estimation via Bridge Functions in Mixed Gaussian Copula Models\n\nThe mixed latent Gaussian copula model jointly models $\\mathbf{W}=$ $({\\bf W}_{1},{\\bf W}_{2},{\\bf W}_{3})\\sim\\mathrm{NPN}({\\bf0},{\\pmb\\Sigma},f)$ such that $X_{1j}=W_{1j},X_{2j}=$ $I(W_{2j}>c_{2j})$ and $W_{3j}=I(W_{3j}>c_{3j})W_{3j}$ . <PARAGRAPH_SEPARATOR> The latent correlation matrix $\\pmb{\\Sigma}$ is the key parameter in Gaussian copula models. Estimation of latent correlations is achieved via the bridge function $F$ such that $\\mathbb{E}(\\widehat{\\tau}_{j k})~=~F(\\sigma_{j k})$ , where $\\sigma_{j k}$ is the latent correlation between variables $j$ and $k,$ and $\\widehat{\\tau}_{j k}$ is the corresponding sample Kendall’s $\\tau$ . Given observed $\\mathbf{x}_{j},\\dot{\\mathbf{x}_{k}}\\in\\mathbb{R}^{n}$ , <PARAGRAPH_SEPARATOR> $$ \\widehat{\\tau}_{j k}=\\widehat{\\tau}(\\mathbf{x}_{j},\\mathbf{x}_{k})=\\frac{2}{n(n-1)}\\sum_{1\\leq i<i^{\\prime}\\leq n}\\mathrm{sign}(x_{i j}-x_{i^{\\prime}j})\\mathrm{sign}(x_{i k}-x_{i^{\\prime}k}), $$ <PARAGRAPH_SEPARATOR> where $n$ is the sample size. Using $F$ , one can construct $\\widehat{\\sigma}_{j k}~=~{\\cal F}^{-1}(\\widehat{\\tau}_{j k})$ with the corresponding estimator $\\widehat{\\pmb\\Sigma}$ being consistent for $\\pmb{\\Sigma}$ (Fan et al. 2017; Quan, Booth, and Wells 2018; Yoon, Carroll, and Gaynanova 2020). The explicit form of $F$ has been derived for all combinations of continuous(C)/binary(B)/truncated(T) variables (Fan et al. 2017; Yoon, Carroll, and Gaynanova 2020). We summarize these results below, and use CC, BC, TC, etc. to denote corresponding combinations."
  },
  {
    "qid": "statistic-posneg-930-1-0",
    "gold_answer": "FALSE. The condition $\\operatorname{pr}\\{J^{\\prime}=(1)\\mathrm{~and~}n^{\\prime}=(1,0)\\}=\\frac{1}{N}\\sum_{i=0}^{N+1}\\pi_{i}$ is derived under the assumption that the prior distribution is non-degenerate. However, when a prior distribution is admitted, the analysis shows that all population elements almost surely belong to the same category, leading to a degenerate prior distribution where $\\pi_{8}=...=\\pi_{N}=0$. This contradicts the initial condition, as the sum $\\sum_{i=0}^{N+1}\\pi_{i}$ would not account for the degenerate nature of the prior distribution, making the condition inconsistent with the Bayesian analysis.",
    "question": "Is the condition $\\operatorname{pr}\\{J^{\\prime}=(1)\\mathrm{~and~}n^{\\prime}=(1,0)\\}=\\frac{1}{N}\\sum_{i=0}^{N+1}\\pi_{i}$ consistent with the assumption that all population elements almost surely belong to the same category when a prior distribution is admitted?",
    "question_context": "Bayesian Analysis of Dempster's Upper and Lower Probabilities\n\nDempster (1967) illustrates the calculation of upper and lower probabilities for samples from a finite univariate population. Since this method of inference is quite novel, it deserves careful scrutiny. To this end we give two simple examples to show that when this method is considered from a Bayesian point of view, it leads to some anomalous results. <PARAGRAPH_SEPARATOR> We first review the calculation of upper and lower probabilities for finite univariate populations. Consider a population of $\\pmb{N}$ elements, each of which belongs to one of $\\pmb{k}$ distinct categories. The set of categories is assumed to have a natural ordering, and category membership is regarded as a univariate observable characteristic. In sampling from the population, suppose that each element has equal probability of being drawn. A sample of $\\pmb{\\mathscr{n}}$ is drawn and we are asked to make inferences about either a further sample of size $\\mathbf{\\nabla}_{m}$ or the whole population. The population $N^{\\prime}$ and the two samples $\\acute{n}$ and $m^{\\prime}$ are represented by vectors $N^{\\prime}=(N_{1},...,N_{k})$ ${\\pmb n^{\\prime}=(n_{1},...,n_{k})}$ ,and $\\boldsymbol{m}^{\\prime}=(\\boldsymbol{m}_{1},...,\\boldsymbol{m}_{k})$ .where the $j$ th coordinate of each vector gives the number of elements belonging to category $j$ For purposes of computation, an unobservable index $\\boldsymbol{I}=\\mathbf{1},...,\\mathbf{\\dot{\\DeltaN}}$ is introduced to label the population elements. In terms of this index, the first and second samples may be represented by $J^{\\prime}=(J_{1},...,J_{n})$ and $\\begin{array}{r}{K^{\\prime}=(K_{1},...,K_{m})_{:}}\\end{array}$ respectively. We may assume that $J_{1}<\\ldots<J_{n}$ and $K_{1}<\\ldots<K_{m}$ . A typical pair of samples is represented by $[J^{\\prime},K^{\\prime}]$ . The probability of drawing any particular sample pair is $\\{n!m!(N{-}n{-}m)!\\}/N!.$ <PARAGRAPH_SEPARATOR> Dempster (1967, pp. 518, 519) makes two assumptions which are basic to the probability calculations: (i) The individual $\\boldsymbol{I}$ is assumed to have observable characteristic $j$ ifandonly if $N^{j-1}<I\\leqslant N^{j}$ where $N^{0}=0,~N^{j}=\\sum_{l=1}^{j}N_{l}.$ In other words, the first $\\pmb{N_{1}}$ individuals, according to $\\pmb{I}$ , belong to the first category, the next $\\pmb{N_{8}}$ individuals belong to the second category, and so forth. (i) All sample pairs $[J^{\\prime},K^{\\prime}]$ have the same probability $\\{n!m!(N{-}n{-}m)!\\}/N!$ even after $\\acute{\\bullet}$ is known. That is, the measure $\\pmb{\\mu}$ remains in effect dven after the first sample is observed. Note that observation of the first sample means observation of $\\acute{n}$ . The vector $J^{\\prime}$ is not observable. <PARAGRAPH_SEPARATOR> Let $\\pmb{T}$ be any event belonging to the space of $[{\\pmb n}^{\\prime},{\\pmb m}^{\\prime},{\\pmb N}^{\\prime}]$ and suppose that $\\acute{\\bullet}$ is observed. Then define $\\pmb{T^{*}}$ as the set of $[J^{\\prime},K^{\\prime}]$ pairs which are consistent with $\\bullet^{\\prime}$ and $\\pmb{T}$ and $\\pmb{T}_{*}$ a8 the set of $[J^{\\prime},K^{\\prime}]$ pairs which are consistent with $\\mathbf{\\nabla}_{n}\\prime$ but with no $[{\\pmb n}^{\\prime},{\\pmb m}^{\\prime},{\\pmb N}^{\\prime}]$ not in $\\pmb{T}$ . Upper and lower probabilities for $\\pmb{T}$ are given by $P^{*}(T)=\\mu(T^{*})$ and $P_{*}({\\cal T})=\\mu(T_{*})$ <PARAGRAPH_SEPARATOR> In both of the examples which follow we assume that it makes sense to speak of the existence of a prior distribution over the space of possible populations, although it is not necessary for these prior probabilities to be known or even knowable. While Dempster does not use prior distributions, we make this assumption in order to examine the relation of his methods to Bayesian methods."
  },
  {
    "qid": "statistic-posneg-1485-1-0",
    "gold_answer": "FALSE. The likelihood displacement is correctly defined as $LD(\\omega) = 2\\{L(\\widehat{\\pmb\\theta}) - L(\\widehat{\\pmb\\theta}_{\\omega})\\}$, but the question incorrectly states that it is used to find influential observations in MLEs. In reality, it measures the influence of perturbations on the parameter estimates, not directly the observations.",
    "question": "Is the likelihood displacement $LD(\\omega)$ correctly defined as $2\\{L(\\widehat{\\pmb\\theta}) - L(\\widehat{\\pmb\\theta}_{\\omega})\\}$ for identifying influential observations in MLEs?",
    "question_context": "Influence Diagnostics in Nonlinear Mixed-Effects Models\n\nThe diagnostic technique developed in Cook (1986) is a widely used tool to check the model assumptions and conduct diagnostic studies. The author proposes looking at the likelihood displacement $L D(\\omega)=2\\{L(\\widehat{\\pmb\\theta})-L(\\widehat{\\pmb\\theta}_{\\omega})\\}$ to find possible influential observations in the MLEs, where $\\begin{array}{r}{L(\\pmb{\\dot{\\theta}})=\\sum_{i}L_{i}(\\pmb{\\theta})}\\end{array}$ is the log-likelihood function abnd $\\omega$ isb an $s\\times1$ vector of perturbation restricted in an open set $\\pmb{\\Omega}\\subset\\mathbb{R}^{s}$ . A vect or with no perturbation is defined as ${\\pmb{\\omega}}_{0}\\in{\\pmb{\\Omega}}$ in which $L D(\\omega_{0})=0$ , i.e., $L(\\pmb{\\theta}_{\\omega_{0}})=L(\\pmb{\\theta})$ . In his seminal paper, Cook shows that the normal curvature at the unit direction ℓ has the following form $C_{\\ell}(\\pmb\\theta)=2|\\pmb\\ell^{\\top}\\pmb{\\varDelta}^{\\top}(\\ddot{\\pmb L}_{\\theta\\theta})^{-1}\\pmb{\\Delta}\\ell|$ , where $\\pmb{\\varDelta}=\\partial^{2}L(\\pmb{\\theta}|\\omega)/\\partial\\pmb{\\theta}\\partial\\omega^{\\top}$ , both $\\pmb{\\varDelta}$ and $\\stackrel{\\cdot\\cdot}{L}_{\\theta\\theta}$ are evaluated at ${\\pmb\\theta}=\\widehat{\\pmb\\theta}$ and $\\omega=\\omega_{0}$ . Thus, $C_{\\mathbf{{d}}_{\\mathrm{{max}}}}$ is twice the largest eigenvalue of $\\pmb{{\\cal B}}=-\\pmb{\\varDelta}^{\\top}\\ddot{\\pmb{L}}_{\\theta\\theta}^{-1}\\pmb{\\varDelta}$ and $\\pmb{d}_{\\mathrm{{max}}}$ is the corresponding eigenvector. T hbe index plot of $\\pmb{d}_{\\mathrm{max}}$ may reveal how to perturb the model (or data) to obtain large changes in the estimate of $\\pmb\\theta$ For more detailed information, we refer the reader to the work of Russo et al. (2009) and the references therein."
  },
  {
    "qid": "statistic-posneg-1711-0-0",
    "gold_answer": "FALSE. The correct derivation should account for all non-zero terms and their contributions. The given formula cov(c, σ) = (2/T)ρ(1 - p²) is incorrect because it does not fully account for the cross-terms and their expected values under the model assumptions. The correct covariance should include additional terms that arise from the interaction of the variates, leading to a more complex expression.",
    "question": "Is the covariance between c and σ correctly derived as cov(c, σ) = (2/T)ρ(1 - p²) based on the given model assumptions and expected values?",
    "question_context": "Sampling Variance of Correlation Coefficients under Fixed and Mixed Variates\n\nThe paper derives the sampling variance of correlation coefficients under assumptions of fixed and mixed variates. Key formulas include the expected values and variances of terms like c, s, and σ, which are used to compute covariances and the asymptotic sampling variance of the zero-order regression coefficient b. The analysis involves evaluating terms containing normal variates and their moments."
  },
  {
    "qid": "statistic-posneg-831-0-0",
    "gold_answer": "FALSE. The condition $m \\geqslant t(t-1)+2$ is sufficient but not necessary for the existence of the design. The text explicitly states that the sufficient conditions are not necessary in certain cases, such as Corollaries 1(b), 2(a), and 2(c).",
    "question": "Is it true that the condition $m \\geqslant t(t-1)+2$ is sufficient for the existence of a BIBRC design as described in Theorem 1(a)?",
    "question_context": "Balanced Incomplete Block Designs with Nested Rows and Columns\n\nThen there exists $\\pmb{a}$ BIBRC $(v,b,r,p,q,\\lambda)$ design with $b=m v$ $r=m p q,p,\\$ I and 入. <PARAGRAPH_SEPARATOR> Proof. Define $\\pmb{m}$ initial ${\\pmb p}\\times{\\pmb q}$ blocks $B_{i}=B(b_{i},b_{i}^{\\prime})$ . These $\\pmb{m}$ blocks, when developed, give our design. By (i) and (ii), $N_{1}$ and $N_{2}$ are incidence matrices of BIB designs. The symmetric differences arising from $B_{i}$ are $\\pmb q$ copies of $d_{i}$ from columns, $\\pmb{p}$ copies of $d_{i}^{\\prime}$ from rows, and the elements of $R_{i}$ , all diagonal differences; so, by (ii) and (ii), $\\pmb{N}$ is the incidence matrix of a BIB design. 口 <PARAGRAPH_SEPARATOR> Example. For $v=19$ and $\\boldsymbol{G}=\\mathcal{X}_{19}$ write $b_{1}=(0,2,3,14)$ , $b_{2}=(0,4,6,9),b_{3}=(0,1,7,11),$ $b_{1}^{\\prime}=(0,6,10)$ , $b_{2}^{\\prime}=\\left(0,1,12\\right)$ and $b_{3}^{\\prime}=\\left(0,2,5\\right)$ . The initial blocks are <PARAGRAPH_SEPARATOR> $$ b_{1},b_{1}^{\\prime})={\\left[\\begin{array}{l l l}{0}&{6}&{10}\\\\ {2}&{8}&{12}\\\\ {3}&{9}&{13}\\\\ {14}&{1}&{5}\\end{array}\\right]},\\quad B(b_{2},b_{2}^{\\prime})={\\left[\\begin{array}{l l l}{0}&{1}&{12}\\\\ {4}&{5}&{16}\\\\ {6}&{7}&{18}\\\\ {9}&{10}&{2}\\end{array}\\right]},\\quad B(b_{3},b_{3}^{\\prime})={\\left[\\begin{array}{l l l}{0}&{2}&{5}\\\\ {1}&{3}&{6}\\\\ {7}&{9}&{12}\\\\ {11}&{13}&{16}\\end{array}\\right]}. $$ <PARAGRAPH_SEPARATOR> A BIBRC (19, 57, 36, 4, 3, 12) is found by successively adding $0,\\ldots,18$ (mod 19) to the initial blocks. <PARAGRAPH_SEPARATOR> If ${\\pmb{p}}={\\pmb q}.$ . it is sufficient for (i) that the $2m p(p-1)$ combined symmetric differences from the $b_{i}$ 's and $b_{i}^{\\prime\\prime}\\mathsf{s}$ are balanced, a fact which is taken advantage of in $\\S3$ below. <PARAGRAPH_SEPARATOR> Applying this technique we obtain several infinite series of designs as presented in the following theorems and corollaries. In all cases $_G$ will be taken as the finite field $\\mathbf{GF_{\\mathit{v}}}$ with primitive element $x.$ <PARAGRAPH_SEPARATOR> THEOREM 1. Let $v=2t m+1$ be a prime power and write $x^{*_{i}}=1-x^{m i}$ <PARAGRAPH_SEPARATOR> $(a)$ If there exists a positive integer $\\pmb{u}$ $\\mathsf{k}\\left(\\pmb{u}_{i}-\\pmb{u}_{j}\\right)$ (mod $m$ ) for $i,j=1,\\ldots,t,$ then there exists a BIBRC with $b=m v$ $\\scriptstyle{r=4m t^{2}}$ , ${\\boldsymbol{p}}={\\boldsymbol{q}}=2t$ and $\\lambda=2t(2t-1)^{2}$   (b) If there exists a positive integer $\\pmb{u}$ 丰 $\\pm{\\pmb u}_{i}$ , $(u_{i}-u_{j})$ (mod $\\pmb{m}$ ) for $i,j=1,\\dots,t,$ then there exists a BIBRC with $b=m v$ $\\boldsymbol{r}=m(2t+1)^{2}$ ., $p=q=2t+1$ and $\\lambda=2t(2t+1)^{2}$   (c)If there exists $\\pmb{a}$ positive integer $\\pmb{u}$ 丰 $u_{i}$ , $(u_{i}-u_{j})$ (mod $\\pmb{m}$ ) for $i,j=1,\\dots,t,$ then there exists a BIBRC with $\\begin{array}{r}{=m v,\\quad r=2m t(2t+1),\\quad p=2t,\\quad q=2t+1}\\end{array}$ and $\\lambda=2t\\bigl(2t+1\\bigr)\\bigl(2t-1\\bigr)$ <PARAGRAPH_SEPARATOR> Proof. (a) Let <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{b_{1}=\\big(x^{0},x^{m},\\ldots,x^{(2t-1)m}\\big),\\quad b_{i}=x^{i-1}b_{1},}}\\\\ {{b_{1}^{\\prime}=\\big(x^{u},x^{m+u},\\ldots,x^{(2t-1)m+u}\\big),\\quad b_{i}^{\\prime}=x^{i-1}b_{1}^{\\prime}.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Sprott (1954) has shown that $b_{1},\\dots,b_{m}$ , and hence $b_{1}^{\\prime},\\ldots,b_{m}^{\\prime}$ ,are $\\pmb{m}$ supplementary difference sets for a BIBD for which <PARAGRAPH_SEPARATOR> $$ d_{1}=b_{1}\\otimes(1+x^{0},1\\pm x^{m},\\ldots,1\\pm x^{(t-1)m}), $$ <PARAGRAPH_SEPARATOR> and $\\otimes$ means the Kronecker product. Using ${{x}^{t m}}=-1$ we obtain <PARAGRAPH_SEPARATOR> $$ 1+x^{i m}=x^{i m}\\bigl(1-x^{(t-i)m}\\bigr)=x^{i m+u_{t-i}} $$ <PARAGRAPH_SEPARATOR> so that $\\boldsymbol{d}_{1}=\\boldsymbol{b}_{1}\\otimes\\omega$ where $\\omega=\\left(x^{u_{1}},x^{u_{1}},\\ldots,x^{u_{t-1}},x^{u_{t-1}},x^{u_{t}}\\right),$ <PARAGRAPH_SEPARATOR> Now $R_{i}=R(d_{i},d_{i}^{\\prime})=x^{i-1}R(d_{1},d_{1}^{\\prime})=x^{i-1}R(d_{1},x^{u}d_{1}),$ sO <PARAGRAPH_SEPARATOR> $$ \\{R_{1},\\ldots,R_{m}\\}=(x^{0},x^{1},\\ldots,x^{m-1})\\otimes R(d_{1},x^{u}d_{1}). $$ <PARAGRAPH_SEPARATOR> Using $R(x^{n_{1}}b_{1},x^{n_{2}}b_{1})=x^{n_{1}}b_{1}\\otimes(1+x^{n_{2}-n_{1}}b_{1})$ , which can be verified easily, we obtain <PARAGRAPH_SEPARATOR> $$ R(d_{1},x^{u}d_{1})=\\smash{\\big\\{\\omega_{j}b_{1}\\otimes\\big(1+\\omega_{l}\\omega_{j}^{-1}x^{u}b_{1}\\big)\\big|j,l=1,\\ldots,2t-1\\big\\}}, $$ <PARAGRAPH_SEPARATOR> $\\omega_{\\j}$ being the jth component of $\\omega$ Since $(x^{0},x^{1},\\ldots,x^{m-1})\\otimes b_{1}=G-\\{0\\}.$ condition (ii) is satisfied and the proof is completed. <PARAGRAPH_SEPARATOR> To prove (b), adjoin 0 to the $b_{i}$ 's and $b_{i}^{\\prime\\prime}$ 's in the proof of (a). <PARAGRAPH_SEPARATOR> To prove (c), adjoin 0 to the $b_{i}^{\\prime\\prime}$ 's in the proof of (a). <PARAGRAPH_SEPARATOR> The conditions on $\\pmb{u}$ are needed to give $d_{i}\\cap d_{i}^{\\prime}=\\emptyset_{:}$ ensuring the binary property, i.e. that a treatment occurs at most once in each block. Suffcient, but often not necessary, conditions for the existence of ${\\pmb u},$ , and hence the designs of Theorem 1, are <PARAGRAPH_SEPARATOR> $$ m\\geqslant t(t-1)+2,\\quad m\\geqslant t(t+1)+2,\\quad m\\geqslant t^{2}+2 $$ <PARAGRAPH_SEPARATOR> for (a), (b) and (c) respectively. <PARAGRAPH_SEPARATOR> The following two corollaries illustrate the application of this theorem. Note that the sufficient conditions are not necessary in Corollaries 1(b), 2(a) and 2(c). <PARAGRAPH_SEPARATOR> COROLLARY 1. Let ${\\pmb v}={\\pmb4}{\\pmb m}+{\\pmb1}$ be a prime power."
  },
  {
    "qid": "statistic-posneg-279-2-2",
    "gold_answer": "FALSE. The paper clearly states that a control chart with a smaller IRAATS value is considered more effective in detecting shifts within the specified range. The IRAATS criterion measures the relative performance of a control chart compared to an optimal CUSUM chart, with smaller values indicating better performance (i.e., fewer samples needed on average to detect a shift).",
    "question": "Is it correct that a control chart with a smaller IRAATS value is considered less effective in detecting shifts within the specified range?",
    "question_context": "Performance Evaluation of Adaptive CUSUM Control Charts Using IRAATS\n\nFor fair comparison for detecting the range mean shifts, a new criterion is introduced, which is similar to the IRARL provided by Zhao et al. (2005). The new criterion, called the integral of the relative AATS (IRAATS), is defined by $${\\mathrm{IRAATS}}(C)=E\\left[{\\frac{{\\mathrm{AATS}}_{c}(\\delta)}{{\\mathrm{AATS}}_{o p}(\\delta)}}\\right]=\\int{\\frac{{\\mathrm{AATS}}_{c}(\\delta)}{{\\mathrm{AATS}}_{o p}}}\\mathrm{d}F(\\delta),$$ where ${\\mathsf{A d T S}}_{c}(\\delta)$ , $\\mathsf{A A T S}_{o p}(\\delta)$ are the OC AATS of control chart C and CUSUM chart with $k=\\delta/2$ for detecting the shift δ, respectively. $F(\\delta)$ is the cumulative distribution function (CDF) of shifts $\\delta$ . If we have no idea about prior information of the shift, the CDF of uniform distribution $U[\\delta_{1},\\delta_{2}]$ could be used as $F(\\delta)$ , which we employ in this paper. To ease the computation, the discrete form of Eq. (12) is used to approximate the value of IRAATS, which is given by $$\\mathrm{IRAATS}\\approx\\frac{1}{m+1}\\sum_{i=0}^{m}\\frac{\\mathsf{A A T S}_{c}(\\delta_{i})}{\\mathsf{A A T S}_{o p}(\\delta_{i})},$$ where $m$ is a given integer, and $\\begin{array}{r}{\\delta_{i}=\\delta_{1}+\\frac{i}{m}(\\delta_{2}-\\delta_{1})}\\end{array}$ . When comparing the performance of VSI ACUSUM with that of VSI CUSUM control chart, it is necessary to evaluate the values of IRAATS. Apparently, a control chart with a smaller IRAATS value of a particular region is considered to be more effective to detect the shifts in that region, and vice versa."
  },
  {
    "qid": "statistic-posneg-603-0-1",
    "gold_answer": "FALSE. The inequality provides a lower bound for E₂⁻¹, which translates to an upper bound for E₂. This means E₂ cannot exceed a certain value determined by the parameters A and P. The bound is derived from the results of Conniffe & Stone (1974) and is used to assess the efficiency of neighbour designs, ensuring they are within theoretically achievable limits, not suggesting unbounded growth.",
    "question": "Does the condition E₂⁻¹ ≥ (v-2)/{A + P(v-1)^(1/2)(v-2)^(-1/2)} + 1/{A - P(v-1)^(1/2)(v-2)^(1/2)} imply that E₂ can be arbitrarily large?",
    "question_context": "Efficiency and Optimality in Neighbour Designs for Field Trials\n\nThe paper discusses the generation of resolvable incomplete block designs for field trials, incorporating a neighbour criterion derived by Williams (1985). The process involves creating efficient α-designs, optimizing them using the harmonic mean of canonical efficiency factors (E₁), and then refining these designs to maximize neighbour efficiency factors (E₂) through (M,S)-optimality criteria. The paper provides detailed mathematical formulations and algorithms for achieving these optimizations, including bounds on efficiency factors."
  },
  {
    "qid": "statistic-posneg-657-2-0",
    "gold_answer": "TRUE. The context explicitly states in Theorem 1(1) that when $\\lambda\\eta\\to0$, the estimators $\\hat{r}_{\\lambda,\\eta}^{(0)}(x)$ and $r_{\\lambda,\\eta}^{N W}(x)$ indeed converge to $\\tilde{r}_{l l}(x)$ and $\\tilde{r}^{N W}(x)$, respectively, with a convergence rate of $O_{p}(\\lambda\\eta)$. This is further supported by the findings in part (a) of the theorem, which confirms that the new estimators perform like the local linear estimator or N-W estimator when $\\lambda$ and $\\eta$ are not very large.",
    "question": "The theorem states that when $\\lambda\\eta\\to0$, the estimators $\\hat{r}_{\\lambda,\\eta}^{(0)}(x)$ and $r_{\\lambda,\\eta}^{N W}(x)$ converge to the local linear estimator and N-W estimator, respectively, with a convergence rate of $O_{p}(\\lambda\\eta)$. Is this statement true based on the provided context?",
    "question_context": "Local-linear-additive-estimation convergence properties\n\nThen we have the following main theorem. Theorem 1. For model (1.1) with conditions (C1)–(C4), the local linear–additive estimator and local N-W-additive estimator have the following properties: (1) if $\\lambda\\eta\\to0$ , then for $x\\in[-1,1]^{p}$ , $$\\hat{r}_{\\lambda,\\eta}^{(0)}(x)=\\tilde{r}_{l l}(x)+O_{p}(\\lambda\\eta)\\quad a n d\\quad r_{\\lambda,\\eta}^{N W}(x)=\\tilde{r}^{N W}(x)+O_{p}(\\lambda\\eta).$$ (2) if $p\\leq8$ and $\\backslash\\eta\\leq c$ for a positive constant $c$ , then for $x\\in(-1,1)^{p}$ , $$\\hat{r}_{\\lambda,\\eta}^{(0)}(x)=r(x)+O_{p}\\left(h^{2}+\\frac{1}{\\sqrt{n h^{p}}}\\right)\\textit{\\textbf{a n d}}\\widetilde{r}_{\\lambda,\\eta}^{N W}(x)=r(x)+O_{p}\\left(h^{2}+\\frac{1}{\\sqrt{n h^{p}}}\\right).$$ (3) if $\\lambda\\eta\\to\\infty$ , then for $x\\in[-1,1]^{p}$ , $$\\hat{r}_{\\lambda,\\eta}^{(0)}(x)=\\tilde{r}_{l a d,\\eta}(x)+O_{p}((\\eta\\lambda)^{-1})\\quad a n d\\quad\\tilde{r}_{\\lambda,\\eta}^{N W}(x)=\\tilde{r}_{l a d,\\eta}(x)+O_{p}((\\eta\\lambda)^{-1}).$$ (4) if model (1.1) is globally additive and $\\lambda\\eta\\to\\infty$ , then for $x\\in(-1,1)^{p}$ , $$\\begin{array}{l}{{\\hat{r}_{\\lambda,\\eta}^{(0)}(x)=r(x)+{\\cal O}_{p}((\\eta\\lambda)^{-1})+{\\cal O}_{p}\\left(h^{2}+\\displaystyle\\frac{1}{\\sqrt{n\\eta^{p-1}h}}\\right)}}\\ {{}}\\ {{a n d\\quad\\tilde{r}_{\\lambda,\\eta}^{N W}(x)=r(x)+{\\cal O}_{p}((\\eta\\lambda)^{-1})+{\\cal O}_{p}\\left(h^{2}+\\displaystyle\\frac{1}{\\sqrt{n\\eta^{p-1}h}}\\right).}}\\end{array}$$ By the theorem, we have the following findings: (a) the conclusions in Theorem 1(1) and (2) together imply that for the case without global additive structure, the new estimators perform like the local linear estimator (or N-W estimator) and therefore they can achieve the optimal (or the standard) convergence rate provided that $\\lambda$ and $\\eta$ are not very large. For achieving this goal, the theoretical constraints on $\\lambda$ and $\\eta$ are quite mild, for example $\\lambda$ , $\\eta\\in[0,c]$ with $c$ being an arbitrary fixed positive constant. By comparison, the new estimators outperform the additive estimators (e.g., the backfitting estimator and marginal integral estimator) when the model under study is nonadditive; (b) the results of Theorem 1(3) indicate that when $\\lambda$ and $\\eta$ is large enough, the new estimators perform like the local additive estimator and therefore they can adapt to the local additivity of the underlying regression function;"
  },
  {
    "qid": "statistic-posneg-299-3-1",
    "gold_answer": "FALSE. The correct expression should be $\\operatorname{pr}\\{N(\\tau)\\leqslant N_{\\mathrm{max}}\\}=1-\\operatorname{pr}(Y_{\\nu+N_{\\mathrm{max}}}<\\infty)$, which is equivalent to $1-q Q^{\\nu+N_{\\mathrm{max}}}1_{k}$. The provided formula incorrectly uses $N_{\\mathrm{max}}-\\nu$ instead of $\\nu+N_{\\mathrm{max}}$, which does not correctly account for the minimal number of jumps $\\nu$ required before considering the additional jumps up to $N_{\\mathrm{max}}$.",
    "question": "Does the probability $\\operatorname{pr}\\{N(\\tau)\\leqslant N_{\\mathrm{max}}\\}$ correctly account for the probability of the process not being absorbed before $N_{\\mathrm{max}}$ jumps?",
    "question_context": "Markov Chain Modeling for Risk-adjusted Monitoring of Time to Event\n\nWe consider $R(t)=N(t)-t/a$ , for some $a>0$ . Let $c$ , S, $\\tau$ be defined as in $\\S2$ . We define a Markov chain with $k=\\lceil c-1\\rceil$ transient states and one absorbing state. As illustrated in Fig. A1, let $\\sigma_{0}=0$ and for $n=1,2,\\ldots$ let $\\sigma_{n}=\\operatorname*{inf}\\{t>\\sigma_{n-1}:S(t)\\in\\{c-k,\\ldots,c-1\\}\\},\\quad Y_{n}=\\left\\{{\\infty},{\\sigma_{n}}\\geq\\tau,\\right.$. The transition probability matrix of the Markov chain $Y_{n}$ is $\\mathcal{Q}=\\mathcal{Q}(\\xi,\\eta)=\\left(\\begin{array}{l l l l}{a_{1}(\\xi,\\eta)}&{a_{2}(\\xi,\\eta)}&{\\cdots}&{a_{k}(\\xi,\\eta)}\\\\{p_{0}(\\xi+\\eta)}&{p_{1}(\\xi+\\eta)}&{\\cdots}&{p_{k-1}(\\xi+\\eta)}\\\\ &{\\ddots}&{\\ddots}&{\\vdots}\\\\{0}&{p_{0}(\\xi+\\eta)}&{p_{1}(\\xi+\\eta)}\\end{array}\\right),$ where $p_{j}(t)=(\\lambda t)^{j}e^{-\\lambda t}/j!$ is the probability of $j$ events in $t$ time units, $\\xi=a(c-k)$ , $\\eta=a-\\xi$ and $a_{j}(x,y)=p_{0}(x)p_{j-1}(y)+\\sum_{\\nu=1}^{j}p_{\\nu}(x)p_{j-\\nu}(y)=p_{j}(x+y)+p_{0}(x)\\{p_{j-1}(y)-p_{j}(y)\\}.$ The minimal number of jumps needed to hit the threshold is $\\upnu=\\lceil c-S(0)\\rceil$. If $S(0)>c-k$ then the distribution of $Y_{1}$ on the transient states is $q=\\{0,\\ldots,0,p_{0}(t_{0}),\\ldots,p_{\\nu-1}(t_{0})\\}$ , where $t_{0}=a\\{S(0)-(c-$ $\\nu)\\}$. If $S(0)\\leqslant c-k$ then $q=(1,0,\\ldots,0)Q\\{a S(0),\\eta\\}$. Using this notation, $E\\{N(\\tau)\\}=\\nu+\\sum_{j=0}^{\\infty}\\mathrm{pr}\\{N(\\tau)>\\nu+j\\}=\\nu+\\sum_{j=1}^{\\infty}\\mathrm{pr}(Y_{j}<\\infty)=\\nu+\\sum_{j=1}^{\\infty}q Q^{j-1}\\mathbb{1}_{k}=\\nu+q(\\mathrm{I}-Q)^{-1}\\mathbb{1}_{k},$ where $1_{k}=(1,\\ldots,1)^{\\mathrm{T}}$. Furthermore, for $N_{\\mathrm{max}}\\geqslant\\nu$, $\\operatorname{pr}\\{N(\\tau)\\leqslant N_{\\mathrm{max}}\\}=1-\\operatorname{pr}\\big(F_{N_{\\mathrm{max}}+1-\\nu}<\\infty\\big)=1-q Q^{N_{\\mathrm{max}}-\\nu}1_{k}.$"
  },
  {
    "qid": "statistic-posneg-1585-5-2",
    "gold_answer": "FALSE. Explanation: While the context mentions that a total score exceeding 3 times the root of the variance may be considered significant, this is a simplified threshold. In practice, the exact threshold for significance would depend on the desired significance level (e.g., p < 0.05) and the distribution of the test statistic. The value of 3 is arbitrary and not a universally accepted cutoff. A more rigorous approach would involve calculating a p-value or using a predefined critical value from the appropriate statistical distribution.",
    "question": "The total score $\\lambda$ for linkage detection is calculated by summing the scores for each sib-pair across families. Is it correct to say that a significant linkage is declared if the total score exceeds 3 times the square root of the total variance?",
    "question_context": "Linkage Detection in Human Genetics: Sib-pair Analysis\n\nThis table is arranged so as to give also the actual probability $p(G^{i}G^{j},T^{r}T^{s})$ of any sib-pair classified as $G^{i}G^{j}$ in the main character, and $T^{r}T^{s}$ in the test character.Write <PARAGRAPH_SEPARATOR> $$ p(G^{i}G^{j},T^{r}T^{s})=\\pi(G^{i}G^{j})\\pi(T^{r}T^{s})\\:\\Psi, $$ <PARAGRAPH_SEPARATOR> where $\\pi(G^{i}G^{j})$ is the probability of pair $G^{i}G^{j}$ when $\\zeta=0$ $\\pi(T^{r}T^{s})$ the same for $T^{r}T^{s}$ and $\\downarrow$ a correcting factor for the effect of linkage. The value of $\\pi(G^{i}G^{j})$ for all kinds of pairs is given in Table 5._The value of $\\pi(T^{r}T^{s})$ follows from symmetry. The value of $\\updownarrow$ can be found from the entry in Table 5 by the use of Table 6. <PARAGRAPH_SEPARATOR> The variances per sib-pair can be found from the formula $\\kappa=\\Sigma\\pi a^{2}$ and are given in Table 7. For recessivity types $[22^{*}]$ and [4] in either character $\\pmb{\\kappa}=\\mathbf{0}$ , since all scores are zero. <PARAGRAPH_SEPARATOR> These tables enable a test for linkage to be made in any set of families in which the parental genotypes are known.For each family a score $\\lambda$ is found, by adding the scores for each sib-pair as given in Table 5, and a variance by multiplying the value of $\\pmb{\\kappa}$ in Table 7 by $\\frac{1}{2}n(n-1)$ ，the number of sib-pairs (where $_n$ is the number of sibs). The scores and variances for the different families are summed to give a total score and total variance. If the total score is positive and exceeds (say) 3 times the root of the variance it may be considered significant. Note that any one phenotype may be given different symbols in different families: e.g., blood-group $\\pmb{A}$ is of type $\\mathbf{G_{2}}$ when themating is $\\mathbf{AA}\\times\\mathbf{AO}$ ,but $\\mathbf{G_{3}}$ when themating is $\\mathbf{AO}\\times\\mathbf{AO}$"
  },
  {
    "qid": "statistic-posneg-402-0-0",
    "gold_answer": "TRUE. The matrix B(ρ) is defined as a 2×2 symmetric positive definite matrix with elements derived from the observed correlation r and the desired correlation ρ. The rotation (W Z)ᵀ = (X Y)ᵀ B(ρ₀) ensures that the resampled pairs (W*, Z*) have the required null correlation ρ₀. The elements of B(ρ) are constructed to adjust the correlation while preserving other properties of the data.",
    "question": "Is the matrix B(ρ) correctly defined to rotate the standardized data (X, Y) to have the required null correlation ρ₀?",
    "question_context": "Bootstrap Hypothesis Testing for Correlation Coefficient\n\nThe paper discusses bootstrap hypothesis testing for the correlation coefficient ρ in a bivariate sample. The null hypothesis H₀: ρ = ρ₀ is tested against the alternative Hₐ: ρ < ρ₀. For ρ₀ ≠ 0, the data is rotated using a matrix B(ρ₀) to impose the null correlation on resamples. The bootstrap p-value is calculated based on resampled correlation coefficients r*_{W,Z}. The paper also discusses bootstrap power calculations by resampling from a distribution with correlation ρₐ."
  },
  {
    "qid": "statistic-posneg-391-2-0",
    "gold_answer": "FALSE. Explanation: While $\\rho=0$ indicates ignorable compliance (no correlation between $U$ and $V$), it does not necessarily imply that ATE equals LATE. The ATE is the average effect over the entire population, while LATE is the average effect for compliers (those with $X(1)>X(0)$). These quantities can differ even under ignorable compliance if the treatment effect varies across subgroups. In the given context, the proportion of compliers is 0.192, meaning the LATE is specific to this subset, whereas the ATE (0.040) is averaged over all individuals.",
    "question": "In the bivariate probit model simulation, setting $\\rho=0$ implies that the compliance mechanism is ignorable, meaning the correlation between the unobserved errors $U$ and $V$ is zero. Does this imply that the average treatment effect (ATE) is equal to the local average treatment effect (LATE) when $\\rho=0$?",
    "question_context": "Bivariate Probit Model for Causal Inference with Noncompliance\n\nTo demonstrate further the important role of NEM, we now conduct two Monte Carlo simulation studies in which the true structural models generating the data are known. The aim of both studies is to show the impact of misinterpreting inferences obtained using SMMs. <PARAGRAPH_SEPARATOR> # 6.2 Bivariate probit model simulation <PARAGRAPH_SEPARATOR> The first illustration is based on structural model (4.1) from Section 3, namely, <PARAGRAPH_SEPARATOR> $$ Y(x)=I\\left(\\alpha+\\beta x-U>0\\right),X(z)=I\\left(\\gamma+\\delta z-V>0\\right), $$ <PARAGRAPH_SEPARATOR> where here we set $(U,V)$ to have the bivariate normal distribution <PARAGRAPH_SEPARATOR> $$ \\left(\\begin{array}{c}{{U}}\\ {{V}}\\end{array}\\right)\\sim N\\left\\{\\left(\\begin{array}{c}{{0}}\\ {{0}}\\end{array}\\right),\\left(\\begin{array}{c c}{{1}}&{{\\rho}}\\ {{\\rho}}&{{1}}\\end{array}\\right)\\right\\}, $$ <PARAGRAPH_SEPARATOR> and $\\operatorname*{Pr}(Z=1)=0.5$ . Note that $\\rho$ indexes the strength of nonignorability in the selection mechanism determining compliance, with $\\rho=0$ corresponding to ignorable compliance. For each set of parameter values $(\\alpha,\\beta,\\gamma,\\delta,\\rho)$ , we can calculate the corresponding values of the key causal parameters. We fix the parameters in the outcome model to $\\alpha=0$ , $\\beta=0.1$ and look at how the causal parameters vary as a function of $(\\gamma,\\delta,\\rho)$ . <PARAGRAPH_SEPARATOR> Figure 1 displays the values of average treatment effects, RRs, and ORs as a function of $\\rho$ for $\\gamma=$ 0 and $\\delta=0.5$ . In the first panel, ATE denotes the average treatment effect $E\\{Y(1)-Y(0)\\}$ , and the parameters of the additive SMM are denoted as follows: $\\psi_{0}+\\psi_{1}=E\\{Y(1)-Y(0)|X=1,Z=1\\}$ by ATEX1Z1, $\\psi_{0}=E\\{Y(1)-Y(0)|X=1,Z=0\\}$ by ATEX1Z0, and the average treatment effect among the treated $E\\{Y(1)-Y(0)|X=1\\}$ by ATEX1; LATE denotes $E\\{Y(1)-Y(0)|X(1)>X(0)\\}$ . The parameters are similarly defined in the second and third panel for the RR and OR, respectively $(\\mathrm{{RRX1Z1}=}$ $\\exp(\\theta_{0}+\\theta_{1})$ , $\\mathrm{ORX1Z0}=\\exp(\\xi_{0})$ , LRR, etc.); for the OR, there is an additional parameter, denoted by DL, corresponding to the estimand of the double-logistic SMM (2.6). <PARAGRAPH_SEPARATOR> For $\\alpha=0$ and $\\beta=0.1$ , the marginal expectations are $E\\{Y\\left(1\\right)\\}=\\Phi\\left(0.1\\right)=0.540$ and $E\\{Y(0)\\}=$ $\\Phi\\left(0\\right)=0.5$ , and so $\\mathrm{ATE}=0.540-0.5=0.040,\\mathrm{RR}=1.080$ and $\\mathrm{OR}=1.173\\$ . Likewise, as $\\gamma\\:=0$ and $\\delta=0.5$ then $E\\{X\\left(1\\right)\\}=\\Phi\\left(0.5\\right)=0.692$ and $E\\{X\\left(0\\right)\\}=\\Phi\\left(0\\right)=0.5$ , indicating a large degree of noncompliance in the control arm. The proportion of compliers in the population is $\\operatorname*{Pr}\\{X(1)>X(0)\\}=$ $E\\{X\\left(1\\right)-X\\left(0\\right)\\}=0.192$ ."
  },
  {
    "qid": "statistic-posneg-710-0-0",
    "gold_answer": "TRUE. The context explicitly states that the Lie algebra $\\mathcal{G}$ of a stratified group $G$ of step $s$ has a vector space decomposition $\\mathcal{G} = \\bigoplus_{j=1}^s V_j$ with the property that $[V_i, V_j] \\subset V_{i+j}$ when $i + j \\leq s$ and $[V_i, V_j] = 0$ when $i + j > s$. This is a defining property of stratified groups.",
    "question": "Is it true that for a stratified group $G$ of step $s$, the Lie algebra $\\mathcal{G}$ decomposes into subspaces $V_j$ such that $[V_i, V_j] \\subset V_{i+j}$ only when $i + j \\leq s$ and $[V_i, V_j] = 0$ otherwise?",
    "question_context": "Central Limit Theorem on Stratified Groups\n\nLet $G$ be a stratified group of step $s$ , that is a simply connected nilpotent Lie  group  whose  Lie  algebra $\\pmb{\\mathscr{G}}$ has  a  vector  space  decomposition $\\mathcal{G}=\\bigoplus_{j=1}^{s}V_{j}$ such that $[V_{i},V_{j}]\\subset V_{i+j}$ when $i+j\\leqslant s$ and $[V_{i},V_{j}]=0$ when $i+j>s$ and $V_{1}$ generates $\\pmb{\\mathscr{G}}$ as an algebra. Let exp: ${\\mathcal{G}}\\mapsto G$ be the exponential mapping (which is now a diffeomorphism). We equip $\\pmb{\\mathcal{G}}$ as well as $G$ with the natural dilations by extending. ${\\check{\\delta}}_{t}(X)=t^{j}X,t>0,X\\in V_{j}$ by linearity to $\\pmb{\\mathcal{G}}$ and putting $\\delta_{t}(\\ensuremath{\\exp}X)=\\ensuremath{\\exp}(\\breve{\\delta}_{t}X)$ .The family $\\left(\\delta_{t}\\right)_{t>0}$ is a continuous one-parameter semigroup of automorphisms of $G$"
  },
  {
    "qid": "statistic-posneg-1457-0-3",
    "gold_answer": "FALSE. The no-gaps algorithm bypasses numerical integration by using parameter augmentation, making it more efficient than methods that rely on numerical integration for non-conjugate models.",
    "question": "Does the no-gaps MCMC algorithm require numerical integration for posterior inference in the DPMM described?",
    "question_context": "Dirichlet Process Mixture Model for Time Series Analysis\n\nThe paper introduces a Dirichlet Process Mixture Model (DPMM) for modeling ergodic time series. The model assumes a nonparametric mixture of normal kernels for the conditional density, with a specific autoregressive (AR) link function involving the hyperbolic tangent function. The DP prior is placed on the mixing distribution, allowing for flexible shapes. Posterior inference is conducted via an MCMC scheme, and sufficient conditions for posterior consistency are established in two different topologies."
  },
  {
    "qid": "statistic-posneg-2140-1-0",
    "gold_answer": "TRUE. The theorem explicitly states that if the partner distribution (i.e., $\\mathbf{X}$) is not Gaussian, the probability for the chosen projection $Y_{t}^{h}$ to be Gaussian is zero. This is a deterministic result, not just an extremely small probability. The reasoning is based on the properties of Gaussian distributions and their projections.",
    "question": "According to Theorem 1.1, if the process $\\mathbf{X}$ is not Gaussian, then the probability that a randomly chosen projection $Y_{t}^{h}$ is Gaussian is zero. True or False?",
    "question_context": "Random Projection Test for Gaussianity in Stationary Processes\n\nMoving back to the Gaussianity problem, let $\\mathbf{X}$ be a process and let us select randomly a one-dimensional projection. According to Theorem 1.1, if the partner distribution is not Gaussian, the probability for the chosen projection to be Gaussian is zero. Let us stress this point: this probability is zero; it is not extremely small, like in the previous example, but zero. Therefore, if the partner distribution is not Gaussian, we will not obtain a Gaussian projection; and if we obtain a Gaussian projection, we will conclude that $\\mathbf{X}$ is Gaussian.<PARAGRAPH_SEPARATOR>Returning to the Gaussianity test of stationary processes; in this setting, we do not have a random sample of trajectories but a sequence of observations taken on a fixed trajectory. Thus, the theory developed in Cuesta-Albertos et al. (2007) cannot be applied directly. The good news is that, similarly as in Example 1.2, Theorem 1.1 transforms the analysis of the Gaussianity of the process $\\mathbf{X}$ in the analysis of the Gaussianity of a randomly chosen one-dimensional projection as follows.<PARAGRAPH_SEPARATOR>Assume that we have the stationary process $\\mathbf{X}=(X_{t})_{t\\in\\mathbb{Z}}$ . Then, randomly select a vector $\\mathbf{h}:=(h_{t})_{t\\in\\mathbb{N}}$ (technical details on its construction are given in Section 3) and construct the new process $\\mathbf{Y}^{h}=(Y_{t}^{h})_{t\\in\\mathbb{Z}}$ where<PARAGRAPH_SEPARATOR>$$Y_{t}^{h}:=\\sum_{i=0}^{\\infty}h_{i}X_{t-i,t\\in\\mathbb{Z}}.$$<PARAGRAPH_SEPARATOR>Theorem 1.1 implies that if $\\mathbf{X}$ is not Gaussian, then, with probability one, the h we have chosen makes $Y_{t}^{h}$ non-Gaussian. In other words, if $\\mathbf{X}$ is not Gaussian, then the one-dimensional marginal of $\\mathbf{Y}^{h}$ is not Gaussian for almost every h.<PARAGRAPH_SEPARATOR>Once h has been fixed, the process $\\mathbf{Y}^{h}$ is also stationary and we can employ one of the above mentioned tests to test the Gaussianity of the marginals of $\\mathbf{Y}^{h}$ as those tests were designed, precisely, to test the Gaussianity of a given one-dimensional marginal. With this, according to the preceding reasoning, in fact, we are testing the full Gaussianity of the $\\mathbf{X}.$<PARAGRAPH_SEPARATOR>The particular procedure used to test the Gaussianity of the marginal distribution of $\\mathbf{Y}^{h}$ is left to the practitioner. Here, we use improved versions of the tests proposed in Epps (1987) and Lobato and Velasco (2004), obtaining a test consistent against every stationary alternative satisfying some regularity conditions (see Section 4.3 the explanation on how we have applied the test)."
  },
  {
    "qid": "statistic-posneg-2053-0-0",
    "gold_answer": "FALSE. The empirical variogram based on $Y_{i}$ observed at $x_{i}$ estimates $\\gamma_{m}(u)$, but the model assumes ${\\boldsymbol{\\gamma}}(u)=\\gamma_{m}(u)$ only when $\\tau^{2}=0$. This is a specific condition and not a general property of the model. The statement oversimplifies the relationship between the empirical variogram and the model's assumptions.",
    "question": "Is the statement 'The empirical variogram based on $Y_{i}$ observed at $x_{i}$ estimates $\\gamma_{m}(u)$' consistent with the model's assumption that ${\\boldsymbol{\\gamma}}(u)=\\gamma_{m}(u)$ when $\\tau^{2}=0$?",
    "question_context": "Geostatistical Inference Under Preferential Sampling: Variogram Analysis\n\nThe class of models that is defined through assumptions 1–3 is a particular case of the geostatistical model for preferential sampling in Ho and Stoyan (2008). The sampling locations, or points $x_{i}$ , are from the log-Gaussian Cox process and the Gaussian variates $Y_{i}$ of the paper correspond to the marks $m(x_{i})=$ $\\mu+S(x_{i})+\\varepsilon(x_{i})$ , where $\\varepsilon(x_{i})\\sim N(0,\tau^{2})$ . <PARAGRAPH_SEPARATOR> In the context of models such as that considered in the paper, it makes sense to speak both about the ‘field variogram’ $\\gamma(u)$ and the ‘mark variogram’ $\\gamma_{m}(u)$ , which belongs to the corresponding marked point process; see for example Illian et al. (2008), page 344. The empirical variogram based on $Y_{i}$ observed at $x_{i}$ estimates $\\gamma_{m}(u)$ , whereas, in geostatistics, a statistician hopes to estimate $\\gamma(u)$ . <PARAGRAPH_SEPARATOR> For the model of the paper, <PARAGRAPH_SEPARATOR> $$ {\\boldsymbol{\\gamma}}(u)=\\gamma_{m}(u), $$ <PARAGRAPH_SEPARATOR> when $\\tau^{2}=0$ (as in Section 3); see Ho and Stoyan (2008), equation (19). This seems to be in contradiction to the words of the paper on page 196, lines 8–11. <PARAGRAPH_SEPARATOR> We made simulations of the model. Fig. 13 shows the results for the model with the random-field parameters of Section 3 (mean 0), $\\alpha=3.0$ and $\\beta{=}1.5$ (about l00 points on average in a unit square). We simulated full randomness whereas a form of conditional simulation was used in the paper. <PARAGRAPH_SEPARATOR> We think that the bias occurs since the estimator for the mark variogram is only asymptotically ratio unbiased. We are surprised that the bias is so large, but it decreases along an increasing window size. For large windows the simulations become extremely long; the variability of the model is high."
  },
  {
    "qid": "statistic-posneg-71-0-1",
    "gold_answer": "FALSE. The paper explicitly states that the MELE $\tilde{\theta}_{G}^{\\mathrm{EX}}$ is different from the GEE estimate $\\hat{\theta}_{G}^{\\mathrm{EX}}$ for the exchangeable correlation structure. The MELE is obtained by maximizing $\\mathcal{R}^{F}(\beta,\\alpha_{1},\\alpha_{2})$ under the constraint $\\alpha_{1}=\\alpha_{2}$, while the GEE estimate is obtained by solving the GEE equation without this additional constraint. The MELE involves overconstrained estimating equations, which can lead to computational issues.",
    "question": "Does the paper claim that the MELE $\tilde{\theta}_{G}^{\\mathrm{EX}}$ is identical to the GEE estimate $\\hat{\theta}_{G}^{\\mathrm{EX}}$ for the exchangeable correlation structure?",
    "question_context": "Empirical Likelihood for Working Correlation Structure Selection in GEE Models\n\nThe paper discusses the use of empirical likelihood to select the optimal working correlation structure in Generalized Estimating Equations (GEE) models. It introduces a full-model empirical likelihood ratio, which serves as a unified measure to compare different working correlation structures. The paper also proposes modified versions of AIC and BIC, called EAIC and EBIC, which use empirical likelihood instead of parametric likelihood for model selection."
  },
  {
    "qid": "statistic-posneg-501-0-0",
    "gold_answer": "FALSE. While the condition states that the expectation of the score function is zero under the model parameter $\\pmb\\theta$, it does not automatically guarantee that this holds under the true parameter $\\pmb\\theta_0$ unless additional assumptions (e.g., correct model specification) are met. The zero expectation of the score under $\\pmb\\theta_0$ is a consequence of the model being correctly specified and the true parameter being in the interior of the parameter space.",
    "question": "Is the condition $E_{\\pmb\\theta}\\left[\\frac{\\partial}{\\partial\\theta_{j}}L(\\pmb\\theta,\\boldsymbol y)\\right]=0$ sufficient to ensure that the score function has zero expectation under the true parameter $\\pmb\\theta_0$?",
    "question_context": "Asymptotic Normality and Fiducial Inference in Maximum Likelihood Estimation\n\nThe paper outlines standard assumptions for proving the asymptotic normality of maximum likelihood estimators (MLEs) and conditions for Bayesian posterior distributions to approximate MLEs. Key assumptions include distinct distributions (A0), common support (A1), iid data (A2), existence of third derivatives (A3), and properties of the log-likelihood derivatives (A4-A6). The paper also discusses fiducial inference and MCMC ergodicity conditions (D1)."
  },
  {
    "qid": "statistic-posneg-336-0-2",
    "gold_answer": "FALSE. According to Donoho and Johnstone (1998), linear procedures like kernel estimators cannot be optimal for some Besov classes. This necessitates the use of other estimation procedures, such as adaptive methods or neural networks, to achieve optimal rates in these cases. The paper emphasizes the need for adaptive procedures that can handle varying interaction orders and smoothness parameters.",
    "question": "Is it true that linear procedures such as kernel estimators can always achieve optimal rates for all Besov classes?",
    "question_context": "Adaptive Estimation in Besov and Neural Network Classes\n\nThe collection $\\mathcal{L}$ may be chosen to include a few familiar parametric procedures (e.g., simple linear or generalized linear models) as well as some nonparametric procedures, for instance, kernel estimator with automatically selected bandwidth (e.g., Hardle and Marron, 1985; and Devroye and Lugosi, 1997), CART (Breiman et al., 1984) and others as mentioned in the Introduction.<PARAGRAPH_SEPARATOR>The following classes will be involved in one choice of $\\mathcal{T}$.<PARAGRAPH_SEPARATOR>1. Besov classes of different interaction orders. For $1\\leqslant\\sigma$ , $q\\leqslant\\infty$ and $\\alpha>0$ , let $B_{q,\\sigma}^{\\alpha,r}(C)$ be the collections of all functions $g\\in L_{q}[0,1]^{r}$ such that the Besov norm satisfies $\\|g\\|_{B_{q_{\\sigma}}^{\\alpha}}\\leqslant C$ (see, e.g., Triebel, 1992, and DeVore and Lorentz, 1993). When $\\alpha/d>1/q$ , the functions in $B_{q,\\sigma}^{\\alpha}$ are uniformly bounded. Consider the following function classes on $[0,\\dot{1}]^{d}$ :<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}&{S_{q,\\sigma}^{x,1}(C)=\\Bigg\\{\\underset{i=1}{\\sum}g_{i}(x_{i})\\colon g_{i}\\in B_{q,\\sigma}^{x,1}(C),1\\leqslant i\\leqslant d\\Bigg\\}}\\ &{S_{q,\\sigma}^{x,2}(C)=\\Bigg\\{\\underset{1\\leqslant i<j\\leqslant d}{\\sum}g_{i,j}\\left(x_{i},x_{j}\\right)\\colon g_{i,j}\\in B_{q,\\sigma}^{x,2}\\left(C\\right),1\\leqslant i<j\\leqslant d\\Bigg\\}}\\ &{\\cdots}\\ &{S_{q,\\sigma}^{x,d}(C)=B_{q,\\sigma}^{x,d}\\left(C\\right).}\\end{array}$$<PARAGRAPH_SEPARATOR>The simplest function class $S_{q,\\sigma}^{\\alpha,1}\\left(C\\right)$ contains additive functions (no interaction). These classes have different input dimensions with increasing complexity when $r$ increases. The metric entropies of these classes are of the same orders as $B_{q,\\sigma}^{\\alpha,1}\\left(C\\right),...,B_{q,\\sigma}^{\\alpha,d}\\left(C\\right)$ , respectively. By results of Yang and Barron (1999), if the unknown design density with respect to Lebesgue measure is bounded above and away from zero on $[0,1]^{d}$ , the minimax rate of convergence under square $L_{2}$ loss for estimating a regression function in $S_{q,\\sigma}^{\\alpha,r}\\left(C\\right)$ is $n^{-2\\alpha/(2\\alpha+r)}$ for $1\\leqslant r\\leqslant d$ , as suggested by the heuristic dimensionality reduction principle of Stone (1985). Rates of convergence and adaptive estimation using wavelets for $B_{q,\\sigma}^{\\alpha,1}(C)$ with $\\alpha$ unknown are studied in Donoho and Johnstone (1998). Results on nonadaptive rates of convergence in more restrictive Sobolev classes are given in Stone (1994) and Nicoleris and Yatracos (1997). Adaptive estimation over Sobolev classes with unknown smoothness and interaction order by model selection is in Yang (1999a).<PARAGRAPH_SEPARATOR>2. Neural network classes. Let $N(C)$ be the closure in $L_{2}[0,1]^{d}$ of the set of all functions $g$ : $R^{d}\\to R$ of the form $\\begin{array}{r}{g(\\boldsymbol{x})=c_{0}+\\sum_{i}c_{i}\\sigma(\\boldsymbol{v}_{i}\\cdot\\boldsymbol{x}+b_{i})}\\end{array}$ , with $\\begin{array}{r}{\\vert c_{0}\\vert+\\sum_{i}\\vert c_{i}\\vert\\leqslant C,}\\end{array}$ , and $\\|v_{i}\\|=1$ , where $\\sigma$ is the step function $\\sigma(t)=1$ for $t\\geqslant0$ , and $\\sigma(t)=0$ for $t<0$ . The minimax rate under square $L_{2}$ loss is shown to be bounded between<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}&{n^{-(1+2/d)/(2+1/d)}(\\log n)^{-(1+1/d)(1+2/d)/(2+1/d)}}\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\ &{\\mathrm{~and~}\\quad\\quad(n/\\log n)^{-(1+1/d)/(2+1/d)}}\\end{array}$$<PARAGRAPH_SEPARATOR>when the design density is bounded above and away from zero on $[0,1]^{d}$ (see Yang and Barron, 1999). When $d$ is moderately large, the rate is slightly better than $n^{-1/2}$ (independent of $d$ ).<PARAGRAPH_SEPARATOR>Let $\\mathcal{T}$ consist of all the Besov classes of different interaction order and smoothness parameters, and the neural network class. Note that for some of the Besov classes, from Donoho and Johnstone (1998), linear procedures such as kernel estimators cannot be optimal, suggesting the need of other estimation procedures. The desire here is that the to-be-constructed adaptive procedure automatically adapt to the interaction order and smoothness over the Besov classes, and retain a good rate $o(n^{-1/2})$ if unfortunately both the interaction order is high and the smoothness parameter $\\alpha$ is small relative to $d$ (i.e., curse of dimensionality, as is well-known) but fortunately it has the neural net representation. More function classes with different characterizations can be considered here as well to increase the chance to capture the underlying regression function to overcome the curse of dimensionality.<PARAGRAPH_SEPARATOR># 5.1. Method of Adaptation."
  },
  {
    "qid": "statistic-posneg-1328-1-1",
    "gold_answer": "TRUE. The paper explicitly states that for a 'wandering' series with $\\alpha_1 = -1$, the limit of the variance of the differenced series becomes infinite. This is because the series is non-stationary under this condition, and the result for stationary series does not hold.",
    "question": "For a 'wandering' series where all $\\alpha_i$ except $\\alpha_1$ are zero and $\\alpha_1 = -1$, does the limit of the variance of the differenced series become infinite?",
    "question_context": "Variate Differences in Autoregressive Series\n\nThe paper discusses the variate differences of autoregressive series, focusing on the limit of the variance of differenced series as the order of differencing approaches infinity. The autoregressive series is defined by the equation $x_{i}+\\alpha_{1}x_{i-1}+\\ldots+\\alpha_{\\infty}x_{i-m}=\\epsilon_{i}$, where $E\\{\\epsilon_{i}\\epsilon_{i+\\circ}\\}=0$ for ${\\pm}0$. The paper derives expressions for the variance and covariance of differenced series and explores the behavior of these statistics under different conditions."
  },
  {
    "qid": "statistic-posneg-1501-1-2",
    "gold_answer": "TRUE. The formula for $\\operatorname{var}(Z_1)$ is derived from the expression for $Z_1$ and the properties of $s_{a w}$ and $B$. The assumption $\\operatorname{var}(s_{a w}) = s_{a a}/k + O(N^{-1})$ is used in the derivation, and the term $2N^{-1/2}\\mathrm{cov}(s_{a w}, B)$ accounts for the covariance between $s_{a w}$ and $B$, which is of order $O(N^{-1/2})$ as stated in the context.",
    "question": "Is the variance of $Z_1$ given by $\\operatorname{var}(Z_1) = (k/s_{a a})\\left\\{\\operatorname{var}(s_{a w}) + 2N^{-1/2}\\mathrm{cov}(s_{a w}, B)\\right\\} + O(N^{-1})$ consistent with the assumption that $\\operatorname{var}(s_{a w}) = s_{a a}/k + O(N^{-1})$?",
    "question_context": "Meta-Analysis Approximations and Radial Plot Statistics\n\nThe paper discusses the asymptotic properties of estimates in meta-analysis, focusing on the joint normality of $(\\hat{\\theta}, \\hat{\\sigma}^2)$ with mean $(\\theta, \\sigma^2)$. It assumes biases of order $O(n^{-1})$, and provides various approximations and expansions for statistics like $\\widetilde{\\theta}$, $Z_1$, and $Q$. The context includes detailed derivations and expectations of these statistics, as well as special cases involving binomial distributions and radial plot statistics."
  },
  {
    "qid": "statistic-posneg-834-1-0",
    "gold_answer": "TRUE. The context explicitly states that according to LPML, the SSGLGRF model outperforms the other models. Additionally, $\\mathrm{{CV}}_{3}$, which checks the goodness of prediction, shows that the SSGLGRF model performs better than the other models. The decreasing values of $\\mathrm{{CV}}_{3}$ for SSGRF-VS and SSGLGRF models further support the inclusion of the scale mixing term in the hierarchical model, reinforcing the superiority of the SSGLGRF model.",
    "question": "Is the claim that the SSGLGRF model outperforms other models based on LPML and $\\mathrm{{CV}}_{3}$ criteria correct, given the context?",
    "question_context": "Skewed and Heavy-Tailed Latent Random Field Model for Spatial Extremes\n\nSpecifically, we consider the hierarchical model defined in the previous section whose large-scale variation is modeled through a second-order polynomial in $\\pmb{s}=(s_{1},s_{2})$ , namely, $\\beta_{1}+\\beta_{2}s_{1}+$ $\\beta_{3}s_{2}+\\beta_{4}s_{1}^{2}+\\beta_{5}s_{1}s_{2}+\\beta_{6}s_{2}^{2}$ . Since annual data at each spatial locations show somewhat trend free behavior, we ignore time dynamics. Prior information is incorporated in the same way as prior set 1 in the simulation study. Besides the model in (9), which is termed as SSGLGRF, several spatial models with different distributions for $W_{j}$ were fitted to the wind speed dataset. These models are r GRF: multivariate normal distribution, r SSGRF: SSN distribution with fixed skewness parameter $\\vartheta$ , r GLGRF: multivariate normal distribution with scale mixing, r SSGRF-VS: SSN distribution with spatially varying skewness. Notice that the distribution of $W_{j}$ is centered at zero by subtracting $\\pmb{v}_{j}$ from ${\\pmb\\mu}_{\\upsilon}$ in skewed models for meaningful inference and model comparisons. For the same purpose, similar prior distributions were assumed for the parameters of the different models. In addition, the skewness parameter of the SSGRF model was taken to be distributed priori as $\\mathrm{N}_{1}(0,10)I(\\vartheta>0)$ . The Bayesian inference for these models proceeded with the same iterations, burn-in, and thinning numbers as in simulation study. As usual, the trace, autocorrelation, and running median plots are monitored to see any departures from convergence. These graphical showings of the MCMC output, not reported here, reveal that the algorithm produces well-mixing chains with low auto-correlations. The model selection criteria were computed for different models in Table 4. According to LPML, the SSGLGRF model outperforms the other models. Based on $\\mathrm{CV}_{1}$ that assesses the unbiasedness of the predictors, the GRF, SSGRF, and GLGRF models perform slightly better than the models with spatially varying skewness. The same conclusions hold for $\\mathrm{CV}_{2}$ that quantifies the accuracy of the mean squared prediction errors. Since the SGRF models are not mean square continuous, the predictive-based criteria, that is, $\\mathrm{CV}_{1}$ and $\\mathrm{CV}_{2}$ exhibit high values of differences from zero and one, respectively, for these skewed models. But, $\\mathrm{{CV}}_{3}$ that checks the goodness of prediction shows that the SSGLGRF model performs better than the other models. Also, based on the decreasing values of $\\mathrm{{CV}}_{3}$ for SSGRF-VS and SSGLGRF models, the scale mixing term should be embedded in the hierarchical model. Altogether, by considering the values of LPML and $\\mathrm{{CV}}_{3}$ statistics our new model with stochastic skewing structure is the best one."
  },
  {
    "qid": "statistic-posneg-2004-2-0",
    "gold_answer": "TRUE. The initial estimator $\\tilde{\\pmb{\\theta}}$ is obtained using the function \"rq\" in the R package quantreg, which performs standard linear quantile regression without any penalty term. This is explicitly stated in Step 1 of the context, where $\\lambda_{n}=0$ is used to obtain the initial estimator.",
    "question": "Is the initial estimator $\\tilde{\\pmb{\\theta}}$ obtained by minimizing the quantile regression loss without any penalty term (i.e., $\\lambda_{n}=0$)?",
    "question_context": "Variable Selection in Quantile Varying Coefficient Models\n\nStep 1: Obtain the initial estimator $\\tilde{\\pmb{\\theta}}$ by using the function ‘‘rq’’ in the R package quantreg, which is designed for linear quantile regression. In this paper, we choose $\\tilde{{\\pmb{\\theta}}}_{k}$ as the minimizer of (3) with $\\lambda_{n}=0$ . <PARAGRAPH_SEPARATOR> Step 2: Set $\\omega_{0,1}~=~0$ and $\\omega_{k,1}~=~\\|\\tilde{\\pmb{\\theta}}_{k}\\|_{1}^{-\\delta}$ , $k=1,\\ldots,p$ as the common penalty weights for the kth group basis coefficients $(\\theta_{k,1},\\ldots,\\theta_{k,q})^{\\prime}$ . The $\\delta$ is some appropriately chosen positive value, for instance $\\delta=0.5,1,2$ as used in Zou (2006). Throughout our empirical studies, we set $\\delta=2$ . <PARAGRAPH_SEPARATOR> Step 3: Use the interior-point method to solve the linear programming problem <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\xi^{+},\\xi^{-},\\theta^{+},\\theta^{-}\\in\\mathbb{R}_{+}^{2N+2(p+1)q}}\\sum_{i=1}^{n}\\sum_{j=1}^{J_{i}}\\left\\{\\tau\\xi_{i j}^{+}+(1-\\tau)\\xi_{i j}^{-}\\right\\}+\\lambda_{n}\\sum_{k=0}^{p}\\omega_{k,1}\\sum_{l=1}^{q}(\\theta_{k,l}^{+}+\\theta_{k,l}^{-})}\\ &{\\displaystyle\\mathrm{s.t.}\\xi_{i j}^{+}-\\xi_{i j}^{-}=y_{i j}-\\Pi_{i j}^{\\prime}\\left(\\theta^{+}-\\theta^{-}\\right),}\\ &{i=1,\\dots,n,\\quad j=1,\\dots,J_{i},\\quad k=1,\\dots,p,\\quad l=1,\\dots,q,}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\pmb{\\theta}^{+}=\\left(\\theta_{0,1}^{+},\\ldots,\\theta_{0,q}^{+},\\ldots,\\theta_{p,1}^{+},\\ldots,\\theta_{p,q}^{+}\\right)^{\\prime}$ , $\\begin{array}{r}{\\pmb{\\theta}^{-}=\\left(\\theta_{0,1}^{-},\\ldots,\\theta_{0,q}^{-},\\ldots,\\theta_{p,1}^{-},\\ldots,\\theta_{p,q}^{-}\\right)^{\\prime},N=\\sum_{i=1}^{n}J_{i}}\\end{array}$ is the total number of observations, and $z^{+}=z I(z>0)$ and $z^{-}=-z I(z<0)$ for any variable $z$ . <PARAGRAPH_SEPARATOR> # Algorithm 2. <PARAGRAPH_SEPARATOR> The minimization of (3) with $L_{2}$ group penalty contains non-differentiable asymmetric $L_{1}$ loss and $L_{2}$ penalty, which leads to a computational challenge. In this paper, we extend the local linear approximation (LLA) (Zou and Li, 2008) to a group version so that it works under the quantile regression settings, and estimate $\\hat{\\pmb\\theta}$ by an iterative algorithm, where in each iteration, the estimate is obtained by standard linear programming. The iteration makes the computation more complicated than Algorithm 1 with $L_{1}$ penalty. <PARAGRAPH_SEPARATOR> Specifically, we first approximate $\\lVert\\pmb{\\theta}_{k}\\rVert_{2}$ by <PARAGRAPH_SEPARATOR> $$ \\lVert\\theta_{k}\\rVert_{2}\\approx\\lVert\\theta_{k}^{(0)}\\rVert_{2}+\\lVert\\theta_{k}^{(0)}\\rVert_{2}^{-1}|\\theta_{k}^{(0)}|^{\\prime}\\left(|\\theta_{k}|-|\\theta_{k}^{(0)}|\\right)=\\lVert\\theta_{k}^{(0)}\\rVert_{2}^{-1}\\sum_{l=1}^{q}|\\theta_{k,l}^{(0)}|\\cdot|\\theta_{k,l}|, $$ <PARAGRAPH_SEPARATOR> where $\\pmb{\\theta}_{k}^{(0)}=\\tilde{\\pmb{\\theta}}_{k}$ is some initial estimator as in Algorithm 1, and $|\\pmb{\\theta}_{k}|=(|\\theta_{k,1}|,\\ldots,|\\theta_{k,q}|)^{\\prime}.$ Noh and Park (2010) extended LLA to the group SCAD penalty, but their method still contained the $L_{2}$ norm in the penalty, which does not work in our setting. Then we can obtain $\\hat{\\pmb\\theta}$ by iteratively conducting the following penalized quantile regression until convergence <PARAGRAPH_SEPARATOR> $$ \\theta^{(j)}=\\arg\\operatorname*{min}_{\\theta}\\sum_{i=1}^{n}\\sum_{j=1}^{J_{i}}\\rho_{\\tau}(y_{i j}-\\Pi_{i j}^{\\prime}\\theta)+\\lambda_{n}\\sum_{k=0}^{p}\\omega_{k,2}\\|\\theta_{k}^{(j-1)}\\|_{2}^{-1}\\sum_{l=1}^{q}|\\theta_{k,l}^{(j-1)}|\\cdot|\\theta_{k,l}|, $$ <PARAGRAPH_SEPARATOR> where $\\omega_{0,2}=0$ , $\\omega_{k,2}=\\|\\tilde{\\pmb{\\theta}}_{k}\\|_{2}^{-\\delta}$ with $\\delta=2$ . The minimization of (5) can be solved by using an algorithm similar to the Step 3 of Algorithm 1. During the iteration, once the whole kth group is shrunk to zero, we will set $\\hat{\\pmb{\\theta}}_{k}=0$ . In our simulation studies, the algorithm often converges in less than five iterations. <PARAGRAPH_SEPARATOR> # 2.3. Selection of tuning parameters <PARAGRAPH_SEPARATOR> For practical implementation, the tuning parameters need to be decided. Throughout the empirical studies in this paper, we consider cubic spline basis $m=3.$ with $k_{n}$ interior knots chosen equally spaced on (0, 1). We employ a data-driven procedure to choose the tuning parameters $k_{n}$ and $\\lambda_{n}$ , where $k_{n}$ controls the smoothness of $\\hat{\\boldsymbol\\beta}(t)$ and $\\lambda_{n}$ determines the sparsity of the solution. Searching over two-dimensional grids, we choose the optimal pair of tuning parameters $(k_{n},\\lambda_{n})$ as the minimizer to the following Schwarz-type Information Criterion (Schwarz, 1978; Koenker et al., 1994), <PARAGRAPH_SEPARATOR> $$ S I C(k_{n},\\lambda_{n})=\\log\\left\\{\\sum_{i=1}^{n}\\sum_{j=1}^{J_{i}}\\rho_{\\tau}(y_{i j}-\\Pi_{i j}^{\\prime}\\hat{\\theta})\\right\\}+\\frac{\\log N}{2N}e d f, $$ <PARAGRAPH_SEPARATOR> where edf denotes the effective degree of freedom, $N$ denotes the total number of observations. In this paper, edf is defined as the number of interpolated $y_{i j}^{\\prime}s$ , i.e., the number of zero residuals, which provides a plausible measure for the effective degree of freedom of the fitted quantile regression model (Koenker et al., 1994; Li and Zhu, 2008). In this paper, a residual is treated as zero if its absolute value is smaller than $10^{-6}$ . <PARAGRAPH_SEPARATOR> Remark 1. Schwarz’s Bayesian information criterion, SIC, was originally given as a criterion for evaluating models estimated by the maximum likelihood methods. Machado (1993) proved that minimizing SIC yields consistent model selection in parametric $M$ -estimation, which cover quantile regression as a special case. For semi-parametric/nonparametric quantile regression, Koenker et al. (1994) and Wang et al. (2009) showed that SIC performs well in the selection of tuning parameters. Another commonly used criterion for selecting tuning parameters is cross-validation, which was shown tending to have an over-fitting effect in the resulting model (Wang et al., 2007b). In contrast, SIC tends to choose smaller models than the cross-validation method. <PARAGRAPH_SEPARATOR> # 3. Large-sample properties"
  },
  {
    "qid": "statistic-posneg-1469-0-1",
    "gold_answer": "TRUE. The asymptotic results for $E_{0}(T_{m,n}/N)$ and $\\operatorname{var}_{0}(T_{m,n}/\\sqrt{N})$ are derived under the condition that $m/N \to \\lambda \\in (0,1)$ as $N \to \\infty$. These results show that $T_{m,n}^{*} = \\sqrt{N}\\{T_{m,n}/N - 2\\lambda(1-\\lambda)\\}$ converges in distribution to $N(0, 4\\lambda^{2}(1-\\lambda)^{2})$, which justifies the use of a normal approximation for large samples. This asymptotic normality facilitates hypothesis testing for large $m$ and $n$.",
    "question": "The asymptotic expectation and variance of $T_{m,n}/N$ under $H_0$ are given by $E_{0}(T_{m,n}/N) \to 2\\lambda(1-\\lambda)$ and $\\operatorname{var}_{0}(T_{m,n}/\\sqrt{N}) \to 4\\lambda^{2}(1-\\lambda)^{2}$ as $N \to \\infty$. Is this claim correct, and does it justify the use of a normal approximation for large samples?",
    "question_context": "Distribution-Free Two-Sample Run Test for High-Dimensional Data\n\nThe paper discusses a distribution-free two-sample run test applicable to high-dimensional data, where the test statistic $T_{m,n}$ generalizes the univariate run test to higher dimensions while retaining the distribution-free property. Under the null hypothesis $H_0$, the exact null distribution of $T_{m,n}$ matches that of the univariate run statistic, given by specific combinatorial probabilities. For large samples, the asymptotic distribution of $T_{m,n}$ is derived, showing convergence to a normal distribution under certain conditions. The computation of $T_{m,n}$ involves solving a shortest Hamiltonian path problem, which is addressed using heuristic algorithms like Kruskal's method."
  },
  {
    "qid": "statistic-posneg-714-1-0",
    "gold_answer": "TRUE. The paper explicitly states that the new CLRT can be applied to general data with unknown means, whereas the CLRT in [2] is only applicable to Gaussian data with known means. This is supported by the asymptotic normality results for $L_{n}$ under the null hypothesis $\\Sigma_{p}=I_{p}$ and the discussion of its applicability to general data.",
    "question": "Is the statement 'The new CLRT statistic $L_{n}$ is applicable to general data with unknown means, unlike the CLRT in [2] which requires Gaussian data with known means' supported by the paper's results?",
    "question_context": "High-Dimensional Covariance Matrix Identity Tests\n\nThe paper introduces two versions of the sample covariance matrices, $S_{n}$ and $B_{n}$, and defines the CLRT and LW test statistics for testing the identity of the covariance matrix in high-dimensional settings. The CLRT is defined as $L_{n}=\\frac{1}{p}\\mathrm{tr}\\left(S_{n}\\right)-\\frac{1}{p}\\log\\left|S_{n}\\right|-1$, and the LW test is defined as $W_{n}=\\frac{1}{p}\\mathrm{tr}\\left(S_{n}-I_{p}\\right)^{2}-\\frac{p}{n-1}\\left(\\frac{1}{p}\\mathrm{tr}\\left(S_{n}\\right)\\right)^{2}-\\frac{(1+A)(n-2)(n-1)-2}{n(n-1)^{2}}$. The paper establishes the asymptotic normality of these statistics under the null hypothesis $\\Sigma_{p}=I_{p}$ and discusses their advantages over existing tests."
  },
  {
    "qid": "statistic-posneg-1383-0-1",
    "gold_answer": "FALSE. The context explicitly mentions that the projection operator $P^{x}:\\varphi\\mapsto P^{x}\\varphi\\in L^{p}({\\mathcal{L}})$ generally depends on $x$. The hypothesis $(^{*})$ is described as a 'uniformity' condition, which suggests that without this condition, the operator $P$ would indeed vary with $x$. Therefore, the statement that $P$ is independent of $x$ is false unless the uniformity condition is satisfied.",
    "question": "The projection operator $P$ defined by $E(\\varphi x)=(P\\varphi)\\cdot x$ is independent of the choice of $x \\in X$. Is this statement true based on the provided context?",
    "question_context": "Contractive Projections and Conditional Expectations in L^p Spaces\n\nThen there exists a 8-ring $A\\subset{\\Sigma}$ such that $E=E^{A}$ <PARAGRAPH_SEPARATOR> Proof. We first remark that, for each ${\\boldsymbol{x}}\\in X$ the equation $E(\\varphi x)=(P\\varphi)\\cdot x$ defines uniquely a continuous projection operator $P^{x}:\\varphi\\mapsto P^{x}\\varphi\\in L^{p}({\\mathcal{L}}).$ but it generally depends on $_{x}$ . Thus our hypothesis $(^{*})$ is a “uniformity” condition. <PARAGRAPH_SEPARATOR> It is clear that $\\|P\\varphi\\|_{p}\\leqslant\\|\\varphi\\|_{p}$ and, as noted, $P$ is a projection operator. Also (ii) of $(^{\\ast})$ implies the averaging property for $P.$ In fact, for $\\varphi,\\psi\\in L^{p}({\\mathcal{Z}})$ with $\\varphi$ or $\\psi$ bounded (say $\\psi_{,}$ , we have <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{P(\\varphi\\cdot P\\psi)x=E(\\varphi\\cdot(P\\psi)x)=E(\\varphi\\cdot E(\\psi x))=P\\varphi\\cdot E(\\psi x)}\\ &{\\qquad=P\\varphi\\cdot(P\\psi)x,\\quad x\\in X.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Since $\\pmb{x}\\in\\pmb{X}$ is arbitrary, it follows that $P(\\varphi\\cdot P\\psi)=(P\\varphi)\\cdot(P\\psi).$ .By Theorem 8, we deduce that $\\pmb{P}=E^{\\b{A}}$ for a $\\delta$ -ring $\\varLambda\\subset\\varSigma.$ Consequently, <PARAGRAPH_SEPARATOR> $$ {\\cal E}(\\varphi x)=({\\cal E}^{A}\\varphi)x={\\cal E}^{A}(\\varphi x),\\qquad\\varphi\\in{\\cal L}^{p}(\\varSigma),x\\in{\\cal X}. $$ <PARAGRAPH_SEPARATOR> This implies, from the density of such functions in $L_{X}{\\mathcal{P}}(Z)$ , and the properties of ${\\pmb E}^{\\pmb A}$ on $L_{X}{\\mathcal{P}}(\\Sigma)$ [4], $E f=E^{A}f,f\\in L_{X}{}^{p}(\\varSigma).$ as was to be proved."
  },
  {
    "qid": "statistic-posneg-407-1-2",
    "gold_answer": "FALSE. When adapting the similarity measure to trees with different structures, 'ghost branches' are added to replace leaves where splits occur in the other tree, but all St terms in the ghost branch are set to 0, not 1. This is because the ghost branch's splits and thresholds are undefined, making them trivially dissimilar to any actual splits in the other tree. The context specifies that St = 0 in such cases.",
    "question": "The similarity measure d(A1, A2) can be adapted to compare trees with different structures by adding 'ghost branches' to replace leaves where splits occur in the other tree, with all St terms in the ghost branch set to 1. True or False?",
    "question_context": "Tree Similarity Measure and Stability in Classification Trees\n\nThe context discusses the implementation of the NLS method for pruning classification trees, highlighting issues with overpruning when using cross-validation. It introduces the Reduced Error Pruning (REP) method as a more suitable alternative. The paper also presents a similarity measure between classification trees, defined by the dissimilarity at each node and aggregated with user-supplied weights. This measure is used to assess the stability of tree structures and to define a central tree in a forest. The application involves a radioecological model for contamination assessment."
  },
  {
    "qid": "statistic-posneg-703-2-1",
    "gold_answer": "FALSE. While the normal approximation performs reasonably well for some probability levels, it does not consistently outperform the F approximations. For example, at the lower 2 percent level, the normal approximation (0.35) is less accurate than both the exact limit (0.84) and the modified F approximation (0.83). The F approximations, particularly the modified version, generally provide better estimates, especially in the tails.",
    "question": "Does the normal approximation consistently outperform the F approximations across all probability levels in Table 3?",
    "question_context": "Comparison of Approximation Methods for Significance Limits in Small Sample Sizes\n\nThe paper compares different methods for determining significance limits in small sample sizes, including 'Exact' limits, Crude F approximation, Modified F approximation, and Normal approximation. The results are presented in tables, showing the probability levels and corresponding limits for each method. The paper concludes that even with extremely small sample sizes, both F approximations and the normal approximation are reasonably good, with a worthwhile improvement in the tail when using the F-distribution with a correcting factor. The extreme tail can be found exactly, as demonstrated by the two highest values of $5\\bar{t}_{1}$ with their probabilities."
  },
  {
    "qid": "statistic-posneg-330-0-0",
    "gold_answer": "TRUE. The eigenvalues $\\lambda_{k l}$ are indeed arranged in non-increasing order as part of the spectral decomposition of the covariance operator $\\Sigma^{\\mathbf{W}_{k}}(s,t)$. This ordering is a fundamental property of the Karhunen-Loève decomposition, where the first eigenfunction $\\psi_{k1}(s)$ corresponds to the largest eigenvalue $\\lambda_{k1}$ and explains the most variance in the functional predictor. Subsequent eigenvalues and eigenfunctions explain progressively less variance, ensuring the decomposition efficiently captures the most significant modes of variation in the data.",
    "question": "In the longitudinal penalized functional regression model, the functional predictors $W_{i j k}(s)$ are expressed using a truncated Karhunen-Loève decomposition, where the principal component scores $c_{i j k l}$ are uncorrelated random variables with variance $\\lambda_{k l}$. Is it correct to assume that the eigenvalues $\\lambda_{k l}$ are arranged in non-increasing order, i.e., $\\lambda_{k1} \\geqslant \\lambda_{k2} \\geqslant \\ldots$?",
    "question_context": "Longitudinal Penalized Functional Regression with Karhunen-Loève Decomposition\n\nThe addition of random effects to this model arises naturally and with a minimal increase in complexity. In what follows, we shall use $i$ to index subject, $j$ to index visit, $k$ to index functional predictor, $l$ to index objects associated with principal component bases and $m$ to index objects that are associated with spline bases. <PARAGRAPH_SEPARATOR> Specifically, given data of the form $[Y_{i j},W_{i j1}(s),\\ldots,W_{i j K}(s),X_{i j}]$ for subjects $1\\leqslant i\\leqslant I$ over visits $1\\leqslant j\\leqslant J_{i}$ , we model the functional effect in the following way. First, express the $W_{i j k}(s)$ in terms of a truncated Karhunen–Loève decomposition, i.e. let $\\Sigma^{\\mathbf{W}_{k}}(s,t)=\\mathrm{cov}\\{W_{i j k}(s),\\\\\\dot{W}_{i j k}(t)\\}$ be the covariance operator on the $k$ th observed functional predictor; thus $\\Sigma^{\\mathbf{W}_{k}}(s,t)$ is a bivariate function providing the covariance between two locations of the functional predictor. Further, let $\\Sigma_{l=1}^{\\infty}\\lambda_{k l}\\psi_{k l}(s)\\psi_{k l}(t)$ be the spectral decomposition of $\\Sigma^{\\mathbf{W}_{k}}(s,t)$ , where $\\lambda_{k1}\\geqslant\\lambda_{k2}\\geqslant...$ are the non-increasing eigenvalues and $\\psi_{k}(\\cdot)=\\{\\psi_{k l}(\\cdot):l\\in\\mathbb{Z}^{+}\\}$ are the corresponding orthonormal eigenfunctions. In practice, functional predictors are observed over finite grids, and often with error. Thus we estimate $\\Sigma^{\\mathbf{W}_{k}}(s,t)$ by using a method-of-moments approach, and then smooth the off-diagonal elements of this observed covariance matrix to remove the ‘nugget effect’ that is caused by measurement error (Staniswalis and Lee, 1998; Yao et al., 2003). Moreover, in the case that the $W_{i j k}(s)$ are sampled sparsely or over different grids, we can construct an estimate of the covariance matrix by using the following two-stage procedure (Di et al., 2009). First, use a fine grid to bin each subject’s observations to construct a rough estimate of the covariance matrix based on these undersmoothed functions; second, smooth the rough covariance matrix that is estimated in the previous step. <PARAGRAPH_SEPARATOR> A truncated Karhunen–Loève approximation for $W_{i j k}(s)$ is given by <PARAGRAPH_SEPARATOR> $$ W_{i j k}(s)=\\mu_{k}(s)+\\sum_{l=1}^{K_{w}}c_{i j k l}\\psi_{k l}(s), $$ <PARAGRAPH_SEPARATOR> where $K_{w}$ is the truncation lag, the $\\begin{array}{r}{c_{i j k l}=\\int_{0}^{1}\\{W_{i j k}(s)-\\mu_{k}(s)\\}\\psi_{k l}(s)\\mathrm{d}s}\\end{array}$ are uncorrelated random variables with variance $\\lambda_{k l}$ and $\\mu_{k}(s)$ is the mean of the $k$ th functional predictor, taken over all subjects and visits. Unbiased estimators of $c_{i j k l}$ can be obtained either as the Riemann sum approximation to the integral $\\begin{array}{r}{\\int_{0}^{1}\\left\\{W_{i j k}(s)-\\mu_{k}(s)\\right\\}\\psi_{k l}(s)\\mathrm{d}s}\\end{array}$ or via the mixed effects model (Crainiceanu et al., 2009; Di et al., 2009): <PARAGRAPH_SEPARATOR> $$ \\left.\\begin{array}{c}{{W_{i j k}(s)=\\displaystyle\\mu_{k}(s)+\\sum_{k=1}^{K_{w}}c_{i j k l}\\psi_{k l}(s)+\\varepsilon_{i j k}(s),}}\\\\ {{\\mathbf{c}_{i j k}\\sim N(0,\\Lambda_{k}),}}\\\\ {{\\varepsilon_{i j k}(s)\\sim N(0,\\sigma_{\\mathbf{W}_{k}}^{2}),}}\\end{array}\\right\\} $$ <PARAGRAPH_SEPARATOR> where $\\mathbf{c}_{i j k}^{\\prime}=(c_{i j k1},\\dots,c_{i j k K_{w}})$ is the vector of subject–visit-specific loadings for the $k$ th functional predictor, $\\Lambda_{k}$ is a $K_{w}\\times K_{w}$ matrix with $(l,l)$ th entry $\\lambda_{k l}$ and 0 elsewhere, and the ${\\bf{c}}_{i j k}$ and $\\varepsilon_{i j k}(s)$ are mutually independent for every $i,j$ and $k$ . Note that we have expressed the $W_{i j k}(s)$ without taking the repeated subject level observations into account; alternatively, we could use longitudinal functional principal components analysis for a decomposition that borrows strength across visits (Greven et al., 2010). <PARAGRAPH_SEPARATOR> The second step in longitudinal penalized functional regression is to model the coefficient functions $\\gamma_{k}(s)$ by using a large spline basis with smoothness induced explicitly via a mixed effects model. For example, let $\\boldsymbol{\\phi}_{k}(s)=\\left\\{\\phi_{k1}(s),\\phi_{k2}(s),\\ldots,\\phi_{k K_{g}}(s)\\right\\}$ be a truncated power series spline basis, so that <PARAGRAPH_SEPARATOR> $$ \\gamma_{k}(s)=\\phi_{k}(s)g_{k}=g_{k1}+g_{k2}t+\\sum_{m=3}^{K_{g}}g_{k m}(t-\\kappa_{k m})_{+} $$ <PARAGRAPH_SEPARATOR> where $g_{k}=(g_{k1},\\ldots,g_{k K_{g}})^{\\mathrm{T}}$ and $\\{\\kappa_{k m}\\}_{m=3}^{K_{g}}$ are knots (note that the index on the knots begins at 3 to match the index of the spline terms in $\\mathbf{\\mathit{~\\mathbf{~\\mathit{~g~}~}}}_{k}$ ). Thus, <PARAGRAPH_SEPARATOR> $$ \\int_{0}^{1}W_{i j k}(s)\\gamma_{k}(s)\\mathrm{d}s=a_{k}+\\int_{0}^{1}\\mathbf{c}_{i j k}^{\\prime}\\psi_{k}^{\\mathrm{T}}(s)\\phi_{k}(s)g_{k}\\mathrm{d}s=a_{k}+\\mathbf{c}_{i j k}^{\\prime}\\mathbf{M}_{k}g_{k}, $$ <PARAGRAPH_SEPARATOR> where $\\mathbf{M}_{k}$ is a $(K_{w}\\times K_{g})$ -dimensional matrix with the $(l,m)$ th entry equal to $\\begin{array}{r}{\\int_{0}^{1}\\psi_{k l}(s)\\phi_{k m}(s)\\mathrm{d}s}\\end{array}$ and $\\begin{array}{r}{a_{k}=\\int_{0}^{1}\\mu_{k}(s)\\gamma_{k}(s)\\stackrel{\\leftarrow}{\\mathrm{d}}s}\\end{array}$ . Then, letting $\\mathbf{C}_{k}$ be the $(\\Sigma_{i=1}^{I}J_{i})\\times K_{w}$ matrix of principal component loadings with rows $\\mathbf{c}_{i j k}^{\\prime},X$ be the design matrix of fixed effects and $Z_{1}$ be the random-effect design matrix that is used to account for repeated observations, the outcome model (1) is posed as <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l r}&{}&{{\\bf Y}\\vert{\\bf W}(s)\\sim{\\mathrm{EF}}(\\mu,\\eta),}\\\\ &{}&{g(\\mu)={\\bf X}\\beta+{\\bf Z}{\\bf u},}\\\\ &{}&{{\\bf\\Phi}{\\bf u}\\sim{\\cal N}\\left\\{\\left(\\begin{array}{l}{{\\bf0}}\\\\ {{\\bf0}}\\\\ {\\vdots}\\\\ {{\\bf0}}\\end{array}\\right),\\left(\\begin{array}{l l l l}{\\sigma_{{\\bf b}}^{2}{\\bf I}_{I}}&{{\\bf0}}&{\\ldots}&{{\\bf0}}\\\\ {{\\bf0}}&{\\sigma_{g_{1}}^{2}{\\bf I}_{K_{g}-2}}&{{\\bf0}}\\\\ {\\vdots}&{}&{\\ddots}&{\\vdots}\\\\ {{\\bf0}}&{{\\bf0}}&{\\ldots}&{\\sigma_{g_{L}}^{2}{\\bf I}_{K_{g}-2}}\\end{array}\\right)\\right\\},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\mathbf{X}=(1X(\\mathbf{C}_{1}\\mathbf{M}_{1})^{[,1:2]}...(\\mathbf{C}_{K}\\mathbf{M}_{K})^{[,1:2]})$ is the design matrix consisting of scalar covariates and fixed effects used to model the $\\gamma_{k}(s)$ , $\\mathbf{Z}=(Z_{1}\\mathbf{\\bar{(C_{l}M_{l})}}^{[,3:K_{g}]}\\dots(\\mathbf{C}_{K}\\mathbf{M}_{K})^{[,3:K_{g}]})$ is the design matrix consisting of subject-specific random effects and random effects used to model the coefficient functions, $\\beta=(\\alpha,\\beta,g_{10},g_{11},\\ldots,g_{K0},g_{K1})$ is the vector of fixed effect parameters and $\\mathbf{u}=(\\{b_{i}\\}_{i=1}^{I},\\{g_{1m}\\}_{m=3}^{K_{g}^{\\prime}},\\dots,\\{g_{K m}\\}_{m=3}^{K_{g}^{\\prime}})$ is the vector of s u1bject-specific random effects and the random effects used to model the $\\gamma_{k}(s)$ . The terms $\\begin{array}{r}{a_{k}=\\int_{0}^{1}\\dot{\\mu_{k}}\\left(s\\right)\\dot{\\gamma_{k}}(s)\\mathrm{d}s}\\end{array}$ are incorporated in the overall model intercept $\\alpha$ . <PARAGRAPH_SEPARATOR> In this way longitudinal functional regression models can be flexibly estimated by using standard, yet carefully constructed, mixed effects models. Once again, we note that the random-effect structure is simple for expositional purposes only; complex combinations of random-effects and covariance structures can be implemented just as in standard generalized mixed models. Similarly to Crainiceanu and Goldsmith (2010), it is possible to model jointly the principal component loadings ${\\bf{c}}_{i j k}$ and the outcome. This is important if there is substantial variability in the estimates of the ${\\bf{c}}_{i j k}$ but may not be necessary if good estimates are available. We have assumed the same truncation lags $K_{w}$ and $K_{g}$ for each functional predictor and coefficient, but this assumption is easily relaxed. For the choice of truncation lags, we refer to Goldsmith et al. (2011a) and Ruppert (2002) and choose them large, subject to the identifiability constraint $K_{w}\\geqslant K_{g}$ ; typically $K_{w}=K_{g}=30$ will suffice. Spline bases other than the truncated power series basis can (and, in the Bayesian software implementation, will) be used with appropriate changes to the specification of the random effects $\\mathbf{u}$ that are used to model the $\\gamma_{k}(s)$ . <PARAGRAPH_SEPARATOR> A related approach, which can be advantageous when the shape of $\\gamma_{k}(s)$ is known, takes $\\phi_{k}(\\cdot)$ as a collection of parametric functions, i.e. $\\phi_{k}(s)=\\{1,t\\}$ if $\\gamma_{k}(s)$ is a linear function, and uses random effects to model the longitudinal structure of the data but not to induce smoothness on $\\gamma_{k}(s)$ . However, absent setting-specific knowledge of the shape of $\\gamma_{k}(s)$ we typically advocate the more flexible penalized approach that was described above."
  },
  {
    "qid": "statistic-posneg-133-0-0",
    "gold_answer": "FALSE. The working covariance matrix $V_{i}(\\beta,\\alpha,\\phi)$ is correctly specified only if both $R(\\alpha)$ is the true correlation matrix and the variance function $\\phi v(\\mu_{i j})$ correctly models the true variance of $Y_{i j}$. The statement is false because it ignores the role of the variance function in the specification of $V_{i}(\\beta,\\alpha,\\phi)$. The working covariance matrix requires both the correlation structure and the variance function to be correctly specified to match the true covariance structure of $Y_{i}$.",
    "question": "In the GEE framework, the working covariance matrix $V_{i}(\\beta,\\alpha,\\phi)=A_{i}^{1/2}(\\beta,\\phi)R(\\alpha)A_{i}^{1/2}(\\beta,\\phi)$ is correctly specified if and only if $R(\\alpha)$ is the true correlation matrix of $Y_{i}$. True or False?",
    "question_context": "Generalized Estimating Equations (GEE) and Empirical Likelihood in Longitudinal Data Analysis\n\nIn a longitudinal dataset, $Y_{i}=(Y_{i1},\\dots,Y_{i t_{i}})^{T}$ denotes the response vector of the $i$ th subject, where $Y_{i j}$ is the response observed at the $j$ th time point on subject $i$ , $i=1,\\ldots,n$ and $j=1,\\ldots,t_{i};X_{i j}=(X_{i j1},\\ldots,X_{i j p})^{T}$ is the $p\\times1$ vector of covariates associated with $Y_{i j}$ , and $X_{i}=(X_{i1},\\ldots,X_{i t_{i}})^{T}$ is the $t_{i}\\times p$ design matrix for the $i$ th subject. Measurements on the same subject are correlated and form what is termed a 'cluster.' In biomedical applications, the response variable $Y_{i j}$ is often binary or a count, for instance the presence or absence of some particular illness, or the number of epileptic seizures in a four-week interval. Among models appropriate for non-Gaussian longitudinal data, generalized estimating equations (GEE; Liang and Zeger 1986) require only a partial specification of the marginal distribution for the response. It is assumed in GEE that $E(Y_{i j})=\\mu_{i j}$ depends upon $X_{i j}$ through a known link function $h(\\mu_{i j})=\\eta_{i j}=X_{i j}^{T}\\beta$ , where $\\beta$ is the $p$ -dimensional parameter of interest, and that $\\operatorname{var}(Y_{i j})=\\phi v(\\mu_{i j})$ , where $v(\\cdot)$ is a known variance function and $\\phi$ is a possibly unknown scale parameter. In addition, a $t_{i}\\times t_{i}$ working correlation matrix $R(\\alpha)$ , $\\boldsymbol{\\alpha}=(\\alpha_{1},\\ldots,\\alpha_{s})^{T}$ , is assumed for $Y_{i}$ , and the corresponding working covariance matrix is $$ V_{i}(\\beta,\\alpha,\\phi)=A_{i}^{1/2}(\\beta,\\phi)R(\\alpha)A_{i}^{1/2}(\\beta,\\phi), $$ where $A_{i}(\\beta,\\phi)$ is a diagonal matrix with $\\operatorname{var}(Y_{i j})=\\phi v(\\mu_{i j})$ , $j=1,\\ldots,t_{i}$ , along the diagonal. $V_{i}=\\operatorname{var}(Y_{i})$ if $R(\\alpha)$ is chosen correctly. The generalized estimating equation for $\\beta$ is defined as $$ \\sum_{i=1}^{n}\\left(\\frac{\\partial\\mu_{i}}{\\partial\\beta^{T}}\\right)^{T}V_{i}^{-1}(\\beta,\\alpha,\\phi)(Y_{i}-\\mu_{i})=0, $$ where $\\mu_{i}=(\\mu_{i1},\\dots,\\mu_{i t_{i}})^{T}$ . The left side of (2.2) can be written as a function of $\\beta$ alone given the data, $$ \\sum_{i=1}^{n}\\Biggl(\\frac{\\partial\\mu_{i}}{\\partial\\beta^{T}}\\Biggr)^{T}V_{i}^{-1}(\\beta,\\alpha,\\phi)(Y_{i}-\\mu_{i})=\\sum_{i=1}^{n}U_{i}\\bigl[\\beta,\\hat{\\alpha}(\\beta,\\hat{\\phi}(\\beta))\\bigr], $$ since $\\phi$ can be replaced by an $n^{1/2}$ -consistent estimator $\\hat{\\phi}(\\beta)$ given $\\beta$ , and $\\alpha$ by an $n^{1/2}$ - consistent estimator $\\hat{\\alpha}(\\beta,\\phi)$ given $\\beta$ and $\\phi$ (Liang and Zeger 1986). Typically, method of moments estimators of $\\phi$ and $\\alpha$ are used. The solution to (2.2), $\\hat{\\beta}_{G}$ , is the GEE estimator of the regression parameter $\\beta$ ."
  },
  {
    "qid": "statistic-posneg-2059-2-1",
    "gold_answer": "FALSE. The paper explicitly discusses the Gauss-Laplace representation in Proposition 3, stating that a generalized Laplace random vector can be represented as √u𝐱, where u ∼ Gamma(s,1) and 𝐱 ∼ N(0, Σ) are independent. This is a key property used in the proofs.",
    "question": "Does the paper claim that the generalized Laplace distribution cannot be represented as an infinite scale mixture of Gaussians with gamma mixing distribution?",
    "question_context": "Generalized Laplace Distribution in PPCA Framework\n\nThe paper discusses the multivariate generalized asymmetric Laplace distribution and its properties within the PPCA framework. Key properties include the characteristic function, density function, and the Gauss-Laplace representation. The distribution is used to model the signal and noise terms in the PPCA model, leading to a closed-form expression for the marginal likelihood."
  },
  {
    "qid": "statistic-posneg-2121-5-1",
    "gold_answer": "FALSE. While the context discusses the possibility of eliminating preferential sampling by finding relevant explanatory variables (latent variable $U$), it does not claim this is always possible. The text emphasizes that this approach is conditional on the ability to find such variables and verify residual independence, which may not always be feasible.",
    "question": "Does the context suggest that preferential sampling can always be eliminated by finding explanatory variables associated with both $X$ and $S$?",
    "question_context": "Geostatistical Inference Under Preferential Sampling\n\nA natural response to a strongly non-uniform sampling design is to ask whether its spatial pattern could be explained by the pattern of spatial variation in a relevant covariate. Suppose, for illustration, that $S$ is observed without error, that dependence between $X$ and $S$ arises through their shared dependence on a latent variable $U$ and that the joint distribution of $X$ and $S$ is of the form $$[X,S]{}=\\int[X|U][S|U][U]\\mathrm{d}U,$$ so that $X$ and $S$ are conditionally independent given $U.$ . If the values of $U$ were to be observed, we could then legitimately work with the conditional likelihood $[X,S|U]{=}[X|U][S|U]$ and eliminate $X$ by marginalization, exactly as is done implicitly when conventional geostatistical methods are used. In practice, ‘observing’ $U$ means finding explanatory variables which are associated both with $X$ and with $S$ , adjusting for their effects and checking that after this adjustment there is little or no residual dependence between $X$ and $S$ . If so, the analysis could then proceed on the assumption that sampling is no longer preferential. In this context, any of the existing tests for preferential sampling can be applied, albeit approximately, to residuals after fitting a regression model for the mean response."
  },
  {
    "qid": "statistic-posneg-1301-0-0",
    "gold_answer": "FALSE. While it is true that trace-class operators are Hilbert-Schmidt operators, the converse is not necessarily true. The context states that $\\Sigma_X$ and $\\Sigma_Y$ are trace-class operators because their traces are finite, which is a stronger condition than being Hilbert-Schmidt.",
    "question": "Is it true that the covariance operators $\\Sigma_X$ and $\\Sigma_Y$ are trace-class operators because they are Hilbert-Schmidt operators?",
    "question_context": "Kernel Canonical Correlation Analysis: Convergence Rates and Lower Bounds\n\nDefine the two covariance operators by $\\langle f,\\Sigma_{X}f\\rangle_{\\mathcal{H}_{X}}=\\mathsf{V a r}\\{f(x)\\},\\forall f\\in\\mathcal{H}_{x}$ and $\\langle g,\\Sigma_{Y}g\\rangle_{\\mathcal{H}_{y}}=\\mathsf{V a r}\\{g(y)\\},\\forall g\\in\\mathcal{H}_{y}$ . Since $\\begin{array}{r}{\\mathrm{Var}\\{f(x)\\}=\\mathrm{E}\\langle f,K_{1}(.,x)-\\mathrm{E}_{x}K_{1}(.,x)\\rangle_{\\mathcal{H}_{x}}^{2}\\le\\|f\\|_{\\mathcal{H}_{x}}^{2}\\{\\mathrm{E}_{x}K_{1}(x,x)-\\|\\mathrm{E}_{x}K_{1}(.,x)\\|_{\\mathcal{H}_{x}}^{2}\\}<\\infty,}\\end{array}$ covariance operators are well-defined bounded operators. In fact we can write $\\Sigma_{X}=\\dot{\\operatorname{E}_{x}}[\\{K_{1}(.,x)-\\operatorname{E}_{x}K_{1}(.,x)\\}\\otimes\\{\\ddot{K}_{1}(.,x)-\\operatorname{E}_{x}K_{1}(.,x)\\}]$ and $\\varSigma_{Y}=\\mathbb{E}_{y}[\\{K_{2}(.,y)-$ $\\mathtt{E}_{y}K_{2}(.,y)\\}\\otimes\\{K_{2}(.,y)-\\mathtt{E}_{y}K_{2}(.,y)\\}]$ where for $f_{1},f_{2}\\in\\mathcal{H}_{x},f_{1}\\otimes f_{2}$ is the operator such that $(f_{1}\\otimes f_{2})h=\\langle f_{2},h\\rangle_{\\mathcal{H}_{x}}f_{1}$ , for example. Since $\\mathtt{E}_{x}K_{1}(x,x)<\\infty$ , we have $\\mathrm{tr}(\\Sigma_{X})=\\operatorname{E}_{x}\\langle K_{1}(.,x)-\\operatorname{E}_{x}K_{1}(.,x)$ , K1(., x) − ExK1(., x)⟩Hx < ∞ and thus $\\Sigma_{X}$ and $\\varSigma_{Y}$ are trace-class operators and in particular are Hilbert–Schmidt operators. Furthermore, we define the cross-covariance operators $\\Sigma_{X Y}=\\Sigma_{Y X}^{\\bar{\\top}}=\\mathsf E_{x,y}[\\{K_{1}(.,\\bar{x})-\\mathsf E_{x}K_{1}(.,x)\\}\\otimes\\{K_{2}(.,y)-\\mathsf E_{y}K_{2}(.,y)\\}]$ where $()^{\\top}$ denotes the adjoint operator. By the reproducing property, we can easily see $\\langle f,\\Sigma_{X Y}g\\rangle_{\\mathcal{H}_{x}}=\\mathsf{C o v}\\{f(x),g(y)\\}$ ."
  },
  {
    "qid": "statistic-posneg-1562-2-0",
    "gold_answer": "FALSE. The calculation shows P(v0) = 0.0504, which is very close to the normal-theory 5% point (0.0500), but the context implies that this is adjusted for non-normality via Edgeworth series terms (λ₃, λ₄). The equality is coincidental, not theoretically exact, as non-normal corrections are applied.",
    "question": "Is the calculated probability P(v0) = 0.0504 consistent with the normal-theory 5% point (v0 = 3.2840) for the variance ratio test in non-normal populations?",
    "question_context": "Variance Ratio Distribution in Non-Normal Populations\n\nSample $\\boldsymbol{I}$ (summer months—barometric height in inches): 29.85, 29.75, 29·15, 30.05, 29·55, 30·35, 30·05, 30·25 and 29·35. <PARAGRAPH_SEPARATOR> Sample $_{I I}$ (winter months): 30·55, 28·45, 30·45, 30·15, 29·15, 29·95, 30·65, 28-85, 30·25, 30.35, 30.55, 29.55 and 29·05. <PARAGRAPH_SEPARATOR> Here ${\\pmb v}=3{\\cdot}3685$ for $\\pmb{n_{1}}=12$ and ${\\mathfrak{n}}_{2}=8$ degrees of freedom. The probability at $v_{0}=3{\\cdot}2840$ (the normal-theory $5\\%$ point), as calculated from (3·8) and Table 4, is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{P(v_{0})=0\\cdot0500+(0\\cdot0680)(0\\cdot0257)+(0\\cdot2065)(-0\\cdot0140}\\ &{\\qquad-(0\\cdot1719)(-0\\cdot0078)-(0\\cdot0297)(-0\\cdot0072)}\\ &{\\qquad=0\\cdot0504.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Hence this point can also be regarded as the $5\\%$ point of $v$ for the present populations. Since the sample value of $v$ falls beyond this point, the difference in variances must be regarded as significant at the $5\\%$ level. <PARAGRAPH_SEPARATOR> # SUMMARY <PARAGRAPH_SEPARATOR> The mathematical forms of the frequency distributions of the variance ratio used for testing (i) the homogeneity of a set of means, in the case of a one-way classification for analysis of variance with equal or unequal numbers (the test function denoted here by $\\boldsymbol{\\mathscr{w}}$ )and (ii) the compatibility of two variances (the test function, $\\pmb{v}$ )have been derived for samples drawn from non-normal universes characterized by a priori values of $\\lambda_{3}(=\\mathcal{N}_{1})$ , a measure of skewness, and $\\lambda_{4}(=\\beta_{2}-3)$ , a measure of excess, and expressed by the first four terms (including the term in $\\lambda_{3,-}^{2}$ of the Edgeworth series. The formulae which have furnished the corrective terms in $\\lambda_{4}$ and $\\lambda_{3}^{2},$ in addition to the normal-theory function in each case, are $(a)$ sufficiently accurate for any size of sample from populations agreeing well with the terms up to $\\lambda_{3}^{2}$ of the Edgeworth series, and $(b)$ valid asymptotically for any form of universe. Thus it is not unlikely that for moderate sizes of sample the distributions have quite an extended range of applicability, provided always that the higher order even-cumulants are small."
  },
  {
    "qid": "statistic-posneg-600-0-2",
    "gold_answer": "TRUE. The formulation is correct for a random-effects ordinal regression model with a probit link. The probability that the ordinal response \\( Y_{i k} \\) is less than or equal to category \\( c \\) is given by the cumulative normal distribution function \\( \\Phi \\) evaluated at \\( (\\gamma_{c} - \\zeta_{i k}) / \\sigma \\), where \\( \\zeta_{i k} \\) is the linear predictor incorporating fixed and random effects. This aligns with standard probit model assumptions.",
    "question": "The probability \\( P(Y_{i k} \\leq c) \\) for the ordinal response is given by \\( \\Phi\\left(\\frac{\\gamma_{c} - \\zeta_{i k}}{\\sigma}\\right) \\), where \\( \\zeta_{i k} = \\pmb{x}_{i k}^{\\prime}\\pmb{\\beta} + \\pmb{z}_{i k}^{\\prime}\\pmb{b}_{i} \\). Is this formulation correct under the assumption of a probit link function?",
    "question_context": "Joint Normal-Ordinal (Probit) Model for Longitudinal Data\n\nThe paper presents a joint model for analyzing longitudinal data with both continuous and ordinal responses. The model employs random effects to account for correlations within and between responses. The continuous response is modeled using a linear mixed model, while the ordinal response is modeled using a random-effects ordinal regression model with a probit link. The joint likelihood integrates over the random effects to estimate parameters via maximum likelihood, with numerical approximations such as adaptive Gaussian quadrature used for the integrals."
  },
  {
    "qid": "statistic-posneg-1084-0-0",
    "gold_answer": "TRUE. The paper demonstrates through simulations that when $\\operatorname{cov}\\{X, \\phi(\\theta^{\\mathrm{T}}X)\\} \\neq 0$, projection pursuit regression leads to results that are less stable and harder to interpret. The proposed iterative algorithm, as described in §3, provides more stable estimates and a better convergence rate, as evidenced by the simulation results in Figures 3 and 4.",
    "question": "Is the claim that the proposed iterative algorithm has a better convergence rate than projection pursuit regression when $\\operatorname{cov}\\{X, \\phi(\\theta^{\\mathrm{T}}X)\\} \\neq 0$ supported by the simulation results?",
    "question_context": "Extended Partially Linear Single-Index Models: Estimation and Comparison with Projection Pursuit Regression\n\nThe paper discusses the estimation of extended partially linear single-index models, comparing the proposed iterative algorithm with projection pursuit regression. The model is given by $y - \\beta^{\\mathrm{T}}X = \\phi(\\theta^{\\mathrm{T}}X) + \\varepsilon$, where $\\beta$ and $\\theta$ are estimated parameters, and $\\phi(.)$ is a nonlinear function. The paper highlights the conditions under which the proposed method outperforms projection pursuit regression, particularly when $\\operatorname{cov}\\{X, \\phi(\\theta^{\\mathrm{T}}X)\\} \\neq 0$."
  },
  {
    "qid": "statistic-posneg-1760-3-0",
    "gold_answer": "TRUE. The profile likelihood ${\\cal L}_{p}(\\vartheta;{\\bf z})$ is invariant under such transformations because $L_{p}(\\pmb{\\vartheta};\\mathbf{z}) \\propto L_{p}(\\pmb{\\vartheta};\\mathbf{z} \\odot \\exp\\{X\\mathbf{a}\\})$. This invariance arises from the property that $A_{\\vartheta}X = X$, which ensures that the term ${\\bf y} - A_{\\vartheta}{\\bf y}$ remains unchanged under the transformation $\\mathbf{y} \\rightarrow \\mathbf{y} + X\\mathbf{a}$ (since $\\mathbf{y} = \\log(\\mathbf{z})$). This demonstrates the log-location invariance of the estimator $\\hat{\\pmb{\\vartheta}}(\\mathbf{z})$.",
    "question": "Is the profile likelihood ${\\cal L}_{p}(\\vartheta;{\\bf z})$ invariant under transformations of the form $\\mathbf{z} \\rightarrow \\mathbf{z} \\odot \\exp\\{X\\mathbf{a}\\}$ for any $\\mathbf{a} \\in \\mathbb{R}^{p}$?",
    "question_context": "Log-Gaussian Prediction Intervals and Invariance Properties\n\nThis work was partially supported by National Science Foundation grant DMS-0719508. <PARAGRAPH_SEPARATOR> # Appendix <PARAGRAPH_SEPARATOR> Proof of Proposition 1. Let ${\\pmb\\beta}_{\\vartheta}({\\pmb z})=(X^{\\prime}{\\pmb\\Sigma}_{\\vartheta}^{-1}X)^{-1}X^{\\prime}{\\pmb\\Sigma}_{\\vartheta}^{-1}\\log({\\bf z})$ be the ML estimate of $\\beta$ based on the observed data z when $\\vartheta$ is known. The maximum likelihood estimate of $\\vartheta$ based on the observed data $\\mathbf{z}$ is the vector $\\hat{\\pmb{\\vartheta}}\\in\\Theta$ that maximizes the profile likelihood of $\\vartheta$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{\\displaystyle{\\cal L}_{p}(\\vartheta;{\\bf z})=(2\\pi)^{-\\frac{n}{2}}\\Bigl(\\prod_{i=1}^{n}z_{i}\\Bigr)^{-1}|{\\cal{S}}_{\\vartheta}|^{-\\frac{1}{2}}\\exp\\{(\\log({\\bf z})-{\\cal X}\\hat{\\beta}_{\\vartheta}({\\bf z}))^{\\prime}{\\cal{\\Sigma}}_{\\vartheta}^{-1}(\\log({\\bf z})-{\\cal X}\\hat{\\beta}_{\\vartheta}({\\bf z}))\\}}}\\ {{\\displaystyle~\\propto|{\\cal{S}}_{\\vartheta}|^{-\\frac{1}{2}}\\exp\\{({\\bf y}-A_{\\vartheta}{\\bf y})^{\\prime}{\\cal{\\Sigma}}_{\\vartheta}^{-1}({\\bf y}-A_{\\vartheta}{\\bf y})\\},}}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\mathbf{y}=\\log(\\mathbf{z})$ and $A_{\\vartheta}=X(X^{\\prime}\\Sigma_{\\vartheta}^{-1}X)^{-1}X^{\\prime}\\Sigma_{\\vartheta}^{-1}$ . Now note that for any $\\mathbf{a}\\in\\mathbb{R}^{p}$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{{\\bf y}-A_{\\vartheta}{\\bf y}={\\bf y}-X{\\bf a}+X{\\bf a}-A_{\\vartheta}\\left({\\bf y}-X{\\bf a}+X{\\bf a}\\right)}}\\ {{{\\bf\\qquad}={\\bf y}-X{\\bf a}-A_{\\vartheta}\\left({\\bf y}-X{\\bf a}\\right),}}\\end{array} $$ <PARAGRAPH_SEPARATOR> since $A_{\\vartheta}X=X$ . This implies that for any $\\mathbf{z}>\\mathbf{0}$ and $\\mathbf{a}\\in\\mathbb{R}^{p},L_{p}(\\pmb{\\vartheta};\\mathbf{z})\\propto L_{p}(\\pmb{\\vartheta};\\mathbf{z}\\odot\\exp\\{X\\mathbf{a}\\})$ , and hence $\\hat{\\pmb{\\vartheta}}(\\mathbf{z})=\\hat{\\pmb{\\vartheta}}(\\mathbf{z}\\odot\\exp\\{X\\mathbf{a}\\})$ .   The proof for the REML estimate o\" (z) follows along the same lines."
  },
  {
    "qid": "statistic-posneg-199-0-2",
    "gold_answer": "FALSE. The weight function w(δig; αg, ηg) is a decreasing function of δig because ν(δig; αg, ηg) decreases as δig increases. This means that outliers (with larger δig) are down-weighted in the estimation of the model parameters, providing robustness.",
    "question": "The weight function w(δig; αg, ηg) in the MVCN model increases as the squared Mahalanobis distance δig increases, giving more weight to outliers. Is this statement accurate?",
    "question_context": "Matrix-Variate Contaminated Normal Mixture Model for Robust Clustering\n\nThe paper introduces a mixture of matrix-variate contaminated normal (MVCN) distributions for robust clustering of matrix-variate data. The MVCN distribution is defined as a two-component mixture where one component models typical observations and the other, with inflated covariance, models atypical observations. The model parameters are estimated using an ECM algorithm, which handles the missing group memberships and typical/atypical classifications. The approach provides robust estimates by down-weighting the influence of outliers and allows for the detection of outlying matrices."
  },
  {
    "qid": "statistic-posneg-194-0-3",
    "gold_answer": "TRUE. The context states that under the proportional hazards alternative and assuming $\\Lambda$ is continuous, the threshold can be defined based solely on $\\rho$ and the head start $S(0)$, without needing to model the arrival process, survival time, or censoring time. This is due to the transformation to the standardized Poisson process.",
    "question": "Does the paper suggest that the threshold for the CUSUM chart can be determined without modeling the arrival process, survival time, or censoring time under the proportional hazards alternative?",
    "question_context": "Risk-adjusted CUSUM Charts for Time-to-Event Monitoring\n\nThe paper discusses the use of CUSUM charts for monitoring time-to-event data, focusing on proportional hazards alternatives where the hazard rate under the alternative hypothesis is a multiple of the in-control hazard rate. The log-likelihood ratio for the CUSUM chart is given by $R(t)=\\log(\\rho)N(t)-(\\rho-1)\\Lambda(t)$, where $N(t)$ is the number of events and $\\Lambda(t)$ is the cumulative hazard. The paper also explores the impact of head starts and delayed information on the performance of the CUSUM chart."
  },
  {
    "qid": "statistic-posneg-335-1-1",
    "gold_answer": "FALSE. While the formula for λ_j appears independent of j, the text specifies κ depends on C_* (from assumption (A)) and the wavelet basis ψ. In practice, thresholding in wavelet methods often uses level-dependent thresholds (e.g., λ_j = κ_j√(ln(n)/n)) to account for varying signal-to-noise ratios across scales. The paper's reference to Kerkyacharian and Picard (2004) suggests implicit level-dependence through κ's dependence on wavelet properties.",
    "question": "Is the threshold λ_j = κ√(ln(n)/n) in Theorem 3.4 independent of the resolution level j?",
    "question_context": "Nonparametric Estimation of Quantile Density via Wavelet Methods\n\nThe paper discusses nonparametric estimation of a quantile density function using wavelet methods. It introduces a Lipschitz (1/2) assumption (A) to improve the efficiency of linear and hard thresholding wavelet estimators. Theorems 3.3 and 3.4 present bounds on the expected L_p error for these estimators under assumption (A), with specific conditions on the smoothness parameters (s, r, p) and choices of resolution levels (j_0, j_1). The hard thresholding estimator's convergence rate ψ_n adapts to the parameter regime (r,s,p) ∈ D."
  },
  {
    "qid": "statistic-posneg-931-0-1",
    "gold_answer": "TRUE. The formula $\\hat{\\sigma}_{j}^{2}$ calculates the residual variance when only the $j$-th lagged variable is included, isolating its explanatory power. This is used to identify the most explicative variable ($i_{1}$) in the model selection process, as described in the context.",
    "question": "Does the formula $\\hat{\\sigma}_{j}^{2}=\\frac{1}{n-1}\\sum_{t=p+1}^{T}(Z_{t}-\\hat{f}_{j}(Z_{t-j}))^{2}$ correctly represent the residual variance when only the $j$-th lagged variable is included in the model?",
    "question_context": "Nonlinear Autoregressive Model Order Selection via Spline PCAIV\n\nAs shown in Imam et al. (1999), spline PCAIV is used to estimate the order $p$ of the nonlinear autoregressive model by proceeding as follows: (1) Choose $P$ big enough to incorporate possible seasonal component. (2) Build a $n\\times1$ matrix $Y$ and a $n\\times P$ matrix $X$. (3) Perform a spline PCAIV. (4) Compute the residual variance of order $j$ , for all $j$ less than $P$ $$\\hat{\\sigma}_{1,\\ldots,j}^{2}=\\frac{1}{n-1}\\sum_{t=P+1}^{T}(Z_{t}-\\hat{f}_{1}(Z_{t-1})-\\cdot\\cdot\\cdot-\\hat{f}_{j}(Z_{t-j}))^{2}.$$ (5) Plot $\\hat{\\sigma}_{1,...,j}^{2}$ against $j$ and choose $p$ as the value for which $\\hat{\\sigma}_{1,...,j}^{2}j>p$ stabilizes."
  },
  {
    "qid": "statistic-posneg-1722-0-0",
    "gold_answer": "TRUE. Explanation: According to Brown & Sundberg (1987), under normality of $\\pmb\\varepsilon$, the classical estimator obtained by minimizing $f_Y(\\xi)$ is not generally the maximum likelihood estimator unless $\\pmb{\\xi}$ and $Y$ have the same dimension. When their dimensions match, the classical estimator coincides with the maximum likelihood estimator. This is because the inversion of the distribution of $Y$ given $\\pmb{\\xi}$ becomes exact, and the minimization of the quadratic form corresponds to maximizing the likelihood function under the normality assumption.",
    "question": "In the context of controlled calibration, the classical estimate $\\pmb{\\hat{\\xi}}$ is obtained by minimizing the function $f(\\xi)=f_{Y}(\\xi)\\equiv\\{Y-\\hat{h}(\\xi)\\}^{\\mathrm{T}}S^{-1}\\{Y-\\hat{h}(\\xi)\\}$. Is it true that this estimator is equivalent to the maximum likelihood estimator under normality of $\\pmb\\varepsilon$ when $\\pmb{\\xi}$ and $Y$ have the same dimension?",
    "question_context": "Nonlinear Calibration in Controlled Experiments\n\nIn calibration problems one wishes to predict a quantity whose precise measurement $\\pmb{\\xi}$ is difficult or expensive to obtain by a less precise but more easily obtained measurement Y. A training sample comprising $\\pmb{n}$ measurements $\\pmb{\\xi}_{(i)}$ with the corresponding $Y_{(i)}$ is used to estimate the relation between $\\pmb{\\xi}$ and $\\mathbf{Y}$ ; this estimated relation will be used to predict the unknown $\\pmb{\\xi}$ corresponding to a measured future Y. <PARAGRAPH_SEPARATOR> Often $Y,$ and sometimes $\\pmb{\\xi}$ as well, are vector valued. For example, in the work of Naes (1985), $\\pmb{\\xi}$ is the fat content of a sample of fish while $Y$ is a vector of near infrared measurements at nine wavelengths; for Fearn (1983) $\\pmb{\\xi}$ is the protein content of a sample of ground wheat and $Y$ is a vector of six logged infrared reflectances; and for Oman & Wax (1984) $\\pmb{\\xi}$ is gestational age while $\\mathbf{Y}$ contains two ultrasound bone measurements of a foetus. Examples with higher dimensional $\\pmb{Y},$ of the order of 1000, are given by Osborne et al. (1984). <PARAGRAPH_SEPARATOR> We consider here controlled calibration (Brown, 1982), in which the $\\pmb{\\xi}_{(i)}$ of the training sample are set or controlled by the experimenter and cannot be considered random. Thus predictions of future $\\pmb{\\xi}$ are done by ‘inverting' the distribution of $\\pmb{Y}$ given $\\pmb{\\xi}$ as estimated from the training sample. If the assumed relation is <PARAGRAPH_SEPARATOR> $$ Y=h(\\xi)+\\varepsilon,\\quad E(\\varepsilon)=0,\\quad\\mathsf{c o v}\\left(\\varepsilon\right)=\\Gamma $$ <PARAGRAPH_SEPARATOR> and if $\\hat{h}$ and $\\pmb{S}$ are estimates of $\\pmb{h}$ and $\\mathbf{\\Gamma}$ from the training sample, the ‘classical' estimate $\\pmb{\\hat{\\xi}}$ for future $\\pmb{Y}$ is obtained by minimizing <PARAGRAPH_SEPARATOR> $$ f(\\xi)=f_{Y}(\\xi)\\equiv\\{Y-\\hat{h}(\\xi)\\}^{\\mathrm{T}}S^{-1}\\{Y-\\hat{h}(\\xi)\\}. $$ <PARAGRAPH_SEPARATOR> If $\\pmb{h}$ is linear in $\\pmb{\\xi},$ then $\\pmb{\\hat{\\xi}}$ is the generalized least squares estimator from (1·1) with least squares estimators $\\pmb{\\hat{h}}$ and $s$ from the training data replacing the unknown $\\pmb{h}$ and $\\Gamma$ <PARAGRAPH_SEPARATOR> Under normality of $\\pmb\\varepsilon$ it is not in general the maximum likelihood estimator unless $\\pmb{\\xi}$ and $Y$ have the same dimension (Brown & Sundberg, 1987). However, it is linear in $Y,$ and will usually be preferred to the maximum likelihood estimator which exhibits an unnatural expansion property and is more complicated, being nonlinear in $Y.$ <PARAGRAPH_SEPARATOR> If $\\pmb{\\hat{h}}$ is nonlinear in $\\pmb{\\xi}$ then minimization of (1·2), which typically is done numerically, can be problematic. For example, Fig. 1 shows $f_{Y}(\\xi)$ , where $Y=(1\\cdot78,38\\cdot73)^{\\mathrm{T}}$ is the fourth observation in the prediction set of the paint data discussed by Brown (1982). Here $\\pmb{\\xi}$ is the coded pigmentation value of a batch of paint, $Y$ comprises two reflectance measurements from a spectrometer and $\\hat{h}$ is quadratic in $\\xi.$"
  },
  {
    "qid": "statistic-posneg-725-0-1",
    "gold_answer": "TRUE. According to the theorem in the paper, for θ in R(c) ∩ Q, the condition requires that the sum of squared θ values for parameters in the first subset (1 ≤ i ≤ s) and the sum of squared θ values for parameters in the second subset (s+1 ≤ i ≤ n) must each be ≤ c^2. This is explicitly stated in the theorem's representation of R(c) ∩ Q.",
    "question": "Does the condition for the sharp bounds over X+ require that the sum of squared θ values for two separate subsets of parameters each be less than or equal to c^2?",
    "question_context": "Sharpening Scheffe Bounds for Linear Functions with Non-negative Arguments\n\nAn extension of Scheffe's (1959) confidence bounds gives sharp confidence bounds for a linear function of non-negative arguments. The length of these bounds is compared with that of the conservative Scheffe bounds which would usually be used in this situation. A table facilitating application of the sharpened bounds is provided."
  },
  {
    "qid": "statistic-posneg-1862-0-2",
    "gold_answer": "FALSE. While the Gaussian assumption simplifies the approximation (yielding $\\mathcal{N}(\\beta\\sigma^{2},\\sigma^{2})$), the paper explicitly notes that a similar argument applies to non-Gaussian cases. The key mechanism—preferential sampling distorting the empirical distribution via the $\\exp(\\beta S(u))$ weighting—is general, though the exact form of the bias may differ.",
    "question": "The marginal distribution $\\Pr\\{S(X)\\leqslant s\\}$ in the preferential sampling model depends on the empirical distribution $\\tilde{G}$ of $S(A)$. Is it accurate to claim that this result holds only for Gaussian processes and fails under non-Gaussian assumptions?",
    "question_context": "Preferential Sampling and Selection Bias in Geostatistical Models\n\nThe paper’s conditional Poisson model represents the possibility that the locations chosen for observation depend on the variable of interest, $\\pmb{\\mu}+S(\\mathbf{x})$ . Initially therefore we might expect that the conditional Poisson sampling intensity would be of the form $\\lambda(\\mathbf{x})=\\lambda_{0}\\exp[\\beta\\{\\mu+S(\\mathbf{x})\\}]$ for a parameter $\\beta$ and baseline intensity $\\lambda_{0}$ . The paper, of course, has this form, but with $\\alpha=\\beta\\mu+\\log(\\lambda_{0})$ , which is a more economical parameterization that recognizes that $\\lambda_{0}$ and $\\beta\\mu$ would not otherwise be separately estimable and corresponds to the commonsense consideration that the overall sample size, which will be largely governed by $\\alpha$ , is likely to have been influenced by factors other than characteristics of the target variable—economic and organizational factors, for example—which will not usually be of direct interest themselves. In fact, it might therefore be reasonable in inference to treat the sample size as fixed and to argue conditionally on its value, eliminating $\\alpha$ from consideration. I suspect that this is what the authors did. <PARAGRAPH_SEPARATOR> The effect of selection on the values of $S(\\mathbf{X})$ (and therefore of $Y$ ) obtained at the sampled locations may be seen by the following approximate argument. Conditionally on a realization $S(A)=\\left\\{S(u):u\\in A\\right\\}$ of the target process over the region $A$ and given the number of points $\\mathbf{X}$ that are generated in the resulting inhomogeneous Poisson process, the points correspond to an independent sample from $A$ with probability density function <PARAGRAPH_SEPARATOR> $$ \\lambda(x)\\Biggl/\\int_{A}\\lambda(u)\\mathrm{d}u=\\exp\\{\\beta S(x)\\}\\Biggl/\\int_{A}\\exp\\{\\beta S(u)\\}\\mathrm{d}u. $$ <PARAGRAPH_SEPARATOR> The (conditional) marginal distribution function of selected values of $S$ is therefore <PARAGRAPH_SEPARATOR> $$ \\operatorname*{Pr}\\{S(X)\\leqslant s\\}=\\frac{\\displaystyle{\\int_{\\{u:S(u)\\leqslant s\\}}\\exp\\{\\beta S(u)\\}\\mathrm{d}u}}{\\displaystyle{\\int_{A}\\exp\\{\\beta S(u)\\}\\mathrm{d}u}}=\\frac{\\displaystyle{\\int_{-\\infty}^{s}\\exp(\\beta w)\\mathrm{d}\\tilde{G}(w)}}{\\displaystyle{\\int_{-\\infty}^{\\infty}\\exp(\\beta w)\\mathrm{d}\\tilde{G}(w)}}, $$ <PARAGRAPH_SEPARATOR> where $\\tilde{G}$ is the empirical distribution function of the values of $S(A)$ . The probability in equation (13) comes from the random selection of sampling point $X$ and therefore the choice of the value $S(X)$ from $S(A)$ . However, under the assumption of a Gaussian model for $S$ we can expect that $\\tilde{G}$ will approximate to a normal ${\\mathcal{N}}(0,\\sigma^{2})$ distribution function (though there will be random variations around the exact Gaussian form, depending on the spatial dependence structure). Approximately therefore, from equation (13), the probability density function of the sampled values of $S$ has value at $s$ that is proportional to $\\exp(\\beta s-s^{2}/2\\sigma^{2})$ , from which it follows that the distribution is approximately normal $\\mathcal{N}(\\bar{\\beta}\\sigma^{2},\\sigma^{2})$ . The effect of selection is therefore to shift the mean $S$ by $\\beta\\sigma^{2}$ . Though the argument here is conditional on the realized field $S(A)$ , the approximation is the same across realizations and so can be expected to give a guide to selection bias generally for this model. A similar argument should work also in non-Gaussian cases."
  },
  {
    "qid": "statistic-posneg-432-1-0",
    "gold_answer": "TRUE. The Edgeworth expansion is correctly derived by expressing the cumulants of the studentized estimate in terms of the coefficients $\\alpha_{jk}$ and then constructing the polynomials $\\lambda_{1}(z)$ and $\\lambda_{2}(z)$ to account for the first and second-order corrections to the normal approximation. The expansion accounts for the skewness and kurtosis of the linear approximation through the coefficients $\\alpha_{32}$ and $\\alpha_{43}$, respectively, and the correction terms are appropriately scaled by powers of $n^{-1/2}$ to ensure the expansion's accuracy up to $o(n^{-1})$.",
    "question": "Is the Edgeworth expansion for the studentized estimate $\\pmb{\\hat{Z}}=(T-\\theta)/\\hat{S}$ correctly derived as $\\mathrm{pr}(\\hat{Z}\\leqslant z)=\\Phi(z)-\\phi(z)\\sum_{j=1}^{2}n^{-\\frac{1}{2}j}\\lambda_{j}(z)+o(n^{-1})$, where $\\lambda_{1}(z)=\\alpha_{11}+\\frac{1}{6}\\alpha_{32}(z^{2}-1)$ and $\\lambda_{2}(z)=\\frac{1}{2}(\\alpha_{11}^{2}+\\alpha_{22})z+\\frac{1}{24}(4\\alpha_{11}\\alpha_{32}+\\alpha_{43})(z^{3}-3z)+\\frac{1}{72}\\alpha_{32}^{2}(z^{5}-10z^{3}+9z)$?",
    "question_context": "Edgeworth Expansion for Studentized Estimates in Jackknife Confidence Limits\n\nIn this section we obtain an Edgeworth expansion for the distribution of $\\pmb{\\hat{Z}}=(T-\\theta)/\\hat{S}$ where $\\hat{s}$ is the infinitesimal jacknife standard error given by (1·4). We begin by obtaining a simple expansion for $\\dot{\\boldsymbol{z}}$ The required Edgeworth expansion then follows by a standard argument in $\\S\\mathbf{2\\cdot2}$ . The results are applied to confidence limits in $\\S2{\\cdot}3$ <PARAGRAPH_SEPARATOR> The studentized quantity $\\hat{z}$ can be expressed in the form <PARAGRAPH_SEPARATOR> $$ \\pmb{\\hat{Z}}=\\sqrt{n}w(\\pmb{\\hat{F}},\\pmb{F}), $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ w(G,F)=\\{t(G)-t(F)\\}/\\sqrt{v(G)} $$ <PARAGRAPH_SEPARATOR> and $v(.)$ is the variance of the influence function of $t(.)$ . Since the second argument, $\\pmb{F}$ of $\\pmb{w}$ in (2·l) is fixed throughout, we shall suppress it and write simply $\\bar{w}(\\hat{F})$ when no confusion can arise. Note that $w(F)\\equiv w(F,F)=0.$ Now the von Mises expansion of $w(G)$ about $w(F)$ has the general form <PARAGRAPH_SEPARATOR> $$ w(G)=w(F)+\\sum_{j=1}^{r}\\frac{1}{j!}\\int\\cdots\\int\\mathcal{D}^{j}w(x_{1},...,x_{j},F)\\prod_{k=1}^{j}d(G-F)(x_{k})+o(\\parallel G-F\\parallel^{r}), $$ <PARAGRAPH_SEPARATOR> where $\\mathcal{D}^{j}w$ is a jth functional derivative of $\\pmb{w}$ , and $\\parallel G-F\\parallel=\\operatorname*{sup}{\\bigl\\{}G(x)-F(x){\\bigr\\}}$ .We choose the unique versions $D^{j}w$ of derivatives $\\mathcal{D}^{j}w$ such that all marginal expectations of $D^{j}w(X_{1},...,X_{j},F)$ are zero. This implies the following simple definitions for the first three derivatives. First, write $F_{\\varepsilon,x}(y)=\\left(1-\\varepsilon\\right)F(y)+\\varepsilon\\Delta(y-x)$ with $\\pmb{\\triangle}$ as in (l·1), and define <PARAGRAPH_SEPARATOR> $$ D^{1}w(x,F)=[d w(F_{\\varepsilon,x})/d\\varepsilon]_{\\varepsilon=0}=I_{w}(x,F). $$ <PARAGRAPH_SEPARATOR> Then <PARAGRAPH_SEPARATOR> $$ D^{2}w(x_{1},x_{2},F)=D^{1}\\big\\{I_{w}(x_{1},F)\\big\\}(x_{2},F)+I_{w}(x_{2},F)=Q_{w}(x_{1},x_{2},F), $$ <PARAGRAPH_SEPARATOR> say, and <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{D^{3}w(x_{1},x_{2},x_{3},F)=D^{1}\\big\\{Q_{w}(x_{1},x_{2},F)\\big\\}(x_{3},F)+Q_{w}(x_{1},x_{3},F)+Q_{w}(x_{2},x_{3},F)}}\\ {{{}}}\\ {{=C_{w}(x_{1},x_{2},x_{3},F),}}\\end{array} $$ <PARAGRAPH_SEPARATOR> say. Next we substitute these derivatives in (2·3), set $G=\\hat{F},w(F)=0$ ,and note that $\\parallel\\hat{F}-F\\parallel=O_{p}(n^{-\\frac{1}{\\alpha}})$ . The result is the expansion <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{w(\\hat{F}_{n},F)\\sim n^{-1}\\Sigma I_{w}(X_{j},F)+\\frac{1}{2}n^{-2}\\Sigma\\Sigma Q_{w}(X_{j},X_{k},F)+\\frac{1}{6}n^{-3}\\Sigma\\Sigma\\Sigma_{w}(X_{j},X_{k},X_{l},F),}\\end{array} $$ <PARAGRAPH_SEPARATOR> with error $o_{p}(n^{-3/2})$ <PARAGRAPH_SEPARATOR> A more convenient form of expansion (2·5) is obtained by expressing the derivatives of $\\boldsymbol{w}$ in terms of derivatives of $t$ , using the definition (2·2) and recalling that <PARAGRAPH_SEPARATOR> $$ v(F)=\\int I_{t}^{2}(x,F)d F(x). $$ <PARAGRAPH_SEPARATOR> In particular, we find that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{c}{{I_{w}(x_{1},F)=v^{-\\frac{1}{4}}I_{t}(x_{1},F),}}\\ {{{}_{2},F)=v^{-\\frac{1}{4}}Q_{t}(x_{1},x_{2},F)-{\\textstyle\\frac{1}{2}}v^{-3/2}\\big\\{I_{t}(x_{1},F)I_{v}(x_{2},F)+I_{t}(x_{2},F)I_{v}(x_{1},F)\\big\\},}}\\ {{I_{v}(x,F)=I_{t}^{2}(x,F)-v+2E\\{I_{t}(Y,F)Q_{t}(x,Y,F)\\}.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> In this expression $Y$ has distribution $\\pmb{F}$ . Derivations of these and other results may be obtained from the first author. <PARAGRAPH_SEPARATOR> The $N(0,1)$ approximation for $\\hat{z}$ comes from applying a central limit theorem to the first term of expansion (2·5), since additional terms are $o_{p}(n^{-\\frac{1}{2}})$ and var $\\{I_{\\varkappa}(X,F)\\}=1$ by (2·6). The second and third terms on the right-hand side of (2?5) are used to make firstand second-order corrections to the normal approximation."
  },
  {
    "qid": "statistic-posneg-952-0-1",
    "gold_answer": "FALSE. While the theorem states that $a<-2$ implies quasi-inadmissibility, the proof and context suggest that this conclusion is derived under specific assumptions about $L(y)$ and the form of the prior. The behavior of $L(y)$ and the resulting shrinkage function $\\phi_{a,L}(w)$ are critical in determining quasi-admissibility or quasi-inadmissibility. Therefore, the implication is not unconditional but depends on the underlying model assumptions.",
    "question": "Does the condition $a<-2$ always imply quasi-inadmissibility of the generalized Bayes estimator, regardless of the behavior of $L(y)$?",
    "question_context": "Quasi-admissibility Boundary for Stein-type Shrinkage Estimators\n\nTheorem 5 (Quasi-inadmissibility). <PARAGRAPH_SEPARATOR> 1. Suppose $a=-2$ and $\\{\\ln(y)\\}^{b}/L(y)$ for $b>1$ is monotone non-increasing. The generalized Bayes estimator is quasiinadmissible. <PARAGRAPH_SEPARATOR> 2. Suppose $a<-2$ . The generalized Bayes estimator is quasi-inadmissible. <PARAGRAPH_SEPARATOR> Proof of Theorems 4 and 5. By following [11,12], the generalized Bayes estimator under the prior given by (20) is $\\delta_{\\phi}$ with <PARAGRAPH_SEPARATOR> $$ \\phi_{a,L}(w)=w\\frac{\\int_{0}^{1}\\lambda^{p/2+a+1}L(1/\\lambda)(1+w\\lambda)^{-(p+n)/2-1}\\mathrm{d}\\lambda}{\\int_{0}^{1}\\lambda^{p/2+a}L(1/\\lambda)(1+w\\lambda)^{-(p+n)/2-1}\\mathrm{d}\\lambda}. $$ <PARAGRAPH_SEPARATOR> By a change of variables $\\mathbf{\\omega}^{\\prime}t=w\\lambda$ ), we have <PARAGRAPH_SEPARATOR> $$ \\phi_{a,L}(w)=\\frac{\\int_{0}^{w}t^{p/2+a+1}L(w/t)(1+t)^{-(p+n)/2-1}\\mathrm{d}t}{\\int_{0}^{w}\\lambda^{p/2+a}L(w/t)(1+t)^{-(p+n)/2-1}\\mathrm{d}t}. $$ <PARAGRAPH_SEPARATOR> By Assumption L1 and the Lebesgue dominated convergence theorem, <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{w\\to\\infty}\\phi_{a,L}(w)=\\frac{\\int_{0}^{\\infty}t^{p/2+a+1}(1+t)^{-(p+n)/2-1}\\mathrm{d}t}{\\int_{0}^{\\infty}t^{p/2+a}(1+t)^{-(p+n)/2-1}\\mathrm{d}t}=\\frac{p/2+a+1}{n/2-a-1} $$ <PARAGRAPH_SEPARATOR> which is increasing in $a$ and is equal to $(p-2)/(n+2)$ when $a=-2$ . Hence, by Theorem 1 $,a>-2$ and $a<-2$ implies quasi-admissibility and quasi-inadmissibility, respectively. <PARAGRAPH_SEPARATOR> When $a=-2$ , take $L(1/\\lambda)=(\\ln1/\\lambda)^{b}$ for $b>0$ and consider <PARAGRAPH_SEPARATOR> $$ \\phi_{-2,b}(w)=w{\\frac{\\int_{0}^{1}\\lambda^{p/2-1}\\{\\ln(1/\\lambda)\\}^{b}(1+w\\lambda)^{-(p+n)/2-1}\\mathrm{d}\\lambda}{\\int_{0}^{1}\\lambda^{p/2-2}\\{\\ln(1/\\lambda)\\}^{b}(1+w\\lambda)^{-(p+n)/2-1}\\mathrm{d}\\lambda}}. $$ <PARAGRAPH_SEPARATOR> Then we have <PARAGRAPH_SEPARATOR> $$ \\operatorname*{lim}_{w\\to\\infty}(\\ln w)\\left\\{\\frac{p-2}{n+2}-\\phi_{-2,b}(w)\\right\\}=b\\frac{2(p+n)}{(n+2)^{2}}=b\\beta_{\\star} $$ <PARAGRAPH_SEPARATOR> where $\\beta_{\\star}$ is given by (11) as used in Theorem 1. See Appendix A.5 for the derivation of (23). Further the inequality <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{\\phi_{-2,b}(w)=\\frac{\\int_{0}^{1}\\lambda^{p/2-1}\\{\\ln(1/\\lambda)\\}^{b}(1+w\\lambda)^{-(p+n)/2-1}\\mathrm{d}\\lambda}{\\int_{0}^{1}\\lambda^{p/2-2}\\{\\ln(1/\\lambda)\\}^{b}(1+w\\lambda)^{-(p+n)/2-1}\\mathrm{d}\\lambda}}\\ {}&{=\\frac{\\int_{0}^{1}\\lambda\\frac{\\{\\ln(1/\\lambda)\\}^{b}}{L(1/\\lambda)}\\lambda^{p/2-2}L(1/\\lambda)(1+w\\lambda)^{-(p+n)/2-1}\\mathrm{d}\\lambda}{\\int_{0}^{1}\\frac{\\{\\ln(1/\\lambda)\\}^{b}}{L(1/\\lambda)}\\lambda^{p/2-2}L(1/\\lambda)(1+w\\lambda)^{-(p+n)/2-1}\\mathrm{d}\\lambda}}\\ {}&{\\leq(\\geq)\\frac{\\int_{0}^{1}\\lambda\\lambda^{p/2-2}L(1/\\lambda)(1+w\\lambda)^{-(p+n)/2-1}\\mathrm{d}\\lambda}{\\int_{0}^{1}\\lambda^{p/2-2}L(1/\\lambda)(1+w\\lambda)^{-(p+n)/2-1}\\mathrm{d}\\lambda}}\\ {}&{=\\phi_{-2,L}(w)}\\end{array} $$ <PARAGRAPH_SEPARATOR> follows for $b>0$ when $\\{\\ln(y)\\}^{b}/L(y)$ is monotone non-increasing (non-decreasing). From (21), (23), (24) and Theorem 1, the two theorems follow. □"
  },
  {
    "qid": "statistic-posneg-2129-0-0",
    "gold_answer": "TRUE. The prior distribution for the coefficients $\\alpha_{j}$ is indeed specified as a normal distribution with mean $\\underline{{\\alpha}}$ and variance $\\underline{{H}}_{\\alpha}^{-1}$. This is explicitly stated in the context: $$ \\alpha_{j}\\sim N(\\underline{{\\alpha}},\\underline{{H}}_{\\alpha}^{-1}) $$. The hyperparameters are further detailed as $\\underline{{\\boldsymbol{\\alpha}}}~=$ $\\begin{array}{r}{\\left[\\frac{\\sum\\Delta y_{t}}{T},0\\right]^{\\prime},\\underline{{H_{\\alpha}}}=\\left[\\begin{array}{l l}{0.01}&{0}\\\\ {0}&{0.01}\\end{array}\\right]$. This choice of prior is part of the Bayesian framework used in the smoothly mixing regressions model.",
    "question": "Is the prior distribution for the coefficients $\\alpha_{j}$ specified as a normal distribution with mean $\\underline{{\\alpha}}$ and variance $\\underline{{H}}_{\\alpha}^{-1}$?",
    "question_context": "Bayesian Smoothly Mixing Regressions Model for Inflation Dynamics\n\nOur econometric methods follow Geweke and Keane (2007) and complete details are given there. Here, we describe our prior hyperparameter choices along with our method of choosing models. We refer the reader to Geweke and Keane (2007) for a description of the MCMC algorithm used to carry out Bayesian inference in the smoothly mixing regressions model. <PARAGRAPH_SEPARATOR> Our modelling framework is given in (2) and (3) and the eight choices for index variables are given in Section 4. The parameters are the error variance $(\\sigma_{j}^{2})$ and the coefficients $(\\alpha_{j})$ in state $j$ for $j=1,\\ldots,m$ and the coefficients $(T)$ of the restricted multinomial probit specification used to model the state probabilities. <PARAGRAPH_SEPARATOR> We use the following priors: <PARAGRAPH_SEPARATOR> $$ \\alpha_{j}\\sim N(\\underline{{\\alpha}},\\underline{{H}}_{\\alpha}^{-1}) $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\underline{{s}}^{2}\\sigma_{j}^{-2}|(\\underline{{s}}^{2},\\underline{{\\nu}})\\sim\\chi^{2}(\\underline{{\\nu}}). $$ <PARAGRAPH_SEPARATOR> As in Geweke and Keane (2007), $\\varGamma$ in the latent multinomial probit model is identified by restricting $\\iota_{m}^{\\prime}{\\cal{T}}\\:=\\:0^{\\prime}$ . Specifically, define an $m\\times m$ orthonormal matrix $P=[p_{1}~P_{2}]$ in which $p_{1}=\\iota_{m}m^{-\\frac{1}{2}}$ . Then define ${\\cal{T}}^{*}={\\cal{P}}_{2}^{\\prime}{\\cal{T}}$ . Letting $\\gamma_{j}^{*}$ be the jth column of the matrix $\\b{\\Gamma^{*}}$ , we have a prior: <PARAGRAPH_SEPARATOR> $$ \\gamma_{j}^{*}\\sim N(0,\\underline{{H}}_{\\gamma^{*}}^{-1}). $$ <PARAGRAPH_SEPARATOR> We have experimented with different choices for the prior hyperparameters $\\underline{{\\alpha}},\\underline{{H}}_{\\alpha},\\underline{{s}}^{2},\\underline{{\\nu}}$ and $\\underline{{H}}_{\\gamma^{*}}$ , including training sample priors. The results presented in the body of the paper involve the relatively noninformative choices of $\\underline{{\\boldsymbol{\\alpha}}}~=$ $\\begin{array}{r}{\\left[\\frac{\\sum\\Delta y_{t}}{T},0\\right]^{\\prime},\\underline{{H_{\\alpha}}}=\\left[\\begin{array}{l l}{0.01}&{0}\\\\ {0}&{0.01}\\end{array}\\right],\\underline{{s}}^{2}=0.01,\\underline{{\\nu}}=2\\mathrm{and}\\underline{{H}}_{\\gamma^{*}}=0.01I.}\\end{array}$ We find that the posterior results of pass through coefficients are robust to prior choice. The following empirical appendix presents some evidence to support this claim. <PARAGRAPH_SEPARATOR> We use the modified cross-validated log scoring rule of Geweke and Keane (2007) to select m and the index variable. This involves randomly selecting a sample of size $T_{1}$ to be used for estimation, leaving a remaining $T-T_{1}$ for cross-validation. Then the metric for comparing models is: <PARAGRAPH_SEPARATOR> $$ \\sum_{t=T_{1}+1}^{T}\\log\\left[p\\left(\\Delta y_{t}|Y_{T_{1}},\\upsilon_{t},z_{t}\\right)\\right] $$ <PARAGRAPH_SEPARATOR> where $Y_{T_{1}}$ denotes the $T_{1}$ observations on the dependent variable used for estimation. The notation in the previous equation denotes the predictive density evaluated at the actual realization $\\Delta{{y}_{t}}$ . This metric can be estimated in our MCMC algorithm since, if we denote all model parameters by $\\lambda$ , we have $p(\\Delta y_{t}|Y_{T_{1}},v_{t},z_{t},\\lambda)$ being Normal and we can simply average these Normal densities over our MCMC draws of $\\lambda$ . <PARAGRAPH_SEPARATOR> The empirical results in the paper choose $T_{1}$ such that $75\\%$ of the observations are used for estimation and $25\\%$ for crossvalidation. This $25\\%$ of the observations is randomly chosen one time and the same set of observations is withheld for every model. Results withholding $50\\%$ of the observations for cross-validation are very similar. The final empirical results for each model (such as those presented in Figs. 1–12) are based on the full sample."
  },
  {
    "qid": "statistic-posneg-2004-1-2",
    "gold_answer": "TRUE. The paper states that the $L_{2}$ penalty corresponds to the group LASSO as discussed in Yuan and Lin (2006) and Wang and Leng (2008) for conditional mean regression. This confirms that the $L_{2}$ penalty in the $\\mathsf{G A L}_{\\gamma}$ method is indeed equivalent to the group LASSO approach.",
    "question": "Is the $L_{2}$ penalty in the $\\mathsf{G A L}_{\\gamma}$ method equivalent to the group LASSO as discussed in Yuan and Lin (2006)?",
    "question_context": "Variable Selection in Quantile Varying Coefficient Models via Group Adaptive LASSO\n\nFor simplicity, we omit $\\tau$ from $\\beta(t,\\tau)$ and $\\epsilon_{i j}(\\tau)$ in model (1) wherever clear from the context, but it is helpful to keep in mind that those quantities are $\\tau$ -specific. <PARAGRAPH_SEPARATOR> Let $\\pmb{\\pi}(t)=\\left(B_{1}(t),\\ldots,B_{k_{n}+m+1}(t)\\right)^{\\prime}$ be a set of normalized $B\\cdot$ -spline basis functions of order $m+1$ , with $k_{n}$ quasi-uniform internal knots. Let $q_{n}=k_{n}+m+1.$ We propose to approximate each $\\beta_{k}(t)$ by a linear combination of $\\pmb{\\pi}(t)\\colon\\beta_{k}(t)\\approx$ $\\begin{array}{r}{\\sum_{l=1}^{q_{n}}\\theta_{k,l}B_{l}(t)=\\pmb{\\pi}(\\bar{t})^{\\prime}\\pmb{\\theta}_{k}}\\end{array}$ where $\\pmb{\\theta}_{k}=(\\theta_{k,1},\\dots,\\theta_{k,q_{n}})^{\\prime}$ is the spline coefficient vector; see Schumaker (1981, Chapter 4) for details on the construction of $B$ -spline basis functions. In what follows, we often suppress the subscript of $q_{n}$ for convenience. For simplicity, define $\\Pi\\left(t,\\pmb{x}\\right)=\\left(x^{(0)}\\pmb{\\pi}(t)^{\\prime},\\dots,x^{(p)}\\pmb{\\pi}(t)^{\\prime}\\right)^{\\prime}$ , $\\pi_{i j}=\\pi(t_{i j})$ , and $\\Pi_{i j}=\\Pi(t_{i j},\\pmb{x}_{i j})$ , $i=1,\\ldots,n$ , $j=1,\\ldots,J_{i}$ . Model (1) can be approximated by <PARAGRAPH_SEPARATOR> $$ y_{i j}\\approx\\Pi_{i j}^{\\prime}\\pmb{\\theta}+\\epsilon_{i j}, $$ <PARAGRAPH_SEPARATOR> where $\\pmb\\theta=(\\pmb\\theta_{0}^{\\prime},\\pmb\\theta_{1}^{\\prime},\\dots,\\pmb\\theta_{p}^{\\prime})^{\\prime}$ . <PARAGRAPH_SEPARATOR> Our interest lies in selecting the covariates that are relevant to the $\\tau$ th quantile of $y$ . We will exclude a covariate, e.g., $x^{(k)}$ , from model (2) if and only if every component of $\\pmb{\\theta}_{k}$ equals 0. Therefore, we treat $\\pmb{\\theta}_{k}$ as a group. Instead of selecting components with nonzero $\\theta_{k,l},$ we need to select groups with nonzero $\\pmb{\\theta}_{k}$ . We propose a penalization method by applying adaptive LASSO penalty to the $L_{\\gamma}$ norm of the coefficient group. Throughout the paper, we refer to this method as $\\mathsf{G A L}_{\\gamma}$ meaning that group adaptive LASSO penalty is applied to the $L_{\\gamma}$ norm of each coefficient group. We define the proposed estimator $\\hat{\\pmb\\theta}$ as the minimizer of the following penalized objective function <PARAGRAPH_SEPARATOR> $$ l(\\pmb\\theta)=\\sum_{i=1}^{n}\\sum_{j=1}^{J_{i}}\\rho_{\\tau}(y_{i j}-\\Pi_{i j}^{\\prime}\\pmb\\theta)+\\lambda_{n}\\sum_{k=0}^{p}\\omega_{k,\\gamma}\\lVert\\pmb\\theta_{k}\\rVert_{\\gamma}, $$ <PARAGRAPH_SEPARATOR> where $\\rho_{\\tau}(h)=h\\left\\{\\tau-I_{(h<0)}\\right\\}$ is the quantile check loss function (Koenker and Bassett, 1978), $\\lambda_{n}$ is a nonnegative regularization parameter that determines the sparsity of the solution, $\\begin{array}{r}{\\|\\pmb{\\theta}_{k}\\|_{\\gamma}=\\left(\\sum_{l=1}^{q}|\\theta_{k,l}|^{\\gamma}\\right)^{1/\\gamma}}\\end{array}$ is the $L_{\\gamma}$ -norm of $\\pmb{\\theta}_{k}$ with $\\gamma\\geq1$ , and $\\omega_{k,\\gamma}$ , $k=0,\\ldots,p$ , are the penalty weights of the coefficient groups. The specifi c choices of $\\omega_{k,\\gamma}$ are described in the computational algorithms in Section 2.2. For each $k=0,1,\\ldots,p$ the coefficient function $\\beta_{k}(t)$ can be estimated by $\\hat{\\beta}_{k}(t)=\\pi(t)^{\\prime}\\hat{\\pmb\\theta}_{k}$ . <PARAGRAPH_SEPARATOR> In the proposed procedure, every $\\beta_{k}(t)$ is characterized by a spline coefficient vector $\\pmb{\\theta}_{k}$ , which can be treated as a group. In this paper, we penalize the $L_{\\gamma}$ norm of the within-group coefficients and consider $\\gamma=1$ and 2 for empirical investigations. The $L_{2}$ penalty corresponds to the group LASSO as discussed in Yuan and Lin (2006) and Wang and Leng (2008) for conditional mean regression. When $\\gamma=1$ , each basis coefficient receives a LASSO penalty. However, different from the adaptive LASSO penalization in Zou (2006), all components within the same group $k$ receive the same group-specific penalty $\\lambda_{n}\\omega_{k,1}$ . For quantile regression, the method based on the $L_{1}$ norm penalty is computationally more convenient, because linear programming can be directly applied to solve the minimization problem."
  },
  {
    "qid": "statistic-posneg-2114-0-0",
    "gold_answer": "TRUE. The condition ensures that the vector $\\mathbf{h}$ is constructed such that its norm is at least $1-\\delta$, where $\\delta$ is a very small positive number (e.g., $10^{-15}$). The term $h_{m}$ is then adjusted to make the norm exactly 1, i.e., $\\left\\|\\mathbf{h}\\right\\|=1$. This normalization is crucial for the random projection method to work correctly, as it ensures the projected data maintains certain statistical properties under the null hypothesis.",
    "question": "Is the condition $m=1+\\operatorname*{min}\\{\\operatorname*{min}\\{t:\\|(h_{0},\\ldots,h_{t})^{T}\\|\\ge1-\\delta\\},n-1\\}$ used to ensure that the vector $\\mathbf{h}$ has a norm close to 1?",
    "question_context": "Random Projection-Based Test of Gaussianity in Time Series Data\n\nOn the other hand, in the case of time series data, only the goodness of fit test for Gaussianity of a fixed finite marginal has been studied (Epps, 1987; Lobato and Velasco, 2004; Moulines and Choukri, 1996, for example). Notice that the regularity assumption has recently been weakened in Ghosh (2013) in order to work with long range dependence processes. Our work appears to be at the crossroad between all these works; that is, those on time series, on i.i.d. data in infinite dimension or on finite dimension. As a matter of fact, the random projection trick allows only a one dimensional marginal to be considered. Nevertheless, as the sample is not i.i.d., tools similar to those used in the multidimensional case appear. For example, we have to whiten empirically the sample in order to obtain an asymptotic $\\chi^{2}$ distribution (under the null hypothesis). Furthermore, the procedure based on the empirical characteristic function is a method related to the one developed in Csorgo (1986) although the sampled frequencies are, in our work, chosen randomly.<PARAGRAPH_SEPARATOR># 3.2. The test in practice<PARAGRAPH_SEPARATOR>Let $X_{0},\\ldots,X_{n}$ be the available measurements. To compute $\\mathbf{h}$ , let $\\delta>0$ be a fixed number (equal to $10^{-15}$ in the simulations that we carry out in Sections 4 and 5), and take $\\mathbf{h}=(h_{0},\\ldots,h_{m})^{T}$ with<PARAGRAPH_SEPARATOR>$$m=1+\\operatorname*{min}\\{\\operatorname*{min}\\{t:\\|(h_{0},\\ldots,h_{t})^{T}\\|\\ge1-\\delta\\},n-1\\},$$<PARAGRAPH_SEPARATOR>where $h_{0},\\ldots,h_{m-1}$ are drawn as follows and $h_{m}$ is such that $\\left\\|\\mathbf{h}\\right\\|=1$ . Then, define<PARAGRAPH_SEPARATOR>$$Y_{t}=\\sum_{i=0}^{\\operatorname*{min}(m,t)}h_{i}X_{t-i}a_{i},\\quad t=0,\\dots,n.$$<PARAGRAPH_SEPARATOR>To draw $h_{0},\\ldots,h_{m-1}$ , let us fix $\\alpha_{1}$ , $\\alpha_{2}>0$ . Then, we choose $(\\beta_{n})_{n\\in\\mathbb{N}}$ independent and identically distributed with beta distribution of parameters $\\alpha_{1}$ and $\\alpha_{2}$ . Further, we consider the probability distribution which selects a random point in $l^{2}$ according to the following iterative procedure:<PARAGRAPH_SEPARATOR>• $l_{0}=\\beta_{0}\\in[0,1]$ .   • Given $n\\geq1,l_{n}\\in[0,1-\\sum_{i=0}^{n-1}l_{i}]$ equal to $\\beta_{n}(1-\\textstyle\\sum_{i=0}^{n-1}l_{i})$ .<PARAGRAPH_SEPARATOR>Let us define $H_{n}=(l_{n}/a_{n})^{1/2}$ for $n\\in\\mathbb{N}$ with $a_{0}=1$ and $a_{n}=n^{-2}$ , $n\\geq1$ , and take $\\mathbf{H}=(H_{n})_{n\\in\\mathbb{N}}$ . Thus, ${\\bf h}=(h_{i})_{i\\in\\mathbb{N}}$ is a fixed realization of the random element $\\mathbf{H}$."
  },
  {
    "qid": "statistic-posneg-753-1-1",
    "gold_answer": "FALSE. While the Cressie-Read statistic (λ=2/3) performs well in some scenarios, the results in the tables show that no single statistic uniformly dominates across all conditions. For example, in the 3-category truncated Poisson data with n=50, the Cressie-Read statistic selects the correct model 8551 times, while the X² statistic (λ=1) selects it 8500 times—a negligible difference. The performance varies by dataset and sample size, with different statistics occasionally achieving the highest proportions (e.g., λ=0.5 in some cases). The boldfaced values in the tables indicate the best performer for each specific condition, not a universal winner.",
    "question": "The paper claims that the Cressie-Read statistic (λ=2/3) consistently outperforms all other divergence statistics in selecting the correct model across all sample sizes and data structures. True or False?",
    "question_context": "Log-linear Model Selection Using Phi-Divergence Statistics\n\nThe paper examines model selection in log-linear models using various phi-divergence statistics. The saturated log-linear model for an 8-category contingency table is specified with parameters for main effects (A, C, M), two-way interactions (AC, AM, CM), and a three-way interaction (ACM). The probability of each cell is given by a multinomial logistic form. The study compares the performance of 11 different divergence statistics (e.g., G², X², C-R) in selecting the correct model across different sample sizes and data structures (genetics of plants, truncated Poisson variates). Results are presented as proportions of correct model selections out of 10,000 replications."
  },
  {
    "qid": "statistic-posneg-1368-1-1",
    "gold_answer": "TRUE. The operation described is a linear combination of elements from the vector X, specifically combining X(J) and X(M+1-J) with a coefficient P. This is a standard linear algebra operation used in various computational procedures.",
    "question": "Does the operation Y(J) ← X(J) - P*X(M+1-J) involve a linear combination of elements from the vector X?",
    "question_context": "Cramér-Wold Factorization and Recursive Solutions in ARMA Processes\n\nThe context discusses auxiliary algorithms for solving specific equations related to polynomial functions in the context of ARMA processes. The key equations involve polynomial functions G(x), H(x), and W(x), and their relationships. The recursive procedure by Laurie (1980) is highlighted for its efficiency, requiring O(n) divisions compared to O(n²) in Wilson (1979). The equations include transformations and operations such as Y(J) ← X(J) - P*X(M+1-J) and H(x) = G(x) + W(x)W(x⁻¹)."
  },
  {
    "qid": "statistic-posneg-1413-0-1",
    "gold_answer": "FALSE. Explanation: The correlation formula holds for any increasing function $g$ only under specific conditions derived from the system of equations. The proof shows that unless $r = 0$ or $F_X = F_Y$, the conditions cannot be satisfied for arbitrary increasing functions $g$. Thus, the invariant correlation property is restrictive and not generally valid for all increasing functions.",
    "question": "Does the correlation formula $\\operatorname{Corr}(g(X),g(Y)) = r$ hold for any increasing function $g$ if $(X,Y) \\in \\mathrm{IC}_{r}^{\\uparrow}$?",
    "question_context": "Invariant Correlation under Marginal Transformations\n\nAssume $\\operatorname{Supp}(X)=\\{x_{1},x_{2}\\}$ for some $x_{1}<x_{2}$ and $\\operatorname{Supp}(Y)=\\{y_{1},y_{2},y_{3}\\}$ for some $y_{1}<y_{2}<y_{3}$ . Let $P=(p_{i j})_{2\\times3}$ be the probability matrix, $\\mathbf{p}=(p_{1},p_{2})^{\\top}$ and $\\mathbf{q}=(q_{1},q_{2},q_{3})^{\\top}$ be the marginal distributions of $X$ and $Y$ , respectively, with $p_{i}=\\mathbb{P}(X=x_{i})$ and $q_{j}=\\mathbb{P}(Y=y_{j})$ for $i\\in\\{1,2\\}$ and $j\\in\\{1,2,3\\}$ and $S=(s_{i j})_{2\\times3}=P-{\\bf p}^{\\top}{\\bf q}$ . We have $p_{i}>0$ for $i=1,2$ and $q_{j}>0$ for $j\\in\\{1,2,3\\}$ . <PARAGRAPH_SEPARATOR> Since a linear transform of $g$ does not change $\\operatorname{Corr}(g(X),g(Y))$ , we can fix $g(x_{1})=0$ and $g(x_{2})=1$ . Assume $g(y_{1})=z_{1}$ , $g(y_{2})=z_{2}$ and $g(y_{3})=z_{3}$ with $z_{1}\\leqslant z_{2}\\leqslant z_{3}$ . As $(X,Y)\\in\\mathrm{IC}_{r}^{\\uparrow}$ , we have <PARAGRAPH_SEPARATOR> $$ \\operatorname{Corr}(g(X),g(Y))={\\frac{z_{1}s_{21}+z_{2}s_{22}+z_{3}s_{23}}{{\\sqrt{p_{2}-p_{2}^{2}}}{\\sqrt{z_{1}^{2}q_{1}+z_{2}^{2}q_{2}+z_{3}^{2}q_{3}-(z_{1}q_{1}+z_{2}q_{2}+z_{3}q_{3})^{2}}}}}=r. $$ <PARAGRAPH_SEPARATOR> By matching the coefficients in front of $z_{1}^{2},z_{2}^{2},z_{3}^{2},z_{1}z_{2},z_{1}z_{3}$ and $z_{2}z_{3}$ terms, we get the system <PARAGRAPH_SEPARATOR> $$ s_{21}^{2}=r^{2}(p_{2}-p_{2}^{2})(q_{1}-q_{1}^{2}), $$ <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{{s_{22}^{2}=r^{2}(p_{2}-p_{2}^{2})(q_{2}-q_{2}^{2}),}}\\ {{}}\\ {{s_{32}^{2}=r^{2}(p_{2}-p_{2}^{2})(q_{3}-q_{3}^{2}),}}\\ {{}}\\ {{s_{21}s_{22}=r^{2}(p_{2}-p_{2}^{2})q_{1}q_{2},}}\\ {{}}\\ {{s_{21}s_{23}=r^{2}(p_{2}-p_{2}^{2})q_{1}q_{3},}}\\ {{}}\\ {{s_{22}s_{23}=r^{2}(p_{2}-p_{2}^{2})q_{2}q_{3}.}}\\end{array} $$ <PARAGRAPH_SEPARATOR> As $g$ can be any increasing function, at most one of $z_{1},z_{2},z_{3}$ can be fixed as 0. If $z_{3}=0$ , we have (B.11a), (B.11b) and (B.11d) hold simultaneously. Thus, $r\\neq0$ implies $q_{3}=0$ . If $z_{2}=0$ , we have (B.11a), (B.11c) and (B.11e) hold simultaneously. Thus, $r\\neq0$ implies $q_{2}=0$ . If $z_{1}=0$ , we have (B.11b), (B.11c) and (B.11f) hold simultaneously. Thus, $r\\neq0$ implies $q_{1}=0$ . If none of $z_{1},z_{2},z_{3}$ is 0, then we have $q_{1}=q_{2}=q_{3}=0$ . As $q_{1},q_{2},q_{3}>0$ , we have $r=0$ . □ <PARAGRAPH_SEPARATOR> Lemma 8. Suppose both $X$ and $Y$ are tri-atomic random variables with the same support. If $(X,Y)\\in\\mathrm{IC}_{r}^{\\uparrow}$ for some $r\\in[-1,1],$ then $r=0$ or $F_{X}=F_{Y}$ . <PARAGRAPH_SEPARATOR> Proof of Lemma 8. As $p_{i}=\\mathbb{P}(X=x_{i})>0$ and $q_{i}=\\mathbb{P}(Y=y_{j})>0$ for all $i,j\\in\\{1,2,3\\}$ , we can use the similar argument in the proof of Lemma 4 to show that if $(X,Y)\\in\\mathrm{IC}_{r}^{\\uparrow}$ with $r\\neq0$ , then $F_{X}=F_{Y}$ . □"
  },
  {
    "qid": "statistic-posneg-1654-0-0",
    "gold_answer": "TRUE. The normalization ${\\cal Z}_{N}(t)=\\{X_{N}(t)-a N\\}/\\sqrt{N}$ is indeed a standard condition for weak convergence of sequences of birth and death processes to an Ornstein-Uhlenbeck process, as discussed in the context. This normalization ensures that the centered and scaled process converges to a Gaussian process with appropriate mean and variance, which is characteristic of the Ornstein-Uhlenbeck process.",
    "question": "Is the condition for weak convergence of the sequence of birth and death processes $\\{X_{N}(t)\\}$ to an Ornstein-Uhlenbeck process given by the normalization ${\\cal Z}_{N}(t)=\\{X_{N}(t)-a N\\}/\\sqrt{N}$?",
    "question_context": "Central Limit Analogues for Markov Population Processes\n\nIN this paper the main discussion is concerned with obtaining asymptotic results for sequences of birth and death processes which are similar to the central limit theorem for sequences of random variables. The motivation is the need to obtain useful approximations to the distributions of sample paths of processes which arise as models for population growth, but for which Kolmogorov differential equations are intractable. <PARAGRAPH_SEPARATOR> In the first section, univariate processes are considered, and conditions are given for the weak convergence of <PARAGRAPH_SEPARATOR> $$ {\\cal Z}_{N}(t)=\\{X_{N}(t)-a N\\}/\\sqrt{N}, $$ <PARAGRAPH_SEPARATOR> where $\\{X_{N}(t),N=1,2,\\ldots\\}$ is a sequence of birth and death processes, to those of an Ornstein-Uhlenbeck process as $N{\\rightarrow}{\\infty}$ . Some examples are given for purpose of illustration."
  },
  {
    "qid": "statistic-posneg-1499-0-1",
    "gold_answer": "FALSE. Remark 2 clarifies that for the SUN sub-family (a specific case of SUE distributions), the condition $\\pmb{\\tau}_{i} = \\mathbf{0}$ is not necessary. The reduction to a simpler form can still occur even if $\\pmb{\\tau}_{i} \\neq \\pmb{0}$, due to the specific properties of Gaussian cumulative distribution functions. This is a special property of the SUN sub-family and does not generalize to all SUE distributions.",
    "question": "Does the condition $\\pmb{\\tau}_{i} = \\mathbf{0}$ always need to hold for the SUE distribution to reduce to a simpler form when $\\pmb{\\mathscr{a}}_{i} = \\mathbf{0}$ and $\\bar{\\boldsymbol{{\\cal T}}}_{i j} = \\bar{\\boldsymbol{{\\cal T}}}_{j i}^{\\top} = 0$?",
    "question_context": "Conjugacy Properties of Multivariate Unified Skew-Elliptical Distributions\n\nProof. Let $\\mathbf{z}\\sim S\\mathrm{UE}_{m,q}(\\xi,\\Omega,A,\\tau,I,g^{(m+q)})$ . Then, according to the selection representation in (2), $\\mathbf{z}\\stackrel{d}{=}(\\bar{\\mathbf{z}}\\mid\\bar{\\mathbf{z}}_{0}>\\mathbf{0})$ , and its density function is given by (3). Notice that, since $\\gamma$ is a diagonal matrix with non-negative entries, then the numerator and denominator in (3) can be alternatively re-written as $\\mathbb{P}(-\\bar{\\mathbf{z}}_{0}\\leq\\mathbf{0}\\mid\\bar{\\mathbf{z}}=\\mathbf{z})=\\mathbb{P}(-\\gamma^{-1}\\bar{\\mathbf{z}}_{0}\\leq\\mathbf{0}\\mid\\bar{\\mathbf{z}}=\\mathbf{z})$ and $\\mathbb{P}(-\\bar{\\mathbf{z}}_{0}\\le\\mathbf{0})=\\mathbb{P}(-\\gamma^{-1}\\bar{\\mathbf{z}}_{0}\\le\\mathbf{0})$ . Therefore, the proof follows directly from (2)–(3) and by the closure under linear combinations and conditioning of elliptical distributions. □\n\nLemma 5 concludes this section by presenting particular cases of SUE distributions obtained under specific constraints on the associated parameters. These results are useful for detecting redundant latent dimensions and identifying interesting examples of constrained representations yielding specific models of interest under the conjugacy results derived in Section 3.\n\nLemma 5. Let $\\mathbf{z}\\sim\\mathbf{SUE}_{m,q}(\\xi,\\Omega,A,\\tau,\\bar{I},g^{(m+q)})$ with parameters $\\mathbf{4},\\pmb{\\tau},$ and $\\bar{\\pmb{T}}$ partitioned as\n\n$$\n\\mathbf{\\Lambda}\\mathbf{A}=\\left[\\mathbf{A}_{1}\\quad\\mathbf{\\Delta}\\mathbf{A}_{2}\\right],\\quad\\pmb{\\tau}=\\left[\\pmb{\\tau}_{1}\\right],\\quad\\bar{\\boldsymbol{T}}=\\left[\\bar{\\boldsymbol{T}}_{11}\\quad\\bar{\\boldsymbol{T}}_{12}\\right],\n$$\n\nwhere $\\pmb{\\mathscr{a}}_{i}\\in\\mathbb{R}^{m\\times q_{i}}$ , $\\pmb{\\tau}_{i}\\in\\mathbb{R}^{q_{i}}$ , and $\\bar{\\boldsymbol{r}}_{i j}\\in\\mathbb{R}^{q_{i}\\times q_{j}}$ , for every $i,j\\in\\{1,2\\}$ , such that $q_{1}+q_{2}=q$ . Then, (i) if $\\mathbf{A}=\\mathbf{0}$ and ${\\pmb\\tau}={\\bf0},$ it follows that $\\mathbf{z}\\sim\\mathrm{EC}_{m}(\\xi,\\Omega,g^{(m)})$ . Additionally, (ii) if ${\\pmb{\\varDelta}}_{i}={\\bf0},{\\pmb{\\tau}}_{i}={\\bf0},\\bar{\\pmb{T}}_{i j}={\\bf0},$ for 𝑖 and $j$ fixed, with $j\\neq i,$ then $\\mathbf{z}\\sim{\\cal S}\\mathrm{UE}_{m,q_{j}}(\\xi,\\Omega,A_{j},\\tau_{j},\\bar{T}_{j j},g^{(m+q_{j})})$ . Finally, (iii) $F_{m+q}\\{[({\\mathbf{z}}-\\pmb{\\xi})^{\\top},\\mathbf{0}^{\\top}]^{\\top}$ $;\\mathrm{diag}(\\pmb{\\mathscr{Q}},\\bar{\\cal T}),g^{(m+q)}\\}=F_{m}(\\mathbf{z}-\\pmb{\\xi};\\pmb{\\mathscr{Q}},g^{(m)})\\cdot F_{q}(\\mathbf{0};\\bar{\\cal T},g^{(q)})_{s}$ where diag $(\\pmb{\\mathcal{Q}},\\bar{\\pmb{T}})$ denotes a block-diagonal matrix with blocks $\\pmb{\\Omega}$ and $\\bar{\\boldsymbol{{\\cal T}}}$ , respectively."
  },
  {
    "qid": "statistic-posneg-1475-0-1",
    "gold_answer": "FALSE. The Koksma–Hlawka inequality guarantees the error bound V(g)D*(P_{n_gen}) only if the function g has finite bounded variation in the sense of Hardy and Krause. If g does not meet this condition, the inequality does not apply, and the error may not be bounded as described.",
    "question": "Does the Koksma–Hlawka inequality guarantee that the error of the GMMN QMC estimator is always bounded by V(g)D*(P_{n_gen}) for any function g?",
    "question_context": "Quasi-Random Sampling and Discrepancy Analysis in GMMN QMC Estimators\n\nThe paper discusses the discrepancy of point sets on the unit hypercube [0,1]^p, defining the discrepancy function D(I; P_{n_gen}) as the difference between the relative frequency of points in an interval I and the probability of a p-dimensional standard uniform random vector falling in I. The star discrepancy D*(P_{n_gen}) is defined as the supremum of |D(I; P_{n_gen})| over all intervals I in A. The paper also analyzes the GMMN QMC estimator, deriving conditions under which the error in approximating E(Ψ(Y)) is small, using the Koksma–Hlawka inequality which bounds the error by V(g)D*(P_{n_gen})."
  },
  {
    "qid": "statistic-posneg-1236-0-1",
    "gold_answer": "FALSE. The basis matrix B does not need to be full column rank. The penalty matrix P adds a ridge penalty to the system of equations, ensuring an explicit solution even when B is not full rank, as mentioned in the context.",
    "question": "The mixed model representation of P-splines requires the basis matrix B to be full column rank for the transformation to fixed and random effects matrices X and Z to be valid. True or False?",
    "question_context": "P-spline Smoothing for Spatial Data with CAR Structure\n\nThe paper discusses the use of P-splines for smoothing spatial data, incorporating conditionally autoregressive (CAR) structures to model both global spatial trends and local regional effects. The models are formulated within a mixed model framework, allowing for flexible smoothing and handling of overdispersion in count data."
  },
  {
    "qid": "statistic-posneg-322-0-2",
    "gold_answer": "TRUE. The multivariate L^1-expectile e_alpha(X, y) is defined as the minimizer of the expected loss function E[alpha * ||(X - x)_+||_1^2 + (1 - alpha) * ||(X - x)_-||_1^2 | Y = y], where alpha is the risk level. This balances the trade-off between positive and negative deviations from the vector x, conditional on Y = y.",
    "question": "Is the multivariate L^1-expectile e_alpha(X, y) defined as the minimizer of the expected loss function involving both positive and negative deviations from a vector x, weighted by the risk level alpha?",
    "question_context": "Estimation of Extreme Multivariate Expectiles with Functional Covariates\n\nThe paper discusses the estimation of multivariate expectiles with functional covariates, focusing on conditional marginal distributions, quantile functions, and tail dependence functions. It introduces a framework for estimating extreme multivariate expectiles using conditional copulas and tail dependence functions, with applications in risk analysis and capital allocation."
  },
  {
    "qid": "statistic-posneg-692-0-2",
    "gold_answer": "FALSE. While the optimal bandwidth h = O(n^(-1/5)) gives a coverage error of O(n^(-2/5)), this is not universally true for all α in -4/5 < α < 4/45. The coverage error's order depends on α: it is O(n^(-2/5 + A)) where A varies with α (e.g., A = 9α/2 for 0 < α < 4/45). Only for α = 0 (optimal bandwidth) does the error strictly remain O(n^(-2/5)).",
    "question": "Is the asymptotic order of the coverage error for the theoretical interval I always O(n^(-2/5)) when using the optimal bandwidth h = O(n^(-1/5))?",
    "question_context": "Edgeworth Expansions for Confidence Intervals in Nonparametric Binary Regression\n\nAs it has been indicated above, confidence intervals for a binary regression function will be constructed from a statistic with asymptotic normal distribution, but the parameters depend on unknown theoretical functions which have to be estimated in practice. Then we will talk about two kinds of intervals that will be denominated theoretical normal and plug-in normal. The first one will use the limit normal distribution with theoretical parameters, and in the second one unknown functions are estimated in a nonparametric way. We will get Edgeworth expansions for the corresponding statistics which confirm the rates of convergence for the normal limit and its plug-in approach obtained in Cao-Abad [3] by means of Berry-Esseen bounds."
  },
  {
    "qid": "statistic-posneg-2121-10-0",
    "gold_answer": "FALSE. The empirical variogram based on $Y_{i}$ observed at $x_{i}$ estimates $\\gamma_{m}(u)$, but the model assumes ${\\boldsymbol{\\gamma}}(u)=\\gamma_{m}(u)$ only when $\\tau^{2}=0$. This is a specific condition and not a general property of the model. The statement oversimplifies the relationship between the empirical variogram and the model's assumptions.",
    "question": "Is the statement 'The empirical variogram based on $Y_{i}$ observed at $x_{i}$ estimates $\\gamma_{m}(u)$' consistent with the model's assumption that ${\\boldsymbol{\\gamma}}(u)=\\gamma_{m}(u)$ when $\\tau^{2}=0$?",
    "question_context": "Geostatistical Inference Under Preferential Sampling: Variogram Analysis\n\nThe class of models that is defined through assumptions 1–3 is a particular case of the geostatistical model for preferential sampling in Ho and Stoyan (2008). The sampling locations, or points $x_{i}$ , are from the log-Gaussian Cox process and the Gaussian variates $Y_{i}$ of the paper correspond to the marks $m(x_{i})=$ $\\mu+S(x_{i})+\\varepsilon(x_{i})$ , where $\\varepsilon(x_{i})\\sim N(0,\tau^{2})$ . <PARAGRAPH_SEPARATOR> In the context of models such as that considered in the paper, it makes sense to speak both about the ‘field variogram’ $\\gamma(u)$ and the ‘mark variogram’ $\\gamma_{m}(u)$ , which belongs to the corresponding marked point process; see for example Illian et al. (2008), page 344. The empirical variogram based on $Y_{i}$ observed at $x_{i}$ estimates $\\gamma_{m}(u)$ , whereas, in geostatistics, a statistician hopes to estimate $\\gamma(u)$ . <PARAGRAPH_SEPARATOR> For the model of the paper, <PARAGRAPH_SEPARATOR> $$ {\\boldsymbol{\\gamma}}(u)=\\gamma_{m}(u), $$ <PARAGRAPH_SEPARATOR> when $\\tau^{2}=0$ (as in Section 3); see Ho and Stoyan (2008), equation (19). This seems to be in contradiction to the words of the paper on page 196, lines 8–11. <PARAGRAPH_SEPARATOR> We made simulations of the model. Fig. 13 shows the results for the model with the random-field parameters of Section 3 (mean 0), $\\alpha=3.0$ and $\\beta{=}1.5$ (about l00 points on average in a unit square). We simulated full randomness whereas a form of conditional simulation was used in the paper. <PARAGRAPH_SEPARATOR> We think that the bias occurs since the estimator for the mark variogram is only asymptotically ratio unbiased. We are surprised that the bias is so large, but it decreases along an increasing window size. For large windows the simulations become extremely long; the variability of the model is high."
  },
  {
    "qid": "statistic-posneg-1696-0-1",
    "gold_answer": "FALSE. The paper states that the proportion of calibrated shortest prediction intervals covering the observed values was 0.95, but it also notes that these figures are not expected to be good estimates of the true coverage probabilities due to measurement errors in the observed values. Therefore, while the observed coverage is close to nominal, it is not strictly accurate to claim that the coverage matches the nominal 95%.",
    "question": "Does the coverage probability of the calibrated shortest prediction intervals match the nominal 95% coverage as stated in the paper?",
    "question_context": "Comparison of Prediction Interval Lengths in Statistical Modeling\n\nThe paper compares the lengths of 95% standard and shortest prediction intervals, both plug-in and calibrated versions, at the first ten locations of a validation set. The results show that the shortest prediction intervals are on average about 14% shorter than the standard prediction intervals. The coverage probabilities of these intervals are also discussed, with the calibrated shortest prediction intervals having coverage close to the nominal 95%."
  },
  {
    "qid": "statistic-posneg-1746-0-2",
    "gold_answer": "FALSE. The time complexity for computing the distance correlation is typically $\\mathcal{O}(n^{2})$ for general cases. However, recent works have expedited the computation to ${\\mathcal{O}}(n\\log(n))$ under specific conditions (e.g., one-dimensional data and Euclidean distance). This is still not as fast as the Pearson correlation, which can be computed in $\\mathcal{O}(n)$ for one-dimensional data.",
    "question": "Is the time complexity for computing the distance correlation $\\mathcal{O}(n)$ for one-dimensional data?",
    "question_context": "Testing Independence with Distance Correlation\n\nGiven pairs of observations $(x_{i},y_{i})\\in\\mathbb{R}^{p}\\times\\mathbb{R}^{q}$ for $i=1,\\ldots,n.$ , assume they are independently identically distributed as $F_{X Y}$ . Two random variables are independent if and only if the joint distribution equals the product of the marginal distributions. The statistical hypothesis for testing independence is $$\\begin{array}{l}{{H_{0}:F_{X Y}=F_{X}F_{Y},}}\\ {{}}\\ {{H_{A}:F_{X Y}\\not=F_{X}F_{Y}.}}\\end{array}$$ <PARAGRAPH_SEPARATOR> To populate these methods to big data analysis, a big hurdle is the time complexity. Computing the distance correlation typically requires $\\mathcal{O}(n^{2})$ ; and to compute its $\\boldsymbol{p}$ -value for testing, the standard approach is to estimate the null distribution of distance correlation via permutation, which requires $\\mathcal{O}(r n^{2})$ where $r$ is the number of random permutations and typically at least 100 or more. In comparison, for one-dimensional $(p=q=1)$ ) data the Pearson correlation can be computed in $\\mathcal{O}(n)$ , and a Pearson correlation $t$ -test is readily available to compute the $\\boldsymbol{p}$ -value in constant time complexity $\\mathcal{O}(1)$ . The computational advantage makes Pearson extremely fast and attractive in practice. Recent works have successfully expedited the distance correlation computation into ${\\mathcal{O}}(n\\log(n))$ under one-dimensional data and Euclidean distance (Huo and Szekely 2016; Chaudhuri and $\\mathrm{Hu}2019\\$ ). The testing part, however, remains difficult and less well-understood, because the null distribution of distance correlation has no fixed nor known density in general. Some notable works include the distance correlation $t$ -test (Szekely and Rizzo 2013), HSIC Gamma test (Gretton and Gyorfi 2010), and the subsampling approach (Zhang et al. 2018), which provided special treatments and very valuable insights into the problem."
  },
  {
    "qid": "statistic-posneg-1754-0-0",
    "gold_answer": "FALSE. The context explicitly states that the efficiency result applies not only to multiplicative intercept models but also to non-multiplicative intercept models, as demonstrated by the direct calculation presented in the note. The argument by J. Robins extends the efficiency result beyond multiplicative intercept models, showing that the case-control likelihood in the larger model (non-multiplicative) and the smaller model (multiplicative) are equivalent in terms of efficiency.",
    "question": "In the context of case-control studies, the joint maximization of the likelihood for the marginal distribution of predictors and regression parameters leads to efficient estimates of regression parameters only in multiplicative intercept models. True or False?",
    "question_context": "Efficient Estimation in Case-Control Studies: Multiplicative and Non-Multiplicative Intercept Models\n\nThe results of Cosslett (1981) imply that joint maximisation of the likelihood for the marginal distribution of the predictors and the regression parameters results in effcient estimates of the regression parameters in the asymptotic setting where the numbers of cases and controls tend to infinity together. Scott & Wild (1986) note that, with randomly sampled data in multiplicative intercept models, the case-control indicators are ancillary for the regression coefficients. Thus joint maximisation of the semiparametric likelihood for the case control data, and maximisation of the likelihood obtained by treating the data as if they were randomly sampled both result in the same estimate of the regression parameters. J. Robins (personal communication) has pointed out that the argument may be applied to show effciency of the approach in non-multiplicative intercept models as well because the case-control likelihood in the larger model and the case-control likelihood in the smaler model are equivalent. This note presents a direct calculation that demonstrates the effciency result both for multiplicative and non-multiplicative intercept models."
  },
  {
    "qid": "statistic-posneg-19-0-0",
    "gold_answer": "TRUE. The test statistic $u$ incorporates the correlation $\\rho$ through the term $r=2a_{12}/(a_{11}+a_{22})$, where $a_{12}$ is the covariance between the paired observations. This adjustment is necessary to account for the dependence between the paired data points when testing the equality of means under the null hypothesis.",
    "question": "Does the proposed test statistic $u$ account for the correlation $\\rho$ between the paired observations when testing $H_{0}\\colon\\mu_{1}=\\mu_{2}$?",
    "question_context": "Test for Equality of Means with Missing Data and Correlated Variates\n\nLet us consider the usual paired-data situation in which observations are obtained under two conditions from the same sampling unit. The model for each unit's observations will be the bivariate normal distribution with mean vector $(\\mu_{1},\\mu_{\\mathfrak{L}})$ and a covariance matrix with common variance ${\\boldsymbol{\\sigma^{2}}}$ and correlation coefficient $\\rho\\geqslant0$ . From this distribution, $\\pmb{m}$ independent pairs of observations $(x_{1},y_{1}),...,(x_{m},y_{m})$ have been drawn, while in addition, $N-m$ observations $\\pmb{y}_{m+1},...,\\pmb{y}_{N}$ are available on the second variate alone. We wish to use all of these data to test the hypothesis $H_{0}\\colon\\mu_{1}=\\mu_{2}$ against the alternative of unequal means."
  },
  {
    "qid": "statistic-posneg-1115-0-2",
    "gold_answer": "TRUE. The set $\\mathcal{D}^{\\mathrm{diag}}$ is defined to include only points $(U_i, V_i)$ where the ranks $U_i$ and $V_i$ are within $0.4n^{5/6}$ of each other. This restriction is based on Johansson's [10] result that deviations from the diagonal are typically of order $n^{5/6}$ under independence. The constant 0.4 was chosen empirically to optimize test power.",
    "question": "Is the condition $|U_i - V_i| \\leq 0.4n^{5/6}$ used in $J L_{n}$ to restrict the analysis to points near the diagonal $U = V$?",
    "question_context": "Independence Tests for Continuous Random Variables Using Longest Increasing Subsequence Statistics\n\nThe paper discusses the distribution of the statistic $L_{n}$, which measures the length of the longest increasing subsequence in a paired sample of size $n$ from continuous random variables $X$ and $Y$. Under the null hypothesis of independence, the exact distribution of $L_{n}$ is derived using permutations and Young tableaux. The asymptotic distribution of $L_{n}$ is shown to follow the Tracy-Widom distribution after appropriate centering and scaling. Modifications to $L_{n}$, such as $J L_{n}$ and $J L M_{n}$, are introduced to improve test power and mitigate discreteness issues."
  },
  {
    "qid": "statistic-posneg-1688-0-2",
    "gold_answer": "TRUE. The weight w_i accounts for the effective sample size adjustment due to intraclass correlation (ρ_SS) and mother-offspring correlation (ρ_MS). The denominator z_i increases with higher correlations, reducing the weight for families with more correlated offspring, which aligns with generalized least squares principles for correlated data.",
    "question": "Is the weight w_i = k_i / z_i (where z_i = 1 + (k_i - 1)ρ_SS - k_iρ_MS^2) inversely proportional to the variance of the ith family's offspring mean?",
    "question_context": "Maximum Likelihood Estimation of Interclass Correlations in Family Data\n\nThe paper presents a maximum likelihood estimation method for interclass correlations in family data, where measurements from N families are modeled with a multivariate normal distribution. The model includes parameters for means (μ_M, μ_S), variances (σ_M^2, σ_S^2), and correlations (ρ_MS, ρ_SS) between mother-offspring and sib-sib pairs. The likelihood function and iterative estimation procedures are derived, with a focus on handling varying sibship sizes and ensuring positive-definite covariance matrices."
  },
  {
    "qid": "statistic-posneg-1044-0-0",
    "gold_answer": "TRUE. The quadratic penalty term Q(B, C, M) is indeed defined as the sum over l from 0 to h of the inner product of M_l and (C_l - B) plus the Frobenius norm squared of (C_l - B) multiplied by ρ/2. This term ensures that the dummy variables C_l remain close to the coefficient matrix B, which is a key component of the ADMM algorithm for enforcing constraints and penalties.",
    "question": "The augmented Lagrangian function in the ADMM algorithm includes a quadratic penalty term Q(B, C, M) that ensures the dummy variables C_l stay close to the coefficient matrix B. Is the statement that Q(B, C, M) is defined as the sum over l from 0 to h of the inner product of M_l and (C_l - B) plus the Frobenius norm squared of (C_l - B) multiplied by ρ/2 correct?",
    "question_context": "Sparse Single Index Models for Multivariate Responses: Optimization via ADMM\n\nThe paper discusses the estimation of sparse single index models for multivariate responses using penalized splines and ADMM optimization. The model involves estimating link functions and a coefficient matrix with various structural constraints (e.g., sparsity, low-rank). The optimization problem is solved using an ADMM algorithm with updates for the coefficient matrix B, dummy variables C, and Lagrange multipliers M."
  },
  {
    "qid": "statistic-posneg-818-2-0",
    "gold_answer": "FALSE. The correct integration of $$y_{x}=a+b x+c x^{3}+d x^{3}+e x^{4}$$ over [-1, 1] should yield $$\\int_{-1}^{1}y_{x}d x = 2a + \\frac{2c}{4} + \\frac{2e}{5} = 2a + \\frac{c}{2} + \\frac{2e}{5}$$, not $$a+\\frac{c}{12}+\\frac{e}{80}$$. The given formula is incorrect because it does not account for the symmetric integration of odd-powered terms (which vanish) and misrepresents the coefficients of the even-powered terms.",
    "question": "Is the formula $$\\int_{-1}^{1}y_{x}d x=a+\\frac{c}{12}+\\frac{e}{80}$$ correctly derived from the polynomial $$y_{x}=a+b x+c x^{3}+d x^{3}+e x^{4}$$ by integrating term by term over the interval [-1, 1]?",
    "question_context": "Quadrature Formulae for Statistical Area Calculation\n\nThe usual quadrature formulae express an area in terms of the ordinates at the beginning and end of the base and a varying number of intermediate ordinates, but in some statistical work it is convenient to have values for the areas in terms of ordinates both within and without the base on which the area stands. Symbolically we have to express $\\int\\limits_{-b}^{b}y d x$ in terms of $y_{0},y_{1},y_{-1},y_{3},y_{-3},\\mathrm{etc}.$ <PARAGRAPH_SEPARATOR> Let then and <PARAGRAPH_SEPARATOR> $$\\begin{array}{c}{{y_{x}\\varpi a+b x+c x^{3}+d x^{3}+e x^{4},}}\\ {{}}\\ {{\\displaystyle\\int_{-1}^{1}y_{x}d x=a+\\frac{c}{12}+\\frac{e}{80},}}\\ {{}}\\ {{}}\\ {{y_{0}=a,}}\\ {{}}\\ {{}}\\ {{y_{-1}+y_{+1}=2\\left(a+c+e\\right)_{\\ast}}}\\ {{}}\\ {{}}\\ {{y_{-1}+y_{+1}=2\\left(a+4e+16e\\right)}}\\end{array} $$ <PARAGRAPH_SEPARATOR> Now assume the required integral can be put in the form <PARAGRAPH_SEPARATOR> $$k y_{0}+k(y_{1}+y_{-1})+l(y_{1}+y_{-1}),$$ <PARAGRAPH_SEPARATOR> substitute the values given just above and equate coefficients of $a_{1}0$ and $\\pmb{\\theta}$ respectively to $\\mathbf{1},\\frac{\\cdot\\mathbf{1}}{18}\\rightsquigarrow\\frac{\\mathbf{1}}{80}$ and we obtain <PARAGRAPH_SEPARATOR> The most obvious purpose for which these formulae can be used in biometric work is for calculating the areas from the values of ordinates when the equation to the curve has been determined. It is clearly convenient to calculate as few ordinates as possible and I have found Formulae I. and II. especially convenient when expressed in the form of differences; thus II. can be written <PARAGRAPH_SEPARATOR> $$\\int_{-\\frac{1}{6}}^{1}\\mathbf{y}d x=y_{0}+\\frac{1}{24}\\left\\{\\Delta y_{0}-\\Delta y_{-1}\\right\\},$$ <PARAGRAPH_SEPARATOR> and the adjustment from the mid-ordinate is easily calculated. <PARAGRAPH_SEPARATOR> Formula I. can be written <PARAGRAPH_SEPARATOR> $$\\int_{-\\frac{1}{3}}^{1}y d x=y_{0}+\\frac{201}{5760}(\\Delta y_{0}-\\Delta y_{-1})-\\frac{17}{5760}(\\Delta y_{1}-\\Delta y_{-2})$$ <PARAGRAPH_SEPARATOR> and the fractions are 0505 and 00295."
  },
  {
    "qid": "statistic-posneg-1134-0-1",
    "gold_answer": "TRUE. The context specifies that $\\varepsilon_{i j k}\\sim N(0,\\sigma_{e}^{2})$, $\\beta_{i j}\\sim N(0,\\sigma_{\\beta}^{2})$, and $\\alpha_{i}\\sim N(0,\\sigma_{\\alpha}^{2})$, all independent. This indicates that the random effects at each level (school, class, student) are modeled as independent normal variables.",
    "question": "Does the model $Y_{i j k}=\\mu+\\alpha_{i}+\\beta_{i j}+\\gamma_{i j k}$ assume that all random effects are independent?",
    "question_context": "Nested Random Effects in Normal Linear Mixed Models\n\nColumns provide the 50th and 95th percentiles of Gelman & Rubin's (1992) variance inflation factor (G&R), and the value of the lag 1 sample autocorrelation (AcF) computed from the first of the five sampled chains. <PARAGRAPH_SEPARATOR> # 4. THE NESTED RANDOM EFFECTS NORMAL LINEAR MODEL <PARAGRAPH_SEPARATOR> Suppose there are additional levels of the hierarchy. For instance, we might denote the response for the $k$ th child in the $j$ th class of the ith school by $Y_{i j k}$ and model it as <PARAGRAPH_SEPARATOR> $$ Y_{i j k}=\\mu+\\alpha_{i}+\\beta_{i j}+\\gamma_{i j k}, $$ <PARAGRAPH_SEPARATOR> where $i=1,\\ldots,I,j=1,\\ldots,J$ and $k=1,\\ldots,n_{i j}$ At each level of the hierarchy we set up a linear model relating the terms in (4.1) to explanatory variables, i.e. <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\alpha_{i}=x_{i}^{\\mathrm{{T}}}\\alpha+\\varepsilon_{i},\\quad\\beta_{i j}=w_{i j}^{\\mathrm{{T}}}\\beta_{i}+\\lambda_{i j},\\quad\\gamma_{i j k}=z_{i j k}^{\\mathrm{{T}}}\\gamma_{i j}+\\xi_{i j k}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Here, $x_{i}$ denotes school-level covariates, $w_{i j}$ class-level covariates, and $z_{i j k}$ student-level covariates. Such models are referred to as multilevel or nested mixed linear models, e.g. Goldstein (1986), Longford (1987). Our centring ideas can be relevant here, applied in sequence from the highest level down. <PARAGRAPH_SEPARATOR> We illustrate using (4·1) with no covariates, assuming $\\varepsilon_{i j k}\\sim N(0,\\sigma_{e}^{2})$ $\\beta_{i j}\\sim N(0,\\sigma_{\\beta}^{2})$ and $\\alpha_{i}\\sim N(0,\\sigma_{\\alpha}^{2})$ , all independent. Define $\\rho_{i j}=\\mu+\\alpha_{i}+\\beta_{i j}$ whence $Y_{i j k}|\\rho_{i j}\\sim N(\\rho_{i j},\\sigma_{e}^{2})$ Define $\\eta_{i}=\\mu+\\alpha_{i}$ so that $\\rho_{i j}|\\eta_{i}\\sim N(\\eta_{i},\\sigma_{\\beta}^{2})$ and $\\eta_{i}|\\mu\\sim N(\\mu,\\sigma_{\\alpha}^{2})$ We assume a flat prior for $\\mu,$ and take $\\sigma_{\\epsilon}^{2},\\sigma_{\\beta}^{2}$ and $\\sigma_{\\alpha}^{2}$ as known. The centred parametrisation is thus $(\\mu,\\eta,\\rho)$ . Exact calculations are more complicated here since marginalisation produces dependence amongst the Yjk. <PARAGRAPH_SEPARATOR> Let <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{Y_{i j}^{\\mathsf{T}}=(Y_{i j1},\\hdots,Y_{i j n_{i j}}),\\quad Y_{i}^{\\mathsf{T}}=(Y_{i1}^{\\mathsf{T}},\\hdots,Y_{i j}^{\\mathsf{T}}),\\quad Y^{\\mathsf{T}}=(Y_{1}^{\\mathsf{T}},\\hdots,Y_{I}^{\\mathsf{T}}).}\\end{array} $$ <PARAGRAPH_SEPARATOR> Then, given $\\mu,$ the $Y_{i}$ are independent normal with $E(Y_{i}|\\mu)=\\mu1_{n_{i+}}$ and variance <PARAGRAPH_SEPARATOR> $$ \\mathrm{var}\\left(Y_{i}|\\mu\\right)=\\Sigma_{i}:=\\sigma_{e}^{2}I_{n_{i+}}+X_{i}D X_{i}^{\\mathrm{T}}. $$ <PARAGRAPH_SEPARATOR> Here $\\begin{array}{r}{n_{i+}=\\sum n_{i j},D=\\mathrm{diag}\\left(\\sigma_{\\alpha}^{2},\\sigma_{\\beta}^{2},...,\\sigma_{\\beta}^{2}\\right)}\\end{array}$ and $X_{i}$ .s $n_{i+}\\times(J+1)$ with first column all 1's, and $(j+1)$ th column $(j=1,\\ldots,J)$ having a 1 in each of the $n_{i j}$ rows corresponding to $Y_{i j k}\\left(k=1,\\ldots,n_{i j}\\right)$ , and ${0}\\mathfrak{s}$ elsewhere. From (4.2), <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{E(\\boldsymbol{\\mu}|\\boldsymbol{Y})=\\hat{\\boldsymbol{\\mu}}:=(\\boldsymbol{X}^{\\mathrm{T}}\\Sigma^{-1}\\boldsymbol{X})^{-1}\\boldsymbol{X}^{\\mathrm{T}}\\Sigma^{-1}\\boldsymbol{Y},\\quad\\mathrm{var}(\\boldsymbol{\\mu}|\\boldsymbol{Y})=(\\boldsymbol{X}^{\\mathrm{T}}\\Sigma^{-1}\\boldsymbol{X})^{-1}=(\\sum\\boldsymbol{X}_{i}^{\\mathrm{T}}\\Sigma_{i}^{-1}\\boldsymbol{X}_{i})^{-1}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Following $\\S2$ we investigate the $|B_{i}D^{-1}|,$ where $B_{i}=(\\sigma_{e}^{-2}X_{i}^{\\mathrm{T}}X_{i}+D^{-1})^{-1}.$ In Appendix 2 we show that if $\\sigma_{\\beta}^{2}\\rightarrow\\infty$ with $\\sigma_{\\alpha}^{2}$ and $\\sigma_{e}^{2}$ fixed, then $|B_{i}D^{-1}|\\to0$ , so centring will be preferred. If on the other hand $\\sigma_{e}^{2}\\to\\infty$ with $\\sigma_{\\alpha}^{2}$ and ${\\sigma}_{\\beta}^{2}$ fixed, then $|B_{i}D^{-1}|\\to1$ and the uncentred parametrisation will be better. <PARAGRAPH_SEPARATOR> We now describe a simulation study of (4.1) with $I=J=3$ $n_{i j}=5$ and the true value of $\\mu$ equal to 0. We are interested in comparing four possible parametrisations: uncentred $(\\mu,\\alpha,\\beta)$ , α's centred $(\\mu,\\eta,\\beta)$ , $\\beta$ 's centred $(\\mu,\\alpha,\\rho)$ , and both $\\pmb{\\alpha\\mathscr{s}}$ and $\\beta$ 's centred $(\\mu,\\eta,\\rho)$ After selecting fixed values for $\\sigma_{\\epsilon}^{2},\\sigma_{\\beta}^{2}$ and $\\sigma_{\\alpha}^{2}$ , designed to favour one parametrisation or another, we first generate ‘true' parameter values from the assumed priors, then data given these parameter values. A single Gibbs sampling chain is then run using the uncentred parametrisation for 10 000 iterations. The output from this chain is used to estimate the posterior correlation matrix among the $I J+I+1=13$ parameters under each of the four parametrisations. <PARAGRAPH_SEPARATOR> Table 2(a) tabulates the resulting estimated correlations arising under the four parametrisations for the case $\\sigma_{e}=10,\\sigma_{\\beta}=1$ and $\\sigma_{\\alpha}=1$ Since the model variability is large relative to that in both stages of the prior, we would expect the uncentred parametrisation to be best. This is in fact the case, with 75 of the 78 sample correlations falling in $(-0{\\cdot}1,0{\\cdot}1)$ Table 2(b) gives results for the case $\\sigma_{\\bullet}=1$ $\\sigma_{\\beta}=10$ and $\\pmb{\\sigma}_{\\pmb{\\alpha}}=1$ . Here, as predicted by our"
  },
  {
    "qid": "statistic-posneg-839-0-0",
    "gold_answer": "TRUE. The lemma first establishes an $L_1$ norm inequality and then uses the Cauchy-Schwarz inequality to transition to the $L_2$ norm. The factor $\\frac{a+a^{\\prime}}{a-a^{\\prime}}$ arises from the geometric properties of the cubes $K$ and $K'$, and the $\\sqrt{m}$ term is a consequence of applying the Cauchy-Schwarz inequality in $m$-dimensional space.",
    "question": "The lemma states that for any $t \\in K'$ and $t^* \\in R^m$, the inequality $\\parallel S(t)-S(t^{*})\\parallel_{2} \\leqslant \\frac{a+a^{\\prime}}{a-a^{\\prime}}\\sqrt{m}\\operatorname*{max}\\parallel S(t^{i})-S(t^{*})\\parallel_{2}$ holds. Is this inequality derived correctly from the preceding steps in the lemma?",
    "question_context": "Convergence of Monotone Random Fields in High Dimensions\n\nThe paper discusses the convergence properties of monotone random fields in high-dimensional spaces. It presents a lemma and a proposition that establish bounds on the difference between a random field $S_n(t)$ and its limit $S(t)$ in terms of the $L_1$ and $L_2$ norms. The key results include inequalities that relate the difference at any point $t$ within a cube to the maximum difference at the vertices of the cube, scaled by a factor depending on the cube's dimensions. The proposition then uses these bounds to show uniform convergence in probability of $S_n(t)$ to $S(t)$ over compact sets."
  },
  {
    "qid": "statistic-posneg-1501-1-3",
    "gold_answer": "TRUE. The context provides the expansion $\\log\\left(\\frac{f+\\frac{1}{2}}{m-f+\\frac{1}{2}}\\right) = \\log\\left(\\frac{p}{1-p}\\right) + \\gamma^{1/2}z m^{-1/2} + \\frac{1}{2}\\gamma(1-2p)(1-z^{2})m^{-1} + O_{p}(m^{-3/2})$, which explicitly includes the $O_{p}(m^{-3/2})$ term.",
    "question": "Does the approximation for $\\log\\left(\\frac{f+\\frac{1}{2}}{m-f+\\frac{1}{2}}\\right)$ include a term of order $O_{p}(m^{-3/2})$?",
    "question_context": "Meta-Analysis Approximations and Radial Plot Statistics\n\nThe paper discusses the asymptotic properties of estimates in meta-analysis, focusing on the joint normality of $(\\hat{\\theta}, \\hat{\\sigma}^2)$ with mean $(\\theta, \\sigma^2)$. It assumes biases of order $O(n^{-1})$, and provides various approximations and expansions for statistics like $\\widetilde{\\theta}$, $Z_1$, and $Q$. The context includes detailed derivations and expectations of these statistics, as well as special cases involving binomial distributions and radial plot statistics."
  },
  {
    "qid": "statistic-posneg-645-0-0",
    "gold_answer": "TRUE. Explanation: The paper shows that the correct asymptotic distribution under Gamma-distributed data is $\\mathbb{N}(0.75,1)$, not $\\mathbb{N}(0,1)$ as assumed in [8]. Calculating $P(Z>\\Phi^{-1}(0.95))$ where $Z\\sim\\mathbb{N}(0.75,1)$ yields 0.185, which matches the empirical size deviation from the nominal 5% level. This occurs because the test's critical region is based on the incorrect $\\mathbb{N}(0,1)$ assumption, leading to over-rejection when the true distribution has a mean shift (0.75). The table data confirms this with LW test sizes around 0.1 for Gamma vectors, systematically higher than the CZZ test's sizes closer to 0.05.",
    "question": "Is it correct that the LW test's size for non-Gaussian data (Gamma random vectors) is approximately 0.185 when the nominal level is 5%, as derived from the asymptotic distribution $(n\\hat{W}_{n}-1)/2\\stackrel{D}{\\to}\\mathbb{N}(0.75,1)$?",
    "question_context": "High-Dimensional Data Identity Tests: LW vs. CZZ Test Performance\n\nFrom the definitions of the LW and the new LW tests, we know $\\begin{array}{r}{n(W_{n}-\\hat{W}_{n})=O(\\frac{1}{n})}\\end{array}$ which means the two statistics are quite similar for normal distributions. Therefore, our first experiment is to investigate the empirical sizes of the LW, the new LW and the CZZ tests on Gaussian data with a small sample size. Results based on $10^{4}$ replications are reported in Table 2. <PARAGRAPH_SEPARATOR> We observe from Table 2 that the sizes of the LW and the new LW tests are always better than the CZZ test for normal distributions. The reason is due to the fact that the LW and the new LW tests are based on the sample covariance matrix $S_{n}$ which is a completely sufficient statistic for $\\Sigma_{p}$ for Gaussian data. When the sample size $n$ is small such as $n=5$ in the experiments, the new LW test has better sizes than the LW test and when $n$ is large, the performances of the LW and the new LW tests are quite similar. This is because the new LW test is always the unique best unbiased estimator of $\\begin{array}{r}{\\frac{1}{p}\\mathrm{tr}(\\varSigma_{p}-I_{p})^{2}}\\end{array}$ while the LW test has a $\\textstyle O({\\frac{1}{n^{2}}})$ bias. <PARAGRAPH_SEPARATOR> For non-Gaussian data, from Theorem 2.2, we know that the CLT of the LW test depends on $\\varDelta$ and it is not reasonable that Chen et al. [8] assumed $\\varDelta=0$ even for gamma random vectors. From Theorem 2.2, when $\\Sigma_{p}=I_{p}$ and $Y_{i,j}\\sim\\mathrm{Gamma}[4,0.5],$ by Slutsky’s theorem we know <PARAGRAPH_SEPARATOR> $$ (n\\hat{W}_{n}-1)/2\\stackrel{D}{\\to}\\mathbb{N}(0.75,1). $$ <PARAGRAPH_SEPARATOR> However, in [8], the authors still thought $(n\\hat{W}_{n}-1)/2\\stackrel{D}{\\to}\\mathbb{N}(0,1)$ . Moreover, since $P(Z>\\Phi^{-1}(0.95))=0.185$ where $Z\\sim\\mathbb{N}(0.75,1)$ and $\\phi$ is the distribution function of standard norm variables, this explains why the size of the LW test in [8] is near 0.185 not $5\\%$ . <PARAGRAPH_SEPARATOR> Table 3 Performances of the new LW test and the CZZ test on non-Gaussian data. <PARAGRAPH_SEPARATOR> <html><body><table><tr><td rowspan=\"2\">p</td><td colspan=\"4\">LWtest</td><td colspan=\"4\">CZZtest</td></tr><tr><td>n=20</td><td>n=40</td><td>y=60</td><td>n=80</td><td>n=20</td><td>n=40</td><td>n=60</td><td>n=80</td></tr><tr><td colspan=\"9\">Gammarandom vectors</td></tr><tr><td>38</td><td>0.1165</td><td>0.0923</td><td>0.0833</td><td>0.0807</td><td>0.0861</td><td>0.0767</td><td>0.0707</td><td>0.0704</td></tr><tr><td>55</td><td>0.1110</td><td>0.0828</td><td>0.0822</td><td>0.0734</td><td>0.0810</td><td>0.0697</td><td>0.0694</td><td>0.0658</td></tr><tr><td>89</td><td>0.1042</td><td>0.0840</td><td>0.0704</td><td>0.0655</td><td>0.0792</td><td>0.0685</td><td>0.0591</td><td>0.0590</td></tr><tr><td>159</td><td>0.0998</td><td>0.0795</td><td>0.0649</td><td>0.0626</td><td>0.0754</td><td>0.0653</td><td>0.0583</td><td>0.0588</td></tr><tr><td colspan=\"9\">Uniformrandomvectors</td></tr><tr><td>38</td><td>0.0574</td><td>0.0517</td><td>0.0490</td><td>0.0501</td><td>0.0678</td><td>0.0565</td><td>0.0527</td><td>0.0530</td></tr><tr><td>55</td><td>0.0614</td><td>0.0592</td><td>0.0539</td><td>0.0543</td><td>0.0728</td><td>0.0659</td><td>0.0563</td><td>0.0563</td></tr><tr><td>89</td><td>0.0548</td><td>0.0503</td><td>0.0530</td><td>0.0563</td><td>0.0650</td><td>0.0548</td><td>0.0570</td><td>0.0582</td></tr><tr><td>159</td><td>0.0592</td><td>0.0556</td><td>0.0546</td><td>0.0518</td><td>0.0718</td><td>0.0607</td><td>0.0563</td><td>0.0535</td></tr></table></body></html>"
  },
  {
    "qid": "statistic-posneg-1781-0-0",
    "gold_answer": "FALSE. The statement is false because the effect of the longitudinal biomarker $B_{i}(t)$ on the hazard is not necessarily constant over time. The model assumes that the hazard ratio for a unit change in $B_{i}(t)$ is constant (given by $\\exp(\\alpha)$), but since $B_{i}(t)$ itself is time-dependent (e.g., $B_{i}(t)=a+b t+b_{i1}+b_{i2}t$), the actual effect on the hazard can vary over time. This reflects the dynamic nature of the relationship between the biomarker and the hazard, which is a key feature of time-dependent covariate models in survival analysis.",
    "question": "In the Cox proportional hazards model with time-dependent covariates, the hazard function $h_{i}\\{t|Z_{i}(t)\\}=h_{0}(t)\\exp\\{\\alpha B_{i}(t)\\}$ implies that the effect of the longitudinal biomarker $B_{i}(t)$ on the hazard is constant over time. Is this statement true? Briefly explain the economic/statistic implications.",
    "question_context": "Survival Analysis with Time-Dependent Covariates in Joint Models\n\nWe generate survival times on the basis of a Cox proportional hazards model with timedependent variable $h_{i}\\{t|Z_{i}(t)\\}=h_{0}(t)\\exp\\{\\alpha B_{i}(t)\\}$ , where we use Weibull baseline hazard fuction $\\bar{h_{0}(t)}=\\lambda t^{\\lambda-1}\\exp(\\eta)$ . Apart from the above commonly used Cox proportional hazards models, we generate event times from two other models to validate the applicability and robustness of our model. One is the accelerated failure time (AFT) model and the other is the first time hitting (FTH) model (Choi et al., 2014), both of which are specified below.<PARAGRAPH_SEPARATOR>Using different combinations of longitudinal and survival submodels, data are generated under four scenarios to assess the performance of our method. The scenarios are a Weibull hazard combined linear mixed effect model, WBL, a Weibull hazard combined quadratic mixed effect model, WBQ, an AFT model combined linear mixed effect model, AFT, and an FTH model combined linear mixed effect model, FTH. The equations and parameter settings are as follows:<PARAGRAPH_SEPARATOR>(a) scenario I, WBL,<PARAGRAPH_SEPARATOR>$$h_{i}\\{t|Z_{i}(t)\\}=h_{0}(t)\\exp\\{\\alpha B_{i}(t)\\}$$<PARAGRAPH_SEPARATOR>with $B_{i}(t)=a+b t+b_{i1}+b_{i2}t$ , where $a=3,b=-0.2,\\lambda=2,\\alpha=1,\\eta=-6$ and $D=$ matrix.0:4, 0:1, 0:1, 0:2/; b) scenario II, WBQ,<PARAGRAPH_SEPARATOR>$$h_{i}\\{t|Z_{i}(t)\\}=h_{0}(t)\\exp\\{\\alpha B_{i}(t)\\}$$<PARAGRAPH_SEPARATOR>with $B_{i}(t)=a(t-b)^{2}+c+b_{i1}+b_{i2}t$ , where $a=0.2,b=3,c=1,\\lambda=2,\\alpha=1,\\eta=-6$ and $D{=}\\mathrm{matrix}(0.4,0.1,0.1,0.2)$ ;<PARAGRAPH_SEPARATOR>(c) scenario III, AFT,<PARAGRAPH_SEPARATOR>$$B_{i}(t)=a+b t+b_{i1}+b_{i2}t,$$<PARAGRAPH_SEPARATOR>and the survival time $T_{i}$ is defined through $\\begin{array}{r}{U_{i}=\\int_{0}^{T_{i}}\\exp\\{B_{i}(s)\\beta\\}\\mathrm{d}s}\\end{array}$ , where $U_{i}=\\mathrm{exp}(\\epsilon_{i})$ , which is independent and identically distributed, $a=-2$ , $b=0.2$ , $\\beta=1$ , $\\epsilon_{i}\\sim N(0,0.1)$ and $D{=}\\operatorname*{matrix}(0.1,0.04,0.04,0.1)$ ;<PARAGRAPH_SEPARATOR>(d) scenario IV, FTH,<PARAGRAPH_SEPARATOR>$$B_{i}(t)=a+b t+b_{i1}+b_{i2}t,$$<PARAGRAPH_SEPARATOR>and the survival time is defined as the first time point to reach a prespecified threshold $\\Upsilon_{i}$ , where $a=4$ , $b=1$ , $\\Upsilon_{i}\\sim$ uniform.7, 10/ and $D=$ matrix.0:1, 0:04, 0:04, 0:1/.<PARAGRAPH_SEPARATOR>For the first two scenarios, we generate their survival times according to the property that survival function $S(T)$ and cumulative hazard functions $\\Lambda(T)$ follow uniform and exponential distributions respectively. Then, to simulate survival or event times from a joint model with timedependent covariates, we invert the cumulative hazard function with the help of uniroot() and integrate() in R (R Core Team, 2016).<PARAGRAPH_SEPARATOR>In all scenarios, observation time points for longitudinal biomarkers are set to $(0,1,2,3,4,5,6$ , 7, 8, 9/ and a minor variation derived from $N(0,0.1^{2})$ is added to obtain irregular observation time points. We assume that the censoring time is independent of the survival time, which is simulated from a uniform distribution of $(0,t_{\\operatorname*{max}})$ , with $t_{\\mathrm{max}}$ adjusted to result in about $35\\%$ censoring rate. Because of the constraint of survival time, observations measured beyond his or her survival time are discarded considering the real situation, so the number of longitudinal observations varies among all patients. Using the models and parameter settings above, we generated the four scenarios of data sets. For each scenario, we simulated 100 data sets with sample sizes $n=400$ . To present the simulated data set intuitively, we split the whole data set into two subgroups. Group 1 includes subjects whose mean longitudinal biomarkers exceed the population median and the remaining subjects constitute group 2. Fig. 2 displays the Kaplan–Meier curves for the four scenarios by group."
  },
  {
    "qid": "statistic-posneg-1572-0-0",
    "gold_answer": "TRUE. The estimator $\\mathsf{L}_{T,N}^{\\times}$ explicitly includes the terms $\\sigma_{+}$ and $\\sigma_{-}$ in its formula, meaning these volatility coefficients must be known or estimated beforehand for the estimator to be computed.",
    "question": "Is it true that the estimator $\\mathsf{L}_{T,N}^{\\times}$ requires knowledge of $\\sigma_{+}$ and $\\sigma_{-}$ for its implementation?",
    "question_context": "Convergence and Estimation of Local Time in Oscillatory Brownian Motion\n\nThe convergence of $\\mathsf{Q}_{T,N}^{\\pm}$ was discussed in Lejay and Pigato (2018) for the OBM (see also Lemma 1). We prove that the speed of convergence is strictly better than $\\sqrt{N}$ , meaning that $\\sqrt{N}(\\mathsf{Q}_{T,N}^{+}-\\mathsf{Q}_{T}^{+})\\xrightarrow[n\\to\\infty]{\\mathbb{P}}0$ . This result can be extended to the drifted process $\\xi$ via the Girsanov theorem.<PARAGRAPH_SEPARATOR>Alternatively to computing $\\mathsf{R}_{T,N}^{\\pm}$ , we may approximate the local time:<PARAGRAPH_SEPARATOR>• The approximation $\\mathsf{L}_{T,N}$ to the local time given by Equation (10) is easily implemented. • In Lejay and Pigato (2018); Lejay et al. (2019), we have also considered two other consistent estimators for the local time. For the OBM, they are<PARAGRAPH_SEPARATOR>$$\\overset{\\mathtt{\\xi}_{\\mathtt{{-}},N}}{\\neg}:=\\frac{-3}{2}\\sqrt{\\frac{\\pi}{2\\Delta t}}\\frac{\\sigma_{+}+\\sigma_{-}}{\\sigma_{+}\\sigma_{-}}\\sum_{k=1}^{N}((\\sharp_{k T/N}\\mathfrak{h}^{+}-\\mathbb{(}(\\xi_{(k-1)T/N}\\mathfrak{h}^{+})\\cdot(\\mathbb{(} \\xi_{k T/N}\\mathfrak{h}^{-}-\\mathbb{(} \\xi_{(k-1)T/N}\\mathfrak{h}^{-}),$$<PARAGRAPH_SEPARATOR>and<PARAGRAPH_SEPARATOR>$$\\mathsf{L}_{T,N}^{\\times}:=\\frac{4}{\\sqrt{2\\pi}}\\times\\frac{1}{\\sigma_{+}+\\sigma_{-}}\\times\\frac{1}{\\sqrt{N}}\\sum_{k=0}^{N-1}1_{\\xi_{k T/N}\\xi_{(k+1)T/N+1}<0}.$$<PARAGRAPH_SEPARATOR>• The expressions of $\\mathsf{L}_{T,N}^{\\dagger}$ and $\\mathsf{L}_{T,N}^{\\times}$ require $\\sigma_{+}$ and $\\sigma_{-}$ , unlike the one of $\\mathsf{L}_{T,N}$ . To the best of our knowledge, this is the first time that such estimator is considered.   • For the Brownian motion, these estimators converge at rate $N^{1/4}$ , thanks to the results of Jacod (1998). The proof has not been adapted to OBM because of the technical difficulties due to the discontinuity of the coefficients at 0. Anyway, we conjecture a rate of $1/4$ .   • The three different estimators for the local time seem to have all comparable accuracy on numerical simulations if the volatility coefficients $\\sigma_{\\pm}$ are known. It looks like estimator $\\mathsf{L}_{T,N}^{\\times}$ is the best one when $\\sigma_{+}\\approx\\sigma_{-}$ , whereas $\\mathsf{L}_{T,N}$ and $\\mathsf{L}_{T,N}^{\\dagger}$ look more accurate when $\\sigma_{+}\\neq\\sigma_{-}$ .   • Anyway, if these coefficients are not known, only $\\mathsf{L}_{T,N}$ can be implemented directly, whereas to implement the other estimators we have first to estimate $\\sigma_{\\pm}$ from observations of $\\xi$ . This problem has been thoroughly investigated in Lejay and Pigato (2018). As a consequence, a good estimation of the local time relies on the estimation of $\\sigma_{\\pm}$ . Therefore, we choose to show here the implementation of the estimator using $\\mathsf{L}_{T,N}$ , which does not suffer of this possible additional problem.<PARAGRAPH_SEPARATOR>In practice, this means that we are actually showing estimator $\\beta_{T,N}^{\\pm}$ in (11) in place of $\\beta_{T}^{\\pm}$ . The numerical results using an approximation of $\\beta_{T}^{\\pm}$ using $\\mathsf{Q}_{T,N}^{\\pm}$ and $\\mathsf{L}_{T,N}^{\\dagger}$ for the local time are very similar, if $\\sigma_{\\pm}$ is supposed to be known.<PARAGRAPH_SEPARATOR>We do not push further in this paper the theoretical discussion on the quality of these discrete-time approximations. Some more insights, based on numerical results, are given in the following section."
  },
  {
    "qid": "statistic-posneg-2140-7-1",
    "gold_answer": "FALSE. The iterative procedure ensures that the coefficients $h_{0},\\ldots,h_{m-1}$ are drawn such that their squared norms sum to a value close to 1, but not necessarily exactly 1. The final coefficient $h_{m}$ is explicitly adjusted to ensure $\\left\\|\\mathbf{h}\\right\\|=1$. Without this adjustment, the sum of the squared coefficients would not necessarily be 1.",
    "question": "Does the iterative procedure for drawing $h_{0},\\ldots,h_{m-1}$ guarantee that the sum of the squared coefficients $\\sum_{i=0}^{m} h_i^2$ will always be exactly 1?",
    "question_context": "Random Projection-Based Test of Gaussianity in Time Series Data\n\nOn the other hand, in the case of time series data, only the goodness of fit test for Gaussianity of a fixed finite marginal has been studied (Epps, 1987; Lobato and Velasco, 2004; Moulines and Choukri, 1996, for example). Notice that the regularity assumption has recently been weakened in Ghosh (2013) in order to work with long range dependence processes. Our work appears to be at the crossroad between all these works; that is, those on time series, on i.i.d. data in infinite dimension or on finite dimension. As a matter of fact, the random projection trick allows only a one dimensional marginal to be considered. Nevertheless, as the sample is not i.i.d., tools similar to those used in the multidimensional case appear. For example, we have to whiten empirically the sample in order to obtain an asymptotic $\\chi^{2}$ distribution (under the null hypothesis). Furthermore, the procedure based on the empirical characteristic function is a method related to the one developed in Csorgo (1986) although the sampled frequencies are, in our work, chosen randomly.<PARAGRAPH_SEPARATOR># 3.2. The test in practice<PARAGRAPH_SEPARATOR>Let $X_{0},\\ldots,X_{n}$ be the available measurements. To compute $\\mathbf{h}$ , let $\\delta>0$ be a fixed number (equal to $10^{-15}$ in the simulations that we carry out in Sections 4 and 5), and take $\\mathbf{h}=(h_{0},\\ldots,h_{m})^{T}$ with<PARAGRAPH_SEPARATOR>$$m=1+\\operatorname*{min}\\{\\operatorname*{min}\\{t:\\|(h_{0},\\ldots,h_{t})^{T}\\|\\ge1-\\delta\\},n-1\\},$$<PARAGRAPH_SEPARATOR>where $h_{0},\\ldots,h_{m-1}$ are drawn as follows and $h_{m}$ is such that $\\left\\|\\mathbf{h}\\right\\|=1$ . Then, define<PARAGRAPH_SEPARATOR>$$Y_{t}=\\sum_{i=0}^{\\operatorname*{min}(m,t)}h_{i}X_{t-i}a_{i},\\quad t=0,\\dots,n.$$<PARAGRAPH_SEPARATOR>To draw $h_{0},\\ldots,h_{m-1}$ , let us fix $\\alpha_{1}$ , $\\alpha_{2}>0$ . Then, we choose $(\\beta_{n})_{n\\in\\mathbb{N}}$ independent and identically distributed with beta distribution of parameters $\\alpha_{1}$ and $\\alpha_{2}$ . Further, we consider the probability distribution which selects a random point in $l^{2}$ according to the following iterative procedure:<PARAGRAPH_SEPARATOR>• $l_{0}=\\beta_{0}\\in[0,1]$ .   • Given $n\\geq1,l_{n}\\in[0,1-\\sum_{i=0}^{n-1}l_{i}]$ equal to $\\beta_{n}(1-\\textstyle\\sum_{i=0}^{n-1}l_{i})$ .<PARAGRAPH_SEPARATOR>Let us define $H_{n}=(l_{n}/a_{n})^{1/2}$ for $n\\in\\mathbb{N}$ with $a_{0}=1$ and $a_{n}=n^{-2}$ , $n\\geq1$ , and take $\\mathbf{H}=(H_{n})_{n\\in\\mathbb{N}}$ . Thus, ${\\bf h}=(h_{i})_{i\\in\\mathbb{N}}$ is a fixed realization of the random element $\\mathbf{H}$."
  },
  {
    "qid": "statistic-posneg-744-2-0",
    "gold_answer": "TRUE. The condition number CN(m) is defined as the ratio of the maximum to the minimum eigenvalues of matrix A, and the selection of m is constrained by the condition that log10 CN(m) must be less than or equal to √n. This ensures numerical stability while allowing the largest possible number of weights.",
    "question": "Is the condition number CN(m) used to select the number of weights m in the density estimation model by ensuring that log10 CN(m) is bounded by √n?",
    "question_context": "Model Selection Criteria in Unimodal Density Estimation\n\nWe present a novel procedure for selecting the number of weights by examining the condition number of A. Since A is a normal matrix the condition number is evaluated by, $\\mathsf{C N}(m)=\\left\\lceil\\frac{\\lambda_{\\operatorname*{max}}(A)}{\\lambda_{\\operatorname*{min}}(A)}\\right\\rceil$ , where $\\lambda_{\\operatorname*{max}}(A)$ and $\\lambda_{\\mathrm{min}}(A)$ are the maximum and minimum eigenvalues of A respectively. We select $m$ by including the largest number of weights possible while still bounding $\\log_{10}\\mathrm{CN}(m)$ by $\\sqrt{n}$ . <PARAGRAPH_SEPARATOR> # 2.1.2. AIC and BIC <PARAGRAPH_SEPARATOR> We also present more traditional methods for selecting the number of weights, specifically, the Akaike information criterion (AIC) and Bayesian information criteria (BIC). In the realm of density estimation, these criteria are given by, <PARAGRAPH_SEPARATOR> $\\mathsf{A I C}(m)=-2\\sum_{i=1}^{n}\\log[\\widehat{f}_{m}(x_{i},\\widehat{\\omega}_{m})]+2(m-1)$, <PARAGRAPH_SEPARATOR> $\\mathrm{BIC}(m)=-2\\sum_{i=1}^{n}\\log[\\hat{f}_{m}(x_{i},\\widehat{\\omega}_{m})]+\\log(n)(m-1)$, <PARAGRAPH_SEPARATOR> where $m$ is the number of weights, $\\hat{f}_{m}$ is the estimated density using $m$ weights, $\\widehat{\\omega}_{m}$ is the vector of estimated weights, and $x_{i}$ for $i=1,\\ldots,n$ are the observations. The effective dimension of the weight vector $\\widehat{\\omega}_{m}$ is $m-1$ since the weights are constrained to sum to one. <PARAGRAPH_SEPARATOR> A variation of Theorem 3.1 in Babu et al. (2002) shows that $\\hat{f}_{m}$ will converge uniformly to $f$ for $2\\leq m\\leq(n/\\log n)$ as $n\\rightarrow\\infty$ , so we implement both the AIC and BIC methods by estimating the density with $m$ weights ranging from 2 to $\\lceil n/\\log n\\rceil$ . The AIC or BIC is calculated for each fit, then the density estimate with the lowest AIC or BIC is selected as the best estimate. <PARAGRAPH_SEPARATOR> # 2.2. Comparison of criterion for selecting the number of weights <PARAGRAPH_SEPARATOR> We compare these three methods of selecting $m$ by performing a small simulation study. We generate data from a mixture of Beta densities with a fixed vector of weights, which follows the required format of our density estimation model. We then generate density estimates using the AIC, BIC, and CN criterion to select m. Our goal is to find the method which provides the best density estimate in terms of a low root mean integrated squared error (RMISE) and selects the true number of weights of the Beta mixture. The RMISE is approximated as follows: <PARAGRAPH_SEPARATOR> $\\mathrm{RMISE}=\\mathrm{E}\\left[\\Vert\\hat{f}_{\\widehat{m}}-f\\Vert_{2}\\right]\\approx\\frac{1}{N}\\sum_{\\ell=1}^{N}\\sqrt{d\\sum_{j=1}^{J}\\left[\\hat{f}_{\\widehat{m}}^{(\\ell)}(x_{j})-f(x_{j})\\right]^{2}}$, <PARAGRAPH_SEPARATOR> where $J=100,\\operatorname*{Pr}[x_{1}\\leq X\\leq x_{J}]=0.999$ , and $d=(x_{j}-x_{j-1})=\\frac{x_{J}-x_{1}}{J-1}$ for all $j\\geq2$ . In the above expression, $\\hat{f}_{\\widehat{m}}^{(\\ell)}$ denotes the estimated density at the ℓth sample for $\\ell=1,\\ldots,N$ with $\\widehat{m}$ chosen by one of the three criteria. <PARAGRAPH_SEPARATOR> We examine a symmetric and right-skewed Beta mixture each with 7 weights (i.e. $m=7$ ). The weight vectors are fixed to be $\\omega_{1}~=~\\{0.05,0.1,0.2,0.3,0.2,0.1,0.05\\}$ for the symmetric distribution, and $\\omega_{2}=\\{0.1,0.4,0.25,0.15,$ 0.05, 0.03, 0.02} for the right-skewed distribution. We generate $N~=~1000$ Monte Carlo (MC) samples of size $n=$ 15, 30, 100, and 500 for each Beta mixture. Table 1 displays the MC estimated RMISE values as well as the average estimate of m across the MC samples. Statistically significant lower RMISE values, based on a paired $t$ -test, are in bold. <PARAGRAPH_SEPARATOR> The AIC method has the lowest average RMISE in most situations. Unfortunately, none of the methods have average number of weight estimates that are close to 7. The CN method appears to select the number of weights at around $\\sqrt{n}$ , which is much larger than 7 for the cases of $n=100$ and 500. Both the AIC and BIC underestimate $m$ for all sample sizes with the AIC having slightly larger estimates. Overall it appears the CN criterion is a better option for samples with fewer observations while the AIC performs the best for larger samples of size 100 and 500."
  },
  {
    "qid": "statistic-posneg-270-0-1",
    "gold_answer": "TRUE. The E-step computes the expectation of the full log-likelihood over the latent variables z given the current parameter estimates, which involves calculating the posterior probabilities α_jk = E[z_jk|Y, θ̂]. The M-step then maximizes this expected log-likelihood with respect to the parameters θ, updating π, w, λ_0(t), and β based on the current estimates of α_jk.",
    "question": "The EM algorithm proposed in the paper estimates the parameters θ = (π, w, λ_0(t), β) by iterating between the E-step and M-step. Is it correct to say that the E-step involves computing the expectation of the full log-likelihood over the latent variables z given the current parameter estimates, and the M-step involves maximizing this expected log-likelihood with respect to the parameters?",
    "question_context": "Non-parametric Frailty Cox Model with Hierarchical Structure\n\nThe paper introduces a non-parametric frailty term in a Cox proportional hazards model, which is modeled through a random variable with a discrete distribution. The model assumes that each group can belong to one latent population with a certain probability. The hazard function for an individual in a group is conditional on the frailty term and an auxiliary indicator variable. The full likelihood of the model for complete data is provided, and an Expectation-Maximization (EM) algorithm is proposed for parameter estimation. The model can be interpreted as a shared frailty Cox model or a mixture model, with the number of latent populations considered as an unknown parameter."
  },
  {
    "qid": "statistic-posneg-562-1-0",
    "gold_answer": "FALSE. The formula provided is a standard GCV score formula, but the derivation and justification for its specific form in this context are not explicitly detailed in the provided paragraphs. The correct GCV score should account for the effective degrees of freedom in the smoothing process, which is typically represented by the trace of the smoothing matrix. The given formula uses the 1-norm of the smoothing tensor, which may not directly correspond to the effective degrees of freedom.",
    "question": "Is the GCV score formula $G C V s=\\frac{\\left\\Vert\\hat{y}-y\\right\\Vert_{F}^{2}/n}{\\left(1-\\left\\Vert T^{N}\\right\\Vert_{1}/n\\right)^{2}}$ correctly derived from the residual norm and the 1-norm of the smoothing tensor?",
    "question_context": "Robust Smoothing of Gridded Data using GCV and DCT\n\nThe paper discusses a method for robust smoothing of gridded data using Generalized Cross-Validation (GCV) scores and Discrete Cosine Transform (DCT). The iterative process involves calculating residuals, bisquare weights, and updating the smoothed estimate until convergence. The GCV score is used to determine the optimal smoothing parameter, and the method can handle weighted or missing data. The robust version uses Studentized residuals and leverages the average leverage formula."
  },
  {
    "qid": "statistic-posneg-1816-6-1",
    "gold_answer": "FALSE. The marginal likelihood for this model involves an intractable integral over $\\mathbb{R}^{3}$, but GVA only requires numerical integration over a one-dimensional integral, making it less computationally intensive than ML estimation.",
    "question": "Does the Gaussian variational approximation (GVA) method require numerical integration over a three-dimensional space for this model?",
    "question_context": "Logistic Regression with Random Quadratics in Longitudinal Data Analysis\n\nWe now alter the model presented above to include random slopes and quadratic terms for each youth. The random intercept model may not accurately capture the dependence structure of a single subject’s marijuana use over time, since the random intercepts model implies an exchangeable marginal correlation structure, which is unrealistic given the longitudinal nature of the data. A more realistic model allows random slopes and quadratic terms as well, so that the latent variable $Z_{i}$ now has three components. For the conditional probability $p_{i j}=P(Y_{i j}=$ $1|Z_{i},\\mathrm{SEX}_{i},\\mathrm{AGE}_{i j} $ ) we now have\n\n$$\n\\begin{array}{r l}&{\\mathrm{logit}(p_{i j})}\\ &{=\\left\\{\\begin{array}{l l}{(Z_{i0}+\\beta_{0})+(Z_{i1}+\\beta_{1})(\\mathrm{AGE}_{i j}/35)}\\ {\\quad+(Z_{i2}+\\beta_{2})(\\mathrm{AGE}_{i j}/35)^{2},}&{\\mathrm{SEX}_{i}=0}\\ {(Z_{i0}+\\beta_{3})+(Z_{i1}+\\beta_{4})(\\mathrm{AGE}_{i j}/35)}\\ {\\quad+(Z_{i2}+\\beta_{5})(\\mathrm{AGE}_{i j}/35)^{2},}&{\\mathrm{SEX}_{i}=1.}\\end{array}\\right.}\\end{array}\n$$\n\nThus, $\\beta_{0},\\beta_{1}$ , and $\\beta_{2}$ are the coefficients of the quadratic curve for the average female, and analogously for males. We model the random effects ${\\mathbf{Z}}_{n}$ as iid mean zero multivariate Gaussian with covariance matrix $\\Sigma$ . Once again we use MLE, a Gaussian variational approximation, and a one-step correction to the Gaussian variational approximation to estimate the average random effects and covariance matrix.\n\nThe marginal likelihood for this model involves an intractable integral over $\\mathbb{R}^{3}$ . GVA only requires numerical integration over a one-dimensional integral, and is hence less computationally intensive than ML estimation. To compare the computational burden of these methods, we simulated 100 datasets each at sample sizes $n=100$ , 250, and 500, and used the lme4 package, which uses a Laplace approximation to the loglikelihood, GVA, the one-step correction, and ML estimation to obtain estimates of the parameters in the logistic regression with random quadratics model. We used the implementation of the\n\nL-BFGS-B algorithm (Byrd et al. 1995) in the optim function in R (R Core Team 2018) to compute the GVA and MLE."
  },
  {
    "qid": "statistic-posneg-1462-0-1",
    "gold_answer": "FALSE. The invariant correlation property is only possible for non-identical marginals if both marginals are bi-atomic (supported on two points). Otherwise, the property does not hold, as the correlation would typically change under non-linear transformations unless specific conditions like identical marginals are met.",
    "question": "Does the invariant correlation property hold for any pair of random variables $(X,Y)$ with non-identical marginals and non-zero correlation?",
    "question_context": "Invariant Correlation under Marginal Transformations\n\nThe (Pearson) correlation coefficient is one of the most popular measures of quantifying the strength of statistical dependence. It is also called the linear correlation coefficient, reflecting on the fact that it measures linear dependence among random variables. As such, the correlation coefficient is preserved under common linear transforms. When non-linear transforms are applied, the correlation coefficient typically changes except some specific dependence structures such as independence. In fact, this invariance is a useful and notable property of independent samples, and plays a fundamental role in statistics and probability theory. <PARAGRAPH_SEPARATOR> To define this invariance property more formally, a $d$ -dimensional random vector $\\mathbf{X}~=~(X_{1},\\ldots,X_{d})$ with $\\operatorname{Var}(X_{i})\\in(0,\\infty),$ $i\\in[d]=\\{1,\\dots,d\\}$ , is said to have an invariant correlation matrix $R=(r_{i j})_{d\\times d}$ if <PARAGRAPH_SEPARATOR> $$ \\operatorname{Corr}(X_{i},X_{j})=\\operatorname{Corr}(g(X_{i}),g(X_{j}))=r_{i j} $$ <PARAGRAPH_SEPARATOR> for all measurable functions $g:\\mathbb{R}\\to\\mathbb{R}$ such that $\\operatorname{Corr}(g(X_{i}),g(X_{j}))$ is well defined for all $i,j\\in[d]$ . Although this property appears strong, it is satisfied by some specific models existing in the literature, such as the conformal $p$ -value, a useful statistical tool in machine learning and non-parametric inference [16]. The recent work [1] showed that (1) holds for a set of null conformal p-values to discuss a broad class of combination tests. <PARAGRAPH_SEPARATOR> Since (1) is essentially a bivariate property, we first focus on the case when $d=2$ . It is straightforward to check that a pair of random variables $(X,Y)$ has an invariant correlation $r\\in[-1,1]$ if $X$ and $Y$ have an identical marginal $F$ and their joint distribution $H$ is given by <PARAGRAPH_SEPARATOR> $$ H(x,y)=r\\operatorname*{min}(F(x),F(y))+(1-r)F(x)F(y),\\quad x,y\\in\\mathbb{R}. $$ <PARAGRAPH_SEPARATOR> We say that this model or its distribution is $r$ -Fréchet due to its connection to the Fréchet copulas; see [4,18] for applications in risk modeling. If $r\\in[0,1]$ , this model is a probabilistic mixture of comonotonic and independent cases, where two random variables $X$ and $Y$ are said to be comonotonic if $X=f(Z)$ and $Y=h(Z)$ for some random variable $Z$ and two increasing functions $f,h:\\mathbb{R}\\to\\mathbb{R}$ . All terms like ‘‘increasing’’ and ‘‘decreasing’’ are in the non-strict sense in this paper."
  },
  {
    "qid": "statistic-posneg-906-2-0",
    "gold_answer": "FALSE. The condition $[\\mathbf{K},\\mathbf{0}][\\mathbf{X},\\Sigma\\mathbf{X}^{\\perp}]^{+}=\\mathbf{K}\\mathbf{X}^{+}$ is equivalent to the OLSE and BLUE being equal definitely (always), not just with probability 1. The equivalence with probability 1 is given by condition (b), which is not the same as condition (e).",
    "question": "Is the condition $[\\mathbf{K},\\mathbf{0}][\\mathbf{X},\\Sigma\\mathbf{X}^{\\perp}]^{+}=\\mathbf{K}\\mathbf{X}^{+}$ equivalent to the statement that the OLSE and BLUE of $\\mathbf{K}\\beta$ are equal with probability 1?",
    "question_context": "Equivalence Conditions for OLSE and BLUE Estimators in Linear Models\n\nTheorem 3.1. Assume $\\mathbf{K}\\beta$ is estimable under (1.1), and let OLSE $\\mathcal{M}^{(\\mathbf{K}\\beta)}$ and BLUE $\\mathcal{M}^{(\\mathbf{K}\\beta)}$ be given as in (2.8) and (2.25), respectively. Then, the following statements are equivalent: (a) $\\operatorname{OLSE}_{\\mathcal{M}}(\\mathbf{K}\\beta)=\\operatorname{BLUE}_{\\mathcal{M}}(\\mathbf{K}\\beta)$ holds definitely. (b) $\\operatorname{OLSE}_{\\mathcal{M}}(\\mathbf{K}\\beta)=\\operatorname{BLUE}_{\\mathcal{M}}(\\mathbf{K}\\beta)$ holds with probability 1. (c) $D[\\operatorname{OLSE}_{\\mathcal{M}}(\\mathbf{K}\\pmb{\\beta})]=D[\\operatorname{BLUE}_{\\mathcal{M}}(\\mathbf{K}\\pmb{\\beta})]$ . (d) $\\mathbf{K}\\mathbf{X}^{+}\\pm\\mathbf{Z}\\mathbf{X}^{\\perp}=\\mathbf{0}$ . (e) $[\\mathbf{K},\\mathbf{0}][\\mathbf{X},\\Sigma\\mathbf{X}^{\\perp}]^{+}=\\mathbf{K}\\mathbf{X}^{+}$ . (f) $[{\\bf K},{\\bf0}][{\\bf X},{\\Sigma}{\\bf X}^{\\perp}]^{+}{\\Sigma}={\\o{\\mathrm{K}}{\\bf X}}^{+}{\\Sigma}.$ (g) $\\mathcal{R}[(\\mathbf{K}\\mathbf{X}^{+}\\Sigma)^{\\prime}]\\subseteq\\mathcal{R}(\\mathbf{X})$ . (h) $\\mathbf{X}^{\\perp}[\\pmb{\\Sigma}\\mathbf{X},\\mathbf{0}]\\mathbf{F}_{[\\mathbf{X}^{\\prime}\\mathbf{X},\\mathbf{K}^{\\prime}]}=\\mathbf{0}.$ (i) $\\mathcal{R}\\left[{\\bf{X^{\\prime}}\\Sigma\\Sigma}^{\\perp}\\right]\\subseteq\\mathcal{R}\\left[{\\bf{X^{\\prime}}X}\\right]$ (j) $\\begin{array}{r}{\\mathcal{R}\\left[\\begin{array}{c}{\\mathbf{K}^{\\prime}}\\\\{\\mathbf{0}}\\end{array}\\right]\\subseteq\\mathcal{R}\\left[\\begin{array}{c c}{\\mathbf{x}^{\\prime}\\mathbf{x}}&{\\mathbf{0}}\\\\{\\Sigma\\mathbf{X}}&{\\mathbf{X}}\\end{array}\\right]}\\end{array}$ (k) $r\\left[\\begin{array}{c c c}{\\mathbf{X}^{\\prime}\\mathbf{X}}&{\\mathbf{0}}&{\\mathbf{K}^{\\prime}}\\\\{\\Sigma\\mathbf{X}}&{\\mathbf{X}}&{\\mathbf{0}}\\end{array}\\right]=r\\left[\\begin{array}{c c c}{\\mathbf{X}^{\\prime}\\mathbf{X}}&{\\mathbf{0}}\\\\{\\Sigma\\mathbf{X}}&{\\mathbf{X}}\\end{array}\\right]=2r(\\mathbf{X}).$ (l) The matrix equation ${\\bf X U}_{1}+{\\bf U}_{2}[{\\bf X}^{\\prime}{\\bf X},~{\\bf K}^{\\prime}]=[~\\Sigma{\\bf X},~{\\bf0}]$ is solvable for $\\mathbf{U}_{1}$ and $\\mathbf{U}_{2}$ . $(\\mathrm{m})\\pmb{\\Sigma}$ can be decomposed as $\\pmb{\\Sigma}=(\\mathbf{I}_{n}-\\mathbf{J}^{+}\\mathbf{J})\\mathbf{W}^{\\prime}(\\mathbf{I}_{n}-\\mathbf{J}^{+}\\mathbf{J})$ for some matrix $\\mathbf{V}$ , where ${\\bf J}={\\bf I}{\\bf X}^{+}-[{\\bf K},{\\bf0}][{\\bf X},{\\bf\\Sigma}{\\bf X}^{\\perp}]^{+}.$ ."
  },
  {
    "qid": "statistic-posneg-1053-1-1",
    "gold_answer": "TRUE. The minimum variance is achieved when $\\bar{c} = \\bar{s} = 0$ because the variance $V = \\sigma^{2}/N D$ is minimized when $D = 1 - \\bar{c}^{2} - \\bar{s}^{2}$ is maximized, which occurs when $\\bar{c} = \\bar{s} = 0$ (i.e., $D = 1$). This condition implies that the data points are equally spaced around the circle, ensuring optimal estimation efficiency. The statistical implication is that experimental designs with equally spaced angular measurements yield the most precise parameter estimates.",
    "question": "Is the condition for the minimum variance of the estimators $\\hat{\\xi}, \\hat{\\eta}, \\hat{\\alpha}, \\hat{\\beta}$ achieved when $\\bar{c} = \\bar{s} = 0$? Explain the statistical implications of this condition.",
    "question_context": "Parameter Estimation for Circular Data with Known Angular Differences\n\nThe assumption that the angular differences $\\theta_{i}=\\tau_{i}-\\tau_{i-1}$ are known is equivalent to the representation $\\tau_{t}=\\theta_{0}+\\theta_{t},i=1,...,N,$ where $\\theta_{i}, i = 1, .. .,N,$ are known (with typically $\\theta_{1} = 0$), and $\\theta_{0}$ is unknown. Substituting into the model, one obtains $X_{i}=\\xi+\\alpha\\cos\\theta_{i}-\\beta\\sin\\theta_{i}+\\epsilon_{i},\\quad Y_{i}=\\eta+\\alpha\\sin\\theta_{i}+\\beta\\cos\\theta_{i}+\\delta_{i},i=1,...,N,$ where $\\alpha=\\rho\\cos\\theta_{0},\\quad\\beta=\\rho\\sin\\theta_{0}.$ The model is linear in the four parameters $\\xi,\\eta,\\alpha,\\beta$ and so one can obtain explicit maximum likelihood estimates for all parameters and an exact covariance matrix. The covariance matrix for $\\hat{\\xi},\\hat{\\eta},\\hat{\\alpha},\\hat{\\beta}$ is given by $V=\\sigma^{2}/N D$, where $D=1-\\bar{c}^{2}-\\overline{s}^{2}$. The normality of $(\\hat{\\xi},\\hat{\\eta})$ implies that the usual exact $100(1-\\gamma)\\%$ confidence ellipse for $(\\xi,\\eta)$ is always a circle, specifically $(\\hat{\\xi}-\\xi)^{2}+(\\hat{\\eta}-\\eta)^{2}=2\\hat{V}F_{\\gamma,2,2N-4},$ where $\\hat{V}=\\hat{\\sigma}^{2}/N D$ and $F_{\\gamma,2,2N-4}$ is the upper $100\\%$ point of the $F$ distribution with degrees of freedom 2 and $2N - 4$."
  },
  {
    "qid": "statistic-posneg-608-3-3",
    "gold_answer": "FALSE. Although this weight function leads to an acceptance probability of 1, it is not necessarily the most efficient choice. Other weight functions that favor proposing points further away from the current state might improve efficiency by exploring the state space more effectively.",
    "question": "Does the weight function $w(z,z^{\\prime})=\\tilde{\\pi}(z^{\\prime})$ always lead to the highest efficiency in the AAPS algorithm?",
    "question_context": "Detailed Balance and Efficiency in the AAPS Algorithm\n\nProposition 1. The AAPS algorithm satisfies detailed balance with respect to the extended posterior $\\tilde{\\pi}(x,p)$ . <PARAGRAPH_SEPARATOR> Proof. Step 1 preserves $\\tilde{\\pi}$ because $p$ is independent of $x$ and is sampled from its marginal. <PARAGRAPH_SEPARATOR> It will be helpful to define the system from the point of view of starting at $z^{\\mathrm{prop}}$ . Let $a^{\\prime},b^{\\prime}$ and $c^{\\prime}$ be as defined in (5), but with $z^{\\prime}=z^{\\mathrm{prop}}$ . Then, since $S_{\\#}(z^{\\mathrm{prop}};z^{\\mathrm{curr}})=-S_{\\#}(z^{\\mathrm{curr}};z^{\\mathrm{prop}})$ , <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{-c\\leq0\\leq K-c,-c\\leq S_{\\#}(z^{\\mathrm{prop}};z^{\\mathrm{curr}})\\leq K-c\\Leftrightarrow-c^{\\prime}}\\ &{\\quad\\leq S_{\\#}(z^{\\mathrm{curr}};z^{\\mathrm{prop}})\\leq K-c^{\\prime},-c^{\\prime}\\leq0\\leq K-c^{\\prime}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Equivalently, $1(c\\in\\{0,\\ldots,K\\})1(z^{\\mathrm{prop}}\\in S_{a:b}(z^{\\mathrm{curr}}))\\equiv1(c^{\\prime}\\in$ $\\{0,\\ldots,K\\})1(z^{\\mathrm{curr}}\\in S_{a^{\\prime}:b^{\\prime}}(z^{\\mathrm{prop}}))$ . Moreover, segment invariance (5) is equivalent to $z\\in S_{a:b}(z^{\\mathrm{curr}})\\iff z\\in S_{a^{\\prime}:b^{\\prime}}(z^{\\mathrm{prop}})$ . <PARAGRAPH_SEPARATOR> The resulting chain satisfies detailed balance with respect to $\\tilde{\\pi}$ because <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\tilde{\\pi}\\left(z^{\\mathrm{curr}}\\right)\\times\\mathbb{P}\\left(c\\right)}\\ &{\\quad\\quad\\times\\mathbb{P}\\left(\\mathrm{propose}z^{\\mathrm{prop}}|c\\right)\\times\\mathbb{P}\\left(\\mathrm{accept}z^{\\mathrm{prop}}|\\mathrm{proposed},c\\right)}\\end{array} $$ <PARAGRAPH_SEPARATOR> is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\tilde{\\pi}\\left(z^{\\mathrm{curr}}\\right)\\times\\frac{1\\left(c\\in\\{0,\\ldots,K\\}\\right)}{K+1}}\\ &{\\qquad\\times\\frac{w\\left(z^{\\mathrm{curr}},z^{\\mathrm{prop}}\\right)1\\left(z^{\\mathrm{prop}}\\in S_{a:b}\\left(z^{\\mathrm{curr}}\\right)\\right)}{\\sum_{z\\in S_{a:b}\\left(z^{\\mathrm{curr}}\\right)}w\\left(z^{\\mathrm{curr}},z\\right)}}\\ &{\\qquad\\times1\\wedge\\frac{\\tilde{\\pi}\\left(z^{\\mathrm{prop}}\\right)w\\left(z^{\\mathrm{prop}},z^{\\mathrm{curr}}\\right)\\sum_{z\\in S_{a:b}\\left(z^{\\mathrm{curr}}\\right)}w\\left(z^{\\mathrm{curr}},z\\right)}{\\tilde{\\pi}\\left(z^{\\mathrm{curr}}\\right)w\\left(z^{\\mathrm{curr}},z^{\\mathrm{prop}}\\right)\\sum_{z\\in S_{a:b}\\left(z^{\\mathrm{prop}}\\right)}w\\left(z^{\\mathrm{prop}},z\\right)},}\\end{array} $$ <PARAGRAPH_SEPARATOR> which is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\frac{1(c\\in\\{0,\\ldots,K\\})1(z^{\\mathrm{prop}}\\in{\\mathcal S}_{a:b}(z^{\\mathrm{curr}}))}{K+1}}\\ &{\\qquad\\times\\left\\{\\frac{\\tilde{\\pi}(z^{\\mathrm{curr}})w(z^{\\mathrm{curr}},z^{\\mathrm{prop}})}{\\sum_{z\\in{\\mathcal S}_{a:b}(z^{\\mathrm{curr}})}w(z^{\\mathrm{curr}},z)}\\wedge\\frac{\\tilde{\\pi}(z^{\\mathrm{prop}})w(z^{\\mathrm{prop}},z^{\\mathrm{curr}})}{\\sum_{z\\in{\\mathcal S}_{a:b^{\\prime}}(z^{\\mathrm{prop}})}w(z^{\\mathrm{prop}},z)}\\right\\}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> This expression is invariant to $(z^{\\mathrm{curr}},a,b,c)\\leftrightarrow(z^{\\mathrm{prop}},a^{\\prime},b^{\\prime},c^{\\prime})$ . 口 <PARAGRAPH_SEPARATOR> Remark 1. The AAPS could be applied with a numerical integration scheme that does not have a Jacobian of 1. In such a scheme, however, the Jacobian would need to be included in the acceptance probability; moreover, a Jacobian other than 1 would lead to greater variability in $\\tilde{\\pi}$ along a path and, we conjecture, to a reduced efficiency. <PARAGRAPH_SEPARATOR> The path $S_{a:b}$ may visit parts of the statespace where the numerical solution to (2) is unstable and the error in the Hamiltonian may increase without bound, leading to wasted computational effort as large chunks of the path may be very unlikely to be proposed and accepted. Indeed it is even possible for the error to increase beyond machine precision. The no Uturn sampler suffers from a similar problem and we introduce a similar stability condition to that in Hoffman and Gelman (2014). We require that <PARAGRAPH_SEPARATOR> $$ \\operatorname*{max}_{z\\in S_{a:b}(z^{\\mathrm{curr}})}H(z)-\\operatorname*{min}_{z\\in S_{a:b}(z^{\\mathrm{curr}})}H(z)<\\Delta, $$ <PARAGRAPH_SEPARATOR> for some prespecified parameter $\\Delta$ . This criterion can be monitored as the forward and backward integrations proceed, and if at any point the criterion is breached, the path is thrown away: the proposal is automatically rejected. Segment invariance means that $z~\\in~S_{a:b}(z^{\\operatorname{curr}})\\quad\\Longleftrightarrow\\quad z~\\in~S_{a^{\\prime}:b^{\\prime}}(z^{\\operatorname{prop}})$ , so the same rejection would have occured if we created the path from any proposal in $S_{a:b}(z^{\\mathrm{curr}})$ , and detailed balance is still satisfied. For the experiments in Section 4 we found that a value of $\\Delta=1000$ was sufficiently large that the criterion only interfered when something was seriously wrong with the integration. Step 3 of the algorithm then becomes: <PARAGRAPH_SEPARATOR> 3. Create $S_{a:b}$ by leapfrog stepping forward from $(x_{0},p_{0})$ and then backward from $(x_{0},p_{0})$ . If condition (7) fails then go to 6. <PARAGRAPH_SEPARATOR> For a $d$ -dimensional Markov chain, $\\{X_{t}\\}_{t=0}^{\\infty}$ , with a stationary distribution of $\\pi$ , the asymptotic variance is $\\begin{array}{r}{V_{f}:=\\operatorname*{lim}_{n\\uparrow\\infty}}\\end{array}$ nvar $\\textstyle\\left[{\\frac{1}{n}}\\sum_{i=1}^{n}f(X_{i})\\right]$ . Thus, after burn in, var $\\begin{array}{r l}{\\left[{\\frac{1}{n}}\\sum_{i=1}^{n}f(X_{i})\\right]}&{{}\\approx}\\end{array}$ $\\mathrm{{\\itV}}_{f}/n$ . The effective sample size is the number of iid samples from $\\pi$ that would lead to the same variance: $\\mathrm{ESS}_{f}=n\\mathrm{var}_{\\pi}\\left[f\\right]/V_{f}$ . Let ${\\mathrm{ESS}}_{i}$ be an empirical estimate of the ESS for the ith component of $X$ (i.e., $f$ gives the ith component) and let $n_{\\mathrm{leap}}$ be the total number of leapfrog steps taken during the run of the algorithm. We measure the efficiency of an algorithm as <PARAGRAPH_SEPARATOR> $$ \\mathrm{Eff}=\\frac{1}{n_{\\mathrm{leap}}}\\operatorname*{min}_{i=1,\\ldots,d}\\mathrm{ESS}_{i}. $$ <PARAGRAPH_SEPARATOR> Since the leapfrog step is by far the most computationally intensive part of the algorithm, Eff is proportional to the number of effective iid samples generated per second in the worst mixing component. <PARAGRAPH_SEPARATOR> # 3.3. Choice of Weight Function <PARAGRAPH_SEPARATOR> The weight function $\\begin{array}{r}{\\ w(z,z^{\\prime})=\\tilde{\\pi}(z^{\\prime})}\\end{array}$ mentioned previously is a natural choice, and substitution into (6) shows that this leads to an acceptance probability of 1; however, it turns out not to be the most efficient choice. For example, intuitively the algorithm might be more efficient if points on the path that are further away from the current point are more likely to be proposed. To investigate the effects of the choice of $w$ we examine six possibilities:"
  },
  {
    "qid": "statistic-posneg-871-1-0",
    "gold_answer": "FALSE. The stratified Cox model explicitly allows the baseline hazard function $\\lambda_{0i}(t)$ to differ across strata (i=1,2,3,4 in this case), while assuming the regression coefficients $\\beta$ are constant across strata. This is evident from the formula $\\lambda_{i}(t;z)=\\lambda_{0i}(t)\\exp{(\\beta^{\\mathrm{T}}z)}$, where the subscript i on $\\lambda_{0i}(t)$ indicates stratum-specific baseline hazards. The stratification was necessary here because age violated the proportional hazards assumption, and the baseline hazards were estimated separately for each age stratum.",
    "question": "Is the statement 'The stratified Cox model assumes that the baseline hazard function $\\lambda_{0i}(t)$ is the same across all strata' true or false?",
    "question_context": "Stratified Cox Proportional Hazards Model for Survival Analysis\n\nA failure is defined here as either a non-fatal recurrence of the disease or a death due to carcinoma of the vulva. The censored cases are those who did not have a recurrence of the disease; either they died from another cause or they were alive at the end of the study. The survival time thus corresponds to the disease-free interval defined above. Of the 408 study patients, there were 174 failures (149 cancer deaths and 25 recurrences) and 234 censored cases (144 non-cancer deaths and 90 alive at the end of the study). <PARAGRAPH_SEPARATOR> Five pre-operative covariates were available: age at referral $(z_{1})$ menses ${\\displaystyle(z_{2})}$ ,tumoursize $(z_{3})$ clinical node involvement $\\left(z_{4}\\right)$ and secondary spread $(z_{5})$ Tumour size was recorded as the product of longest and smallest dimension of the tumours. Menses was coded 0 or 1 for pre- or post-menopausal respectively. Clinical node involvement was scored 0 or 1 for impalpable or palpable groin nodes respective.y. Secondary spread was scored 0 or 1 for the absence or presence of secondary spread to the surrounding organs. <PARAGRAPH_SEPARATOR> With age ranges as wide as those here (45-90 years) it is often found that age as a covariate does not satisfy the proportional hazards model (1). To allow for the effect of age a simple extension of model (1) is required using age strata described by Kalbfeisch and Prentice (1980, p. 87). This device was found to be necessary so the patients were divided into four age groups: Stratum 1: 45-54 years, Stratum 2: 55-64 years, Stratum 3: 65-69 years, Stratum 4: 70 years or over, containing 51, 115, 106 and 136 patients respectively. These strata were chosen to give both adequate sample size and roughly equal mortality expectation within the strata. Cox's model (1) amended to allow for strata is then <PARAGRAPH_SEPARATOR> $$ \\lambda_{i}(t;z)=\\lambda_{0i}(t)\\exp{(\\beta^{\\mathrm{T}}z)},\\quad i=1,2,3,4, $$ <PARAGRAPH_SEPARATOR> where the parameters $\\beta$ are assumed to take the same values in all four strata but the function $\\lambda_{0i}(.)$ is estaimted separately in each stratum. To estimate $\\beta$ we use Cox's (1972) conditional likelihood with Peto's (1972) approximation to handle tied observations and for $\\lambda_{0i}(.)$ weuse the smooth estimates proposed by ANSN80. <PARAGRAPH_SEPARATOR> A step-wise forward procedure was used to select a good set of predictors. The yard-stick for inclusion or exclusion was based on the asymptotic chi-square distribution of twice the change in the above log-likelihood on entering a new predictor variable. The final model was restricted to the two covariates, clinical node involvement $\\left(z_{4}\\right)$ and secondary spread $(z_{5})$ The parameter estimates and the estimated standard errors are given in Table 1. Since the variables $z_{4}$ and $z_{5}$ are binary they define four prognostic categories: <PARAGRAPH_SEPARATOR> (i) impalpable nodes and no secondary spread;   (i) palpable nodes and no secondary spread;   (ii) impalpable nodes and secondary spread;   (iv) palpable nodes and secondary spread."
  },
  {
    "qid": "statistic-posneg-1408-0-1",
    "gold_answer": "TRUE. The term \\(\\frac{L_{T}^{X}(\rho^{\\ast\\mathrm{new}})L_{T}^{N}(\\mu^{\\mathrm{new}})}{L_{T}^{X}(\rho^{\\ast\\mathrm{old}})L_{T}^{N}(\\mu^{\\mathrm{old}})}\\) is the ratio of the likelihoods under the proposed and current configurations, which is a key component of the Metropolis-Hastings acceptance probability. This ensures that the Markov chain satisfies detailed balance and converges to the correct stationary distribution.",
    "question": "The condition for accepting a new marked point \\((\tau_{M+1}, \\psi_{M+1}, d_{M+1})\\) in the infection process includes the term \\(\\frac{L_{T}^{X}(\rho^{\\ast\\mathrm{new}})L_{T}^{N}(\\mu^{\\mathrm{new}})}{L_{T}^{X}(\rho^{\\ast\\mathrm{old}})L_{T}^{N}(\\mu^{\\mathrm{old}})}\\). Does this term ensure that the Metropolis-Hastings acceptance probability maintains detailed balance?",
    "question_context": "MCMC Algorithm for Infection Process Modeling\n\nThe paper describes an MCMC algorithm for updating the infection process, including the addition or deletion of infection points with associated marks. The likelihood processes for antibodies and AOM counting are defined, and detailed update steps for the infection process are provided, including conditions for compatibility with data and proposal distributions for durations and marks."
  },
  {
    "qid": "statistic-posneg-1494-3-0",
    "gold_answer": "FALSE. The sequential ranks $r_{i}^{+}$ are invariant under scale changes, meaning the CUSUM cannot detect changes from a specified value of $\\sigma$. It can only detect changes away from the current value of $\\sigma$, whatever it may be, and only if the change occurs after a sufficiently long time lapse $\\tau>2$.",
    "question": "The $W^{2}$-CUSUM based on the statistic $\\xi_{i}=\\frac{6(r_{i}^{+})^{2}}{(2i+1)(i+1)}-1$ can detect changes in dispersion from a known, specified value of $\\sigma$. True or False?",
    "question_context": "Sequential Rank CUSUM for Dispersion Monitoring\n\nWhile the in-control properties of the SSR CUSUM do not depend upon the variability of the underlying data, its proper application does require the variability to remain unchanged. Suppose that after $\\tau>0$ observations there is a change to a distribution with density $g(x)=f(x/\\sigma)/\\sigma,\\sigma\\neq1.$ . Then $\\sigma$ is the fraction by which the current, unknown, dispersion changes. To detect an increase in dispersion, one can use a CUSUM based on the scores $J^{2}(u),$ , thus eliminating the effect of the sign of $X$ . With the Wilcoxon score, the corresponding sequential rank statistic to be used in (1) is then $$\\xi_{i}=\\frac{6(r_{i}^{+})^{2}}{(2i+1)(i+1)}-1,$$ which has zero in-control expected value. The corresponding CUSUM will be referred to as the $'W^{2}$ -CUSUM''. Since the sequential ranks $r_{i}^{+}$ are invariant under scale changes, it is clear that a CUSUM based on them cannot detect changes from a specified value of $\\sigma$ . Only changes away from the current value of $\\sigma$ , whatever it may be, will be detectable, and this only if the change occurs after a sufficiently long time lapse $\\tau>2$ . Furthermore, the effect of the pre-change value of the scale parameter becomes negligible as observations continue to accrue after a change to a different value. Thus, the CUSUM will eventually return to a nominally in-control state after a change has occurred. This behaviour is similar to that of self-starting CUSUMs, and is a warning to users of the need for corrective action as soon as a change is diagnosed — see Hawkins and Olwell (1997, Section 7.1)."
  },
  {
    "qid": "statistic-posneg-454-0-1",
    "gold_answer": "FALSE. In the SVM-RFE method, the variable to be removed at each step is the one with the smallest value of wᵢ², as this variable is considered the least important for the classification task.",
    "question": "Is the statement 'In the SVM-RFE method, the variable to be removed at each step is the one with the largest value of wᵢ²' true or false?",
    "question_context": "Kernel Fisher Discriminant Analysis and Variable Selection\n\nThe paper discusses Kernel Fisher Discriminant Analysis (KFDA) and its application to variable selection. The KFDA classifier is given by the sign of a discriminant function involving a kernel function. The paper also introduces recursive feature elimination (RFE) for variable selection in support vector machines (SVMs), comparing different criteria for variable elimination."
  },
  {
    "qid": "statistic-posneg-186-0-0",
    "gold_answer": "FALSE. While the statement about the form of $\\mu(X)$ is true for the logistic single-index model, the conditional independence $Y \\perp X \\mid \\beta^{\\top}X$ is not directly implied by this specification alone. The conditional independence is a stronger condition that defines the central subspace $S_{Y\\mid X} = \\operatorname{span}(\\beta)$, which is assumed in the context but not guaranteed by the logistic link function form. The logistic form specifies the conditional mean of $Y$ given $X$, but additional assumptions about the error structure are needed to ensure full conditional independence.",
    "question": "In the logistic single-index model, the probability $\\mu(X)$ is correctly specified as $\\mu(X)=\\frac{1}{1+\\exp\\{-h(\\beta^{\\top}X)\\}}$. Is this statement true, and does it imply that the response $Y$ is conditionally independent of $X$ given $\\beta^{\\top}X$?",
    "question_context": "Sufficient Dimension Reduction in Single-Index Models\n\nRegression analysis is widely used to study the relationship between a response variable $Y$ and a $p\\times1$ predictor vector $X$ . In practice, this relationship often depends solely on a few linear combinations $\\beta^{\\intercal}\\boldsymbol{X}$ of $X$ , where $\\beta$ is a $p\\times d$ matrix with $d<p$ . For instance, a single-index model (Hardle et al., 1993) involves only one linear combination of the predictors, and has the form $$\\boldsymbol{Y}=f(\\boldsymbol{\\beta}^{\\intercal}\\boldsymbol{X})+\\boldsymbol\\varepsilon,$$ where $d=1$ , $\\|\\beta\\|=1$ , $f$ is an unknown link function, and $\\varepsilon$ is an error term independent of $X$ with its distribution unspecified. Other well-known models with $d=1$ include the heteroscedastic model, $$Y=f(\\beta^{\\intercal}X)+g(\\beta^{\\intercal}X)\\varepsilon,$$ and the logistic single-index model, $$Y\\mid X\\sim\\operatorname{Ber}\\{\\mu(X)\\},\\mathrm{with}\\mu(X)=\\frac{1}{1+\\exp\\{-h(\\beta^{\\top}X)\\}},$$ where $g$ and $h$ are unknown smooth link functions. Sufficient dimension reduction methods estimate linear combinations $\\beta^{\\intercal}\\boldsymbol{X}$ that convey full regression information. The methodology stems from considering dimension reduction subspaces $S\\subset\\mathbb{R}^{p}$ , which are subspaces with the property $Y\\perp X\\mid P_{S}X$ , where $P_{(\\cdot)}$ denotes a projection operator with respect to the standard inner product. Under mild conditions (Cook, 1996) the intersection of all dimension reduction subspaces is itself a dimension reduction subspace, and is then called the central subspace $S_{Y\\mid X}$ (Cook, 1998, p.105). By definition, $S_{Y\\mid X}$ is a parsimonious population parameter that contains full information about the regression of $Y$ on $X$ , and thus is our main object of interest. We assume the existence of $S_{Y\\mid X}$ throughout this article, and let $d=\\dim(S_{Y\\mid X})$ . For models (1), (2) and (3), $S_{Y\\mid X}=\\operatorname{span}(\\beta)$ with $d=1$ . Commonly used pioneering model-free methods for estimating a basis of $S_{Y\\mid X}$ include ordinary least squares (Li & Duan, 1989) when $d=1$ , sliced inverse regression (Li, 1991) and sliced average variance estimation (Cook & Weisberg, 1991). Unfortunately, all of those methods require the number of observations $n$ to be greater than $p$ , and this limits applicability. By contrast, the method of partial least squares (Wold, 1975) does not have this limitation. Background about partial least squares estimator can be found in Helland (1988, 1990, 1992), Næs & Helland (1993), Garthwaite (1994) and Stone & Brooks (1990)."
  },
  {
    "qid": "statistic-posneg-1543-1-2",
    "gold_answer": "FALSE. The context states that the power of the proposed test converges to unity as $d \\to \\infty$ if either $\\nu^{2} > 0$ or $\\sigma_{1}^{2} \\neq \\sigma_{2}^{2}$. It does not require both conditions to be satisfied simultaneously for the power to converge to unity.",
    "question": "Does the power of the proposed test converge to unity as $d \\to \\infty$ only if $\\nu^{2} > 0$ and $\\sigma_{1}^{2} \\neq \\sigma_{2}^{2}$ simultaneously?",
    "question_context": "High-Dimensional Two-Sample Run Test: Assumptions and Convergence Properties\n\nWe assume m and $n$ to be fixed and study the power functions of the run tests as $d\\to\\infty$ . In usual large-sample asymptotics, we assume $d$ to be fixed and expect to gain more information about the separation of $F$ and $G$ as the sample sizes increase. Here, however, we consider the sample sizes to be fixed and expect to get more information as the dimension increases. We have m independent observations on $X=(\\bar{X}^{1},\\ldots,X^{d})^{\\mathrm{T}}$ from $F$ and $n$ independent observations on $Y=(\\bar{Y^{1}},\\dots,Y^{d})^{\\mathrm{T}}$ from $G$ , and the observations on $X$ and $Y$ are independent. Following Hall et al. (2005), we consider the assumptions given below. <PARAGRAPH_SEPARATOR> Assumption 1. The fourth moments of $X^{q}$ and $Y^{q}$ are uniformly bounded for $q\\geqslant1$ . <PARAGRAPH_SEPARATOR> Assumption 2. Let $X_{1}$ and $X_{2}$ be two independent copies of $X$ , and let $Y_{1}$ and $Y_{2}$ be two independent copies of Y . Under some permutation of the $X^{q}$ and the same permutation of the $Y^{q}$ , for $(U^{q},V^{\\bar{q}})=(X_{1}^{q},X_{2}^{q}),$ $(X_{1}^{q},Y_{1}^{q})$ or $(Y_{1}^{q},Y_{2}^{q})$ we have <PARAGRAPH_SEPARATOR> $$ \\operatorname*{sup}_{|q_{1}-q_{2}|>r}\\left|\\operatorname{corr}\\{(U^{q_{1}}-V^{q_{1}})^{2},(U^{q_{2}}-V^{q_{2}})^{2}\\}\\right|\\leqslant\\rho(r) $$ <PARAGRAPH_SEPARATOR> where $\\rho(r)\\to0$ as $r\\rightarrow\\infty$ . <PARAGRAPH_SEPARATOR> Assumption 3. There exist $\\sigma_{1}^{2}$ , $\\sigma_{2}^{2}>0$ and $\\nu^{2}$ such that: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{d^{-1}\\sum_{q=1}^{d}\\{E(X^{q})-E(Y^{q})\\}^{2}\\rightarrow\\nu^{2};}\\ &{d^{-1}\\sum_{q=1}^{d}\\operatorname{var}(X^{q})\\rightarrow\\sigma_{1}^{2};\\mathrm{and}}\\ &{d^{-1}\\sum_{q=1}^{d}\\operatorname{var}(Y^{q})\\rightarrow\\sigma_{2}^{2}\\mathrm{as}d\\rightarrow\\infty.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Hall et al. (2005) treated $d$ -dimensional observations as infinite time series truncated at time $d$ and studied the behaviour of the interpoint distances as $d$ increases. Here we look at these observations from a multivariate data perspective, so we make some minor modifications to the assumptions of Hall et al. (2005), in particular Assumption 2. Ahn et al. (2007) studied the geometry of high-dimension, low-sample-size data under similar assumptions. Jung & Marron (2009) <PARAGRAPH_SEPARATOR> assumed almost the same set of conditions for the high-dimensional consistency of estimated principal component directions. <PARAGRAPH_SEPARATOR> Under Assumptions 1 and 2, the weak law holds for the sequence $\\{(U^{q}-V^{q})^{2}\\}$ . The proof is straightforward, so it is omitted. Again, depending on the choice of $(U^{q},V^{q})=$ $(X_{1}^{q},X_{2}^{q})$ , $(X_{1}^{\\bar{q}},Y_{1}^{q})$ or $(Y_{1}^{q},Y_{2}^{q})$ , under Assumption $\\begin{array}{r}{\\hat{3}d^{-1}\\check{\\sum}_{q=1}^{d}E(U^{q}-V^{q})^{2}}\\end{array}$ converges to $2\\sigma_{1}^{2}$ , $\\sigma_{1}^{2}+\\sigma_{2}^{2}+\\nu^{2}$ or $2\\sigma_{2}^{2}$ as $d$ tends to infinity. So, under Assumptions 1–3, the pairwise distance between any two independent observations, when divided by $\\hat{d}^{1/2}$ , converges in probability to a positive constant. If both observations are from the same distribution, then depending on whether they are from $F$ or $G$ this scaled distance converges to $\\sigma_{1}\\sqrt{2}$ or $\\sigma_{2}\\sqrt{2}$ . If one of the observations is from F and the other is from G, the distance converges to (σ 12 + σ 22 + ν2)1/2. However, if the components of $X$ and $Y$ are independent and identically distributed, as for the normal examples in $\\S2\\cdot4$ , we only require the existence of second-order moments of the component variables for these convergence results. In this case, we do not need Assumption 1, while Assumptions 2 and 3 hold automatically under the existence of second-order moments. Under Assumptions 1–3, if we have $\\nu^{2}>0$ or $\\sigma_{1}^{2}\\neq\\sigma_{2}^{2}$ , the power of the proposed test converges to unity as the dimension increases."
  },
  {
    "qid": "statistic-posneg-1970-0-2",
    "gold_answer": "FALSE. The term $I[\\cap_{i\\sim j}\\{-\\beta_{i j}I[x_{i}=x_{j}]<u_{i j}\\}]$ induces clustering, but the cluster colors are not independent of each other. The dependence on neighboring colors $\\mathbf{x}_{\\partial\\mathbf{C}}$ necessitates sequential updating, as explained in the context.",
    "question": "The term $I[\\cap_{i\\sim j}\\{-\\beta_{i j}I[x_{i}=x_{j}]<u_{i j}\\}]$ in the Potts model posterior ensures that cluster colors are independent of each other.",
    "question_context": "Markov Chain Monte Carlo (MCMC) Estimation and Potts Model Sampling\n\nThe idea that the multivariate exploratory data analysis toolkit has something to offer to Bayesian statistics as well as the recent use by Draper et al. (1993) of exchangeability ideas in an approach to developing a theory of data analysis show a remarkable continuity in statistical ideas which may be seen as an extension to Good's views on a Bayes-non-Bayes compromise and Box's statistical ecumenism. <PARAGRAPH_SEPARATOR> Xiao-liang Han (University of Bristol): When we estimate the integrated autocorrelation time of a Markov chain Monte Carlo (MCMC) algorithm by any of the spectral window estimators commonly used in the analysis of stationary time series, we might decide on the window widths, either in the time domain or in the frequency domain depending on which window estimator we adopt, by an approach which is close to optimal for our specific purpose, i.e. we are only interested in estimating the spectral densityatfrequencyO. <PARAGRAPH_SEPARATOR> The estimators of $\\tau(f)$ based on spectral windows have the general form <PARAGRAPH_SEPARATOR> $$ \\hat{\\tau}(f)=\\sum_{t=-(N-1)}^{N-1}\\lambda_{N}(t)\\hat{\\rho}_{f}(t) $$ <PARAGRAPH_SEPARATOR> where $\\lambda_{N}(t)$ is the window function. In typical cases, the asymptotic mean-square error (MSE) of $\\hat{\\tau}(f)$ is a function of $M,\\tau(f)$ and $\\tau^{(1)}(f)$ or $\\tau^{(2)}(f)$ ,where $M$ is the window width, <PARAGRAPH_SEPARATOR> $$ \\tau^{(1)}(f)\\equiv\\sum_{t=-\\infty}^{\\infty}|t|\\rho_{f}(t),\\tau^{(2)}(f)\\equiv\\sum_{t=-\\infty}^{\\infty}|t|^{2}\\rho_{f}(t). $$ <PARAGRAPH_SEPARATOR> Consider the fact that, while a window estimator is a consistent estimator of $\\tau(f)$ ,it can also be used as a consistent estimator of $\\tau^{(1)}(f)$ and $\\tau^{(2)}(f)$ under certain conditions; we suggest an EM-like algorithm to estimate $\\tau(f)$ by window estimators as follows: <PARAGRAPH_SEPARATOR> (a) calculate $\\hat{\\rho}_{f}(t),t=1,2,...,N-1$ , by fast Fourier transformation;   (b) start from a small $M$   (c) use the window estimator to estimate ${\\hat{\\tau}}(f),{\\hat{\\tau}}^{(1)}(f)$ or $\\hat{\\tau}^{(2)}(f)$ under the current value of $M$ (d) calculate $\\hat{M}_{\\operatorname*{min}}$ , which minimizes the MSE under the current $\\hat{\\tau}(f)$ and $\\hat{\\tau}^{(1)}(f)$ or $\\hat{\\tau}^{(2)}(f)$ (e) unless $\\hat{M}_{\\mathrm{min}}$ is sufficiently close to $M.$ replace $M$ by $\\hat{M}_{\\mathrm{min}}$ and go back to step (c); otherwise print out $\\widehat{\\tau}(f)$ and stop. <PARAGRAPH_SEPARATOR> Our experiment shows that this approach works well in common cases especially when $\\tau(f)$ is large. <PARAGRAPH_SEPARATOR> David Higdon (University of Washington, Seattle): It seems that multimodality in posterior distributions based on Potts priors (Besag and Green, Section 4) does not always yield to the Swendsen-Wang algorithm. In describing an alternative, suppose that the posterior is written as <PARAGRAPH_SEPARATOR> $$ p(\\mathbf{x})\\propto\\exp\\left(\\sum_{i}\\alpha_{i}(x_{i})+\\sum_{i\\sim j}\\beta_{i j}I[x_{i}=x_{j}]\\right)\\qquad\\mathrm{for~}\\mathbf{x}\\in\\{1,\\dots,r\\}^{n}, $$ <PARAGRAPH_SEPARATOR> where the $\\alpha_{i}$ depend also on the fixed data $\\mathbf{y}$ . Let u denote an auxiliary bond variable with one component ${\\pmb u}_{i j}$ for each neighbour pair $(i,j)$ , and define the $u_{i j}$ to be conditionally independent with <PARAGRAPH_SEPARATOR> $$ p(u_{i j}|\\mathbf{x})\\propto\\exp\\left\\{-\\frac{1}{\\uplambda_{i j}}(u_{i j}+\\beta_{i j}I[x_{i}=x_{j}])\\right\\}I[\\mathrel{-\\beta_{i j}}I[x_{i}=x_{j}]<u_{i j}], $$ <PARAGRAPH_SEPARATOR> where the $\\lambda_{i j}$ are as yet arbitrary. Then <PARAGRAPH_SEPARATOR> $$ p(\\mathbf{x}|\\mathbf{u})\\propto\\exp\\left\\{\\sum_{i}\\alpha_{i}(x_{i})+\\sum_{i=j}\\left(1-\\frac{1}{\\uplambda_{i j}}\\right)\\beta_{i j}I[x_{i}=x_{j}]\\right\\}I\\left[\\bigcap_{i=j}\\large\\{-\\beta_{i j}I[x_{i}=x_{j}]<u_{i j}\\}\\right]. $$ <PARAGRAPH_SEPARATOR> The term $I[\\cap_{i\\sim j}\\{-\\beta_{i j}I[x_{i}=x_{j}]<u_{i j}\\}]$ induces clustering as in the Swendsen-Wang algorithm. However, the cluster colours are no longer independent of one another. Now, consider each cluster $\\mathbf{c}$ in turn and give it colour $\\pmb{k}$ withprobability <PARAGRAPH_SEPARATOR> $$ \\jmath(k|\\mathbf{x}_{\\partial\\mathbf{C}},\\mathbf{u})\\propto\\exp\\left\\{\\sum_{i\\in\\mathbf{C}}\\alpha_{i}(k)+\\sum_{\\substack{\\{i,-j:i\\in\\mathbf{C},j\\in\\partial\\mathbf{C}\\}}}\\left(1-\\frac{1}{\\lambda_{i j}}\\right)\\beta_{i j}I[x_{j}=k]\\right\\}\\qquad\\mathrm{for}~k\\in\\{1,,...,r\\} $$ <PARAGRAPH_SEPARATOR> noting the dependence on the colours $\\mathbf{x}_{\\partial\\mathbf{C}}$ of its neighbours, and hence the need for sequential updating. Thus, adjacent, like coloured, pixels are bonded with probability $\\mathbf{l}-\\mathsf{e x p}(-\\beta_{i j}/\\lambda_{i j})$ and each cluster is then updated according to its conditional distribution. When all the $\\lambda_{i j}$ are 1, this is the SwendsenWang algorithm, and when all the $\\lambda_{i j}$ are very large, it is essentially Gibbs sampling. <PARAGRAPH_SEPARATOR> As an ilustration of the new algorithm, an artificial, bimodal, posterior surface based on the archaeology example (Besag et al. (1991), section 3) is constructed on a $16\\times16$ binary array, with second. order neighbourhoods. The surface can be given two main modes by inflating the variance. The first is a nearly all-white image, containing about $70\\%$ of the probability. The second is an image which resembles the original restoration. Among several plausible methods of choosing the $\\lambda_{i j}$ to facilitate mode swapping, one simple scheme is to choose $\\lambda_{i j}=1+\\lvert y_{i}-y_{j}\\rvert$ . This encourages clustering of pixels with similar data values and makes the chain jump modes about 1 in 100 iterations, compared with about 1 in 1000 for Gibbs sampling and 1 in 700 for the Swendsen-Wang algorithm. <PARAGRAPH_SEPARATOR> John E. Kolassa and Martin A. Tanner (University of Rochester): The authors are to be commended for their review of the state of the art of Markov chain Monte Carlo methods to implement the Bayesian paradigm. We briefly illustrate how the Gibbs sampler can be used by frequentists who prefer to eliminate nuisance parameters via conditioning, rather than via integration (as championed by Bayesians and likelihoodists). The classically oriented reader may also be interested in the work of Gelfand and Carlin (1991) and Geyer and Thompson (1992) who illustrate how these methods can be used to locate the modes of the likelihood (or posterior). Besag and Clifford (1989) discuss the application of Markov chain methods to the problem of significance testing. Assuming that we can devise an ergodic Markov chain whose equilibrium distribution is the same as that of the sufficient statistic, and whose transition probabilities are easy to sample from, Besag and Clifford (1989, 1991) suggest methods for computing $p$ -values."
  },
  {
    "qid": "statistic-posneg-1239-0-1",
    "gold_answer": "FALSE. The continuity of $x\\rightarrow c_{x}$ alone does not directly imply the invariance property $\\eta_{(\\exp{t A})x}=\\eta_{x}$. This property is derived from additional conditions and transformations involving the measures $\\eta_{x}^{\\prime}$ and $\\eta_{x}$, as well as the specific form of the operator $A$ and its action on the measures.",
    "question": "Does the continuity of the mapping $x\\rightarrow c_{x}$ imply that $\\eta_{(\\exp{t A})x}=\\eta_{x}$ for all $x\\in V^{*}$?",
    "question_context": "Operator Stability and Semistability in Measure Theory\n\n(i) Obviously we have $\\eta_{(\\mathrm{exp}t A)x}^{\\prime}=e^{t}\\cdot\\eta_{x}^{\\prime}$ and hence $c_{({\\tt e x p t A})x}=e^{t}c_{x}$ (i) $x\\rightarrow c_{x}$ is a continuous mapping of $V^{*}$ into $\\mathbb{R}_{+}^{*}$\n\n[Choose $\\delta>0$ such that $\\begin{array}{r}{\\mathbb{R}\\mathfrak{e}\\mathfrak{\\alpha}-\\frac{1}{2}>\\delta}\\end{array}$ for   all ${\\mathfrak{x}}\\in{\\mathfrak{S p e c}}(A)$ Then lim $\\|e^{-n(A-I/2)}\\|^{1/n}~=~\\rho{\\big(}e^{-(A-I/2)}{\\big)}<e^{-{\\bar{\\delta}}}$ . Hence there exists some $\\pmb{n}_{0}\\in\\mathbb{N}$ such that ${\\|e^{\\sim n(A-I/2)}\\|}<e^{-n\\delta}$ for all $n\\geqslant n_{0}$ . Put $c:=\\exp\\|A-I/2\\|$\n\nFor  every $t\\in[n_{0},\\infty[$ there exists some $n\\in\\mathbb{N}$ ， $n\\geqslant n_{0}$ ，such that $n-1<t\\leqslant n$ . Consequently\n\n$$\n\\big\\|e^{-t(A-I/2)}\\big\\|\\leqslant e^{(n-t)\\|A-I/2\\|}\\|e^{-n(A-I/2)}\\|\\leqslant c e^{-n\\delta}\\leqslant c e^{-t\\delta}.\n$$\n\nMoreover  we  have $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow-\\infty}\\Vert e^{t A}\\Vert=0}\\end{array}$ and $\\varphi(x)\\leqslant\\operatorname*{min}\\{1,|x|_{2}^{2}\\}$ for all $x\\in V^{*}$ . Hence for every compact subset $K$ of $V^{\\ast}$ and for every $\\varepsilon>0$ there exists some $N\\in\\mathbb{N}$ ， $N\\geqslant n_{0}$ , such that\n\n$$\n\\begin{array}{r}{\\int_{\\{t\\}>N}\\varphi(e^{t A}x)e^{-t}d t<\\varepsilon\\qquad\\mathrm{for~all~}x\\in K.}\\end{array}\n$$\n\nNow let $(x_{m})_{m\\geqslant1}$ be a sequence in $V^{*}$ converging to $x\\in V^{*}$ .Applying the observation above to the compact set $K=\\left\\{x,x_{1},x_{2},...\\right\\}$ we arrive at\n\n$$\n|c_{x_{m}}-c_{x}|<2\\varepsilon+\\int_{-N}^{N}|\\varphi(e^{t A}x_{m})-\\varphi(e^{t A}x)|e^{-t}d t\n$$\n\nfor all $m\\in\\mathbb{N}$ .Since $\\varphi$ is  continuous  we  have $\\overline{{\\operatorname*{lim}}}|c_{x_{m}}-c_{x}|<2\\varepsilon$ by Lebesgue's convergence theorem. Hence our assertion.]\n\n(ii) $x\\to\\eta_{x}^{\\prime}$ is a (vaguely) continuous mapping of $V^{*}$ into $\\mathfrak{M}_{+}(V^{*})$\n\n[Let $f\\in\\Re(V^{*})$ and let $(x_{m})_{m\\geqslant1}$ be a sequence in $V^{*}$ converging to $x_{0}\\in V^{*}$ . (Recall that $\\mathfrak{M}_{+}(V^{*})$ is (vaguely) metrizable.) Since $\\left\\{x_{0},x_{1},x_{2},\\dots\\right\\}$ is a compact subset of $V^{*}$ we have\n\n$$\n\\operatorname*{lim}_{t\\to-\\infty}\\operatorname*{sup}\\{e^{t A}x_{m}\\|\\colon m\\in\\mathbb{Z}_{+}\\}=0\n$$\n\nand\n\n$$\n\\underset{t\\rightarrow\\infty}{\\operatorname*{lim}}\\operatorname*{inf}\\{\\|e^{t A}x_{m}\\|\\colon m\\in\\mathbb{Z}_{+}\\}=\\infty.\n$$\n\nConsequently since $f$ has compact support there exists some $N\\in\\mathbb{N}$ such that $f(e^{t A}x_{m})=0$ for all $m\\in\\mathbb{Z}_{+}$ and $t\\in\\mathbb{R}$ such that $|t|>N$ Hence\n\n$$\n\\int f d\\eta_{x_{m}}^{\\prime}=\\int_{-N}^{N}f{\\left(e^{t A}x_{m}\\right)}e^{-\\iota}d t\\qquad{\\mathrm{for~all~}}m\\in\\mathbb{Z}_{+}.\n$$\n\nAgain Lebesgue's convergence theorem yields the assertion.]\n\nFor all $x\\in V^{*}$ let $\\eta_{x}:=c_{x}^{-1}\\cdot\\eta_{x}^{\\prime}$ . In view of (i), (i) and (ii) we have (iv) $\\eta_{(\\exp{t A})x}=\\eta_{x}$ for all $x\\in V^{*}$ ; and $x\\rightarrow\\eta_{x}$ is a continuous mapping of $V^{*}$ into $\\Re_{+}(V^{*})$\n\nIn view of (iv) for every $\\sigma\\in\\mathfrak{M}^{1}(C)$ there isdefined a measure $\\eta_{\\sigma}=\\int_{C}\\eta_{x}\\sigma(d x)$ in $\\Re_{+}(V^{*})$ . Obviously every $\\eta_{\\sigma}$ belongs to the convex set $\\mathfrak{L}$ of Levy measures $\\pmb{\\eta}$ on $V^{*}$ such that $e^{t A}(\\eta)=e^{t}\\cdot\\eta$ for all $t\\in\\mathbb{R}$ and $\\int\\varphi d\\eta=1$ Let $F(\\sigma):=\\eta_{\\sigma}$ for all $\\pmb{\\sigma}\\in\\mathfrak{M}^{1}(C)$"
  },
  {
    "qid": "statistic-posneg-1550-0-0",
    "gold_answer": "TRUE. The $(L,S^{0})$-compatibility condition (11) implies the stable nullspace property (9) when $\\rho = L^{-1}$. This is because (11) ensures that for any $\\beta \\in \\ker X$, the $\\ell_1$-norm of $\\beta_{S^{0}}$ is bounded by $\\rho$ times the $\\ell_1$-norm of $\\beta_{\\overline{S^{0}}}$, where $\\rho = L^{-1}$. Since the $\\theta$-uniform irrepresentable condition implies the $(L,S^{0})$-compatibility condition for any $L \\in (1, \\theta^{-1})$, it is indeed stronger than the stable nullspace property with any constant $\\rho \\in (\\theta, 1)$.",
    "question": "Is it true that the stable nullspace property (9) is implied by the $(L,S^{0})$-compatibility condition (11) when $\\rho = L^{-1}$, making the $\\theta$-uniform irrepresentable condition stronger than the stable nullspace property?",
    "question_context": "Model Selection with Lasso-Zero: Sign Recovery and False Discovery Trade-offs\n\nThe following result provides a lower bound for the probability of correct sign recovery. <PARAGRAPH_SEPARATOR> Theorem 1. Under the linear model (1) and Assumptions (A)– (D), there exist universal constants $c,c^{\\prime},c^{\\prime\\prime}>0$ such that for any $\\rho\\in(0,1)$ , <PARAGRAPH_SEPARATOR> provided that <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{s^{0}<\\displaystyle\\frac{c^{\\prime\\prime}\\nu^{2}}{(1+\\rho^{-1})^{2}}\\displaystyle\\frac{n}{\\log p},}\\ {\\left\\vert\\beta^{0}\\right\\vert_{\\mathrm{min}}>C_{\\rho}\\displaystyle\\frac{2\\sqrt{2}\\sigma\\sqrt{p}}{\\nu(\\sqrt{p/n}-1)},}\\end{array} $$ <PARAGRAPH_SEPARATOR> awchheireev $\\begin{array}{l}{{C_{\\rho}={\\frac{2(3+\\rho)}{1-\\rho}}}}\\end{array}$ 2(13+ρρ ) . Consequently, if p/n → C > 1, TBP ency with $\\beta_{\\mathrm{min}}^{0}~=~\\Omega(\\sqrt{n})$ $s^{0}=$ $O(n/\\log n)$ . <PARAGRAPH_SEPARATOR> The proof is provided in Appendix A. Its main argument is that, no matter what $\\mathrm{sign}(\\beta_{S^{0}}^{0})$ is, TBP recovers the signs of $\\beta^{0}$ under some beta-min condition and the stable nullspace property: <PARAGRAPH_SEPARATOR> $$ \\beta\\in\\ker X\\implies\\left\\lVert\\beta_{S^{0}}\\right\\rVert_{1}\\leq\\rho\\left\\lVert\\beta_{\\overline{{S^{0}}}}\\right\\rVert_{1},\\qquad\\rho\\in(0,1). $$ <PARAGRAPH_SEPARATOR> Let us compare this to the theory of the Lasso. It is known (Zhao and Yu 2006) that the Lasso achieves sign consistency under the $\\theta$ -irrepresentable condition <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\left\\lVert X\\frac{T}{S^{0}}X_{S^{0}}(X_{S^{0}}^{T}X_{S^{0}})^{-1}\\mathrm{sign}(\\beta_{S^{0}}^{0})\\right\\rVert_{\\infty}<\\theta,}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\theta\\in\\quad(0,1)$ . For sign consistency independently of $\\mathrm{sign}(\\beta_{S^{0}}^{0})$ , the $\\theta$ -uniform irrepresentable condition— requiring (10) for all $\\mathrm{sign}(\\beta_{S^{0}}^{0})$ —should be assumed. It is known (see Bühlmann and van de Geer 2011, Theorem 7.2) that the $\\theta$ -uniform irrepresentable condition is stronger than the $(L,S^{0})$ -compatibility condition for any $L\\in(1,\\theta^{-1})$ , which states that there exists a $\\phi>0$ such that <PARAGRAPH_SEPARATOR> $$ \\big\\|\\beta_{\\overline{{S^{0}}}}\\big\\|_{1}\\leq L\\big\\|\\beta_{S^{0}}\\big\\|_{1}\\implies\\big\\|\\beta_{S^{0}}\\big\\|_{1}^{2}\\leq\\frac{\\big|S^{0}\\big|}{n\\phi^{2}}\\|X\\beta\\|_{2}^{2}. $$ <PARAGRAPH_SEPARATOR> Clearly, (11) implies (9) for $\\rho=L^{-1}$ , so the $\\theta$ -uniform irrepresentable condition implies the stable nullspace property with any constant $\\rho~\\in~(\\theta,1)$ . Hence, TBP achieves sign recovery under a weaker condition on $X$ and $S^{0}$ compared to the Lasso."
  },
  {
    "qid": "statistic-posneg-1573-5-0",
    "gold_answer": "FALSE. The formula for $p$ is correct as derived from the scale function and the Feller test, but the question incorrectly links it to the transient behavior condition. The transient behavior is determined by $S(+\\infty) < +\\infty$ and $S(-\\infty) > -\\infty$, which is a separate condition from the probability $p$ of the process tending to $+\\infty$. The probability $p$ describes the likelihood of the process diverging to $+\\infty$ versus $-\\infty$, given the transient nature, but does not define the transient condition itself.",
    "question": "Is the probability $p$ of the process $\\xi_T$ tending to $+\\infty$ given by the formula $p = \\frac{\\sigma_{-}^{2}b_{+}}{\\sigma_{+}^{2}b_{-} + \\sigma_{-}^{2}b_{+}}$ consistent with the transient behavior of the process when $b_{-} < 0$ and $b_{+} > 0$?",
    "question_context": "Asymptotic Behavior and Hypothesis Testing in Drift-Diffusion Models\n\nWhen $b_{-}<0$ and $b_{+}>0$ , the process is also transient as $S(+\\infty)<+\\infty$ and $S(-\\infty)>-\\infty$ . The scale function $S$ map $\\mathbb{R}$ to $(-\\gamma_{-},\\gamma_{+})$ with $\\gamma_{\\pm}=|\\sigma_{\\pm}^{2}|2b_{\\pm}$ . From the Feller test (Karatzas & Shreve, 1991, Theorem 5.29), the process does not ex |plod|e. Thus, as $\\xi_{0}=0$ , it follows from Watanabe (1995) or Karatzas and Shreve (1991), Proposition 5.5.22, p. 354 that $$ p:=\\mathbb{P}\\left(\\operatorname*{lim}_{T\rightarrow\\infty}\\xi_{T}=+\\infty\right)=1-\\mathbb{P}\\left(\\operatorname*{lim}_{T\rightarrow\\infty}\\xi_{T}=-\\infty\right)=\\frac{\\gamma_{-}}{\\gamma_{-}+\\gamma_{+}}=\\frac{\\sigma_{-}^{2}b_{+}}{\\sigma_{+}^{2}b_{-}+\\sigma_{-}^{2}b_{+}}. $$ The event that $\\{\\mathrm{lim}_{T\rightarrow\\infty}|\\xi_{T}|=+\\infty\\}$ arise when the process starts an excursion with infinite lifetime, thus after the last p|ass|age time at 0. We denote by $A^{\\pm}$ the event $\begin{array}{r}{\\{\\operatorname*{lim}_{T\rightarrow\\infty}Q_{T}^{\\pm}/T=1\\}}\\end{array}$ , so that $A^{+}\\cap A^{-}=\\varnothing$ . Hence, $\\mathbb{P}(A^{+})=1-\\mathbb{P}(A^{-})=p$ ."
  },
  {
    "qid": "statistic-posneg-1288-2-0",
    "gold_answer": "TRUE. The moments $\\mu_{1}^{\\prime}(T)$, $\\mu_{2}^{\\prime}(T)$, $\\mu_{8}^{\\prime}(T)$, and $\\mu_{4}^{\\prime}(T)$ are highly complex and depend on $n$ in a non-linear fashion, which suggests that the distribution of $\\pmb{T}$ (and by extension $\\pmb{S}$) is not normal. The complexity of these moments indicates skewness and kurtosis that deviate from those of a normal distribution, supporting the claim that the distribution of $\\pmb{S}$ is far from normal even for large $\\pmb{\\mathscr{n}}$.",
    "question": "Is the claim that the distribution of $\\pmb{S}$ is far from normal for large values of $\\pmb{\\mathscr{n}}$ supported by the moments $\\mu_{1}^{\\prime}(T)$, $\\mu_{2}^{\\prime}(T)$, $\\mu_{8}^{\\prime}(T)$, and $\\mu_{4}^{\\prime}(T)$ provided in the context?",
    "question_context": "Moments and Distribution of Transformed Variables in Infinite Plane Theory\n\nUnder infinite plane theory $-2n\\log G$ is a special case of Bartlett's statistic, $\\pmb{M}$ , which has been extensively studied by Nair (1938), Hartley (1940), Glaser (1976) and others. However, since large values of $\\pmb{M}$ correspond to small values of ${\\pmb G}$ , the tabulated upper tail percentage points of M (Pearson $\\&$ Hartley, 1958, p. 180) are not appropriate to the test for local regularity. We obtained upper tail percentage points of $\\pmb{G}$ using the well-known $x_{n-1}^{2}$ approximation to M with Bartlett's correction. The accuracy of these percentage points was confrmed using computer simulation and Pearson curve approximation. <PARAGRAPH_SEPARATOR> Pearson curve approximation has been used to obtain lower percentage points for $s$ Calculation of the moments of $s$ and of $\\pmb{G}$ , can be simplified by transforming the squared nearest neighbour distances to a set of Dirichlet variates (Hogg $\\&$ Craig, 1965, p. 130). <PARAGRAPH_SEPARATOR> Here $\\pmb{S}_{i}$ . or more conveniently $\\pmb{T}$ given by $\\ S=\\pmb{n}(\\pmb{n}T-1)/(\\pmb{n}-1)$ . can be simply expressed in terms of the transformed variables. The frst four moments of $\\pmb{T}$ about the origin are, in the usual factorial power notation, <PARAGRAPH_SEPARATOR> $$ \\mu_{1}^{\\prime}(T)=2/(n+1),\\quad\\mu_{2}^{\\prime}(T)=4(n+5)/(n+3)^{(3)}, $$ <PARAGRAPH_SEPARATOR> $$ \\mu_{8}^{\\prime}(T)=8(n^{2}+18n+74)/(n+\\tilde{\\delta})^{(5)},\\quad\\mu_{4}^{\\prime}(T)=16(n^{3}+30n^{2}+371n+2118)/(n+7)^{(7)}. $$ <PARAGRAPH_SEPARATOR> The Pearson curve $5\\%$ and $2\\%$ points have been checked by computer simulation and for sample sizes of 15 and above are adequate. <PARAGRAPH_SEPARATOR> Table 2 gives means, standard deviations, coefficients of skewness and kurtosis and approximate $5\\%$ and $2\\%$ points for $\\pmb{S}$ for four sample sizes. The intermediate Pearson curve percentage points can be obtained to within 0.0002 by fitting & cubic in log n to these four points. Table 2 shows that even for large values of $\\pmb{\\mathscr{n}}$ , the distribution of $\\pmb{S}$ is far from normal. For $\\pmb{n}$ larger than 100, $1/\\sqrt{s}$ is approximately normal with mean $(1+\\frac{3}{8}c^{2})/\\sqrt{\\mu}$ and variance $\\yen1\\mu$ ,where $\\pmb{\\mu}$ and $\\pmb{c}$ are the mean and coefficient of variation of $\\pmb{S}$ <PARAGRAPH_SEPARATOR> Table 2. Mean, $\\pmb{\\mu}$ , 8tandard deviation, $\\pmb{\\upsigma}$ , coeficient of skewne88, $\\sqrt{\\beta_{1}}$ , and coeficient of kurtosi8, $\\beta_{\\mathbf{2}},$ of $\\pmb{S}$ for infinite plane theory; lower $5\\%$ and $2\\%$ points are based on Pearson curve approximation"
  },
  {
    "qid": "statistic-posneg-1908-0-0",
    "gold_answer": "TRUE. The halfspace depth function is affine invariant, as shown by the equation $l d e p t h(A\\Theta+\\ensuremath{\\mathbf{b}};A X_{n}+\\ensuremath{\\mathbf{b}})=l d e p t h(\\ensuremath{\\mathbf{\\Theta}};X_{n})$. This property holds because affine transformations map halfspaces to halfspaces, preserving the relative positions of the point and the data.",
    "question": "Is the halfspace depth function affine invariant, meaning that applying an affine transformation to the data and the point does not change the depth value?",
    "question_context": "Halfspace Depth and Its Properties in Multivariate Data Analysis\n\nTake any data set $X_{n}=\\{\\mathbf{x}_{i};i=1,...,n\\}$ with data points $\\mathbf{x}_{i}=(x_{i1},...,x_{i p})^{\\prime}$ $\\in\\mathbb{R}^{p}$ . This data set determines an empirical distribution ${\\hat{P}}_{n}$ which is the discrete probability distribution on $\\mathbb{R}^{p}$ given by $\\hat{P}_{n}(A)=\\#\\left\\{\\mathbf{x}_{i}\\in A\\right\\}/n$ . When the sample size $n$ is given, ${\\hat{P}}_{n}$ characterizes the data set $X_{n}$ . <PARAGRAPH_SEPARATOR> Tukey (1975) and Donoho and Gasko (1992) defined the halfspace depth of an arbitrary point ${\\bf\\uptheta}=(\\theta_{1},...,\\theta_{p})^{\\prime}\\in\\mathbb{R}^{p}$ relative to $X_{n}$ as the smallest number of data points in any closed halfspace with boundary hyperplane through %. We also call this the location depth, and it can be written as <PARAGRAPH_SEPARATOR> $$ l d e p t h(\\ \\ 6;X_{n})=\\operatorname*{min}_{\\|\\mathbf{u}\\|=1}\\#\\left\\{i;\\mathbf{u}^{\\prime}\\mathbf{x}_{i}\\leqslant\\mathbf{u}^{\\prime}\\mathbf{\\uptheta}\\right\\}, $$ <PARAGRAPH_SEPARATOR> where u ranges over all vectors in $\\mathbb{R}^{p}$ with $\\left\\|\\mathbf{u}\\right\\|=1$ . Interestingly, (1.1) is affine invariant. That is, if we consider a regular matrix $A\\in\\mathbb{R}^{p\\times p}$ and some vector $\\ensuremath{\\mathbf{b}}\\in\\mathbb{R}^{p}$ , it holds that <PARAGRAPH_SEPARATOR> $$ l d e p t h(A\\Theta+\\ensuremath{\\mathbf{b}};A X_{n}+\\ensuremath{\\mathbf{b}})=l d e p t h(\\ensuremath{\\mathbf{\\Theta}};X_{n}) $$ <PARAGRAPH_SEPARATOR> due to the fact that halfspaces are mapped to halfspaces. <PARAGRAPH_SEPARATOR> Since (1.1) is defined for any $\\mathbf{\\uptheta}\\in\\mathbb{R}^{p}$ we call it the depth function. Its values are nonnegative integers. When $p=1$ we have $u\\in\\{-1,1\\}$ so we can write (1.1) as <PARAGRAPH_SEPARATOR> $$ l d e p t h_{1}(\\theta;X_{n})=\\operatorname*{min}\\{n\\hat{F}_{n}(\\theta;X_{n}),n\\hat{F}_{n}(-\\theta;-X_{n})\\}, $$ <PARAGRAPH_SEPARATOR> where $\\hat{F}_{n}(\\theta;X_{n})=\\#\\left\\{x_{i}\\leqslant\\theta\\right\\}/n$ is the usual empirical cdf. Figure 1(a) shows the depth function of a univariate data set with $n=30$ . The data values were randomly generated from a $\\chi_{6}^{2}$ -distribution. The depth function clearly reflects the skewness. The increasing part of the function coincides with $\\hat{F}_{n}$ , whereas the decreasing part coincides with the empirical cdf of the image of the data under an affine transformation $x\\to a x+b$ with $a<0$ . Figure 1(b) is the depth function of a bivariate data set ( $p=2,$ ). The two variables are the batting average and the number of home runs of 14 baseball teams in the American League in 1987 (Moore and McCabe 1989; the data are also available at http:\\\\\\\\lib.stat.cmu.edu\\\\\\\\DASL\\\\\\\\). For any dimension $p\\geqslant2$ it holds that <PARAGRAPH_SEPARATOR> $$ l d e p t h}(\\ \\mathbf{\\Theta};X_{n})=\\operatorname*{min}_{\\|\\mathbf{u}\\|=1}{l d e p t h}_{1}(\\mathbf{u}^{\\prime}\\mathbf{\\Theta}\\mathbf{\\Theta};\\mathbf{u}^{\\prime}X_{n}), $$ <PARAGRAPH_SEPARATOR> which can also be written as <PARAGRAPH_SEPARATOR> $$ \\operatorname*{min}_{g\\in\\mathcal{A}}l d e p t h_{1}([g(\\mathbf{\\theta})]_{1};[g(X_{n})]_{1})=\\operatorname*{min}_{g\\in\\mathcal{A}}n\\hat{F}_{n}([g(\\mathbf{\\theta})]_{1};[g(X_{n})]_{1}), $$ <PARAGRAPH_SEPARATOR> where $\\mathcal{A}$ is the set of all affine transformations of $\\mathbb{R}^{p}$ and $[g(\\mathbf{\\theta})]_{1}$ denotes the first component of $g({\\bf\\theta})$ . Therefore, location depth can be seen as a natural affine equivariant generalization of the univariate empirical cdf. The usual multivariate empirical cdf is not affine equivariant because it depends on the coordinate system used. <PARAGRAPH_SEPARATOR> The depth contours defined as <PARAGRAPH_SEPARATOR> $$ D_{k}=\\{\\pmb{\\uptheta}\\in\\mathbb{R}^{p};l d e p t h(\\pmb{\\uptheta};X_{n})\\geqslant k\\} $$ <PARAGRAPH_SEPARATOR> are convex, and $D_{k+1}\\subseteq D_{k}$ for each $k$ . The outermost contour $D_{1}$ is the convex hull of the data set. Each data set also has an innermost depth contour $D_{k^{*}}$ where $k^{*}$ is the maximum of the function ldepth $\\iota(\\boldsymbol{\\mathbf{\\theta}};\\boldsymbol{X}_{n})$ over all $\\mathbf{\\uptheta}\\in\\mathbb{R}^{p}$ . Therefore, the complete set of contours is $D_{k^{*}}\\subseteq D_{k^{*}-1}\\subseteq\\cdots\\subseteq$ $D_{2}\\subseteq D_{1}$ . Figure 2 shows such a collection of contours. The depicted data set (from the Wall Street Journal of March 1, 1984, and provided at http:\\\\\\\\lib.stat.cmu.edu\\\\\\\\DASL\\\\\\\\) gives the 1983 TV advertising budget of several well-known companies, in millions of dollars. The second variable is based on a survey, where people had to cite a commercial for that product they had seen in the past week. The number of retained impressions (in millions) are plotted on the vertical axis. We see that the depth contours reflect the shape of the data set. <PARAGRAPH_SEPARATOR> From definition (1.1) we can derive an equivalent expression for the $k$ th depth contour: <PARAGRAPH_SEPARATOR> Lemma 1. <PARAGRAPH_SEPARATOR> $$ D_{k}=\\bigcap_{\\scriptstyle A\\in{\\mathcal{A}}(n-k+1)}A, $$ <PARAGRAPH_SEPARATOR> where $\\mathcal{A}(m)$ is the set of all closed halfspaces containing at least m data points. <PARAGRAPH_SEPARATOR> Proof. We first prove $\\subseteq$ . If % does not belong to the intersection there exists a closed halfspace in $\\mathcal{A}(n-k+1)$ which does not contain %, hence % belongs to an open halfspace containing at most $n-(n-k+1)=k-1$ observations. Therefore ldepth $\\iota(\\boldsymbol\\Theta;X_{n})\\leqslant k-1$ , and thus $\\mathbf{\\uptheta}\\notin D_{k}$ . On the other hand, suppose that $\\mathbf{\\uptheta}\\notin D_{k}$ . Then ldepth $(\\mathbf{\\theta}\\in X_{n})<k$ , hence % belongs to a closed halfspace $A$ containing fewer than $k$ data points. The complement of $A$ is an open halfspace containing at least $n-k+1$ data points, from which we immediately obtain a closed halfspace that does not contain % and has at least $n-k+1$ data points. Therefore also $\\supseteq$ holds. K"
  },
  {
    "qid": "statistic-posneg-908-0-3",
    "gold_answer": "TRUE. Lemma 4 provides high-probability bounds on the uniform deviation of the stochastic matrix S^(k)(t,β^0) from its population counterpart s^(k)(t,β^0). The bounds are given in terms of the Rademacher complexity and hold uniformly over t ∈ [0,τ], ensuring uniform convergence with probability at least 1-δ.",
    "question": "Does Lemma 4 guarantee uniform convergence of the stochastic matrix S^(k)(t,β^0) to its population counterpart s^(k)(t,β^0) with high probability?",
    "question_context": "Rademacher Complexity and Empirical Process Bounds\n\nThe paper discusses the use of Rademacher complexity to characterize the functional capacity in empirical processes. It defines the empirical Rademacher average and the population Rademacher average, and provides properties and bounds for these averages using Dudley’s entropy integral. The paper also presents lemmas to handle the concentration of large matrices/vectors in the context of time-dependent data."
  },
  {
    "qid": "statistic-posneg-1643-0-0",
    "gold_answer": "FALSE. The assumption $d_{D}=d_{T}/2$ is not directly linked to the spatial process simulation defined by $y_{s}\\mid\\beta,\\lambda,\\sigma^{2}\\sim\\mathcal{N}(r_{s},\\sigma^{2})$. The spatial process simulation models the revenue $y_s$ as normally distributed around $r_s$ with variance $\\sigma^2$, but the distance assumption $d_{D}=d_{T}/2$ is an independent assumption about customer behavior, not derived from the spatial process model.",
    "question": "Is the assumption that the reasonable distance a customer is willing to travel is half of the maximum extent prepared to travel ($d_{D}=d_{T}/2$) consistent with the spatial process simulation defined by $y_{s}\\mid\\beta,\\lambda,\\sigma^{2}\\sim\\mathcal{N}(r_{s},\\sigma^{2})$?",
    "question_context": "Spatial Process Simulation for Competitive Facility Location\n\nWe design a simulation study to experiment with the optimisation problem using the three objective functions introduced and compare the performance using the three sampling methods proposed in the Section 2. We also compare the computational performance of the methods by observing the run time of each optimisation problem. All the experiments are executed on a Intel Core i5 CPU (2.3 GHz Dual-Core and 8 GB of RAM). Furthermore, a simulation study is presented in the Supplementary Material to examine the inferences obtained for BSIM under different synthetic settings characterised by an increasing number of stores and customers.<PARAGRAPH_SEPARATOR>First, we simulate the data from a spatial process that closely matches the extended BSIM framework introduced in Section 2 with the dummy facilities. The process is defined as<PARAGRAPH_SEPARATOR>$$y_{s}\\mid\\beta,\\lambda,\\sigma^{2}\\sim\\mathcal{N}(r_{s},\\sigma^{2}),$$<PARAGRAPH_SEPARATOR>where we assume the reasonable distance a customer is willing to travel is half of the maximum extent prepared to travel $(d_{D}=d_{T}/2$ ). The locations of stores and customers are simulated within a square. Customer budgeted spending is generated, with a strong spatial correlation where rich and poor areas are demonstrated to reflect the real-world scenario closely, as shown in Figure 3a. The customers’ satisfied demand $\\left({\\boldsymbol{p}}_{n}\\right)$ from the existing stores are shown in Figure 3b. The store locations are randomly sampled, and their current revenue is displayed in Figure 3c. We assume two possible designs $(r=2)$ , say small and large facility structures, are available for development. Suppose the cost of a large building is six times the smaller facility, and the cost of each design $(c_{l r})$ remains unchanged despite the locations.<PARAGRAPH_SEPARATOR>Algorithm 2: Multiresoltion grid structure<PARAGRAPH_SEPARATOR><html><body><table><tr><td>x ← constructPointsMeshgrid(region);</td><td></td></tr><tr><td colspan=\"2\">grid ← decompose(region, m) ;</td></tr><tr><td>foreach c; in grid do</td><td>// for each cell ci in the grid</td></tr><tr><td>μ ←mean(f,(x)) ;</td><td>// Mean of f (x) of points in cell Ci</td></tr><tr><td>save μ, in Ci</td><td></td></tr><tr><td colspan=\"2\">end</td></tr><tr><td colspan=\"2\">q←max(μ,)/q;</td></tr><tr><td>foreach c; in grid do points ← midpoint(ci) ;</td><td>// Midpoints of cell Ci</td></tr><tr><td>save points in out;</td><td></td></tr><tr><td>for j = 1 to q do if (j-1)×α≥μr≤j×q then</td><td></td></tr><tr><td></td><td></td></tr><tr><td>for g = 1 to j do</td><td></td></tr><tr><td>Ci ←decompose(ci, 4) ;</td><td>// Decompose c; into 4 square</td></tr><tr><td></td><td>submatrices and repeat recursively</td></tr><tr><td></td><td></td></tr><tr><td>points ← midpoint(ci);</td><td></td></tr><tr><td>save points in out;</td><td></td></tr><tr><td></td><td></td></tr><tr><td>end</td><td></td></tr><tr><td></td><td></td></tr><tr><td>exit</td><td></td></tr><tr><td></td><td></td></tr><tr><td>end</td><td></td></tr><tr><td>end</td><td></td></tr></table></body></html>"
  },
  {
    "qid": "statistic-posneg-1741-1-1",
    "gold_answer": "TRUE. Assumption A5 states that $\\sigma_{j,t} \\in (1/C_{2}, C_{2})$ for all $j=1,\\dots,p$ and $t\\in[0,1)$, where $C_{2}<\\infty$. This means the individual volatilities are uniformly bounded away from zero and infinity, ensuring they do not become too small or too large, which is crucial for the consistency of the PA-RCov estimator.",
    "question": "Does assumption A5 imply that the individual volatilities $\\sigma_{j,t}$ are bounded both above and below for all $j$ and $t$?",
    "question_context": "Spiked Model for Large Volatility Matrix: ICV Spectrum Separation\n\nThe paper introduces a spiked model for the Integrated Covariance Volatility (ICV) matrix, separating its spectrum into a bulk part and a spikes part. The ICV matrix is represented as $\\breve{\\Sigma}=\\left(\\begin{array}{c c}{{{\\bf V}_{s}}}&{{{\\bf0}}}\\\\ {{{\\bf0}}}&{{{\\bf V}_{p}}}\\end{array}\\right)$, where ${\\bf V}_{s}$ is a $K\\times K$ matrix with eigenvalues $\\breve{\\alpha}_{1}>\\cdot\\cdot\\cdot>\\breve{\\alpha}_{K}>0$ (the spikes), and $\\mathbf{V}_{p}$ is a $(p-K)\\times(p-K)$ matrix representing the bulk. The main task is to infer the spiked eigenvalues of the ICV from those observed in the PA-RCov, under a set of assumptions (A1-A7) that ensure the consistency of the sequence $B_{m}$."
  },
  {
    "qid": "statistic-posneg-1864-0-0",
    "gold_answer": "TRUE. Explanation: When γ_i=0 for all i, the objective function simplifies to the sum of squared residuals without the penalty term, which is the traditional MAVE formulation. This is explicitly shown in the context where the objective function becomes ∑[y_i - {a_j + b_j^T B^T (X_i - X_j)}]² w_ij when γ_i=0, matching the standard MAVE approach.",
    "question": "The robust MAVE method reduces to traditional MAVE when all mean-shift parameters γ_i are zero. Is this statement true based on the provided objective function?",
    "question_context": "Robust MAVE with Nonconvex Penalization for Dimension Reduction\n\nThe paper introduces a robust version of the Minimum Average Variance Estimation (MAVE) method using nonconvex penalization to handle outliers in dimension reduction. The method involves minimizing an objective function that includes a mean-shift parameter γ_i for each observation, which is penalized using an indicator function I(|γ_i|≠0) to induce sparsity. The problem is separable in each γ_i, leading to a trimmed least squares formulation when γ_i=0, and a penalty term λ²/2 when γ_i≠0. The conditions C1-C7 ensure the theoretical properties of the estimators, including stationarity, moment conditions, and smoothness of density functions."
  },
  {
    "qid": "statistic-posneg-441-0-2",
    "gold_answer": "TRUE. The TE formula (Section 3.4) incorporates spatial effects through: (1) μ_it = Z_itϕ + Σw_ijZ_jtδ (spatial inefficiency determinants), and (2) ε_it = Y_it - X_itβ - ρΣw_ijY_jt - Σw_ijX_jtθ (spatial frontier residuals). Both terms feed into the exponential and Φ components of TE_it, meaning efficiency estimates inherently capture spatial spillovers from neighbors' inputs (θ), outputs (ρ), and inefficiency determinants (δ).",
    "question": "The technical efficiency score formula TE_it accounts for spatial dependence through both μ_it (which includes WZδ) and ε_it (which includes ρWY). True or False?",
    "question_context": "Spatial Stochastic Frontier Model with Technical Efficiency Spillovers\n\nThe paper introduces a Spatial Durbin Frontier model with Spatially correlated Technical Efficiency (SDF-STE) for panel data, incorporating spatial lags in both production inputs and inefficiency determinants. The model is specified as: $$Y_{i t}=X_{i t}\\beta+\\rho\\sum_{j=1}^{N}w_{i j}Y_{j t}+\\sum_{j=1}^{N}w_{i j}X_{j t}\\theta+\\nu_{i t}-u_{i t}$$ with inefficiency term $u_{i t}$ following a truncated normal distribution whose mean depends on spatial lags of inefficiency determinants: $$\\mu_{i t}=Z_{i t}\\phi+\\sum_{j=1}^{N}w_{i j}Z_{j t}\\delta$$. The model nests several existing spatial and non-spatial stochastic frontier models through parametric restrictions."
  },
  {
    "qid": "statistic-posneg-992-0-0",
    "gold_answer": "FALSE. The formula for $\\mathcal{E}_{0}^{o o b}$ is correctly defined as the average misclassification rate for observations with label 0, but it uses the ensemble of trees not containing each observation i (denoted by $\\hat{\\pmb{g}}_{-i}$). The explanation incorrectly states that it uses 'only trees that did not include those observations in their training set,' which is not the case as it uses the average over the ensemble of learners not containing the observation.",
    "question": "Is the out-of-bag error for class 0, $\\mathcal{E}_{0}^{o o b}$, correctly defined as the average misclassification rate for observations with label 0, using only trees that did not include those observations in their training set?",
    "question_context": "Random Forest Out-of-Bag Error for Two-Sample Testing\n\nNaturally, the split in training and test set is not ideal. For finite sample sizes, one would like to have as many (test) samples as possible to detect differences. At the same time, it would be preferable to have the classifier trained on many data points. This in fact resembles a bias-variance trade-off, similar to what was described in Lopez-Paz and Oquab (2018): Let $g_{1/2}^{*}$ and $L_{\\pi}^{(g_{1/2}^{*})}$ be the Bayes classifier and Bayes error respectively, both defined in Section 2.3. For $\\pi=1/2$ , there is a trade-off between the closeness of $L^{(\\hat{g})}$ to $L_{\\pi}^{(g_{1/2}^{*})}$ , which may be achieved through a large training set and the closeness of [(g) to Lg), which is generally only true in large test sets.\n\n<PARAGRAPH_SEPARATOR>\n\n# 2.2. Out-of-bag test\n\n<PARAGRAPH_SEPARATOR>\n\nFor the purpose of overcoming the arbitrary split in training and testing, Random Forest delivers an interesting tool: the OOB error introduced in Breiman (2001). Since each tree is built on a bootstrapped sample taken from $D_{N}$ , approximately $1/3$ of the trees will not use the ith observation $(\\ell_{i},\\mathbf{Z}_{i})$ . Thus we may use this ensemble of trees not containing observation i to obtain an estimate of the out-of-sample error for i. We slightly generalize this here, in assuming we have an ensemble learner $_g$ : That is, we assume to have iid copies of a random element $\\nu$ , $\\nu_{1},\\ldots,\\nu_{B}$ , such that each $\\hat{\\{\\mathbfcal g}}_{\\nu_{b}}({\\mathbf Z}):=g({\\mathbf Z},D_{N_{t r a i n}},\\nu_{b})$ is a different classifier. We then consider the average\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\hat{g}(\\mathbf{Z}):=\\frac{1}{B}\\sum_{b=1}^{B}\\hat{g}_{\\nu_{b}}(\\mathbf{Z}).$$\n\n<PARAGRAPH_SEPARATOR>\n\nFor $B\\to\\infty$ , it holds that (a.s.) $\\hat{g}(\\mathbf{Z})\\rightarrow\\mathbb{E}_{\\nu}[\\hat{g}_{\\nu}(\\mathbf{Z})]$ . For Random Forest, $\\nu$ usually represents the bootstrap sampling of observations and the sampling of variables to consider at each splitpoint for a given tree.\n\n<PARAGRAPH_SEPARATOR>\n\nLet as before, $\\begin{array}{r}{n_{0}:=\\sum_{i=1}^{\\bar{N}}\\bar{\\mathbb{I}}\\{\\ell_{i}=0\\}}\\end{array}$ and $\\begin{array}{r}{n_{1}:=\\sum_{i=1}^{N}\\mathbb{I}\\{\\ell_{i}=1\\}}\\end{array}$ , with $n_{0}\\geq1$ , $n_{1}\\geq1$ . We assume in the following that each $\\hat{g}_{\\nu_{b}}(\\mathbf{Z})$ uses a bootstra pped sample from the or iginal data, as Random Forest does. The class-wise OOB error of such an ensemble of learners trained on $N$ observations is defined as\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\mathcal{E}_{0}^{o o b}=\\frac{1}{n_{0}}\\sum_{i=1}^{N}\\mathbb{I}\\{\\ell_{i}=0\\}\\mathbb{I}\\{\\hat{\\pmb{g}}_{-i}(\\mathbf{Z}_{i})\\neq0\\},$$\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\begin{array}{l}{\\displaystyle\\mathcal{E}_{1}^{o o b}=\\frac{1}{n_{1}}\\sum_{i=1}^{N}\\mathbb{I}\\{\\ell_{i}=1\\}\\mathbb{I}\\{\\hat{\\pmb{g}}_{-i}(\\mathbf{Z}_{i})\\neq1\\},}\\ {\\displaystyle\\mathcal{E}_{p}^{o o b}=(1-p)\\mathcal{E}_{0}^{o o b}+p\\mathcal{E}_{1}^{o o b},}\\end{array}$$\n\n<PARAGRAPH_SEPARATOR>\n\nwhere $\\hat{g}_{-i}$ , represents the average over the ensemble of learners not containing the $i^{\\mathrm{th}}$ observation for training.\n\n<PARAGRAPH_SEPARATOR>\n\nUnfortunately, the test statistic $\\mathcal{E}_{1/2}^{o o b}$ is difficult to handle; due to the complex dependency structure between the elements of the sum, it is not clear what the (asymptotic) distribution under the null is. For theoretical purposes, we consider in Section 3 a solution based on the concept of U-statistics. Here, we recommend using the OOB error together with a permutation test. See e.g., Good (1994) or Kim et al. (2021), who use it in conjunction with the out-of-sample error evaluated on a test set: We first calculate the class-wise OOB errors $\\mathcal{E}_{0}^{o o b},\\mathcal{E}_{1}^{o o b}$ and then reshuffle the labels $K$ times to obtain $K$ permutations, $\\sigma_{1},\\dots,\\sigma_{K}$ say. For each of these new datasets $\\left(\\mathbf{Z}_{i},\\ell_{\\sigma_{k}\\left(i\\right)}\\right)_{i=1}^{N}$ , $k\\in\\{1,\\ldots,K\\}$ , we calculate the OOB errors\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\mathcal{E}_{j}^{o o b,k}:=\\frac{1}{n_{j}}\\sum_{i=1}^{N}\\mathbb{I}\\{\\ell_{\\sigma_{k}(i)}=j\\}\\mathbb{I}\\{\\hat{\\mathbf{g}}_{-i}(\\mathbf{Z}_{i})\\neq\\ell_{\\sigma_{k}(i)}\\},$$\n\n<PARAGRAPH_SEPARATOR>\n\nfor $j\\in\\{0,1\\}$ . Under $H_{0}$ , $(\\ell_{1},\\dots,\\ell_{N})$ and $(\\mathbf{Z}_{1},\\ldots,\\mathbf{Z}_{N})$ are independent and each $\\mathcal{E}_{1/2}^{o o b}$ is simply an iid draw from the distribution $F$ of the random variable $\\mathcal{E}_{1/2}^{o o b}|({\\mathbf{Z}}_{1},\\ldots,{\\mathbf{Z}}_{N})$ . As such we can accurately approximate the $\\alpha$ quantile $F^{-1}(\\alpha)$ of said distribution by performing a large number of permutations and use the decision rule\n\n<PARAGRAPH_SEPARATOR>\n\n$$\\delta_{o o b}(D_{N})=\\left\\{\\mathcal{E}_{1/2}^{o o b}\\leq F^{-1}(\\alpha)\\right\\}.$$\n\n<PARAGRAPH_SEPARATOR>\n\nThus, as in the decision in Equation (2), the rejection region depends on the data at hand. Nonetheless, the level will be conserved, as proven e.g. in Hemerik and Goeman (2018, Theorem 1).\n\n<PARAGRAPH_SEPARATOR>\n\nHeuristically, this procedure will have power under the alternative, as in this case there is some dependence between $(\\ell_{1},\\dots,\\ell_{N})$ and $(\\mathbf{Z}_{1},\\ldots,\\mathbf{Z}_{N})$ , formed by the difference in the distribution of the $\\mathbf{Z}_{i}$ . The OOB error $\\mathcal{E}_{1/2}^{o o b}$ will thus be different than those observed under permutations.\n\n<PARAGRAPH_SEPARATOR>\n\nThe whole procedure is described in Algorithm 2. We name this test “hypoRF”."
  },
  {
    "qid": "statistic-posneg-1161-0-0",
    "gold_answer": "FALSE. The normality assumption is a simplification for analytical tractability, not a reflection of the actual distribution of refractive indices, which has a pronounced peak and a long tail to the right. This simplification allows for easier computation and understanding but may not fully capture the complexities of real-world forensic data.",
    "question": "Is the assumption that the true values of refractive indices are normally distributed about μ with variance τ² justified in practical forensic scenarios, given that the actual distribution has a pronounced peak and a long tail to the right?",
    "question_context": "Forensic Evidence Evaluation: Bayesian Approach to Glass Fragment Analysis\n\nWe suppose that the measurements are normally distributed about the true values with known, constant variance. If measurements are made at the scene, the sufficient statistic is their mean, normally distributed about the unknown true value with variance. Let denote the mean of similar measurements made on material found on the suspect: this is normally distributed. In the case of identity: otherwise it is supposed that. One remaining piece of information is the distribution of the true values. In the case of window glass there is considerable evidence about the distribution of refractive indices, some values being common, some rare. That such information is relevant is seen intuitively by considering the case where and are close together, both being unusual indices. This gives greater evidence of identity than does the case where and are equally close but are frequently occurring indices. In much of this paper we assume that the true values are normally distributed about with variance, both values being known. Typically will be larger, sometimes much larger, than. The normality assumption does not correspond to the practical situation where the distribution of refractive indices has a pronounced peak and a long tail to the right. Our justification for using the normal distribution is that we can get analytic results and consequently understand the situation more easily. We extend the argument to a general distribution where resort may have to be made to simple numerical integration though a possible approximation avoiding this is also provided."
  },
  {
    "qid": "statistic-posneg-217-0-1",
    "gold_answer": "FALSE. While the paper states that $\\hat{g}_H(x)$ generally achieves better rates of convergence than $\\hat{g}_L(x)$, this is not universally true for all parameter combinations. The exact rates depend on the relationship between $r$, $s$, and $p$, as shown in the expression for $\\varphi_n$. For some parameter values, the rates may be comparable or even favor the linear estimator in specific cases.",
    "question": "The hard thresholding wavelet estimator $\\hat{g}_H(x)$ achieves faster convergence rates than the linear wavelet estimator $\\hat{g}_L(x)$ under the $\\mathbb{L}_p$ risk for all values of $p \\geq 1$ and smoothness parameters $s > 1/r$. Is this statement true based on the theoretical results presented in the paper?",
    "question_context": "Nonparametric Estimation of Quantile Density Function using Wavelet Methods\n\nThe paper discusses nonparametric estimation of the quantile density function $g(x) = Q'(x) = \\frac{1}{f(Q(x))}$ using wavelet methods. It introduces linear and hard thresholding wavelet estimators, $\\hat{g}_L(x)$ and $\\hat{g}_H(x)$, and analyzes their convergence rates under $\\mathbb{L}_p$ risk over Besov balls."
  },
  {
    "qid": "statistic-posneg-756-1-1",
    "gold_answer": "FALSE. While the method of regarding each response value as missing in turn and estimating the residual mean square can be more robust in some cases, it does not guarantee higher power in all scenarios. The power of any detection method depends on the specific nature of the spurious values and the underlying model assumptions. The paper mentions that Anscombe's method may have diminished power when a spurious value is included in the model fitting, but the alternative method's performance is context-dependent and not universally superior.",
    "question": "Does the method of regarding each response value as missing in turn and estimating the residual mean square guarantee higher power in detecting spurious values compared to Anscombe's method?",
    "question_context": "Detection of Spurious Values in Unreplicated Factorial Experiments\n\nA strong case for fully replicated experiments can thus be made when there is prior evidence of frequent spurious values. In practice, however, experimenters are reluctant to spend much time in replication, although the statistician should always urge partial duplication. At the very least the first treatment combination should be repeated at the end of the experiment. <PARAGRAPH_SEPARATOR> The present paper is largely concerned with unreplicated experiments in which the residual variance is estimated from the pooled effects of high-order interactions. The significance of main-effects and low-order interactions is usually tested against this residual variance, but in some cases the experimenter may have a well-founded prior estimate of the run-to-run variance, ${\\pmb\\sigma}^{2}$ <PARAGRAPH_SEPARATOR> A number of methods for identifying spurious values in this situation were proposed at a meeting of the American Statistical Association in December 1959, and were subsequently published in Technometrics. Anscombe (1960) suggested tests based on the size of the largest individual residual, $|z_{M}|$ .Forknown $\\pmb{\\sigma}_{i}$ heproposed rejection of the corresponding response value $y_{M}$ when <PARAGRAPH_SEPARATOR> $$ \\{z_{M}\\vert>C\\sigma $$ <PARAGRAPH_SEPARATOR> and calculated the “insurance premium\" involved for various values of the multiplying factor C. He also extended the analysis to the case when only an estimate of $\\sigma_{:}$ based on a limited number of degrees of freedom, is available. Daniel (1960) studied the pattern of the residuals and also devised a significance test based on the size of the largest residual. He also pointed out that since a bad value enters into every factorial contrast (see, for example, Table 1), a half-Normal plot of the ordered contrasts would have too few contrasts near zero and hence that such a graph would constitute a useful detective device. A general procedure for calculating critical values of the maximum normed residual has recently been given by Stefansky (1972). <PARAGRAPH_SEPARATOR> The drawback of these tests based on the largest residual is that its identity and size depend on a model which has been fitted with a possible spurious value included. Consequently, for a given risk of false detection, the power of the test at revealing a truly spurious value is diminished. <PARAGRAPH_SEPARATOR> The method we have adopted, therefore, is to regard each response value in turn as a missing value and to estimate the treatment effects and the residual mean square for the assumed model from the remaining data. Then if a rogue is present in the set of response values, the analysis carried out when this value is currently regarded as missing will result in a much lower residual mean square. Hence the method consists of scanning the list of residual mean squares, identifying the lowest level and testing whether it shows a statistically significant reduction from the residual mean square obtained with all the given data."
  },
  {
    "qid": "statistic-posneg-1763-0-1",
    "gold_answer": "TRUE. The proof involves the decomposition $\\left(\\frac{S}{T} - \\frac{a}{b}\\right) = \\frac{a}{b}\\left(\\frac{S}{a} - 1\\right) + \\frac{a}{b}\\left(1 - \\frac{T}{b}\\right) + \\left(\\frac{S}{T} - \\frac{a}{b}\\right)\\left(1 - \\frac{T}{b}\\right)$. Using the triangular inequality and the condition $0 < \\eta < \\frac{1}{2}$, it is shown that $\\frac{2\\eta}{1-\\eta} < 4\\eta$, validating the inclusion.",
    "question": "Does Lemma 6 correctly assert that $\\left\\{\\left|\\frac{S}{T}-\\frac{a}{b}\\right|>4\\eta\\left|\\frac{a}{b}\\right|\\right\\}\\subseteq\\left\\{\\left|\\frac{S}{a}-1\\right|>\\eta\\right\\}\\cup\\left\\{\\left|\\frac{T}{b}-1\\right|>\\eta\\right\\}$ for $0 < \\eta < \\frac{1}{2}$?",
    "question_context": "Frontier Estimation via Kernel Regression: Asymptotic Normality and Convergence\n\nThe context discusses the asymptotic properties of kernel regression estimators for frontier estimation. It includes lemmas and proofs related to the convergence and asymptotic normality of estimators such as $\\widehat{g}_{n}(x)$, $\\widehat{r}_{n}(x)$, and $\\widehat{\\varphi}_{n}(x)$. Key formulas involve bounds on deviations, convergence in probability, and distributional limits."
  },
  {
    "qid": "statistic-posneg-751-1-1",
    "gold_answer": "FALSE. While the covariance function suggests an ARMA(1,1)-like decay pattern, the process $\\left\\{X_{t}\\right\\}$ is not strictly ARMA(1,1). It is a state-switching model where the dynamics alternate between an AR(1) process and white noise, leading to a more complex dependence structure. The ARMA(1,1) analogy is only an approximation based on second-order properties.",
    "question": "Does the covariance function $E(X_{t+h+1}X_{t})=s(1-q)E(X_{t+h}X_{t})$ imply that the process $\\left\\{X_{t}\\right\\}$ is strictly an ARMA(1,1) process?",
    "question_context": "Moment Structure of Autoregressive Models with State Switching\n\nAlthough it is known from linear time series theory that the method of moments may not be very efficient, it has been used to some extent for non-linear models (Robinson, 1977; Hall and Heyde, 1980), since, at least in certain situations, it is the only method that is analytically tractable. The difficulties of analysing least squares estimates for general non-linear models are discussed in Tjostheim (1986b). <PARAGRAPH_SEPARATOR> To be able to evaluate moments of $\\{X_{t}\\},\\{X_{t}\\}$ must be stationary, and the moments of the desired order must exist. The problem of finding the most general conditions for which this is true for the class of processes that we consider is far from trivial. Some preliminary results were obtained in Tjostheim (1986a). <PARAGRAPH_SEPARATOR> Here we shall avoid that problem by assuming that $\\vert s_{i}\\vert<1$ for $i=1,\\ldots,k,$ and that $\\left\\{{{e}_{t}}\\right\\}$ has moments of any order. Then it is not diffcult to show (see Tjostheim (1986a)) that <PARAGRAPH_SEPARATOR> $$ X_{t}=\\sum_{j=0}^{\\infty}\\bigg(\\prod_{i=0}^{j-1}\\theta_{t-i}\\bigg)e_{t-j} $$ <PARAGRAPH_SEPARATOR> defines a strictly stationary and ergodic process which is a solution to equation (2.1). Moreover, using independence of $\\left\\{\\theta_{t}\\right\\}$ and $\\left\\{{{e}_{t}}\\right\\}$ and the zero mean property of $\\left\\{\\boldsymbol{e}_{t}\\right\\}$ it follows that $E(X_{t})=0$ and, for $h\\geqslant0$ <PARAGRAPH_SEPARATOR> $$ \\operatorname{cov}(X_{t},X_{t+h})=E(X_{t}X_{t+h})=\\sigma^{2}\\sum_{j=0}^{\\infty}E\\biggl(\\prod_{k=1}^{h}\\theta_{t+k}\\prod_{i=0}^{j-1}\\theta_{t-i}^{2}\\biggr), $$ <PARAGRAPH_SEPARATOR> where by convention <PARAGRAPH_SEPARATOR> $$ \\prod_{k=1}^{h}\\theta_{t+k}=1 $$ <PARAGRAPH_SEPARATOR> for $h=0$ and <PARAGRAPH_SEPARATOR> $$ \\prod_{i=0}^{j-1}\\theta_{t-i}^{2}=1 $$ <PARAGRAPH_SEPARATOR> for $j=0$ <PARAGRAPH_SEPARATOR> Assume that $\\left\\{X_{t}\\right\\}$ is observed for $t=1,\\ldots,n$ An application of the ergodic theorem implies that, as $n\\to\\infty$ ， <PARAGRAPH_SEPARATOR> $$ {\\frac{1}{n}}\\sum_{t=1}^{n-h}X_{t}X_{t+h}\\xrightarrow{\\mathrm{a.s.}}E(X_{t}X_{t+h}), $$ <PARAGRAPH_SEPARATOR> and this is the basis for using the method of moments and for identifying a process by its empirical moments. Formula (3.3) can be extended to higher order moments when these exist, but explicit expressions of these moments in terms of the parameters are difficult to obtain. <PARAGRAPH_SEPARATOR> # 3.1. Moment Structure for some Simple Models <PARAGRAPH_SEPARATOR> We shall examine the second-moment structure for a rather special model, where $p=1$ and $k=3$ , and where the states $s_{1},s_{2}$ and $s_{3}$ are given by the real numbers s, <PARAGRAPH_SEPARATOR> $0,-s$ with $|s|<1$ . We denote the transition probability matrix $Q$ by <PARAGRAPH_SEPARATOR> $$ Q=\\left[\\begin{array}{c c c}{1-q-r}&{q}&{r}\\\\ {v}&{1-v-w}&{w}\\\\ {u}&{t}&{1-u-t}\\end{array}\\right] $$ <PARAGRAPH_SEPARATOR> Weassume that $Q$ is non-singular, so that the stationary probabilities $\\pi_{1},\\pi_{2}$ and $\\pi_{3}$ exist. Using equation (3.2) it can be shown (Tyssedal and Tjostheim, 1985) that <PARAGRAPH_SEPARATOR> $$ \\operatorname{var}(X_{t})=E(X_{t}^{2})=\\sigma^{2}{\\biggl[}1+{\\frac{(\\pi_{1}+\\pi_{3})s^{2}}{1-a s^{2}}}{\\biggr]} $$ <PARAGRAPH_SEPARATOR> where $a=[\\pi_{1}(1-q)+\\pi_{3}(1-t)](\\pi_{1}+\\pi_{3})^{-1}$ . Similarly if we define $b$ and $c$ by $b=[\\pi_{3}u+\\pi_{1}(1-q-r)](\\pi_{1}+\\pi_{3})^{-1}$ and $c=[\\pi_{1}r+\\pi_{3}(1-u-t)](\\pi_{1}+\\pi_{3})^{-1}$ , then <PARAGRAPH_SEPARATOR> $$ E(X_{t}X_{t+1})=\\sigma^{2}s[(\\pi_{1}-\\pi_{3})+(b-c)R] $$ <PARAGRAPH_SEPARATOR> $$ E(X_{t}X_{t+2})=\\sigma^{2}s^{2}\\{\\pi_{1}(1-q-2r)+\\pi_{3}(1-t-2u)+R[(1-q-2r)b+(1-t)\\pi_{3}(1-q)]\\} $$ <PARAGRAPH_SEPARATOR> where $R=(\\pi_{1}+\\pi_{3})s^{2}(1-a s^{2})^{-1}$ <PARAGRAPH_SEPARATOR> Higher lags of the covariance function can be obtained in the same manner, but the computations quickly become cumbersome. If we assume that $Q$ is known, then $\\pi_{1},\\pi_{2}$ and $\\pi_{3}$ can be computed using standard methods, and the method of moments can now be used to obtain consistent estimates (via equations (3.3), (3.5), (3.6) and (3.7)) of $s$ and $\\sigma^{2}$ . The problem becomes much more complicated when $Q$ is unknown, the case of practical interest. Then in general we need to evaluate higher order moments to obtain a sufficient number of equations for evaluating the parameters of $Q$ Even for our simple model an explicit evaluation of higher order moments is difficult to obtain. <PARAGRAPH_SEPARATOR> To obtain an idea about the properties of the moment estimators we shall look at a model which is even more special. This is a two-state model with $s_{1}=s$ and $s_{2}=0$ and with the transition matrix <PARAGRAPH_SEPARATOR> $$ \\ Q=\\left[\\begin{array}{c c}{{1-q}}&{{q}}\\\\ {{u}}&{{1-u}}\\end{array}\\right] $$ <PARAGRAPH_SEPARATOR> so that the local correlation structure of $\\left\\{X_{t}\\right\\}$ alternates between that of a white noise process and that of a first-order AR process. This process was treated in Tjostheim (1986a), and it was shown that <PARAGRAPH_SEPARATOR> and for $h\\geqslant1$ <PARAGRAPH_SEPARATOR> $$ E(X_{t}^{2})=\\sigma^{2}\\bigg\\{1+\\frac{u s^{2}}{(u+q)[1-(1-q)s^{2}]}\\bigg\\} $$ <PARAGRAPH_SEPARATOR> $$ E(X_{t+h+1}X_{t})=s(1-q)E(X_{t+h}X_{t}), $$ <PARAGRAPH_SEPARATOR> which means that as far as second-order properties are concerned $\\left\\{X_{t}\\right\\}$ couldbe thought of as an autoregressive moving average (ARMA(1,1) process. <PARAGRAPH_SEPARATOR> # 3.2. Simulation Experiments <PARAGRAPH_SEPARATOR> To study the properties of the moment estimators we did several simulation experiments for the simplified model. The process $\\left\\{\\boldsymbol{e}_{t}\\right\\}$ was generated using a Gaussian white noise generator with mean zero and unit variance, while $\\left\\{\\theta_{t}\\right\\}$ was generated with $q=u=0.05.$ . In each case the AR order, the number of states, the state $s_{2}=0$ and the probabilities $q$ and $\\pmb{u}$ were assumed to be known. Thus the only unknowns were $s_{1}=s$ and $\\sigma^{2}$ . These were estimated using equations (3.3), (3.9) and (3.10). The results are displayed in Table 1 for several values of s and several sample sizes. In Tyssedal and Tjostheim (1985) we also considered a two-state model with $s_{1}=s$ and $s_{2}=-s.$ The results were similar."
  },
  {
    "qid": "statistic-posneg-541-0-2",
    "gold_answer": "FALSE. While the covariance matrix Σ is expressed in terms of μ₃, μ₅, and the geometry of E (via Φ₂(E)), it also implicitly depends on the parameters of the random field ε(u) (κ, τ, α) through the moments μ₃ and μ₅. The paper derives Σ as Σ = (3μ₅)/(2πμ₃a₁a₂a₃) Φ₂(E), but μ₃ and μ₅ themselves are functions of κ, τ, and α.",
    "question": "Is it true that the covariance matrix Σ of the cover density f_K₀ depends only on the moments μ₃ and μ₅ of the random field ε(u) and the geometry of the ellipsoid E?",
    "question_context": "Stochastic Particle Model with Isotropic Random Field\n\nThe paper presents a stochastic model for star-shaped particles in ℝ³, where each particle K₀ is defined as K₀ = c₀ + M, with M being star-shaped with respect to the origin. The radial function R of M is modeled multiplicatively as R(u) = M(u)ε(u), where M is the radial function of a fixed ellipsoid E and ε(u) is an isotropic random field on the unit sphere 𝕊². The random field ε(u) is given by ε(u) = ∫𝕊² k(u,v)Z(dv), where Z is a Gamma Lévy basis and k(u,v) is a von Mises–Fisher kernel. The paper derives moments of the cover density and discusses parameter estimation for the model."
  },
  {
    "qid": "statistic-posneg-1342-0-2",
    "gold_answer": "FALSE. The asymptotic independence between MLEs of regression parameters for $\\pi(x)$ and $p(x)$ in multinomial splitting GLMs holds under usual assumptions for GLMs, but it is not guaranteed for all possible link function choices. The choice of link function on $\\pi(\\pmb{x})$ affects the symmetry and properties of the resulting splitting GLM, and only the canonical link function (multinomial logit link) implies the symmetry of the splitting GLM.",
    "question": "Is the asymptotic independence between the MLEs of regression parameters for $\\pi(x)$ and $p(x)$ in multinomial splitting GLMs guaranteed under all possible link function choices?",
    "question_context": "Multinomial Splitting Distributions and Their Properties\n\nAssume now that $\\mathcal{L}(\\boldsymbol{\\psi})$ is a standard beta compound distribution. We obtain three new multivariate distributions, which are multivariate extensions of the non-standard beta binomial, non-standard beta negative binomial and beta Poisson distributions (see Table 1 of Supplementary Material S2 for details about these three multivariate distributions and Supplementary Material S1 for definitions of the non-standard beta binomial and the non-standard beta negative binomial distributions). All the characteristics of these six multinomial splitting distributions (pmf, expectation, covariance, pgf and marginal distributions) have been calculated using (6), (7), (8), (9), (10) according to the sum distribution $\\mathcal{L}(\\boldsymbol{\\psi})$"
  },
  {
    "qid": "statistic-posneg-589-0-1",
    "gold_answer": "TRUE. The selection bias formula correctly measures the absolute deviation of the estimator $\\delta_{(i)}$ from the adjusted true parameter $\\hat{a}\\theta_{(i)}$. This adjustment is necessary due to the systematic underestimation of $\\theta_{i}$ in the data. The economic intuition is that by using the least squares estimator $\\hat{a}$ to adjust the true parameter, the model accounts for the systematic bias, allowing for a more accurate assessment of the estimator's performance.",
    "question": "Does the selection bias formula $\\mathrm{SB}_{i}=|\\delta_{(i)}-\\hat{a}\\theta_{(i)}|$ correctly measure the deviation of the estimator from the adjusted true parameter?",
    "question_context": "Bias Correction and Variance Estimation in Gene Expression Analysis\n\nAfter taking $\\log_{2}$ -transformation on the six expression levels over all genes, we denote the 3 expression levels of the control group by $Y_{i1}$ , $Y_{i2}$ and $Y_{i3}$ and the 3 expression levels of the treatment by $Z_{i1}$ , $Z_{i2}$ and $Z_{i3}$ , where the subscript i denotes the ith gene. Let $X_{i}=\\bar{Y}_{i}-\\bar{Z}_{i}$ , where ${\\bar{Y}}_{i}$ and $\\bar{Z}_{i}$ are the sample average. Let $\\theta_{i}$ , for the ith gene, denote the ‘‘true’’ $\\log_{2}$ fold change of the concentrations between the treatment and control groups; and we assume that $\\theta_{i}$ is the mean of $X_{i}$ . For the group of nonspike-in genes, the $\\theta_{i}$ ’s can be regarded as 0 because both treatment and control groups should have the same level of background noise. <PARAGRAPH_SEPARATOR> Many previous works (Bolstad et al., 2003; Cope et al., 2004; Irizarry et al., 2003; Wu and Irizarry, 2004) discovered a phenomenon, namely, that the data of Choe et al. (2005) consistently underestimate the true parameter $\\theta_{i}$ s, which violates the model assumption that $\\theta_{i}$ is the mean of $X_{i}$ . This is a systematic bias produced by experiments, and creates difficulty in using the data to check how procedures work since none of them would work well. See Zhao and Hwang (2012), albeit, for confidence intervals. To overcome the difficulty, we apply a data preprocessing process proposed in Zhao and Hwang (2012), which multiples $\\theta_{i}$ ’s by a factor ‘ $a$ before applying various estimators. This factor $a$ is obtained from the least square estimator $\\hat{a}$ of $a$ , assuming the linear model $X_{i}=a\\theta_{i}$ . As in Zhao and Hwang (2012), we then consider $\\hat{a}\\theta_{i}$ to be the ‘‘true parameter’’ which will then be estimated. This way, $X_{i}$ is nearly unbiased. Although in a real application the value of $\\hat{a}$ cannot be obtained because $\\theta_{i}$ ’s are unknown, all the procedures are designed to estimate the mean of $X_{i}$ , which closely resembles $\\hat{a}\\theta_{i}$ . <PARAGRAPH_SEPARATOR> Fig. 8(a) plots the residuals $X_{i}-\\hat{a}\\theta_{i}$ corresponding to the spike-in genes, nonspike-in genes and differential expressed genes. The plot shows that the variances of these three groups are quite different, especially between the nonspike-in genes and the others. Hence we shall apply the various estimators to these three groups separately, and each group has its own variance estimator. In either case, the variance $\\sigma^{2}$ of $X_{i}$ is estimated by ${\\textstyle\\sum_{i=1}^{p}S_{i}^{2}}/{\\bar{p}}$ , where <PARAGRAPH_SEPARATOR> $$ S_{i}^{2}=\\frac{\\biggl[\\sum_{j=1}^{3}(Y_{i j}-\\bar{Y}_{i})^{2}+\\sum_{j=1}^{3}(Z_{i j}-\\bar{Z}_{i})^{2}\\biggr]}{6}. $$ <PARAGRAPH_SEPARATOR> In Fig. 8(b), (c) and (d) we plot the selection biases (SB) <PARAGRAPH_SEPARATOR> $$ \\mathrm{SB}_{i}=|\\delta_{(i)}-\\hat{a}\\theta_{(i)}|, $$ <PARAGRAPH_SEPARATOR> where $\\delta_{(i)}$ is an estimator for $\\theta_{(i)}$ . In order to have a clear graph, we first rearrange the order of ${\\mathrm{SB}}_{i}{\\mathrm{'}}$ s so that they are increasing and then plot against $i=1,\\ldots,p$ , where $p$ is the number of $X_{i}^{\\prime}{\\mathsf{s}}.$ We report the average selection bias $\\textstyle\\sum_{i=1}^{p}S{\\mathsf{B}}_{i}/p$ in the boxes inside Fig. 8(b), (c) and (d). These numbers show that the proposed estimator GMSS has the smalles t average selection bias and is the winner in all the three groups. <PARAGRAPH_SEPARATOR> It is interesting to study the risks of $\\theta_{(1)}$ and $\\theta_{(p)}$ for various estimators in these three groups. In either group, we randomly draw a quarter of genes and replicate it 1000 times to obtain the risks for estimating $\\theta_{(1)},\\theta_{(p)}$ and ‘‘all’’ $\\theta_{(i)}$ ’s. (Note that various groups have different $p$ , shown in Table 2.) The risks of ‘‘all’’ $\\theta_{(i)}$ ’s refer to the average of the risks for estimating $\\theta_{(i)}$ ’s, $i=1,\\ldots,p$ . Table 2 summarizes the ratio of the risks of GMSS to those of the other estimators. Note that a ratio less than 1 indicates that GMSS has a smaller risk. The results in Table 2 show that the GMSS has the best performance for the non-differential and differential expressed data sets. The LJS estimator has the best performance for the nonspike-in data set, where the true values are zero’s."
  },
  {
    "qid": "statistic-posneg-1640-0-3",
    "gold_answer": "FALSE. The condition actually implies that Z lies in a specific region relative to the normal vector, but not necessarily on the half-line starting from the evolute point. The paper's geometric interpretation is more nuanced and involves the curvature and normal vectors.",
    "question": "Does the condition {Z - g(ξi)}^T g''(ξi) < ||g'(ξi)||^2 imply that Z lies on the half-line along the normal to g at g(ξi), starting from the evolute point g*(ξi)?",
    "question_context": "Double Points in Nonlinear Calibration: Geometric Characterization\n\nThe paper analyzes double points in nonlinear calibration, where a function has two or more local minima. It defines sets of points where the derivative conditions for local minima are satisfied and transforms these sets using a matrix S. The analysis involves differential geometry concepts like curvature and normal vectors to characterize the geometric properties of these double points."
  },
  {
    "qid": "statistic-posneg-172-0-0",
    "gold_answer": "TRUE. The context explicitly states that 'the $D$ -optimal design does not depend on $\\theta_{0}$' and provides the justification: 'Since, for any constant $r>0$ , $|M(\\xi,r\\theta)|=|M(\\xi^{\\prime},\\theta)|$ , where $\\xi^{\\prime}\\{(r x)\\}=\\xi\\{(x)\\}$'. This invariance under scaling of $\\theta_{0}$ confirms that the D-optimal design is independent of $\\theta_{0}$.",
    "question": "Is the statement 'The D-optimal design for the model $Y=\\theta_{0}(e^{-\\theta_{1}x}-e^{-\\theta_{2}x})+\\epsilon$ does not depend on the parameter $\\theta_{0}$' true based on the provided context?",
    "question_context": "D-optimal Designs for Nonlinear Pharmacokinetic Models\n\nCompartmental models are frequently used to model the profile of the drug concentration in plasma and tissue compartments over time. If we assume rapid drug equilibrium throughout the body and linear drug absorption and elimination, the drug concentration in the plasma compartment, when administered orally, might be adequately characterized by the one-compartment pharmacokinetic model. The expected plasma drug concentration is described by the function $$f(t)=\\frac{\\mathrm{Dose}\\times k_{a}}{V\\times(k_{a}-k_{e})}\\times(e^{-k_{e}t}-e^{-k_{a}t}),$$ where $k_{a}$ and $k_{e}$ are the absorption constant and elimination constant, respectively. Atkinson et al. (1993) and Atkinson $\\&$ Haines (1996) have considered the optimal designs for this model. In this section, we apply the sufficient condition in Theorem 1 to obtain some analytical results about the locally $D$ -optimal design. By reparameterization, we consider the model $$Y=\\theta_{0}(e^{-\\theta_{1}x}-e^{-\\theta_{2}x})+\\epsilon,$$ where the $\\epsilon\\mathbf{S}$ are independently $N(0,1),\\theta_{0}>0,0<\\theta_{1}<\\theta_{2}$ and $\\theta=(\\theta_{0},\\theta_{1},\\theta_{2})^{\\mathrm{T}}$ is the parameter vector of interest. The $D$ -optimal design is invariant under reparameterization. The Fisher information matrix for $\\theta$ is $\\begin{array}{r}{M(\\xi,\\theta)=\\int_{\\chi}h(x,\\theta)h(x,\\theta)^{\\mathrm{T}}d\\xi(x)}\\end{array}$ and $\\boldsymbol{d}(\\xi,x)=h(x,\\theta)^{\\mathrm{T}}\\boldsymbol{M}^{-1}(\\xi,\\theta)h(x,\\theta)$ for the nonsingular design $\\xi$ , where $h(x,\\theta)=(e^{-\\theta_{1}x}-e^{-\\theta_{2}x},-\\theta_{0}x e^{-\\theta_{1}x},\\theta_{0}x e^{-\\theta_{2}x})^{\\mathrm{T}}$ Let $m_{i j}$ denote the $(i,j)$ th element of $M^{-1}(\\xi,\\theta)$ . It is easy to show that $d(\\xi,0)=0$ and $\\begin{array}{r}{\\operatorname*{lim}_{x\\rightarrow\\infty}d(\\xi,x)=0}\\end{array}$ . For any design $\\xi$ based on $t\\geqslant3$ points $x_{1},\\ldots,x_{t}$ and corresponding probabilities $p_{1},\\ldots,p_{t}$ , $p_{i}>0$ , $\\textstyle\\sum_{i=1}^{t}p_{i}=1$ , let $M_{i j}$ be the minor corresponding to the $(i,j)$ th element of $M(\\xi,\\theta)$ . Then $\\begin{array}{r}{M_{23}=\\theta_{0}^{2}\\sum_{1\\leqslant i<j\\leqslant t}p_{i}p_{j}f(x_{i},x_{j})}\\end{array}$ , where $f(r,s)=\\{f_{1}(r,s)f_{2}(r,s)\\}/e^{2\\theta_{2}r+2\\theta_{2}s}$ , $f_{1}(r,s)=s e^{r(\\theta_{2}-\\theta_{1})}-$ $r e^{s(\\theta_{2}-\\theta_{1})}+r-s$ and $f_{2}(r,s)=s e^{s(\\theta_{2}-\\theta_{1})}-r e^{r(\\theta_{2}-\\theta_{1})}+(r-s)e^{(r+s)(\\theta_{2}-\\theta_{1})}$ . It can be shown that $f_{1}(r,s)$ and $f_{2}(r,s)$ always have the same sign. Hence, $m_{23}=-M_{23}/|M|<0$ . If we let $u(x)=m_{22}\\theta_{1}x^{2}e^{-2\\theta_{1}x}+m_{33}\\theta_{2}x^{2}e^{-2\\theta_{2}x}-m_{23}(\\theta_{1}+\\theta_{2})x^{2}e^{-(\\theta_{1}+\\theta_{2})x}$ , then $d^{\\prime}(\\xi,x)$ is a linear combination of $$\\left\\{e^{-2\\theta_{1}x},x e^{-2\\theta_{1}x},e^{-2\\theta_{2}x},x e^{-2\\theta_{2}x},e^{-(\\theta_{1}+\\theta_{2})x},x e^{-(\\theta_{1}+\\theta_{2})x},u(x)\\right\\}.$$ It follows from Karlin $\\&$ Studden (1966, p. 10) that each of the following is a $T$ -system: $$\\begin{array}{r l}&{\\left\\{e^{-2\\theta_{1}x},x e^{-2\\theta_{1}x},e^{-2\\theta_{2}x},x e^{-2\\theta_{2}x},e^{-(\\theta_{1}+\\theta_{2})x},x e^{-(\\theta_{1}+\\theta_{2})x},x^{2}e^{-2\\theta_{1}x}\\right\\},}\\ &{\\left\\{e^{-2\\theta_{1}x},x e^{-2\\theta_{1}x},e^{-2\\theta_{2}x},x e^{-2\\theta_{2}x},e^{-(\\theta_{1}+\\theta_{2})x},x e^{-(\\theta_{1}+\\theta_{2})x},x^{2}e^{-2\\theta_{2}x}\\right\\},}\\ &{\\left\\{e^{-2\\theta_{1}x},x e^{-2\\theta_{1}x},e^{-2\\theta_{2}x},x e^{-2\\theta_{2}x},e^{-(\\theta_{1}+\\theta_{2})x},x e^{-(\\theta_{1}+\\theta_{2})x},x^{2}e^{-(\\theta_{1}+\\theta_{2})x}\\right\\}.}\\end{array}$$ Since $m_{23}<0,u(x)$ is a linear combination of $x^{2}e^{-2\\theta_{1}x},x^{2}e^{-2\\theta_{2}x}$ and $x^{2}e^{-(\\theta_{1}+\\theta_{2})x}$ with positive coefficients. By Lemma 1, (3) is a $T$ -system, and thus $d^{\\prime}(\\xi,x)$ has at most six real roots. Hence, by Theorem 1, the $D$ -optimal design for $\\chi_{0b}$ is unique and minimally supported, and so is the $D$ -optimal design for $\\chi_{a b}$ if $0<a<b$ . It is easy to show that the $D$ -optimal design does not depend on $\\theta_{0}$ . Since, for any constant $r>0$ , $|M(\\xi,r\\theta)|=|M(\\xi^{\\prime},\\theta)|$ , where $\\xi^{\\prime}\\{(r x)\\}=\\xi\\{(x)\\}$ , without loss of generality we assume that $\\theta_{2}=1$ . For some values of $\\theta_{1}$ the support points of the $D$ -optimal designs are displayed in Table 1. In the case of $\\theta_{1}=0.1$ , for instance, the $D$ -optimal design for [0, 16] is a three-point design supported on $x_{1}^{*}=0{\\cdot}889$ , $x_{2}^{*}=3{\\cdot}824$ and $x_{3}^{*}=14{\\cdot}442$ . It can be shown that for a sufficiently large $b_{0}$ , the zeros of $d(\\xi,x)-3$ are all interior to $[0,b_{0}]$ ; for $\\theta_{1}=0.1$ this value may be taken as $b_{0}=16$ . The relationships among optimal designs for different design spaces established in Corollary 1 are also illustrated in Table 1. For example, the $D$ -optimal design for [0, 8] has a design point at $x=8$ since $x_{3}^{*}>8$ , while the $D$ -optimal design for [0·8, 16] is the same as that for [0, 16] since $0.8<x_{1}^{*}<x_{3}^{*}<16$ ; the $D$ -optimal design for [2, 16] has a support point at $x=2$ since $x_{1}^{*}<2$ , while the $D$ -optimal design for [2, 8] has both $x=2$ and $x=8$ in the support because $x_{1}^{*}<2<8<x_{3}^{*}$ . From part (i) of Corollary 1, the uniform design supported on $\\{x_{1}^{*},x_{2}^{*},x_{3}^{*}\\}$ is the $D$ -optimal design for $\\chi_{0b}$ for any $b>x_{3}^{*}$ . The support points of the locally $D$ -optimal designs are sensitive to the value of $\\theta_{1}$ particularly for small value of $\\theta_{1}$ ; see Fig. 1. To account for the uncertainty in the parameter value, Bayesian optimal designs could be an alternative choice (Chaloner $\\&$ Larntz, 1989)."
  },
  {
    "qid": "statistic-posneg-820-0-0",
    "gold_answer": "FALSE. Explanation: When β_t = I, the term (β_t - I) in the equation ΔY_it = α_t + (β_t - I)^T Y_i,t-1 + γ_t^T X_i + ε_it becomes zero. This means the expectation of ΔY_it reduces to E(ΔY_it) = α_t + γ_t^T X_i, which does not depend on Y_i,t-1. The condition β_t = I implies that the change in outcome is independent of the previous outcome level, given the covariates.",
    "question": "In the linear increments model, if β_t = I for all t, does the expectation of ΔY_it depend on Y_i,t-1?",
    "question_context": "Linear Increments Model with Measurement Error and Missing Data\n\nThe paper introduces a linear increments model where the change in the underlying outcome, ΔY_it, is related to the previous outcome Y_i,t-1 and covariates X_i through the equation ΔY_it = α_t + (β_t - I)^T Y_i,t-1 + γ_t^T X_i + ε_it. The model accounts for measurement errors e_it and missing data mechanisms R_it. Key assumptions include independence of measurement errors from other processes and conditions on missing data (DTIC). The paper discusses parameter estimation via least squares and conditions under which estimators are unbiased and consistent."
  },
  {
    "qid": "statistic-posneg-290-0-1",
    "gold_answer": "FALSE. The matrix variate hypergeometric beta type II distribution does not reduce to the Wishart distribution under these conditions. Instead, it is the matrix variate generalized hypergeometric distribution (Definition 8(2)) that reduces to the Wishart distribution when $b = c$, $\\Xi = \\frac{1}{2}\\pmb{\\Sigma}$, and $\\alpha = n/2$. The hypergeometric beta type II distribution is a different family of distributions, and its reduction properties are not directly related to the Wishart distribution.",
    "question": "Does the matrix variate hypergeometric beta type II distribution reduce to the Wishart distribution when $b = c$, $\\Xi = \\frac{1}{2}\\pmb{\\Sigma}$, and $\\alpha = n/2$?",
    "question_context": "Matrix Variate Hypergeometric Distributions and Mellin Transforms\n\nThis paper introduces several families of matrix variate elliptical distributions. Section 2 gives some results on integration, using zonal polynomials. In terms of these results, various matrix variate hypergeometric type distributions are proposed, as particular cases. These include well-known distributions such as central and noncentral matrix variate inverted gamma (inverted Wishart) distributions and matrix variate central and noncentral beta type II distributions; see Section 3. In Section 4.1, assuming a hypergeometric type distribution for the matrix parameter in matrix variate normal and matricvariate $T$ distributions, several families of matrix variate elliptical distributions are obtained using the compound matrix variate approach. Section 4.3 introduces the scale mixture of a matrix variate elliptical distribution, which is derived as a particular case of the compound matrix variate approach; all the results given in Section 4.1 can be particularized to this case."
  },
  {
    "qid": "statistic-posneg-161-0-0",
    "gold_answer": "FALSE. The CUSUM control chart $S(t)$ does not signal when $R(t)$ exceeds $c$ directly, but rather when the difference between $R(t)$ and the minimum value of $R(s)$ up to time $t$ exceeds $c$. This is captured by the definition $S(t)=R(t)-\\mathrm{min}_{s\\leqslant t}R(s)$, and $\\tau$ is the first time $S(t)$ exceeds $c$. The distinction is important because $S(t)$ accounts for the cumulative deviation of $R(t)$ from its historical minimum, not just the current value of $R(t)$.",
    "question": "Is the statement 'The CUSUM control chart $S(t)$ signals at time $\\tau$ when the loglikelihood ratio test statistic $R(t)$ exceeds a threshold $c$' consistent with the definition $S(t)=R(t)-\\mathrm{min}_{s\\leqslant t}R(s)$ and $\\tau=\\operatorname*{inf}\\{t:S(t)>c\\}$?",
    "question_context": "CUSUM Control Chart for Hazard Rate Monitoring\n\nSuppose individuals arrive at times $B_{1},B_{2},\\ldots$ with covariate vectors $Z_{1},Z_{2},\\ldots$ and experience an event after $T_{1},T_{2},...$ time units. An individual may be censored after $C_{1}$ , $C_{2}$ , . . . time units. We assume for every individual that $T_{i}$ and $C_{i}$ are conditionally independent given $B_{i}$ and $Z_{i}$ . The only addition to the standard survival analysis set-up are the arrival times $B_{i}$ . <PARAGRAPH_SEPARATOR> At calendar time $t\\geqslant B_{i}$ we observe for the $i$ th individual the covariate vector $Z_{i}$ , the current time at risk <PARAGRAPH_SEPARATOR> $$ X_{i}(t)=\\operatorname*{min}\\{T_{i},C_{i},t-B_{i}\\}, $$ <PARAGRAPH_SEPARATOR> and the event indicator $\\delta_{i}(t)=I\\{T_{i}=X_{i}(t)\\}$ , where $I$ denotes the indicator function. <PARAGRAPH_SEPARATOR> Suppose that in control the hazard rate of $T_{i}$ conditional on the covariates $Z_{i}$ is $h_{0i}(\\cdot)=$ $h_{0}(\\cdot,Z_{i})$ . This hazard rate intuitively corresponds to an acceptable state, e.g. based on past performance. Assume that after the unknown time $\\eta\\in[0,\\infty]$ the hazard rate switches to an out-of-control hazard rate $h_{1i}(\\cdot)=h_{1}(\\cdot,Z_{i})$ . Thus for an individual born at time $B_{i}\\leqslant\\eta$ , the hazard rate of $T_{i}$ is $h_{0i}(s)$ for $s\\leqslant\\eta-B_{i}$ and $h_{1i}(s)$ for $s>\\eta-B_{i}$ . We denote the corresponding integrated hazard rates by $\\begin{array}{r}{H_{j i}(t)=\\int_{0}^{t}h_{j i}(s)d s,j=0,1}\\end{array}$ . <PARAGRAPH_SEPARATOR> Moustakides (1986, 2004) has shown that CUSUM charts based on the likelihood ratio are optimal in a certain sense. Motivated by this, we will define a CUSUM chart based on a partial likelihood. The partial likelihoods for being in-/out-of-control up to calendar time $t$ are <PARAGRAPH_SEPARATOR> $$ L_{j}(t)=\\prod_{i\\colon B_{i}\\leqslant t}h_{j i}(T_{i})^{\\delta_{i}(t)}\\exp[-H_{j i}\\{X_{i}(t)\\}]\\quad(j=0,1). $$ <PARAGRAPH_SEPARATOR> Note that the contributions from the censoring and arrival time distributions are omitted (Aalen et al., 2008, p. 213). <PARAGRAPH_SEPARATOR> The corresponding loglikelihood ratio test statistic for in-control versus out-of-control is <PARAGRAPH_SEPARATOR> $$ R(t)=\\sum_{i:B_{i}\\leqslant t}\\delta_{i}(t)\\log\\left\\{\\frac{h_{1i}(T_{i})}{h_{0i}(T_{i})}\\right\\}-\\sum_{i:B_{i}\\leqslant t}[H_{1i}\\{X_{i}(t)\\}-H_{0i}\\{X_{i}(t)\\}]. $$ <PARAGRAPH_SEPARATOR> Based on this we define a CUSUM control chart $S(t)=R(t)-\\mathrm{min}_{s\\leqslant t}R(s)$ , which signals at time $\\tau=\\operatorname*{inf}\\{t:S(t)>c\\}$ for some threshold $c>0$ . The next section discusses how to choose $c$ . <PARAGRAPH_SEPARATOR> # 2·2. Choosing the threshold <PARAGRAPH_SEPARATOR> Several criteria can be used to choose the threshold $c$ . A common approach is to prescribe an in-control average run length in calendar time, which is the expected time until the threshold is crossed, i.e. ${E(\\tau\\mid\\eta=\\infty)}$ . Another approach is to prescribe a false alarm probability in a given time window, e.g. p $\\mathbf{\\Psi}\\left(\\tau\\leqslant U\\mid\\eta=\\infty\\right)=\\boldsymbol{\\alpha}$ for some $U>0$ and $0<\\alpha<1$ ."
  },
  {
    "qid": "statistic-posneg-414-1-2",
    "gold_answer": "TRUE. The paper states that the quasi-maximum likelihood estimator is supposed to be more efficient than the estimators obtained via the non-parametric method, which is why it is applied in the second step after the initial non-parametric estimation and variable selection.",
    "question": "Is the quasi-maximum likelihood estimator used in the second step of the estimation process more efficient than the estimators obtained via the non-parametric method?",
    "question_context": "GARCH-type Factor Model Estimation in High-Dimensional Time Series\n\nIn large panels, the dimension of time series is comparable to, or even larger than, the sample size. Under such settings, the maximum likelihood method and the corresponding statistical theory are complicated. The number of parameters grows as the sample size and the number of time series increases. Moreover, a nonlinear objective function must be addressed. Compared to classic factor models, the GARCH-type factor model bypasses the above-mentioned challenges, and can be estimated by the quasi-maximum likelihood approach proposed in the last section. The dynamics of the factors are driven by the previous value of the factors and the previous observation $\\pmb{y}_{t-1}$ through the weights $\\mathbf{H}_{1}$ . When the dimension of the time series is large, a sparsity assumption of $\\mathbf{H}_{1}$ that is introduced later on is appropriate since not all of the components of the time series contribute to the common factors. For example, in the analysis of stock/bond return data, it may be beneficial to identify companies that lead the entire economic sector. The idiosyncratic factors of such leading companies then contribute to the common factors of the subsequent observations.<PARAGRAPH_SEPARATOR>The computational burden can be largely reduced if the positions of the nonzero elements in $\\mathbf{H}_{1}$ are identified before the quasi-likelihood estimation is used. Note that the recursive relationship in (1) that governs the dynamics of $\\mathbf{F}_{t}$ is linear. If crude estimates of $\\mathbf{F}_{t}$ are first obtained, the identification of nonzero elements in $\\mathbf{H}_{1}$ can be considered as a standard variable selection problem of linear regression that can be easily carried out by existing computer packages. As such, we propose a two-step estimation method. In the first step, the nonzero support of $\\mathbf{H}_{1}$ , denoted as $\\mathcal{A}$ , is estimated. To achieve this, $\\mathbf{F}_{t}$ in the second equation of (1) is first approximated. In the large panel, a non-parametric method based on principle component (PC) is proposed by [2,21] for estimating the factor loading and factor process. The consistency of the PC estimators, $\\widehat{\\pmb{\\Lambda}}^{P C}$ and $\\widehat{\\mathbf{F}}_{t}^{\\hat{P C}}$ are also established, where $\\widehat{\\Lambda}^{P\\bar{C}}$ is the eigenvectors of the sample covariance matrix of ${\\pmb y}_{t}$ corresponding to i tˆs $r$ larg eˆst eigenvalues and $\\widehat{\\mathbf{F}}_{t}^{P C}=(\\widehat{\\mathbf{A}}^{P C})^{\\top}\\mathbf{y}_{t}/N$ . In the above-mentioned articles, the estimators $\\widehat{\\mathbf{F}}_{t}^{P C}$ are further utilized to be the regressors for coeffi cˆient estˆimation in a new regression model. It will later be shown  thˆat influences of the errors in the estimated factors to the variable selection procedure diminish asymptotically. Therefore, in high-dimensional time series, the PC method provides a valid estimator of factors in the GARCH-type factor model since this non-parametric approach does not assume the specific structure of common latent factors, as long as the covariance matrix of factors is time-invariant. Once the non-parametric estimator, $\\widehat{\\mathbf{F}}_{t}^{P C}$ , is obtained, what remains is to address the coefficient estimation of the following noiseless high-dimensional regre sˆsion model:<PARAGRAPH_SEPARATOR>$$\\widehat{\\mathbf{F}}_{t}^{P C}=\\mathbf{H}_{0}+\\mathbf{H}_{1}\\mathbf{y}_{t-1}+\\mathbf{H}_{2}\\widehat{\\mathbf{F}}_{t-1}^{P C}.$$<PARAGRAPH_SEPARATOR>A multitude of high-dimensional variable selection methods have been proposed in recent years. The ${\\mathsf{M}}{\\mathsf{C}}+$ method of [24] is adopted here:<PARAGRAPH_SEPARATOR>$$\\operatorname*{min}\\frac{1}{T}\\sum_{t=1}^{T}\\big\\lVert\\widehat{\\mathbf{F}}_{t}^{P C}-\\mathbf{H}_{0}-\\mathbf{H}_{1}\\mathbf{y}_{t-1}-\\mathbf{H}_{2}\\widehat{\\mathbf{F}}_{t-1}^{P C})\\big\\rVert_{2}^{2}+\\sum_{i,j}\\rho(H_{1,i,j};\\lambda),$$<PARAGRAPH_SEPARATOR>where $H_{1,i,j}$ is the jth element in the ith row of $\\pmb{H}_{1}$ , $\\begin{array}{r}{\\rho(t;\\lambda)=\\lambda\\int_{0}^{t}(1-x/(\\gamma\\lambda))_{+}d x;}\\end{array}$ and $\\|\\cdot\\|_{2}$ is the Euclidean norm. Denote the estimator of coefficients as $\\widehat{\\mathbf{H}}^{P C}$ . The nonzero support $\\mathcal{A}$ can be obtained from $\\widehat{\\mathbf{H}}^{P C}$ and denoted as $\\hat{\\boldsymbol{\\mathcal{A}}}$ Denote the nonzero sub-vector of the jth row o fˆ the matrix $\\mathbf{H}_{1}$ as $\\mathbf{H}_{1}^{j}$ and the corresponding sub-v eˆctor of $\\mathbf{y}_{t-1}$ as $\\mathbf{y}_{t-1}^{j}$ . Then, the GARCH-type factor model reduces to the following parsimonious form:<PARAGRAPH_SEPARATOR>$$\\begin{array}{r}{{\\bf y}_{t}={\\boldsymbol\\Lambda}{\\bf F}_{t}+\\epsilon_{t},\\quad{\\bf F}_{j t}={\\bf H}_{0,i}+{\\bf H}_{1}^{j}{\\bf y}_{t-1}^{j}+{\\bf H}_{2,i}{\\bf F}_{j,t-1},\\quad\\mathrm{for}j\\in\\lbrace{1,\\ldots,r\\rbrace}.}\\end{array}$$<PARAGRAPH_SEPARATOR>Since the quasi-maximum likelihood estimator is supposed to be more efficient than the estimators obtained via the non-parametric method and the penalty function introduces biases, the quasi-maximum likelihood estimation approach proposed in Section 2 is further applied to the reduced model (10) for coefficient estimation in the second step. The<PARAGRAPH_SEPARATOR>Table 1 The true values, means, and empirical standard deviations of the quasi-maximum likelihood estimators of $\\begin{array}{r l}{\\pmb{\\Lambda}}&{{}=}\\end{array}$ $(\\lambda_{1,1},\\ldots,\\lambda_{10,1})^{\\top}$ , $H_{0}$ , $\\mathbf{H}_{1}=(H_{1,1},\\dots,H_{1,10})$ and $H_{2}$ of the GARCH-type factor model with $r=1$ factor.<PARAGRAPH_SEPARATOR><html><body><table><tr><td></td><td>入1,1</td><td>入2.1</td><td>入3.1</td><td>入4,1</td><td>入5,1</td><td>入6,1</td><td>入7.1</td><td>入8.1</td><td>入9.1</td><td>入10.1</td><td>Ho</td></tr><tr><td rowspan=\"4\">Truevalue Mean</td><td>0.307</td><td>-0.717</td><td>0.989</td><td>1.967</td><td>0.027</td><td>0.419</td><td>1.491</td><td>0.842</td><td>0.051</td><td>1.194</td><td>0.28</td></tr><tr><td>0.307</td><td>0.717</td><td>0.989</td><td>1.967</td><td>0.024</td><td>0.419</td><td>1.490</td><td>0.841</td><td>0.053</td><td>1.194</td><td>0.280</td></tr><tr><td>0.033</td><td>0.033</td><td>0.030</td><td>0.026</td><td>0.032</td><td>0.034</td><td>0.029</td><td>0.032</td><td>0.034</td><td>0.031</td><td>0.010</td></tr><tr><td>H1.1</td><td>H1.2</td><td>H1,3</td><td>H1,4</td><td>H1.5</td><td>H1.6</td><td>H1.7</td><td>H1.8</td><td>H1.9</td><td>H1,10</td><td>H2</td></tr><tr><td>Truevalue</td><td>0.56</td><td>0</td><td>-0.18</td><td>0</td><td>0.53</td><td>0</td><td>0</td><td>-0.4</td><td>0.76</td><td>0</td><td>0.33</td></tr><tr><td>Mean</td><td>0.559</td><td>0.001</td><td>-0.180</td><td>0.000</td><td>0.529</td><td>-0.001</td><td>0.001</td><td>-0.401</td><td>0.760</td><td>0.000</td><td>0.338</td></tr><tr><td>ps</td><td>0.013</td><td>0.012</td><td>0.012</td><td>0.012</td><td>0.012</td><td>0.012</td><td>0.012</td><td>0.012</td><td>0.013</td><td>0.012</td><td>0.042</td></tr></table></body></html>"
  },
  {
    "qid": "statistic-posneg-1265-1-2",
    "gold_answer": "TRUE. The paper explicitly states that the bottom-up method, by construction, cannot separate mutual clusters, whereas the top-down method can break mutual clusters. This is because mutual clusters are preserved in the bottom-up approach, while the top-down method may join points in mutual clusters with external points before fully joining all points within the mutual cluster.",
    "question": "Does the paper claim that the bottom-up method cannot break mutual clusters by construction, while the top-down method can?",
    "question_context": "Hybrid Hierarchical Clustering Performance Metrics\n\nThe paper evaluates clustering methods using three performance measures: the number of 'broken' mutual clusters, the percentage of misclassified points, and the relative within-cluster sum of squares (WSS). The misclassification measure M(C,T) is defined as the normalized Hamming distance between the true and estimated clusterings. The WSS is calculated based on partitioning the dendrogram into the true number of clusters, and relative WSS is reported as the ratio of WSS for the estimated clustering to WSS for the true clustering."
  },
  {
    "qid": "statistic-posneg-1086-1-0",
    "gold_answer": "TRUE. Explanation: The formula $\\hat{\\sigma}=\\hat{\\sigma^{*}}(1-F_{n}(u))^{\\hat{\\xi}}$ shows that $\\hat{\\sigma}$ depends on both $\\hat{\\sigma^{*}}$ and $(1-F_{n}(u))^{\\hat{\\xi}}$. Since $1-F_{n}(u)$ is a probability term, it lies between 0 and 1. When $\\hat{\\xi}$ is large, $(1-F_{n}(u))^{\\hat{\\xi}}$ becomes very small (approaching zero), causing $\\hat{\\sigma}$ to tend to zero regardless of the value of $\\hat{\\sigma^{*}}$. This explains why the MSE of $\\hat{\\sigma}$ becomes close to the square of the true $\\sigma$ when $\\hat{\\xi}$ is overestimated.",
    "question": "Is it true that the formula $\\hat{\\sigma}=\\hat{\\sigma^{*}}(1-F_{n}(u))^{\\hat{\\xi}}$ implies that for large values of $\\hat{\\xi}$, $\\hat{\\sigma}$ tends to zero regardless of $\\hat{\\sigma^{*}}$?",
    "question_context": "Quantile Estimation in Generalized Pareto Distribution (GPD) with Large Shape Parameters\n\nAlthough MLE is very popular, we did not include its performance in the simulation results because the estimation could not be done in most of the cases when the shape parameter $\\xi$ is greater than 0.5 due to the observed information matrix becoming singular in many cases. The ‘‘mdpd’’ method also gives very unstable results when the shape parameter is greater than 1. Therefore, we did not include its results. <PARAGRAPH_SEPARATOR> As shown in Table 2 with a fixed scale parameter and a varying shape parameter, the performance of the estimators is very close when the shape parameter $\\xi$ is zero or 0.5. However, ‘‘mme’’,‘‘pwmu’’,‘‘pwmb’’, and ‘‘mdpd’’ methods start to break down when the shape parameter reaches 1. When $\\xi>1$ , these methods under-estimate the shape parameter and heavily over-estimate the scale parameter, although the actual estimated values are not reported in Table 2. The ‘‘lme’’ method performs well until the shape parameter is 2 but starts to break down when the shape parameter is greater than 2. Only ‘‘med’’, ‘‘pickands’’, and ‘‘Zhang’’ methods give results comparable with our proposed methods. In fact, Zhang’s method gives results very close to ours. Nevertheless, our proposed methods clearly give the best results. <PARAGRAPH_SEPARATOR> In Table 3, we compare the performances of some estimators when the shape parameter is fixed and the scale parameter is varying. Again, our proposed methods clearly perform best in this case. Note that the ‘‘lme’’ estimator performs well with small values of the scale parameter, but it gives very high MSE values for large values of the scale parameter. Especially, the MSE of $\\hat{\\sigma}$ is almost the same as the square of the true $\\sigma$ , which is explained as follows. ‘‘lme’’ overestimates the shape parameter, $\\xi$ , when $\\sigma$ is large. With very large values of $\\hat{\\xi}$ , the estimated scale parameter in the original GPD distribution tends to zero. When the ‘‘lme’’ estimator (as well as other estimators) is computed, the scale parameter of GPD is obtained by the following formula <PARAGRAPH_SEPARATOR> $$ \\hat{\\sigma}=\\hat{\\sigma^{*}}(1-F_{n}(u))^{\\hat{\\xi}}, $$ <PARAGRAPH_SEPARATOR> where $\\sigma^{*}$ is the scale parameter of GPD when the observations over the threshold value are treated as if they are an original sample from GPD. (See McNeil and Saladin (1997) for more details.) So if values of $\\hat{\\xi}$ are large, $\\hat{\\sigma}$ tends to zero regardless of $\\hat{\\sigma^{*}}$ and, thus, the MSE is close to the square of the true value of $\\sigma$ . <PARAGRAPH_SEPARATOR> Our proposed method should perform better with larger sample sizes since the empirical distribution function converges to the true distribution function as the sample size increases. When we computed means and standard deviations for our proposed estimators with different sample sizes in Table 4, our methods estimated the parameters very accurately with the large sample size, as expected. <PARAGRAPH_SEPARATOR> Table 4 Means of NLS parameter estimators with increasing sample size (standard deviations are in the parenthesis)."
  },
  {
    "qid": "statistic-posneg-2067-0-1",
    "gold_answer": "FALSE. While the paper states that G=5 works well in practice, this claim is based on empirical observations rather than mathematical properties of T(x,y;h_n,s). The accuracy of the approximation depends on the behavior of T(x,y;h_n,s) within the interval I(x,y;h_n), which may not be guaranteed to be well-approximated by only 5 points. The paper acknowledges that larger values of G can improve the approximation, but chooses G=5 for computational efficiency.",
    "question": "The paper suggests that the optimal threshold S_0 can be accurately approximated by evaluating T(x,y;h_n,s) at only G=5 regularly spaced values in the interval I(x,y;h_n). Is this claim supported by the mathematical properties of T(x,y;h_n,s)?",
    "question_context": "Image Denoising via Local Clustering and Adaptive Smoothing\n\nThe paper presents a method for image denoising using a local clustering framework. The observed image is modeled as a 2D regression surface with discontinuities at object boundaries. The method involves clustering pixels into two groups based on intensity values within a neighborhood, using a separation measure to determine the optimal threshold. The denoised image is then estimated by weighted averaging within the identified clusters, with weights based on local similarity measures."
  },
  {
    "qid": "statistic-posneg-977-1-1",
    "gold_answer": "TRUE. The moments of λ₄ are given by a complex expression that includes products of Gamma functions and terms involving nα and nα*, as shown in the formula for E(λ₄^h). This reflects the intricate structure of the likelihood ratio statistic in the context of testing covariance matrices.",
    "question": "Does the expression for the moments of λ₄ involve products of Gamma functions and terms involving nα and nα*?",
    "question_context": "Likelihood Ratio Statistics for Testing Covariance Structures in Complex Multivariate Normal Populations\n\nThe paper discusses the modified likelihood ratio statistic for testing the hypothesis that the covariance matrix is equal to a specified matrix. The moments of the likelihood ratio statistic are derived and used to approximate its distribution. The modified likelihood ratio test statistic is obtained by changing certain parameters in the original likelihood ratio test statistic. The paper also provides tables with significance levels and upper percentage points for the distribution of the test statistic."
  },
  {
    "qid": "statistic-posneg-2029-0-1",
    "gold_answer": "FALSE. The paper indicates that the performance of the method is sensitive to the difference Δ=|τϕ,A - τA*|. When Δ is small to moderate, π=4% and 8% are preferable, but when Δ is large, π=1% and 4% are better. Thus, π=4% is suggested as a compromise but not universally optimal for all scenarios.",
    "question": "Does the paper suggest that π=4% is always the optimal choice for estimating A* regardless of the value of Δ?",
    "question_context": "Archimax Distributions and Estimation Sensitivity to Dependence Parameters\n\nThe paper examines the effect of the choice of π on the quality of the estimation of A* using pseudo-random samples from three families of Archimax distributions with the same dependence function A and different Archimedean generators ϕ. The generators are rigged to have the same degree of regular variation -m=-6/5, ensuring the value of Kendall's tau of their common maximum attractor A* is approximately 1/4. The generators used are ϕ₁,αᵐ, ϕ₂,αᵐα, and ϕ₃,αᵐ, where ϕ₃,α(t) is the generator of Frank's family of copulas with parameter α∈ℝ. The study involves generating samples of various sizes and estimating A* using different proportions π, analyzing the bias in Kendall's tau estimation and the sensitivity to the difference Δ=|τϕ,A - τA*|."
  },
  {
    "qid": "statistic-posneg-2121-12-0",
    "gold_answer": "FALSE. While the factorization $[S,X,Y|B]=[Y|S(X),B][X|B][S|B]$ is mathematically correct under the conditional independence assumption $[X|S,B]=[X|B]$, it does not fully return to a non-preferential setting. The term $[X|B]$ still encodes the preferential sampling design (e.g., $X\\sim U[x:\\mathbb{E}\\{S(x)\\}>\\varepsilon]$), meaning the sampling locations $X$ are still influenced by prior beliefs about $S$ (via $B$). The dependence between $S$ and $X$ is mediated through $B$, but the sampling remains preferential because $X$ is chosen based on expectations about $S$. The factorization simplifies the likelihood but does not eliminate the need to account for the sampling design's bias.",
    "question": "Is the statement 'Given prior knowledge $B$, the distributional relationship becomes $[S,X,Y|B]=[Y|S(X),B][X|B][S|B]$, which returns us to a non-preferential setting' mathematically justified based on the conditional independence assumption $[X|S,B]=[X|B]$?",
    "question_context": "Preferential Sampling and Conditional Independence in Geostatistical Inference\n\nPreferential sampling occurs when sampling locations $X$ and the process $S$ are dependent. This implies that, when choosing $X$ , the survey designer had prior knowledge of $S$ which was used in some manner in the design. For example, the locations in the 1997 sample in the Galicia data set were chosen to lie in regions where the surveyors believed a priori that there were likely to be large gradients of lead concentrations. This dependence complicates the analysis as we can then no longer factorize the distribution as $$[S,X,Y]{=}[Y|S(X)][X][S].$$ My question concerns whether we can bypass the problem of preferential sampling by conditioning on the information that induced the dependence between $S$ and $X$ Suppose that $B$ is the surveyor’s prior belief about $S$ before observing $Y$ and that this information includes the model for how $X$ was chosen. For example, we might hope to elicit the prior expected response surface $\\mathbb{E}\\{S(x)\\}$ and perhaps also the variance. This corresponds to specifying a mean and correlation function in assumption $I$ for the Gaussian process that is assumed in the paper. Models for how the locations $X$ were chosen can be decided after discussion with the surveyor. Examples might include choosing $X$ where we expected $S$ to be large (e.g. $X\\sim U[x\\colon\\mathbb{E}\\{S(x)\\}>\\varepsilon])$ , or where the derivative is large $(X\\sim U[x:|\\mathrm{d}\\mathbb{E}\\{S(x)\\}/\\mathrm{d}x|>\\varepsilon])$ etc. The model stated in assumption $^2$ in the paper, namely that $X$ is a Poisson process with rate $\\lambda(x){=}\\exp\\{\\alpha+\\beta S(x)\\}$ , could never arise in practice as the surveyors do not know $S$ . However, it is feasible that the rate could be $\\lambda(x){=}\\exp[\\alpha+\\beta\\mathbb{E}\\{S(x)\\}].$ , for example. For each of these models, $S$ and $X$ are dependent. However, they are conditionally independent given $\\mathbb{E}\\{S(x)\\}$ . Hence, if $B$ contains the information that induced the dependence between $S$ and $X$ , then $[X|S,B]=[X|B]$ . Thus, given prior knowledge $B$ , the distributional relationship becomes $$\\begin{array}{r l}{[S,X,Y|B]=[Y|S(X),B][X|S,B][S|B]}&{{}}\\ {=[Y|S(X),B][X|B][S|B],}\\end{array}$$ which returns us to a non-preferential setting (see equation (17)). Typically, $[Y|S(X),B]=[Y|S(X)]$ if the prior information solely concerns $S$ and $X$ , and the distribution $[X|B]$ is the model that is used by the surveyors to choose $X$ . The final term in equation (18), $[S|B]$ , is tractable for many types of prior information if we assume that $S$ is a Gaussian process. In conclusion, by using expert knowledge there may be an alternative approach to dealing with preferential sampling."
  },
  {
    "qid": "statistic-posneg-1632-0-0",
    "gold_answer": "TRUE. The paper derives the differential equation (1/c(θ))(d/dθ)logπ(θ) = -(d/dθ){1/c(θ)} for the prior π(θ). Solving this equation yields π(θ) ∝ c(θ), where c(θ) = E{(∂/∂θ)log f(X,θ)} = a₁'(θ)f(a₁(θ),θ) - a₂'(θ)f(a₂(θ),θ). This ensures the posterior and frequentist probabilities of the event {nσ(θ-θ̂) ≤ t} match up to O(n^{-2}).",
    "question": "In the one-parameter non-regular case, the probability matching prior π(θ) is proportional to the function c(θ) defined as c(θ) = E{(∂/∂θ)log f(X,θ)}. Is this statement true based on the derived differential equation and its solution?",
    "question_context": "Probability Matching Priors in Non-Regular Cases\n\nThe paper discusses probability matching priors for non-regular cases where the support of the density depends on the parameter θ. It derives conditions under which a prior π ensures that the posterior and frequentist probabilities of certain events match up to O(n^{-2}). The key differential equation derived is (1/c(θ))(d/dθ)logπ(θ) = -(d/dθ){1/c(θ)}, with solution π(θ) ∝ c(θ). The paper extends this to multi-parameter cases involving an additional regular parameter φ."
  },
  {
    "qid": "statistic-posneg-1520-0-1",
    "gold_answer": "FALSE. The paper does not claim that DP12 always outperforms ridge regression. It specifically states that ridge regression may perform better than DP12 when there are very few observations (e.g., 10 or less). However, when there are enough data to estimate the tuning parameters γ and λ well, DP12 can substantially outperform ridge regression, especially if the relationship between y and X is nonlinear.",
    "question": "Does the paper claim that the DP12 method always outperforms ridge regression, regardless of the number of observations?",
    "question_context": "Double Penalization in Additive Models: Combining Ridge and Smoothness Penalties\n\nThe paper discusses the prediction of a response variable y from a p-dimensional predictor variable x using an additive model. The model is defined as E(y|x) = μ(x) = β0 + s1(x1) + ... + sp(xp), where β0 is an intercept and s1(x1), ..., sp(xp) are smooth functions of the predictor variables. Estimation is performed by minimizing a penalized least squares criterion, which can include penalties on the first and second derivatives of the functions. The paper introduces a double penalization method (DP12) that combines both first and second order penalizations to improve predictions, especially in scenarios with few training data or nonlinear relationships."
  },
  {
    "qid": "statistic-posneg-1836-0-2",
    "gold_answer": "FALSE. The optimization problem involves clustering $N$ curves into $k$ groups, where $k$ is typically much smaller than $N$. The goal is to find $k$ templates and $N$ warping functions that maximize the mean similarity measure, not to have each curve in its own cluster.",
    "question": "The optimization problem requires the number of clusters $k$ to be equal to the number of curves $N$. True or False?",
    "question_context": "k-Means Alignment for Curve Clustering with Unknown Templates\n\nCase of unknown templates. Ideally, in order to cluster and align the set of $N$ curves $\\{\\mathbf{c}_{1},\\hdots,\\mathbf{c}_{N}\\}$ with respect to $k$ unknown templates we should first solve the following optimization problem: (i) find $\\underline{{\\varphi}}=\\{\\pmb{\\varphi}_{1},\\dots,\\pmb{\\varphi}_{k}\\}\\subset\\mathcal{C}$ and $\\mathbf{\\underline{{h}}}=\\{h_{1},\\dots,h_{N}\\}\\subset W$ such that $\\frac{1}{N}\\sum_{i=1}^{N}\\rho(\\varphi_{\\lambda(\\underline{{\\varphi}},\\mathbf{c}_{i})},\\mathbf{c}_{i}\\circ h_{i})\\geq\\frac{1}{N}\\sum_{i=1}^{N}\\rho(\\psi_{\\lambda(\\underline{{\\psi}},\\mathbf{c}_{i})},\\mathbf{c}_{i}\\circ g_{i}),$ for any other set of $k$ templates $\\underline{{\\psi}}=\\{\\pmb{\\psi}_{1},\\dots,\\pmb{\\psi}_{k}\\}\\subset\\mathcal{C}$ and any other set of $N$ warping functions $\\mathbf{\\underline{{g}}}=\\{g_{1},\\ldots,g_{N}\\}\\subset W$ ; and then, for $i=1,\\ldots,N$, (ii) assign ${\\bf c}_{i}$ to the cluster $\\lambda(\\underline{{\\pmb\\varphi}},\\mathbf{c}_{i})$ , and align it to the corresponding template, ${\\pmb\\varphi}_{\\lambda(\\underline{{\\varphi}},{\\bf c}_{i})}$ , using the warping function $h_{i}$. warping functions. Note that the solution $(\\underline{{\\pmb{\\varphi}}},\\underline{{\\pmb{\\ h}}})$ to (i) has mean similarity $\\frac{1}{N}\\sum_{i=1}^{N}\\rho(\\pmb{\\varphi}_{\\lambda(\\underline{{\\varphi}},\\mathbf{c}_{i})},\\mathbf{c}_{i}\\circ h_{i})$ $k$ equal to 1 if and on $N$ if it is possible to perfectly align and cluster in $k$ groups the set of curves, i.e., if and only if there exist $\\underline{{\\mathbf{h}}}$ and a partition $\\mathcal{P}=\\{P_{1},\\ldots,P_{k}\\}$ of $\\{1,\\ldots,N\\}$ in $k$ elements, such that $\\rho(\\mathbf{c}_{i}\\circ h_{i},\\mathbf{c}_{j}\\circ h_{j})=1$ for all i and $j$ belonging to the same element of $\\mathcal{P}$."
  },
  {
    "qid": "statistic-posneg-630-4-0",
    "gold_answer": "FALSE. Explanation: While the paper states that the negative binomial fit is 'slightly better,' the provided metrics do not directly support this. P=0.951 indicates a high probability that the observed data fits the negative binomial model, but without the corresponding P-value for the Poisson-Exponential sum (only χ²=5.88 is given), a direct comparison is not possible. Additionally, the χ² value alone does not indicate goodness-of-fit without degrees of freedom or P-value context. The claim of 'better fit' is thus not fully substantiated by the presented metrics.",
    "question": "The paper claims that a single negative binomial distribution (with two significant constants) provides a better fit to the accidental deaths data than the sum of 11 Poisson-Exponential distributions (with 11 insignificant constants). Is this claim supported by the provided goodness-of-fit metrics (P=0.951 for negative binomial vs. χ²=5.88 for Poisson-Exponential sum)?",
    "question_context": "Negative Binomial vs. Poisson-Exponential Fit for Accidental Deaths Data\n\nThe paper analyzes accidental deaths in 11 trade-societies, comparing the fit of a single negative binomial distribution to the sum of 11 Poisson-Exponential distributions. The negative binomial is parameterized with m=4.3636 and σ²=7.5849, leading to the formula 99(1.7382 - 0.7382)^{-5.6111}. The fit is evaluated using chi-square goodness-of-fit tests, resulting in P=0.951 for the negative binomial and χ²=5.88 for the Poisson-Exponential sum. The paper critiques the use of the 'Law of Small Numbers' (Poisson-Exponential) for cases with small absolute frequencies, emphasizing the importance of relative frequency."
  },
  {
    "qid": "statistic-posneg-1299-0-1",
    "gold_answer": "FALSE. While Table 1 shows that the W-CUSUM generally has higher values of $\\theta_{0}$ (indicating better performance) for Student $t$-distributions with heavier tails ($\\nu=1, 2, 3, 4$), the VdW-CUSUM performs better when the distribution is normal ($\\nu=\\infty$). The paper explicitly states that the W-CUSUM should be preferred except when the distribution is normal. Therefore, the claim that the W-CUSUM always outperforms the VdW-CUSUM for all distributions is false.",
    "question": "Does the Wilcoxon SSR CUSUM (W-CUSUM) always outperform the Van der Waerden SSR CUSUM (VdW-CUSUM) for all distributions, as suggested by the values of $\\theta_{0}$ in Table 1?",
    "question_context": "Signed Sequential Rank CUSUMs for Change Detection\n\nThe paper introduces Signed Sequential Rank (SSR) CUSUMs for detecting changes in symmetric distributions with zero median. The CUSUMs are defined using recursive sequences $D_{i}^{+}$ and $D_{i}^{-}$, which are updated based on a score function $\\xi_{i}$. The score function $\\xi_{i}$ is constructed from signed sequential rank statistics, ensuring zero mean and unit variance under the in-control regime. The paper discusses the properties of SSR CUSUMs, including their ability to detect shifts in the median or onset of asymmetry, and compares the performance of Wilcoxon (W-CUSUM) and Van der Waerden (VdW-CUSUM) score functions."
  },
  {
    "qid": "statistic-posneg-2111-1-1",
    "gold_answer": "FALSE. While Theorem 1 states that the empirical distribution is uniquely determined by its halfspace depth contours, it does not guarantee that any arbitrary collection of nested convex contours corresponds to a valid empirical distribution. The context explicitly mentions that Section 3 provides an example showing that not every collection of nested convex contours originates from the halfspace depth function of an empirical distribution.",
    "question": "Does Theorem 1 imply that any empirical distribution can be uniquely reconstructed from its halfspace depth contours $\\{D_{1},...,D_{k^{*}}\\}$?",
    "question_context": "Halfspace Depth and Empirical Distribution Characterization\n\nTheorem 1. The empirical distribution of any data set $X_{n}\\subset\\mathbb{R}^{p}$ is uniquely determined by its halfspace depth function, i.e. the list of contours $\\{D_{1},...,D_{k^{*}}\\}$ . <PARAGRAPH_SEPARATOR> An analogous property was already proved for the zonoid depth in (Koshevoy and Mosler 1997). Koshevoy (1997) proves the same property for the Oja depth (Oja 1983) and the simplicial depth (Liu 1990) when $X_{n}$ is in general position. Together with the result of He and Wang (1997) that empirical depth contours converge to population depth contours, Theorem 1 suggests that one can use depth contours to understand distributional properties. <PARAGRAPH_SEPARATOR> The proof of Theorem 1 will be given for a data set $X_{n}$ of arbitrary dimension $p$ , with data points in any position. Throughout this paper, the interior of a set $A$ will be denoted as $\\mathring{A}$ and its boundary as $\\hat{O}A$ . We will often mention the dimension $\\dim(C)$ of a convex set $C\\in\\mathbb{R}^{p}$ , which is defined as the dimension of the affine span of $C$ : <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathrm{affinespan}(C)=\\mathbf{c}+\\mathrm{linearspan}(C-\\mathbf{c})\\qquad\\mathrm{with}\\quad\\mathbf{c}\\in C}\\ &{\\qquad=\\displaystyle\\left\\{\\sum_{i=1}^{m}{l_{i}}\\mathbf{x}_{i};m\\in\\mathbb{N}\\mathrm{~and~}\\sum_{i=1}^{m}{l_{i}}=1\\mathrm{~and~all~}\\mathbf{x}_{i}\\in C\\right\\}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> In Section 2 we prove Theorem 1 for data sets in general position, and then generalize this to arbitrary position in Section 3. Moreover, Section 3 gives an example showing that not every collection of nested convex contours originates from the halfspace depth function of an empirical distribution. Finally, Section 4 focuses on regression depth. This depth concept was introduced by Rousseeuw and Hubert (1996), who showed that its properties are similar to those of halfspace depth. In Section 4 we will prove that a property analogous to Theorem 1 also holds for regression depth. <PARAGRAPH_SEPARATOR> # 2. PROOF FOR A DATA SET IN GENERAL POSITION <PARAGRAPH_SEPARATOR> In this section we will prove Theorem 1 under the assumption that $X_{n}=$ $\\left\\{\\mathbf{x}_{1},...,\\mathbf{x}_{n}\\right\\}\\subset\\mathbb{R}^{p}$ is in general position, i.e. that no $p$ points lie in a $(p-1)$ - dimensional affine subspace. In order to prove Theorem 1, we need some auxiliary results."
  },
  {
    "qid": "statistic-posneg-290-0-0",
    "gold_answer": "TRUE. The Mellin transform of ${}_{2}F_{1}(a,b;c;-\\mathbf{Y})$ is indeed given by $\\frac{\\upbeta_{m}[\\alpha,b-\\alpha]\\upbeta_{m}[a-\\alpha,c-a]}{\\upbeta_{m}[a,c-a]}$ under the specified conditions. This result is derived from the properties of multivariate beta functions and hypergeometric functions with matrix arguments, as discussed in the context. The conditions ensure the convergence of the integral defining the Mellin transform and the validity of the hypergeometric function's series expansion.",
    "question": "Is the Mellin transform of the hypergeometric function ${}_{2}F_{1}(a,b;c;-\\mathbf{Y})$ correctly given by $\\frac{\\upbeta_{m}[\\alpha,b-\\alpha]\\upbeta_{m}[a-\\alpha,c-a]}{\\upbeta_{m}[a,c-a]}$ under the conditions $\\mathsf{Re}(\\alpha)>(m-1)/2$, $\\mathsf{Re}(a-\\alpha)>(m-1)/2$, $\\mathsf{Re}(b-\\alpha)>(m-1)/2$, and $\\mathsf{Re}(c-\\alpha)>(m-1)/2$?",
    "question_context": "Matrix Variate Hypergeometric Distributions and Mellin Transforms\n\nThis paper introduces several families of matrix variate elliptical distributions. Section 2 gives some results on integration, using zonal polynomials. In terms of these results, various matrix variate hypergeometric type distributions are proposed, as particular cases. These include well-known distributions such as central and noncentral matrix variate inverted gamma (inverted Wishart) distributions and matrix variate central and noncentral beta type II distributions; see Section 3. In Section 4.1, assuming a hypergeometric type distribution for the matrix parameter in matrix variate normal and matricvariate $T$ distributions, several families of matrix variate elliptical distributions are obtained using the compound matrix variate approach. Section 4.3 introduces the scale mixture of a matrix variate elliptical distribution, which is derived as a particular case of the compound matrix variate approach; all the results given in Section 4.1 can be particularized to this case."
  },
  {
    "qid": "statistic-posneg-1198-0-2",
    "gold_answer": "TRUE. The analysis shows that preseizure epochs have significantly higher energy at lower frequencies $(0{-}25\\mathrm{Hz})$ even $10\\mathrm{min}$ before seizure onset. The differences at higher frequencies $(25{-}100\\mathrm{Hz})$ become significant only around 1 min prior to seizure onset, indicating a build-up of power in the lower frequencies before spreading to higher frequencies as the seizure approaches.",
    "question": "Is it correct to interpret that the preseizure epochs show significantly higher energy at lower frequencies $(0{-}25\\mathrm{Hz})$ even $10\\mathrm{min}$ before seizure onset, while higher frequencies $(25{-}100\\mathrm{Hz})$ only show significant differences around 1 min prior to seizure onset?",
    "question_context": "Time-Frequency Functional Model for Locally Stationary Time Series Data\n\nWe apply model (2.2) to the IEEG time series data shown in Figure 1. We started from the full model for the 70 time series: $$\\begin{array}{c}{{g_{l}(\\omega_{k},u_{j})=\\beta_{1}(\\omega_{k},u_{j})+I(\\mathrm{pre-seizure})\\beta_{2}(\\omega_{k},u_{j})+I(\\mathrm{awake})\\beta_{3}(\\omega_{k},u_{j})}}\\ {{{}}}\\ {{+I(\\mathrm{pre-seizure})I(\\mathrm{awake})\\beta_{4}(\\omega_{k},u_{j}),}}\\end{array}$$ where $\\beta_{1}(\\omega,u)$ denotes the log-spectrum for the baseline segments collected from asleep patients, $\\beta_{2}(\\omega,u)\\left(\\beta_{3}(\\omega,u)\\right)$ denotes the difference in the log-spectra between preseizure and baseline (awake and asleep) segments, and $\\beta_{4}(\\omega_{k},u_{j})$ denotes the interaction effect. $I(\\cdot)$ is the indicator function and $k=1,\\ldots,20$ , $j=1,\\ldots,40,l=1,\\ldots,70$ The interaction term $\\beta_{4}(\\omega_{k},u_{j})$ dropped off due to lack of significance and our final model fit to the main effects only. Figure 4 presents the estimated log-spectra and difference log-spectra with their $95\\%$ Bayesian confidence intervals. It shows that when patients are asleep, the baseline segments are approximately stationary as the time-varying spectrum reduces to a smooth curve in frequency. The log-spectra difference estimates between the awake and asleep segments are presented on the second row of Figure 4. The confidence intervals exclude zero, confirming that epileptic patients generally have higher brain energy when they are awake. The difference between the preseizure log-spectrum and the baseline log-spectrum displays an overall increasing trend over time, implying that power of the IEEG builds up during the preseizure state. This agrees with previous studies, which demonstrate an increase in energy derived from the IEEG signal before seizure onset, appearing as an eruption of energy across the whole frequency domain (Litt et al. 2001). As shown in the (3, 2) panel of Figure 4, even $10\\mathrm{min}$ ahead of the seizure onset, the preseizure epochs already have significantly higher energy at the lower frequencies $(0{-}25\\mathrm{Hz})$ . At the higher frequencies $(25{-}100\\mathrm{Hz})$ ), the differences between the two group-average logspectra do not become significant until around 1 min prior to seizure onset. We interpret this as meaning that the difference of the power distribution between the preseizure and the baseline segments mainly locates in the lower frequency end $\\bf\\left(\\leq20\\mathrm{Hz}\\right)$ , which then abruptly spreads to higher frequencies as it moves closer to seizure onset. It indicates that changes between the baseline and preseizure EEG time series are already prominent in their spectra $10\\mathrm{min}$ before the seizure’s clinical onset, which is useful information for developing seizure prediction tools. It also suggests that the lower frequency end contains the most important information about the seizure, because their lower $95\\%$ Bayesian confidence intervals of the difference log-spectrum are consistently above the zero plane. This information allows us to design new experiments to study low-frequency activities, and offers potential clues to mechanisms underlying seizure generation."
  },
  {
    "qid": "statistic-posneg-1953-1-1",
    "gold_answer": "FALSE. While X₂ represents the cumulative population, the reaction R₂ (X₁ + X₂ → X₂) does not explicitly increase or decrease X₂. However, the paper does not state that X₂ only increases; it is a cumulative measure influenced by the dynamics of X₁ and the reactions. The exact behavior depends on the rates c₁ and c₂.",
    "question": "The reaction network includes R₁: X₁ → 2X₁ + X₂ and R₂: X₁ + X₂ → X₂. Does the model imply that the cumulative population X₂ only increases over time and never decreases?",
    "question_context": "Stochastic Kinetic Model for Aphid Population Dynamics\n\nThe paper presents a stochastic kinetic model for aphid population dynamics, focusing on two species: the current population (X₁) and the cumulative population (X₂). The model includes two reactions: R₁ (reproduction) and R₂ (death due to honeydew accumulation). Synthetic data were generated using Gillespie’s direct method, and observations were corrupted with Gaussian error. The inferential model uses the CLE (Chemical Langevin Equation), and the observation model is given by Yₜ = P'Xₜ + εₜ, where εₜ ~ N(0, σ²P'Xₜ). The paper compares PMMH and CPMMH methods for inference, using either myopic simulation or a residual bridge construct."
  },
  {
    "qid": "statistic-posneg-140-0-6",
    "gold_answer": "TRUE. This formulation correctly represents the hypothesis that $X$ is conditionally independent of $Y$ given $Z$, as derived in the paper.",
    "question": "Is the hypothesis of conditional independence in a three-way table correctly formulated as $H_{0}: p_{i j k} = \\frac{p_{i*k} p_{*j k}}{p_{**k}}$ for all $i,j,k$?",
    "question_context": "Power Divergence Measures in Three-Way Contingency Tables\n\nThe paper discusses the use of power divergence measures in three-way contingency tables, focusing on hypotheses of independence, partial independence, and conditional independence. It introduces the $(h,\\phi)$-divergence measures and examines their properties and applications in statistical testing."
  },
  {
    "qid": "statistic-posneg-807-0-1",
    "gold_answer": "FALSE. The EM algorithm described in Algorithm 4 does not necessarily guarantee convergence to the global maximum of the likelihood function. While the EM algorithm is known to converge to a local maximum, the likelihood function for bivariate matrix-Pareto distributions can be non-convex and may have multiple local maxima. The algorithm's convergence to the global maximum depends on the initialization of the parameters $(\\pmb{\\alpha}, \\pmb{T}, \\pmb{\\beta})$ and the specific structure of the likelihood surface. Practical implementations often use multiple random initializations to mitigate this issue.",
    "question": "Does the EM algorithm described in Algorithm 4 guarantee convergence to the global maximum of the likelihood function for the bivariate matrix-Pareto distribution?",
    "question_context": "Parameter Estimation for Bivariate Matrix-Pareto Distributions\n\nIn the sequel we will provide an algorithm for parameter estimation for which an explicit expressions of the bivariate density is needed. We therefore restrict it to the bivariate case. <PARAGRAPH_SEPARATOR> # 4.2 Parameter estimation in the bivariate case <PARAGRAPH_SEPARATOR> In the bivariate case, for any ${\\pmb Y}\\sim\\mathrm{{MPH}}^{*}({\\pmb\\pi},{\\pmb T},{\\pmb R})$ with parameters (5), it is easy to see that the density of $\\pmb{X}=(g_{1}(Y^{(1)}),g_{2}(Y^{(2)}))^{\\prime}$ is given by <PARAGRAPH_SEPARATOR> $$ f_{X}(x^{(1)},x^{(2)})=\\alpha\\mathrm{e}^{T_{11}g_{1}^{-1}(x^{(1)})}T_{12}\\mathrm{e}^{T_{22}g_{2}^{-1}(x^{(2)})}(-T_{22})e~{\\frac{1}{g_{1}^{\\prime}(g_{1}^{-1}(x^{(1)}))g_{2}^{\\prime}(g_{2}^{-1}(x^{(2)}))}}. $$ <PARAGRAPH_SEPARATOR> If we assume that $g_{j}(\\cdot;\\beta_{j})$ is a parametric nonnegative function depending on the vector $\\beta_{j}$ , $j=1,2$ , and let $\\beta=(\\beta_{1},\\beta_{2})$ . Then, we can formulate an algorithm analogous to Algorithm 1: <PARAGRAPH_SEPARATOR> Algorithm 4. (EM algorithm for bivariate inhomogeneous $\\mathrm{MPH^{*}}$ distributions) <PARAGRAPH_SEPARATOR> 0. Initialize with some “arbitrary” $(\\pmb{\\alpha},\\pmb{T},\\pmb{\\beta})$ . 1. Transform the data into $y_{i}^{(j)}:=g_{j}^{-1}(x_{i}^{(j)};\\beta_{j}),i=1,\\dots,N,j=1,2,$ , and apply the $E$ - and M-steps of Algorithm 3 by which we obtain the estimators $(\\hat{\\pmb{\\alpha}},\\hat{\\pmb{T}})$ . <PARAGRAPH_SEPARATOR> 2. Compute <PARAGRAPH_SEPARATOR> $$ \\hat{\\beta}=\\arg\\operatorname*{max}_{\\beta}\\sum_{i=1}^{N}\\log(f_{X}(x_{i}^{(1)},x_{i}^{(2)};\\hat{\\alpha},\\hat{T},\\beta)). $$ <PARAGRAPH_SEPARATOR> 3. Assign $(\\pmb{\\alpha},\\pmb{T},\\pmb{\\beta})=(\\hat{\\pmb{\\alpha}},\\hat{\\pmb{T}},\\hat{\\pmb{\\beta}})$ and GOTO 1. <PARAGRAPH_SEPARATOR> We now consider particular multivariate distributions obtained through such a transformation of an $\\mathrm{MPH^{*}}$ random vector. <PARAGRAPH_SEPARATOR> # 4.3 Multivariate matrix–Pareto models <PARAGRAPH_SEPARATOR> Let $X=(g_{1}(Y^{(1)}),\\dots,g_{d}(Y^{(d)}))^{\\prime}$ , where ${\\pmb Y}\\sim\\mathrm{{MPH}}^{*}({\\pmb\\pi},{\\pmb T},{\\pmb R})$ and <PARAGRAPH_SEPARATOR> $$ g_{j}(y)=\\beta_{j}(e^{y}-1),\\quad\\beta_{j}>0,j=1,\\dots,d. $$ <PARAGRAPH_SEPARATOR> Then we say that $\\pmb{X}$ follows a multivariate matrix–Pareto distribution. Some special properties of this class of distributions are: <PARAGRAPH_SEPARATOR> 1. Marginal distributions are matrix–Pareto distributed.   2. Moments and cross–moments of $\\pmb{X}$ can be obtained from the moment generating function of Y (see (Bladt & Nielsen, 2017, Theorem 8.1.2)), provided that they exist.   3. Products of the type $\\prod_{i=1}^{M}{(X^{(j_{i})}/\\beta_{j_{i}}+1)^{a_{i}}}$ are matrix–Pareto distributed, $a_{i}>0,j_{i}\\in\\{1,\\ldots,d\\}.$ , $i=1,\\ldots,M,M{\\le}N.$ ,  since linear combinations of $\\mathbf{Y}$ are PH distributed. <PARAGRAPH_SEPARATOR> In the bivariate case, (8) and (7), lead to <PARAGRAPH_SEPARATOR> $$ f_{X}(x^{(1)},x^{(2)})=\\alpha\\Biggl(\\frac{x^{(1)}}{\\beta_{1}}+1\\Biggr)^{T_{11}-I}T_{12}\\Biggl(\\frac{x^{(2)}}{\\beta_{2}}+1\\Biggr)^{T_{22}-I}(-T_{22})e\\frac{1}{\\beta_{1}\\beta_{2}}. $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ \\overline{{F}}_{X}(x^{(1)},x^{(2)})=\\alpha(-T_{11})^{-1}\\left(\\frac{x^{(1)}}{\\beta_{1}}+1\\right)^{T_{11}}T_{12}\\left(\\frac{x^{(2)}}{\\beta_{2}}+1\\right)^{T_{22}}e. $$ <PARAGRAPH_SEPARATOR> Moreover, in this bivariate case, linear combinations of $X^{(i)}$ are regularly varying, and the respective index is the real part of the eigenvalue with largest real part of the sub-intensity matrices of the marginals, which follows from (Davis & Resnick, 1996, Lem. 2.1) and asymptotic independence of $\\mathbf{Y}.$ . The general case is not clear since the condition of asymptotic independence does not hold. <PARAGRAPH_SEPARATOR> Remark 3. The Marshall–Olkin Pareto distribution (see Hanagal (1996a)) is a particular case of this class of distributions. <PARAGRAPH_SEPARATOR> # 4.3.1 Parameter estimation"
  },
  {
    "qid": "statistic-posneg-1064-0-0",
    "gold_answer": "FALSE. The efficiency of the centered versus uncentered parametrisation depends on the value of |BiD−1|. If |BiD−1| is near zero, the centered parametrisation is efficient, while if |BiD−1| is near one, the uncentered parametrisation is preferred. The paper explicitly states that the choice between centered and uncentered parametrisations is not absolute but depends on the model specification and the relative contributions of the first stage (error variance) and the second stage (random effect variance).",
    "question": "In the hierarchical normal linear model, the centered parametrisation (μ,η1,…,ηm) is always more efficient than the uncentered parametrisation (μ,α1,…,αm) regardless of the values of |BiD−1|. True or False?",
    "question_context": "Hierarchical Centering in Normal Linear Mixed Models\n\nThe paper discusses the effects of various parametrisations for generalised linear mixed models, focusing on hierarchical centering to improve the behavior of Markov chain Monte Carlo algorithms. It provides analytical and empirical arguments suggesting that hierarchical centering generally results in better performance. The paper includes detailed mathematical formulations and comparisons between centered and uncentered parametrisations."
  },
  {
    "qid": "statistic-posneg-428-0-1",
    "gold_answer": "FALSE. The integration by parts technique is correctly applied in the derivation, as evidenced by the recursive application and the resulting valid expressions for the kernel function. The derivation follows standard mathematical procedures and yields correct results.",
    "question": "The integration by parts technique used in the derivation of $K_{\\lambda}(t_{i},v)$ is incorrectly applied, leading to an invalid expression for the kernel function. True or False?",
    "question_context": "Spatially Adaptive Smoothing Splines with Piecewise-Constant Smoothing Parameters\n\nThe paper derives expressions for the kernel function $K_{\\lambda}(t_{i},v)$ used in spatially adaptive smoothing splines, where the smoothing parameter $\\lambda(u)$ is piecewise constant. The derivation involves integration by parts and recursive application to handle the piecewise nature of $\\lambda(u)$. The kernel function is expressed in terms of basis functions and their integrals over intervals defined by knots $\\tau_k$."
  },
  {
    "qid": "statistic-posneg-989-0-1",
    "gold_answer": "FALSE. The model reduces to a standard linear regression only when $\\phi(\\theta_{0}^{\\mathrm{T}}X) = 0$ or is a constant. If $\\phi(\\theta_{0}^{\\mathrm{T}}X)$ is linear, say $\\phi(\\theta_{0}^{\\mathrm{T}}X) = c\\theta_{0}^{\\mathrm{T}}X$, the model becomes $y = (\\beta_{0} + c\\theta_{0})^{\\mathrm{T}}X + \\varepsilon$, which is still a linear model but with a combined coefficient vector. The statement is false because it incorrectly assumes that any linear $\\phi$ would reduce the model to the standard form, ignoring the combination of coefficients.",
    "question": "The extended partially linear single-index model reduces to a standard linear regression model when the function $\\phi(\\theta_{0}^{\\mathrm{T}}X)$ is linear. Is this statement correct?",
    "question_context": "Extended Partially Linear Single-Index Models: Structure and Applications\n\nThe paper introduces an extended version of a generalized partially linear single-index model, which takes the form $$y=\\beta_{0}^{\\mathrm{{T}}}X+\\phi(\\theta_{0}^{\\mathrm{{T}}}X)+\\varepsilon,$$ where $E(\\varepsilon|X)=0$ and $E\\varepsilon^{2}=\\sigma^{2}$, $\\beta_{0}$ and $\\theta_{0}$ are unknown vector parameters and $\\phi(.)$ is an unknown function. To ensure estimability, the authors assume that $\\beta_{0}$ and $\\theta_{0}$ are perpendicular to each other with $\\|\\theta_{0}\\|=1$ and its first nonzero element positive. The model includes many existing models as special cases, such as the single-index model and the partially linear model. The paper also discusses the application of the model to nonlinear time series data, exemplified by the analysis of annual sunspot numbers."
  },
  {
    "qid": "statistic-posneg-657-3-1",
    "gold_answer": "FALSE. The step-function values $\\tilde{h}_x$ and $\\tilde{\\eta}(x)$ are smoothed via local averaging (using a weight function $w(\\cdot)$) to produce the final choices $\\hat{h}_x$ and $\\hat{\\eta}(x)$. This smoothing step ensures continuity and reduces abrupt changes in parameter values across region boundaries.",
    "question": "The paper claims that the final choices of $h_x$ and $\\eta(x)$ are obtained by directly using the step-function values $\\tilde{h}_x$ and $\\tilde{\\eta}(x)$ from each region $R_k$ without any further smoothing. Is this statement correct?",
    "question_context": "Local Linear-Additive Estimation and Bandwidth Selection via AIC\n\nThe paper discusses a method for selecting local regularization parameters and bandwidths in nonparametric regression using an AIC-type criterion. The approach involves splitting the support of covariates into rectangular regions, fitting local additive estimators within each region, and smoothing the selected parameters. The AIC criterion is used to choose optimal bandwidths and regularization parameters both locally and globally."
  },
  {
    "qid": "statistic-posneg-853-0-1",
    "gold_answer": "TRUE. The formula for E(y_ij) integrates over the random effect ξ_i, weighted by its normal density ϕ(ξ_i), and π_ij explicitly includes σξ_i in the logistic function. This correctly captures how the variance σ² of the random effects influences the marginal expectation of y_ij, as larger σ² increases the variability in the linear predictor x_ij^Tβ + σξ_i, thereby affecting the expected probability.",
    "question": "Does the formula for E(y_ij) correctly account for the influence of σ² on the expected value of the binary response, given that π_ij is a function of σξ_i?",
    "question_context": "Binary Mixed Model Analysis: Likelihood vs. Moment Estimation in COPD Data\n\nThe paper discusses the application of binary mixed models to COPD data, focusing on the estimation of parameters β (covariate effects) and σ² (variance of family effects). The model assumes a logistic link function for the probability of binary responses, incorporating random family effects γ_i ~ N(0, σ²). The paper compares the performance of Simulated Maximum Likelihood (SML) and Simulated Moment (SM) methods, highlighting the inefficiency of the SM approach and the challenges in estimating σ², especially under model misspecification."
  },
  {
    "qid": "statistic-posneg-188-0-1",
    "gold_answer": "FALSE. The formula shows that P_0^{n+1} depends on a sum of positive terms weighted by ρ/(1+ρ)^2. For ρ < 1, the terms in the sum decrease as r increases, but the cumulative sum increases with n, causing P_0^{n+1} to increase (not decrease) monotonically toward the steady-state value 1 - ρ. This reflects the system tending toward equilibrium.",
    "question": "For the M/M/1 queue with exponential service times, the probability P_0^{n+1} that the system is empty after the (n+1)th departure is given by P_0^{n+1} = 1 - (1+ρ)Σ_{r=0}^{n-1} [(2r)!/(r!(r+1)!)] [ρ/(1+ρ)^2]^{r+1}. Is it correct to say that this formula implies P_0^{n+1} decreases monotonically with n for ρ < 1?",
    "question_context": "Transient Behavior of M/G/1 Queue: Probability Distribution of Queue Size After nth Departure\n\nWe consider a single server queue with Poisson arrivals and general distribution of service time and seek to determine the probability distribution of queue size after the nth departure from the system. A generating function is obtained which solves this problem formally. When the service time distribution is negative exponential an explicit solution is obtained."
  },
  {
    "qid": "statistic-posneg-786-0-0",
    "gold_answer": "TRUE. The formula for $\\mathcal{P}(\\tau)$ correctly combines these two scenarios: (a) accounts for the next event being from the same sequence (ith) with no other sequences emitting events earlier, while (b) accounts for the next event being from a different sequence (jth) with no events from any sequence (including ith) occurring earlier. The weighting by $\\theta_i^{-1}/\\sum_{j=1}^N \\theta_j^{-1}$ ensures the correct conditional probability given the source of the initial event at $t_0$. This is consistent with the superposition principle for independent event sequences.",
    "question": "Is the derived formula for $\\mathcal{P}(\\tau)$ correctly expressed as the sum of two probabilities: (a) the probability that the first event after $t_0$ is from the same sequence, and (b) the probability that it is from a different sequence, weighted by the probability that the event at $t_0$ belongs to the ith sequence?",
    "question_context": "Superposition of Random Event Sequences: Interval Probability and Autocorrelation Analysis\n\nThe paper analyzes the superposition of multiple random event sequences, each characterized by interval probability density functions $p_i(\\tau)$. The combined sequence's interval probability density $\\mathcal{P}(\\tau)$ is derived, considering the probabilities of events from different sequences. The autocorrelation function $R(\\tau)$ and expectation density $\\mathcal{E}(\\tau)$ are also examined, with $\\mathcal{E}(\\tau)$ defined as the probability of an event occurring in $[\\tau, \\tau+d\\tau]$ given an event at $\\tau=0$. Special cases for $N=2$ sequences are explored, including applications to neural discharge data."
  },
  {
    "qid": "statistic-posneg-1826-0-1",
    "gold_answer": "FALSE. The symmetry condition $a_1 = a_2$ holds only for $k = 1$ or $2$ populations, as stated in Theorem 2.1. For $k > 2$, the optimal interval becomes asymmetric ($a_2 < \\frac{1}{2}L < a_1$), reflecting the overestimation bias of the largest sample mean and the need to adjust the interval bounds differently to maintain the coverage probability $\\gamma$.",
    "question": "Does the optimal confidence interval length $(a_1 + a_2)\\sigma/\\sqrt{n}$ always require $a_1 = a_2$ for any number of populations $k$?",
    "question_context": "Optimal Confidence Intervals for the Largest Mean in Repeated Measurements Design\n\nIn this section we consider an optimal confidence interval for the largest population mean $\\mu^{*}$ based on the largest sample mean $\\overline{{X}}^{*}$ . For given $\\gamma$ the distance $\\pmb{a_{1}+a_{2}}$ or $\\ensuremath{b_{1}}+\\ensuremath{b_{2}}$ is minimized for the case where the correlation coeficients are assumed to be a given equal nonnegative value, with the variance ${\\pmb\\sigma}^{\\mathbf{2}}$ being known or unknown. It is known that the largest sample mean overestimates the largest population mean (Dudewicz $\\&$ Tong, 1971). This suggests that an unsymmetric confidence interval should be considered. That is, ${\\pmb a}_{1}>{\\pmb a}_{2}$ and $\\pmb{b_{1}>b_{2}}$ in the intervals (2·1) and (2:5) respectively. In addition, by knowing the correlation coefficient one can increase the precision of estimation."
  },
  {
    "qid": "statistic-posneg-1113-1-1",
    "gold_answer": "FALSE. The paper states that the chi-squared mixture approximation (41) performs better in the right-hand tail of the distribution (larger values of x) compared to Fisher's z-transform, especially for small sample sizes. Fisher's z-transform, derived from large sample theory, deteriorates more rapidly for larger x and smaller N, whereas the chi-squared mixture approximation remains robust in these regions.",
    "question": "Does the approximation based on Fisher's z-transform perform better than the chi-squared mixture approximation for small sample sizes (N) and large values of x?",
    "question_context": "Approximation to the Distribution of U and R Using Chi-Squared Mixtures\n\nThe paper discusses an approximation to the distribution of U, which is distributed as a ratio Y1/Y2, where Y1 is a mixture of chi-squared distributions and Y2 is independently distributed as χ²_{n-p+1}. The approximation suggested is U ≈ g(χ²_f / χ²_{n-p+1}), where g and f are constants determined by equating the first two moments of gχ²_f to those of Y1. The values of g and f are given by specific formulas involving n, θ, and p. The approximation is validated through comparisons with exact values and Fisher's z-transform, showing better performance in the right-hand tail of the distribution."
  },
  {
    "qid": "statistic-posneg-1510-4-0",
    "gold_answer": "TRUE. The context explicitly states that NACs are copulas for which there is no known tractable CDM (Conditional Distribution Method). This is a key characteristic mentioned in the paper to highlight the complexity of NACs and the challenge they present for modeling, which is why the authors use GMMNs to capture their dependence structures.",
    "question": "Is the statement 'Nested Archimedean copulas (NACs) are copulas for which there is no known tractable CDM' true based on the provided context?",
    "question_context": "Nested Archimedean Copulas and Marshall-Olkin Copulas in Multivariate Distributions\n\nNested Archimedean copulas (NACs) are Archimedean copulas with arguments possibly replaced by other NACs; see McNeil (2008) or Hofert (2012). In particular, this class of copulas allows us to construct asymmetric extensions of Archimedean copulas. Important to note here is that NACs are copulas for which there is no known (tractable) CDM. To demonstrate the ability of GMMNs to capture such dependence structures, we consider the simplest three-dimensional copula for visualization and investigate higher-dimensional NACs in Sections 3.3 and 4. The three-dimensional NAC we consider here is $$C(\\pmb{u})=C_{0}(C_{1}(u_{1},u_{2}),u_{3}),\\quad\\pmb{u}\\in[0,1]^{3},$$ where $C_{0}$ is a Clayton copula with Kendall’s tau $\\tau_{0}=0.25$ and $C_{1}$ is a Clayton copula with Kendall’s tau $\\tau_{1}=0.50$ . In Sections 3.3 and 4, we will present examples of 5- and 10-dimensional NACs. Bivariate Marshall–Olkin copulas are of the form $$C(u_{1},u_{2})=\\operatorname*{min}\\{u_{1}^{1-\\alpha_{1}}u_{2},u_{1}u_{2}^{1-\\alpha_{2}}\\},\\quad u_{1},u_{2}\\in[0,1],$$ where $\\alpha_{1},\\alpha_{2}~\\in~[0,1]$ . A notable feature of Marshall–Olkin copulas is that they have both an absolutely continuous component and a singular component. In particular, the singular component is determined by all points which satisfy $u_{1}^{\\alpha_{1}}~=$ $u_{2}^{\\alpha_{2}}$ . Accurately capturing this singular component may present a different challenge for GMMNs, which is why we included this copula despite the fact that there also exists a CDM for this copula. As an example for visual assessment, we consider a Marshall–Olkin copula with $\\alpha_{1}=0.75$ and $\\alpha_{2}=0.60$ ."
  },
  {
    "qid": "statistic-posneg-2121-13-1",
    "gold_answer": "TRUE. If the spatially continuous process S(·) is Gaussian, then any spatially averaged process T(x) = ∫w(u)S(x-μu)du will also be Gaussian. This property holds because the integral of a Gaussian process remains Gaussian under linear transformations.",
    "question": "In the context of geostatistical models, does the Gaussian assumption for the spatially continuous process S(·) imply that any spatially averaged process T(x) = ∫w(u)S(x-μu)du is also Gaussian?",
    "question_context": "Geostatistical Inference and Preferential Sampling\n\nProfessor Besag’s more fundamental point, which was also raised in different ways by Fuentes and by Rue, Martino, Mondal and Chopin, is whether a stationary Gaussian process with Matérn correlation structure is appropriate at all. Within the Gaussian process framework, the most important practical difference between stationary and intrinsic models is that, when data are irregularly distributed over the spatial region of interest, the predictions from stationary models are mean reverting in sparsely sampled areas, whereas those from intrinsic models are not. Which of these properties is preferable could be context dependent. On balance, we agree that, in the absence of explanatory variables, allowing a global mean to have a major influence on local predictions is a questionable strategy. When explanatory variables are available, we are less convinced. <PARAGRAPH_SEPARATOR> Professor Myers questions whether any form of Gaussian process is an appropriate model for geostatistical data. Our counter to this is that we think of the model-based approach as a device through which we determine a principled solution to a prediction problem. The Gaussian assumption leads naturally to linear prediction, whereas, in the classical geostatistical approach that is known as ordinary kriging, linearity is imposed as an a priori constraint. In similar vein, a model-based way to answer Professor Scott’s request for a ‘measure of representativeness is to derive the score statistic for the relevant parameter. For our model, if we assume that all parameters other than $\\beta$ are known, the score statistic to test $\\beta=0$ is <PARAGRAPH_SEPARATOR> Fig. 16. (a) Empirical and (b) fitted (by maximum likelihood, assuming non-preferential sampling) variograms for the Galicia 1997 data: the different curve styles correspond to four different imputations to replace the two outliers (see the text for details) <PARAGRAPH_SEPARATOR> $$ T=n^{-1}\\sum_{i=1}^{n}Y_{i}-|A|^{-1}\\int\\hat{S}(x)\\mathrm{d}x. $$ <PARAGRAPH_SEPARATOR> This has an obvious interpretation beyond the model assumed. It does not provide a useful test for preferential sampling because, as shown explicitly by Professor Anderson, when $\\beta\\neq0$ fitting under the incorrect assumption that $\\beta=0$ leads to biased estimation of the mean. However, it provides a more tangible measure of non-representativeness than does the product $\\hat{\\beta}\\hat{\\sigma}$ . <PARAGRAPH_SEPARATOR> Professor Myers’s comment about non-point support is well taken, but we reply that, if the spatially continuous process $S(\\cdot)$ is Gaussian, then so is any spatially averaged process of the form $T(x)=$ $\\begin{array}{r}{\\int w(u)S(x-\\mathbf{\\dot{\\mu}}u)\\mathrm{d}u}\\end{array}$ . Whether one should then derive the correlation structure of $T(\\cdot)$ from an assumed correlation structure for $S(\\cdot)$ and a specified weighting function $w(\\cdot)$ or, more pragmatically, specify the assumed correlation structure of $T(\\cdot)$ directly is open to debate in view of the empirical status of most geostatistical models. <PARAGRAPH_SEPARATOR> Somewhat analogous to the non-point support problem is the aggregated data problem that was raised by Professor Gelfand and Professor Chakraborty, in which observations $Y_{i}$ relate to each of $n$ subregions $A_{i}$ that together form a partition of the study region. Most applications involving data in this form treat the study region as discrete and use Markov random-field models for the vector $Y=(Y_{1},\\dots,Y_{n})$ (Rue and Held, 2005). An alternative is to use a spatially continuous latent process $S(\\cdot)$ ; for example, in an epidemiological setting where the $Y_{i}$ are disease counts a starting point might be to assume that, conditional on $S(\\cdot)$ , the $Y_{i}$ are independent Poisson variates with conditional means $\\begin{array}{r}{\\mu_{i}=N_{i}\\int_{A_{i}}S(x)\\mathrm{{d}}x}\\end{array}$ , where $N_{i}$ denotes the number of people at risk in the subregion $A_{i}$ . Note, however, that both approaches are susceptible to aggregation bias if spatial variation in population density within subregions is correlated with spatial variation in disease risk (Diggle and Elliott, 1995)."
  },
  {
    "qid": "statistic-posneg-1300-0-1",
    "gold_answer": "FALSE. The paper introduces $\\breve{f}_{n}(x) = \\hat{f}_{n}(x) + k_{n}/(n c)$ as a theoretical step, but $\\tilde{f}_{n}(x)$ is designed to estimate $k_{n}/(n c)$ ad hoc using $Z_{n}$, thus not requiring explicit knowledge of $c$.",
    "question": "Does the estimator $\\tilde{f}_{n}(x)$ always require knowledge of the constant $c$ to reduce bias effectively?",
    "question_context": "Asymptotic Distribution of Poisson Process Estimators\n\nThe paper discusses the asymptotic behavior of estimators derived from Poisson processes. Key formulas include the distribution function $G_{n,x}(u)$ and the characteristic function $\\psi_{n,x}(t)$, which are used to prove convergence to standard Gaussian variables under certain conditions."
  },
  {
    "qid": "statistic-posneg-560-5-0",
    "gold_answer": "FALSE. The context explicitly states that the $x^{\\frac{8}{3}}$ test is inadequate for small samples (50 or below) and requires sample sizes of the order 100 or more to be effective. The test's inadequacy is highlighted for small samples, and safety is suggested to begin between 105 and 150.",
    "question": "False or True: The $x^{\\frac{8}{3}}$ test is adequate to discriminate between raw samples from different parent populations even when the sample sizes are as small as 50.",
    "question_context": "Goodness-of-Fit Test for Small Samples and Graduated Populations\n\nIt will be clear from this table that even with parent populations so different as those here dealt with, the $x^{\\frac{8}{3}}$ test is inadequate to discriminate between raw samples from these populations, if the samples have not sizes of the order 100, and even then not on the ·02 probability basis. Safety may be Baid to begin between 105 and 150, and if 50 or below be said to be “small\" sample sizes, it is not dogmatic to assert that the $x^{\\ldots}$ test ought never to be applied to such small samples. <PARAGRAPH_SEPARATOR> (7) Conclusion8. An endeavour has been made in this paper to mark more clearly the distinctions the writer had in view in introducing $x^{\\pm}$ and $\\phi^{\\tt A}$ into statistical theory and practice. <PARAGRAPH_SEPARATOR> (i) If $x^{2}$ be defned as then $\\pmb{\\pi}_{*}$ is in a succession of samples a constant and equal to the mean value of ${\\pmb n}_{\\pmb{\\imath}}$ If this condition be satifed, $x^{\\pm}$ is given approximately, but only approximately, by thecurve <PARAGRAPH_SEPARATOR> $$ y=y_{0}\\sigma^{-}\\mathbf{i}^{x^{0}}(\\mathbf{\\hat{r}}\\chi^{3})^{\\mathbf{f}(\\theta-3)}[d(\\mathbf{\\hat{s}}\\chi^{\\mathbf{8}})], $$ <PARAGRAPH_SEPARATOR> where $v$ is the number of cells ${\\bf\\Phi}={\\bf\\Phi}{\\pmb n}^{\\prime}$ of Elderton's $(x^{\\mathfrak{a}},P)$ table <PARAGRAPH_SEPARATOR> The mean of $x^{\\star}$ .13 $\\pmb{v}-\\mathbf{1}$ , but its true standard deviation is not $\\sqrt{2\\left(\\pmb{\\mathscr{v}}-1\\right)}$ but is given by Equation (vi) above. It is suggested that either Equation (vi) or Equation (xi) will give a better value for the $\\mathcal{P}$ corresponding to & given $x^{\\mathfrak{i}},$ using either the Incomplete $\\mathbf{B}\\mathbf{-}$ or T-Function Tables, than Elderton's $({\\pmb x}^{\\bullet},{\\pmb P})$ Table, when $\\pmb{\\mathscr{N}}$ is not very large or any $1/\\mathfrak{n}_{\\bullet}$ is not negligible as compared to unity. <PARAGRAPH_SEPARATOR> (ii) It has been pointed out that the main use of $x^{8}$ was intended to be the comparison of a considerable graduated sample of a parent population with further samples in order to test whether such samples were or were not likely to be samples from the parent population, only known through this graduated sample. It is shown from a series of experimental examples that the ${\\pmb x}^{\\prime{\\pmb\\Psi}_{\\I}}{\\bf8}$ and $\\mathcal{P}$ 'sfrom the graduated sample are very highly correlated with the ${x^{\\mathfrak{s}}}$ and $\\pmb{P8}$ from the parent \\* I mean the form of the $x^{\\mathfrak{i}}$ test based on the distribation $y=y_{0}e^{-\\frac{1}{2}x^{2}}(\\frac{1}{2}x^{2})^{\\frac{1}{6}(x-3)},$ population, so that without frequent wrong judgment we may use the step-parental population in place of the unknown parent population. This amounts to using for $\\pmb{\\widetilde{n}}_{\\pmb{\\imath}}$ the values found from the graduated sample population They still remain constant throughout the comparison with further samples, and the larger the size of the graduated sample the higher on the average will be the correlation between the $x^{\\mathfrak{s}}{}^{}$ from the step-parental and true parental population8. <PARAGRAPH_SEPARATOR> (ii) Incidentally it is pointed out that even for small samples an immensely better $\\boldsymbol{P}$ is obtained from a graduated than from a raw sample, and this even when the size of the sainple is small. <PARAGRAPH_SEPARATOR> (iv) More than twenty years ago I gave a test for two samples of sizes $\\pmb{N}$ a&nd $N^{\\prime}$ ,categories ${\\pmb{\\mathscr{n}}}_{\\pmb{\\mathscr{n}}};$ being drawn_ from the same parent population with relative frequency of the 8th category ${\\pmb{\\mathscr{P}}}{\\pmb{\\mathscr{s}}}$ .It consisted in calculating <PARAGRAPH_SEPARATOR> $$ \\chi^{\\sharp}=\\mathop{\\displaystyle\\sum_{\\bullet=1}^{\\bullet=\\circ}}_{N+N^{\\prime}}\\frac{\\left(\\displaystyle\\frac{n_{\\bullet}}{N}-\\displaystyle\\frac{n_{\\bullet}}{N^{\\prime}}\\right)^{\\sharp}}{p_{\\bullet}}, $$ <PARAGRAPH_SEPARATOR> therebeing $\\pmb{v}$ categories in both samples, and then applying the $(\\chi^{\\tt A},~P)$ table.   Here ${\\pmb p}_{\\pmb{\\imath}}$ is supposed throughout the further sampling to be a constant. <PARAGRAPH_SEPARATOR> If the parent population be supposed unknown, I suggested that the best value availablewas $p_{\\bullet}{=}(n_{\\bullet}+n_{\\bullet}^{\\prime})/(N+N^{\\prime})$ ; this would not be very accurate for small samples. <PARAGRAPH_SEPARATOR> In this case <PARAGRAPH_SEPARATOR> $$ {\\chi^{2}=\\operatorname*{\\s}_{\\pm=1}^{\\circ}\\frac{N N^{\\prime}\\left(\\frac{n_{\\bullet}}{N}-\\frac{n_{\\bullet}{}^{\\prime}}{N^{\\prime}}\\right)^{2}}{n_{\\bullet}+n_{\\bullet}^{\\prime}}.} $$ <PARAGRAPH_SEPARATOR> But it has been frequently overlooked that in adopting this value of $x^{8}$ we must in measuring $\\mathcal{P}$ remember that in further samples the denominator $n_{\\bullet}+{n_{\\bullet}}^{\\prime}$ i8 supposed to remain constant, while we vary $\\pmb{n}_{0}$ and $\\scriptstyle{n_{*}^{\\prime}}$ in the numerator, otherwise the distribution of $x^{8}$ on which the $(x^{\\mathfrak{s}},P)$ table is based is incorrect. The whole matter has in my opinion been rendered unnecessarily obscure by writing the two samples in the form of a spurious biserial contingency table. This is said to have $\\boldsymbol{2\\mathfrak{v}}$ cells, and to lack ${\\pm1}$ \" degrees of freedom.\" If the first pair of samples gives a contingency table the second will not, and from this manner of approach we lose sight of the true difference between & real and a spurious contingency table. In the former all the constituents of the marginal totals are free, and the only limitation is the total size of the sample in the table. <PARAGRAPH_SEPARATOR> (v) If we, however, write $x^{2}$ in the form of <PARAGRAPH_SEPARATOR> $$ (N+N^{\\prime})\\phi^{\\mathfrak{L}}=\\overset{\\mathfrak{S}}{\\underset{s=1}{\\overset{\\mathfrak{S}}{\\operatorname{S}}}}\\frac{\\left(n_{s}-\\frac{N}{N+N^{\\prime}}(n_{s}+n_{s}^{\\prime})\\right)^{\\mathfrak{L}}}{\\frac{N}{N+N^{\\prime}}(n_{s}+n_{s}^{\\prime})}+\\overset{\\mathfrak{S}}{\\underset{s=1}{\\overset{\\mathfrak{S}}{\\operatorname{S}}}}\\frac{\\left(n_{s}^{\\prime}-\\frac{N^{\\prime}}{N+N^{\\prime}}(n_{s}+n_{s}^{\\prime})\\right)^{\\mathfrak{L}}}{\\frac{N}{N+N^{\\prime}}(n_{s}+n_{s}^{\\prime})} $$ <PARAGRAPH_SEPARATOR> we are not only giving ourselves double work, but are apt to forget that to get the approximate equation for $x^{\\frac{8}{2}}$ wemustconsider $n_{\\bullet}+{n_{\\bullet}}^{\\prime}$ constant in the numerator, <PARAGRAPH_SEPARATOR> which it is not if we take another pair of samples. There is nothing whatever to prevent our choosing <PARAGRAPH_SEPARATOR> $$ \\phi^{\\mathfrak{g}}=\\overset{c}{\\underset{\\ell=1}{\\overset{r}{\\otimes}}}\\frac{N N^{\\prime}}{\\underset{N+N^{\\prime}}{\\overset{N N^{\\prime}}{\\otimes}}},\\frac{\\left(\\frac{\\eta_{\\bullet}}{N}-\\frac{n_{\\bullet}}{N^{\\prime}}\\right)^{\\mathfrak{g}}}{n_{\\bullet}+n_{\\bullet}} $$ <PARAGRAPH_SEPARATOR> as our measure of accordance of the two samples, making $n_{\\bullet},{\\pmb n_{\\bullet}}^{\\prime}$ vary in both numerator and denominator. But if this be done, the distribution of $\\phi^{\\tt A}$ is not that ${\\pmb x}^{\\bullet}/(N+N^{\\prime}),$ and it cannot be deduced from the $({\\pmb x}^{\\bullet},{\\pmb P})$ table. Even approximate values of the mean and variance are complicated, and the experimental study of the distribution of $\\phi^{\\tt A}$ has only been started by Professor Kondo's recent paper. <PARAGRAPH_SEPARATOR> (vi)  If we take a sample, graduate it by aid of $\\pmb{\\ell}$ moments and then compare it with any population, it is, apart from fractions of a unit, a possible sample from that population, and we are at liberty to look out $\\pmb{P}$ in the ordinary $(x^{\\mathbf{1}},P)$ table and judge where it stands among other possible samples of the same size and the same categories. We have not limited anything by obtaining our graduated sample by moments. When we do limit by moments is when we fit a series of curves to & given distribution by moments, the curve moments being in each case those of the given distribution. In such a case we compare the relative goodness of ft of various curves obtained by t moments and our degrees of freedom are reduced by t. An application of such limitation of degrees of freedom was made by me in 1915 and applied to the case of death-rates. It was shown in the memoirs concerned with this topic that in certain cases not only the degrees of freedom, but the value of $x^{\\bullet}$ with which the table was entered might need modification\\*. Another case of the modification of both $\\pmb{v}$ and $x^{\\tt3}$ to get a better measure of $P$ is indicated in this paper: see pp. 366--367 above. <PARAGRAPH_SEPARATOR> (vii) We have also seen in this paper that it is still probably legitimate to calculate $\\pmb{P}$ from $x^{\\mathfrak{s}}$ when, owing to the smallness of the sample and of its cate. gories, the distribution $y=y_{0}e^{-}\\hat{x}^{2}(\\frac{1}{2}x^{3})\\dag^{(0-3)}$ is no longer accurate. In such cases we are thrown back on frequency curves which are generalisations of the usual $x^{2}$ curve, and which can be integrated by aid of the Incomplete T-Function and thc Incomplete B-Function Tables. There is here a field for much experimental work of a useful kind."
  },
  {
    "qid": "statistic-posneg-708-0-0",
    "gold_answer": "FALSE. The context explicitly states that while the random walk hypothesis is roughly correct for a majority of stock data, there are examples that deviate significantly from it. These deviations include non-Gaussian distributions, stepwise time dependence in the variance, and correlations in parts of the series. This suggests that the random walk hypothesis is not universally accurate for all stock price data.",
    "question": "Is the random walk hypothesis for stock prices, which assumes that logarithmic price changes are independent identically distributed Gaussian random variables, always accurate according to the context?",
    "question_context": "Random Walk Hypothesis and Stock Price Modeling\n\nFrom an economic point of view (see Fama (1965)) it is most sensible to analyse stock prices in terms of logarithmic price changes; i.e. if $\\left\\{S_{t}\\right\\}$ is the stock price series then $\\left\\{\\bar{U}_{t}\\right\\}=\\left\\{\\ln S_{t}-\\ln S_{t-1}\\right\\}$ is the series that should be investigated. The assumption that $\\left\\{U_{t}\\right\\}$ consists of independent identically distributed Gaussian random variables is the so-called random walk hypothesis for stock prices, and many empirical investigations (see, for example, Ali and Giacotto (1982) and references therein) have been carried out to test this hypothesis. Similar studies exist for commodity prices (Bird, 1985). <PARAGRAPH_SEPARATOR> Although roughly correct for a majority of stock data, it has been noted by several researchers (e.g. Fielitz (1971), Hsu et al. (1974), Wichern et al. (1976) and Ali and Giacotto (1982)) that examples can be found which deviate significantly from the random walk hypothesis. There are examples of a non-Gaussian distribution, a stepwise time dependence in the variance and of correlations in parts of $\\left\\{U_{t}\\right\\}$ .Since correlations mean that the eficient market hypothesis does not hold (Fama, 1970), it is important to detect and pinpoint sections of $\\left\\{U_{t}\\right\\}$ where correlations exist. Also this should be examined in the context of time dependence of the variance since (Bird, 1985) certain relationships between the correlation and variance can be given an economic interpretation. Correlations and variance can be measured in terms of AR parameters and residual variance, and our methods are especially designed for finding data segments with constant parameters and for pinpointing change points. In addition we would like to give an economic interpretation of the transition points between various states. <PARAGRAPH_SEPARATOR> It should also be noted that the random walk hypothesis is directly related to more recent work based on continuous time models for the stock prices. In a very influential paper on option pricing Black and Scholes (1973) assumed that stock prices are described by an Ito stochastic differential equation <PARAGRAPH_SEPARATOR> $$ \\frac{\\mathrm{d}S(t)}{S(t)}=\\mu~\\mathrm{d}t+\\sigma~\\mathrm{d}W(t) $$ <PARAGRAPH_SEPARATOR> where $\\left\\{W(t)\\right\\}$ is a Wiener process, $\\pmb{\\mu}$ the drift parameter and $\\sigma^{2}$ the variance. The process $\\langle S(t)\\rangle$ itself is termed geometric Brownian motion. Equation (5.1) may be solved analytically for $s(t)$ and for $t\\geqslant0$ we obtain <PARAGRAPH_SEPARATOR> $$ S(t)=S(0)\\exp[(\\mu-{\\textstyle{\\frac{1}{2}}}\\sigma^{2})t+\\sigma W(t)]. $$ <PARAGRAPH_SEPARATOR> Sampling $S(t)$ at equal time intervals $\\Delta$ it follows that <PARAGRAPH_SEPARATOR> $$ \\ln S(k\\Delta)-\\ln S[(k-1)\\Delta]=\\Delta(\\mu-{\\textstyle{\\frac{1}{2}}}\\sigma^{2})+e_{k} $$ <PARAGRAPH_SEPARATOR> where $\\left\\{\\boldsymbol{e}_{k}\\right\\}=\\sigma[W(k\\Delta)-W((k-1)\\Delta)]$ consists of independent identically distributed Gaussian random variables. This means that the random walk model emerges irrespective of the length of the sampling interval $\\Delta.$ . Thus, if we examine the properties of the time series $\\left\\{U_{t}\\right\\}=\\left\\{\\ln S_{t}-\\ln\\bar{S}_{t-1}\\right\\}$ , this implies that we are also examining the validity of the continuous time model (5.1), which has been extensively used recently in the theory of finance (Merton, 1982). The original paper by Black and Scholes already contained some evidence that equation (5.1) may not always be an accurate model, and more general models have been introduced (see, for example, Aase (1984) and references therein), where the stock price can change suddenly by introducing a <PARAGRAPH_SEPARATOR> Poisson process component. The models proposed in this paper are meant to describe sudden structural changes via the AR parameters, and this offers an interesting alternative to the generalized continuous time models. We now illustrate our methods on the IBM example. <PARAGRAPH_SEPARATOR> # 5.1. IBM series"
  },
  {
    "qid": "statistic-posneg-577-1-1",
    "gold_answer": "FALSE. Proposition 1.1 states that if the margins are $\\mathbb{P}(Y_{i}=0)=0.5$ for all $i\\in\\{1,2,3\\}$, the three-way interaction $u_{123}$ is zero due to the radial symmetry of the Normal copula.",
    "question": "Is the three-way interaction term $u_{123}$ always non-zero for a trivariate Bernoulli distribution realized with a Normal copula?",
    "question_context": "Trivariate Bernoulli Distribution and Normal Copula Compatibility\n\nExample 1.2 shows that dependences of Bernoulli distributed random vector depend not only on the copula, but also on the marginal distributions. This observation is in line with previous contributions in Denuit and Lambert (2005), Mesfioui and Tajar (2005) and Nešlehová (2007). <PARAGRAPH_SEPARATOR> # 1.2. Trivariate Bernoulli distribution with copulas <PARAGRAPH_SEPARATOR> In the case of a three-dimensional Bernoulli distribution, the Sklar’s equality (6) takes the following form: <PARAGRAPH_SEPARATOR> $$ \\mathbb{P}(X_{1}\\leq x_{1},X_{2}\\leq x_{2},X_{3}\\leq x_{3})=C(\\mathbb{P}(X_{1}\\leq x_{1}),\\mathbb{P}(X_{2}\\leq x_{2}),\\mathbb{P}(X_{3}\\leq x_{3})). $$ <PARAGRAPH_SEPARATOR> Given that the univariate margins of the Bernoulli random vector $(X_{1},X_{2},X_{3})$ are fixed and all probabilities have to sum to one, a copula $C$ needs to satisfy the following four equations <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\left\\{\\mathbb{P}(X_{1}\\leq0,X_{2}\\leq0,X_{3}\\leq0)=C(\\mathbb{P}(X_{1}\\leq0),\\mathbb{P}(X_{2}\\leq0),\\mathbb{P}(X_{3}\\leq0)),\\right.}\\ &{\\left.\\mathbb{P}(X_{1}\\leq0,X_{2}\\leq0,X_{3}\\leq1)=C(\\mathbb{P}(X_{1}\\leq0),\\mathbb{P}(X_{2}\\leq0),\\mathbb{P}(X_{3}\\leq1))=C(\\mathbb{P}(X_{1}\\leq0),\\mathbb{P}(X_{2}\\leq0),1),\\right.}\\ &{\\left.\\mathbb{P}(X_{1}\\leq0,X_{2}\\leq1,X_{3}\\leq0)=C(\\mathbb{P}(X_{1}\\leq0),\\mathbb{P}(X_{2}\\leq1),\\mathbb{P}(X_{3}\\leq0))=C(\\mathbb{P}(X_{1}\\leq0),1,\\mathbb{P}(X_{3}\\leq1)),\\right.}\\ &{\\left.\\mathbb{P}(X_{1}\\leq1,X_{2}\\leq0,X_{3}\\leq0)=C(\\mathbb{P}(X_{1}\\leq1),\\mathbb{P}(X_{2}\\leq0),\\mathbb{P}(X_{3}\\leq0))=C(1,\\mathbb{P}(X_{2}\\leq0),\\mathbb{P}(X_{3}\\leq1)),\\right.}\\end{array} $$ <PARAGRAPH_SEPARATOR> to model the dependence of $(X_{1},X_{2},X_{3})$ . The second, third, and fourth equations of (13) correspond to the three bivariate margins of the copula. The first equation completes the information needed to construct a trivariate Bernoulli distribution. Consider a Normal copula $C_{R}$ with a symmetric and positive definite matrix $R$ of bivariate correlations <PARAGRAPH_SEPARATOR> $$ R=\\left(\\begin{array}{c c c}{{1}}&{{\\rho_{12}}}&{{\\rho_{13}}}\\\\{{\\rho_{12}}}&{{1}}&{{\\rho_{23}}}\\\\{{\\rho_{13}}}&{{\\rho_{23}}}&{{1}}\\end{array}\\right). $$ <PARAGRAPH_SEPARATOR> It is easy to see why there might be a problem with the existence of a Normal copula that realizes a given trivariate Bernoulli distribution. The three correlations in the correlation matrix are determined by the second, third, and fourth equations in (13), and each can be computed as in Section 1.1. These three correlations have to (1) form a positive definite matrix $R.$ If this is the case, additionally (2) the first equation in (13) has to be satisfied. In the following example, these problems are highlighted. <PARAGRAPH_SEPARATOR> Example 1.3. Consider a trivariate Bernoulli distribution with margins $\\mathbb{P}(X_{1}~=~0)~=~0.4$ , $\\mathbb{P}(X_{2}~=~0)~=~0.8$ , and $\\mathbb{P}(X_{3}=0)=0.2$ and joint probabilities <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{\\bullet\\mathbb{P}(X_{1}=0,X_{2}=0,X_{3}=0)=0.01\\quad}&{\\bullet\\mathbb{P}(X_{1}=0,X_{2}=0,X_{3}=1)=0.36}\\\\{\\bullet\\mathbb{P}(X_{1}=1,X_{2}=0,X_{3}=0)=0.16\\quad}&{\\bullet\\mathbb{P}(X_{1}=1,X_{2}=0,X_{3}=1)=0.27}\\\\{\\bullet\\mathbb{P}(X_{1}=0,X_{2}=1,X_{3}=0)=0.02\\quad}&{\\bullet\\mathbb{P}(X_{1}=0,X_{2}=1,X_{3}=1)=0.01}\\\\{\\bullet\\mathbb{P}(X_{1}=1,X_{2}=1,X_{3}=0)=0.01\\quad}&{\\bullet\\mathbb{P}(X_{1}=1,X_{2}=1,X_{3}=1)=0.16.}\\end{array} $$ <PARAGRAPH_SEPARATOR> The bivariate margins of this distribution are: <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{\\bullet\\mathbb{P}(X_{1}=0,X_{2}=0)=0.37\\quad}&{\\bullet\\mathbb{P}(X_{2}=0,X_{3}=0)=0.17}\\\\{\\bullet\\mathbb{P}(X_{1}=0,X_{3}=0)=0.03.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Notice that the bivariate margin of $(X_{1},X_{2})$ of the Bernoulli has been discussed in Section 1.1. From the last three equations in (13), we find $\\rho_{12}=0.4868$ , $\\rho_{13}=-0.4868$ , $\\rho_{23}=0.1340$ . These form a positive definite matrix. However with the obtained matrix R, $,C_{R}(0.4,0.8,0.2)=0.0298\\neq0.01=\\mathbb{P}(X_{1}=0,X_{2}=0,X_{3}=0)$ . Hence the Bernoulli distribution cannot be recovered with a Normal copula. <PARAGRAPH_SEPARATOR> A Bernoulli distribution inherits some properties from its corresponding copula. However, it is not easy to specify conditions under which a Bernoulli distribution can be constructed with a given copula. For some special cases we can give conditions under which the construction is possible. <PARAGRAPH_SEPARATOR> Proposition 1.1. If the trivariate Bernoulli distribution $(Y_{1},Y_{2},Y_{3})$ obtained from a trivariate Normal copula $C_{R}$ has margins $\\mathbb{P}(Y_{i}=0)=0.5$ for all $i\\in\\{1,2,3\\}$ , then the three-way interaction $u_{123}$ is zero. <PARAGRAPH_SEPARATOR> Proof. Since the Normal copula realizes the trivariate Bernoulli distribution of $(Y_{1},Y_{2},Y_{3})$ and $\\forall y_{1},y_{2},y_{3}\\in\\{0,1\\}$ when $\\mathbb{P}(Y_{i}=0)=0.5$ , the radial symmetry of the trivariate Normal distribution implies: <PARAGRAPH_SEPARATOR> $$ p(y_{1},y_{2},y_{3})=p(1-y_{1},1-y_{2},1-y_{3}). $$ <PARAGRAPH_SEPARATOR> Therefore, in this case we get <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{u_{123}=\\log\\left(\\frac{p(1,1,1)p(1,0,0)p(0,1,0)p(0,0,1)}{p(1,1,0)p(1,0,1)p(0,1,1)p(0,0,0)}\\right)}\\\\ &{\\qquad=\\log\\left(\\frac{p(0,0,0)p(0,1,1)p(1,0,1)p(1,1,0)}{p(1,1,0)p(1,0,1)p(0,1,1)p(0,0,0)}\\right)=0.\\quad\\square}\\end{array} $$ <PARAGRAPH_SEPARATOR> By the construction of the Bernoulli distribution from a copula, we can immediately get the following result. <PARAGRAPH_SEPARATOR> Proposition 1.2. If the margins of the Bernoulli distribution are $\\mathbb{P}(X_{i}=0)=0.5$ for all $i\\in\\{1,2,3\\}$ and the distribution has a radial symmetry, i.e. $p(x_{1},x_{2},x_{3})=p(1-x_{1},1-x_{2},1-x_{3})$ for $x_{i}\\in\\{0,1\\}$ , $(X_{1},X_{2},X_{3})$ can be realized with $a$ trivariate Normal copula. <PARAGRAPH_SEPARATOR> Proof. Since the margins are equal to 0.5, writing $p(1,1,1)$ in terms of $p(0,0,0)$ leads to: <PARAGRAPH_SEPARATOR> $$ p(1,1,1)=-0.5+p(0,0,0)+p(0,0,1)+p(0,0,0)+p(0,1,0)+p(0,0,0)+p(1,0,0)-p(0,1,0) $$ <PARAGRAPH_SEPARATOR> Because of the radial symmetry, the above equation becomes: <PARAGRAPH_SEPARATOR> $$ p(0,0,0)=\\frac{\\mathbb{P}(X_{1}\\leq0,X_{2}\\leq0,X_{3}\\leq1)+\\mathbb{P}(X_{1}\\leq0,X_{2}\\leq1,X_{3}\\leq0)+\\mathbb{P}(X_{1}\\leq1,X_{2}\\leq0,X_{3}\\leq0)}{2} $$ <PARAGRAPH_SEPARATOR> The numerator of the right hand side of the above equation is determined from the second, third, and fourth equations of (13). Therefore, the first equation of (13) is automatically satisfied by the parameters obtained from the other three equations. □ <PARAGRAPH_SEPARATOR> Proposition 1.3. If the three-way interaction of a trivariate Bernoulli distribution $(X_{1},X_{2},X_{3})$ is zero and $\\mathbb{P}(X_{i}=0)=0.5$ for all $i\\in\\{1,2,3\\}$ , a trivariate Normal copula is able to realize $(X_{1},X_{2},X_{3})$ . <PARAGRAPH_SEPARATOR> Proof. The proof can be found in the Appendix."
  },
  {
    "qid": "statistic-posneg-2059-0-0",
    "gold_answer": "TRUE. The formula $\\mathbf{W}_{\\mathrm{ML}}=\\mathbf{A}(\\mathbf{A}-\\sigma^{2}\\mathbf{I}_{d})^{1/2}\\mathbf{R}$ ensures equivalence because: (1) The principal eigenvectors $\\mathbf{A}$ capture the directions of maximum variance in the data, identical to PCA. (2) The term $(\\mathbf{A}-\\sigma^{2}\\mathbf{I}_{d})^{1/2}$ scales the eigenvectors by the variance explained, adjusted for noise ($\\sigma^2$). (3) The arbitrary orthogonal matrix $\\mathbf{R}$ accounts for rotational invariance in the latent space, which does not affect the subspace spanned by the principal components. Thus, the columns of $\\mathbf{W}_{\\mathrm{ML}}$ span the same subspace as the PCA loadings, confirming the equivalence.",
    "question": "In the probabilistic PCA (PPCA) model, the maximum likelihood estimator of the loading matrix $\\mathbf{W}_{\\mathrm{ML}}$ is given by $\\mathbf{W}_{\\mathrm{ML}}=\\mathbf{A}(\\mathbf{A}-\\sigma^{2}\\mathbf{I}_{d})^{1/2}\\mathbf{R}$, where $\\mathbf{A}$ is the matrix of principal eigenvectors of $\\mathbf{X}^{T}\\mathbf{X}$ and $\\mathbf{R}$ is an arbitrary orthogonal matrix. Is it true that this formulation ensures the equivalence between PPCA and traditional PCA?",
    "question_context": "Probabilistic PCA and Model Selection via Penalized Likelihood\n\nLet us assume that a centered independent and identically distributed (i.i.d.) sample $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}\\in$ $\\mathbb{R}^{p}$ is observed that we aim at projecting onto a $d$ -dimensional subspace while retaining as much variance as possible. All the observations are stored in the $n\\times p$ matrix $\\mathbf{X}{=}(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})^{T}$ . <PARAGRAPH_SEPARATOR> # 2.1 Probabilistic PCA <PARAGRAPH_SEPARATOR> The PPCA model $\\mathcal{M}_{d}$ assumes that, for all $i\\in\\{1,\\ldots,n\\}$ , each observation is driven by the following generative model <PARAGRAPH_SEPARATOR> $$ \\mathbf{x}_{i}=\\mathbf{W}\\mathbf{y}_{i}+\\boldsymbol\\varepsilon_{i}, $$ <PARAGRAPH_SEPARATOR> where $\\mathbf y_{i}\\sim\\mathcal N(0,\\mathbf I_{d})$ is a low-dimensional Gaussian latent vector, $\\mathbf{w}$ is a $p\\times d$ parameter matrix called the loading matrix, and $\\varepsilon_{i}\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbf{I}_{p})$ is a Gaussian noise term. <PARAGRAPH_SEPARATOR> This model is an instance of factor analysis and was first introduced by Lawley (1953). Tipping and Bishop (1999) then presented a thorough study of this model. In particular, expanding a result of Theobald (1975), they proved that this generative model is indeed equivalent to PCA in the sense that the PCs of $\\mathbf{X}$ can be retrieved using the maximum likelihood (ML) estimator ${\\bf{W}}_{\\mathrm{{ML}}}$ of W. More specifically, if A is the $p\\times d$ matrix of ordered principal eigenvectors of $\\mathbf{X}^{T}\\mathbf{X}$ and if $\\pmb{\\Lambda}$ is the $d\\times d$ diagonal matrix with corresponding eigenvalues, we have <PARAGRAPH_SEPARATOR> $$ \\mathbf{W}_{\\mathrm{ML}}=\\mathbf{A}(\\mathbf{A}-\\sigma^{2}\\mathbf{I}_{d})^{1/2}\\mathbf{R}, $$ <PARAGRAPH_SEPARATOR> where $\\mathbf{R}$ is an arbitrary orthogonal matrix. <PARAGRAPH_SEPARATOR> Under this sound probabilistic framework, dimension selection can be recast as a model selection problem, for which standard techniques are available. We review a few important ones in the next subsection. <PARAGRAPH_SEPARATOR> # 2.2 Model selection for PPCA <PARAGRAPH_SEPARATOR> The problem of finding an appropriate dimension can be seen as choosing a “best model\" within a family of models $({\\mathcal{M}}_{d})_{d\\in\\{1,...,p-1\\}}.$ . A first popular approach would be to use likelihood penalization, leading to the choice <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{d^{*}\\in\\underset{d\\in\\{1,\\ldots,p-1\\}}{\\mathrm{argmax}}\\{\\mathrm{log}p({\\mathbf X}|{\\mathbf W}_{\\mathrm{ML}},\\sigma_{\\mathrm{ML}},\\mathcal{M}_{d})-\\mathrm{pen}(d)\\},}\\end{array} $$ <PARAGRAPH_SEPARATOR> where pen is a penalty which grows with $d$ . These methods include the popular Akaike information criterion (AIC, (Akaike, 1974)), the Bayesian information criterion (BIC, (Schwarz, 1978)), or other refined approaches (Bai & Ng, 2002). However, their merits are mainly asymptotic, and our main interest in this paper is to investigate non-asymptotic scenarios. While the penalty term is usually necessary to avoid selecting the largest model, under a constrained PPCA model, called isotropic PPCA, Bouveyron et al. (2011) proved that regular ML was surprisingly consistent. While the theoretical optimality of this method is also asymptotic, the fact that it directly maximizes a likelihood criterion which is not derived based on asymptotic considerations makes it of particular interest within the scope of this paper."
  },
  {
    "qid": "statistic-posneg-926-0-0",
    "gold_answer": "TRUE. The assumption of no interference (i.e., $Y_{i}$ is not affected by $W_{j}$ for any $j\\neq i$) is necessary for the ATE to be unbiased under random assignment. Without this assumption, the potential outcomes for a unit could depend on the treatments of other units, violating the Stable Unit Treatment Value Assumption (SUTVA). This would make the simple estimator $\\tau_{\\mathrm{ATE}}$ biased because the observed outcomes would not solely reflect the direct effect of the treatment on the unit itself but also the indirect effects from other units' treatments. The classical framework relies on SUTVA to ensure that the difference in potential outcomes $Y_{i}(1) - Y_{i}(0)$ captures only the direct effect of the treatment.",
    "question": "The formula for the average treatment effect (ATE) $\\tau_{\\mathrm{ATE}}=\\frac{1}{n}\\sum_{i=1}^{n}\\{Y_{i}(1)-Y_{i}(0)\\}$ assumes no interference between units. Is this assumption necessary for the ATE to be unbiased under random assignment of treatment?",
    "question_context": "Average Treatment Effect Estimation Under Interference\n\nThe classical way of analysing randomized trials, following Neyman (1923) and Rubin (1974), centres on the average treatment effect defined using potential outcomes. Given a sample of $i=1,\\ldots,n$ units used to study the effect of a binary treatment $W_{i}\\in\\{0,1\\}$ , we posit potential outcomes $Y_{i}(0),Y_{i}(1)\\in\\mathbb{R}$ corresponding to the outcomes we would have measured had we assigned the ith unit to control and to treatment, respectively, i.e., we observe $Y_{i}=Y_{i}(W_{i})$ . We then proceed by arguing that the sample average treatment effect $$ \\tau_{\\mathrm{ATE}}=\\frac{1}{n}\\sum_{i=1}^{n}\\{Y_{i}(1)-Y_{i}(0)\\} $$ admits a simple unbiased estimator under random assignment of treatment. One limitation of this classical approach is that it rules out interference and instead introduces an assumption that the observed outcome for any given unit does not depend on the treatments assigned to other units, i.e., $Y_{i}$ is not affected by $W_{j}$ for any $j\\neq i$ (Halloran & Struchiner, 1995). However, in a wide variety of applied settings, such interference effects not only exist, but are often of considerable scientific interest (Sacerdote, 2001; Miguel & Kremer, 2004; Bakshy et al., 2012; Bond et al., 2012; Cai et al., 2015; Rogers & Feller, 2018). For example, in an education setting, it may be of interest to understand how a pedagogical innovation affects not just certain targeted students, but also their peers. This has led to a recent surge of interest in methods for studying randomized trials under interference (Hudgens & Halloran, 2008; Tchetgen Tchetgen & VanderWeele, 2012; Manski, 2013; Aronow & Samii, 2017; Eckles et al., 2017; Leung, 2020; Li & Wager, 2020; Sävje et al., 2021)."
  },
  {
    "qid": "statistic-posneg-542-0-0",
    "gold_answer": "FALSE. The particle cover density is defined as $f_{\\mathbf{K}_{0}}(x) = P(x\\in\\mathbf{K}_{0})/\\mathbb{E}V(\\mathbf{K}_{0})$, but this is not a probability density function (PDF) because it does not necessarily integrate to 1 over $\\mathbb{R}^{3}$. Instead, it is a normalized version of the probability that a point $x$ is covered by the typical particle $\\mathbf{K}_{0}$, scaled by the expected volume of $\\mathbf{K}_{0}$. The correct interpretation is that $f_{\\mathbf{K}_{0}}(x)$ represents the relative frequency of coverage of point $x$ by the particle, not a PDF.",
    "question": "Is the particle cover density $f_{\\mathbf{K}_{0}}(x)$ correctly defined as $P(x\\in\\mathbf{K}_{0})/\\mathbb{E}V(\\mathbf{K}_{0})$ for $x\\in\\mathbb{R}^{3}$?",
    "question_context": "Estimation of Volume Tensors and Particle Cover Density in Stereology\n\nIn the present paper, we focus on the more difficult situation where the construction of such 3D voxel images is not feasible. Even in conventional microscopy, it is difficult to construct such images. For this case, we develop stereological methods of estimating volume tensors. The main practical purpose of stereology is to estimate quantitative parameters of a spatial object from microscopy images of sections through the object (Baddeley & Jensen, 2005). A recent account of the mathematical and statistical foundations of stereology and the closely related field of stochastic geometry can be found in Schneider & Weil (2008). Local stereology is concerned with estimators based on observations in sections through fixed lower dimensional subspaces (Jensen, 1998). <PARAGRAPH_SEPARATOR> Local stereology of volume tensors (and more generally Minkowski tensors) has been studied in the recent papers by Auneau-Cognacq et al. (2013) and Jensen & Ziegel (2014). In Jensen & Ziegel (2014), the focus is on local stereological estimation, using lines and planes passing through fixed points. In the present paper, we show how a number of other geometric designs can be used for estimating volume tensors, including the Cavalieri design and the optical rotator design (Baddeley & Jensen, 2005; Tandrup et al., 1997). <PARAGRAPH_SEPARATOR> If the estimator variances are large, we cannot use these estimators in a direct study of the distribution of volume tensors of the particle population. We will show that the estimators can then instead be combined to provide consistent (in a probabilistic sense) estimators of moments of the so-called particle cover density. If we let $\\mathbf{K}_{0}$ be a typical particle with reference point at the origin, the cover density takes the form <PARAGRAPH_SEPARATOR> $$ f_{\\mathbf{K}_{0}}(x)=P(x\\in\\mathbf{K}_{0})/\\mathbb{E}V(\\mathbf{K}_{0}),\\quad x\\in\\mathbb{R}^{3}, $$ <PARAGRAPH_SEPARATOR> where $V$ denotes volume. The density $f_{{\\bf K}_{0}}$ may be envisaged as a kind of probability cloud, indicating how often the different points in space are visited by the typical particle $\\mathbf{K}_{0}$ . The covariance matrix of the cover density can be visualized by a centred ellipsoid called the Miles ellipsoid,1 containing information about average particle shape and orientation. We develop a non-parametric test of the hypothesis that the distribution of $\\mathbf{K}_{0}$ is rotation invariant in which case the Miles ellipsoid is a ball."
  },
  {
    "qid": "statistic-posneg-838-0-1",
    "gold_answer": "FALSE. The condition actually implies the opposite. According to Theorem 3(b), if there exists a $j\\in S$ such that $\\beta_{j}^{0}$ lies within the interval $(0,h(\\lambda))$ for $h(\\lambda)>0$ or $(h(\\lambda),0)$ for $h(\\lambda)<0$, then the probability of sign recovery failure is at least 1/2, i.e., $\\operatorname*{Pr}\\{\\operatorname{sign}({\\hat{\\pmb{\\beta}}})\\neq\\operatorname{sign}({\\pmb{\\beta}}^{0})\\}\\geq1/2$. This means the Lasso estimator is likely to fail in recovering the correct signs under these conditions.",
    "question": "Is it true that the condition $\\beta_{j}^{0}\\in(0,h(\\lambda))$ for $h(\\lambda)>0$ or $\\beta_{j}^{0}\\in(h(\\lambda),0)$ for $h(\\lambda)<0$ implies that the Lasso estimator will always recover the correct sign of the coefficients?",
    "question_context": "Sign Recovery Consistency in Lasso Estimation\n\nNow we turn to the results related to the failure of the sign recovery consistency, providing that either mutual incoherence condition or the constraint on minimum value of the true coefficient is violated. <PARAGRAPH_SEPARATOR> Theorem 3. Suppose that Assumptions 1–3 hold. <PARAGRAPH_SEPARATOR> (a) As opposed to Assumption 4 (the mutual incoherence condition), we assume that, for some $\\nu>0$ , <PARAGRAPH_SEPARATOR> $$ \\operatorname*{max}_{j\\in S^{c}}\\vert e_{j}^{\\top}\\Sigma_{S^{c}S}\\big(\\Sigma_{S S}\\big)^{-1}{\\ s i g n(\\pmb\\beta_{1}^{0})}\\vert=1+\\nu. $$ <PARAGRAPH_SEPARATOR> Then for any $\\lambda>0$ and sufficiently large $n$ , the probability that sign support recovery fails is no less than $1/2$ , i.e., $\\operatorname*{Pr}\\{\\operatorname{sign}(\\hat{\\pmb{\\beta}})\\neq\\operatorname{sign}({\\pmb{\\beta}}^{0})\\}\\geq1/2$ . (b) For each $j\\in S$ , we define the quantity $h(\\lambda)=\\lambda e_{j}^{\\top}(\\Sigma_{S S})^{-1}\\mathsf{s i g n}(\\beta_{1}^{0})$ . If there exists $j\\in S$ with $\\beta_{j}^{0}\\in(0,h(\\lambda))f o r h(\\lambda)>0$ or $\\beta_{j}^{0}\\in(h(\\lambda),0)$ for $h(\\lambda)<0$ , then $\\operatorname*{Pr}\\{\\operatorname{sign}({\\hat{\\pmb{\\beta}}})\\neq\\operatorname{sign}({\\pmb{\\beta}}^{0})\\}\\geq1/2$ . <PARAGRAPH_SEPARATOR> Theorem 3(a) implies that, the mutual incoherence condition in Assumption 4 is essential to ensure the corrected sign recovery for the Lasso partial likelihood estimator. The same result has been established in linear regression models [27,29]. For sign consistency, Theorem 3(b) shows that the value $\\operatorname*{min}_{\\mathbf{\\Phi}_{\\mathbf{\\Lambda}}}|\\beta_{j}^{0}|$ cannot decay to zero faster than the penalty parameter $\\lambda$ . Note that (7) is not a complete complementary event of Assumption 4, where $\\gamma>0$ is used to guarantee the uniqueness of the estimator $\\hat{\\boldsymbol{\\beta}}$ . <PARAGRAPH_SEPARATOR> ![](images/af0564164bfc1355848da3a494817352e193051ae587d9d33f188cd61e9b9e1e.jpg)  Fig. 1. Example 1—relating incoherence measure to sign consistency."
  },
  {
    "qid": "statistic-posneg-1403-1-0",
    "gold_answer": "FALSE. While the Stieltjes transform $m(z)$ is indeed analytic in $\\mathbb{C}_{+}$ as stated, it can also be analytically continued to the lower half-plane $\\mathbb{C}_{-}$ under certain conditions, particularly if the spectral measure $\\pi$ has a density with sufficient regularity. The assumption specifies the behavior in $\\mathbb{C}_{+}$ but does not preclude analyticity elsewhere.",
    "question": "Is it true that under Assumption A3, the Stieltjes transform $m(z)$ of the limiting spectral distribution of $X_{n}X_{n}^{*}$ is analytic only in the upper half-plane $\\mathbb{C}_{+}$?",
    "question_context": "Fixed-Rank Perturbations and Stieltjes Transform in Random Matrix Theory\n\nAssumptions and notations <PARAGRAPH_SEPARATOR> We now state the general assumptions of the paper. Consider the sequence of $N\\times n$ matrices $\\varSigma_{n}=X_{n}+P_{n}$ where the following assumption holds. <PARAGRAPH_SEPARATOR> Assumption A1. The dimensions $N$ , $n$ satisfy: $N\\leq n,n\\to\\infty$ and <PARAGRAPH_SEPARATOR> $$ \\frac{N}{n}\\to c\\in(0,1] $$ <PARAGRAPH_SEPARATOR> (notation for this asymptotic regime: $n\\to\\infty$ ). <PARAGRAPH_SEPARATOR> The following assumption on $X_{n}$ is widely used in the random matrix literature [18,23]. <PARAGRAPH_SEPARATOR> Assumption A2. Matrices $X_{n}$ are random $N\\times n$ bi-unitarily invariant matrices, i.e., each $X_{n}$ admits the singular value decomposition $X_{n}=L_{n}\\varGamma_{n}R_{n}^{*}$ where $L_{n}$ , the $N\\times N$ matrix $T_{n}$ and $R_{n}$ are independent, $L_{n}$ is Haar distributed on the group $\\mathcal{U}(N)$ of unitary $N\\times N$ matrices, and $R_{n}$ is a $n\\times N$ submatrix of a Haar distributed matrix on $\\mathcal{U}(n)$ . <PARAGRAPH_SEPARATOR> We recall that the Stieltjes transform of a probability measure $\\pi$ on the real line is the complex function <PARAGRAPH_SEPARATOR> $$ m(z)=\\int\\frac{1}{t-z}\\pi(d t), $$ <PARAGRAPH_SEPARATOR> analytic on $\\mathbb{C}_{+}=\\{z:\\Im(z)>0\\}$ . <PARAGRAPH_SEPARATOR> Assumption A3. Let $Q_{n}(z)=(X_{n}X_{n}^{*}-z I_{N})^{-1}$ be the resolvent associated with $X_{n}X_{n}^{*}$ and let $\\alpha_{n}(z)=N^{-1}\\mathrm{tr}Q_{n}(z)$ . For every $z\\in\\mathbb{C}_{+},\\alpha_{n}(z)$ a.s. converges to a deterministic function $m(z)$ which is the Stieltjes transform of a probability measure $\\pi$ supported by the compact interval $[\\lambda_{-},\\lambda_{+}]$ . <PARAGRAPH_SEPARATOR> Assumption A4. The quantity $\\|X_{n}X_{n}^{*}\\|$ a.s. converges to $\\lambda_{+}$ as $n\\to\\infty$ , where $\\|\\cdot\\|$ denotes the spectral norm."
  },
  {
    "qid": "statistic-posneg-412-3-0",
    "gold_answer": "FALSE. The condition $R(\\mathcal{U};\\lfloor n/2\\rfloor) \\asymp R(\\mathcal{U};n)$ is necessary but not sufficient to ensure rate-regularity. Rate-regularity also requires that the risk sequence behaves consistently across different sample sizes and does not exhibit erratic behavior. The context implies that additional regularity conditions are needed, though they are not explicitly stated in the provided text.",
    "question": "Is the condition $R(\\mathcal{U};\\lfloor n/2\\rfloor) \\asymp R(\\mathcal{U};n)$ sufficient to ensure that the minimax risk of the class $\\boldsymbol{\\mathcal U}$ is rate-regular, as defined in the context?",
    "question_context": "Minimax-Rate Adaptation in Regression Function Estimation\n\nIn light of Theorem 1, the adaptation recipe can be used to derive minimax-rate adaptive estimators over function classes. For simplicity, in this section, we assume $\\sigma^{2}(\\boldsymbol{x})$ is a unknown constant bounded above by $\\bar{\\sigma}^{2}$ . <PARAGRAPH_SEPARATOR> # 4.1. Minimax-Rate Adaptation <PARAGRAPH_SEPARATOR> Let $\\boldsymbol{\\mathcal U}$ be a class of regression functions. Consider the minimax risk for estimating a regression in $\\boldsymbol{\\mathcal U}$ , <PARAGRAPH_SEPARATOR> $$ R(\\mathcal{U};n)=\\operatorname*{min}_{\\hat{u}_{n}}\\operatorname*{max}_{u\\in\\mathcal{U},\\sigma\\leqslant\\bar{\\sigma}}E\\|u-\\hat{u}_{n}\\|^{2}, $$ <PARAGRAPH_SEPARATOR> where ${\\hat{u}}_{n}$ is over all estimators based on $\\textstyle Z^{n}=(X_{i},Y_{i})_{i=1}^{n}$ and the expectation is taken under $u$ and $\\sigma$ . The minimax risk measures how well one can estimate $u$ uniformly over the class $\\boldsymbol{\\mathcal U}$ . Let $\\{\\mathcal{U}_{j},j\\geqslant1\\}$ be a collection of classes of regression functions uniformly bounded between $-A$ and $A$ . Assume the true function $u$ is in (at least) one of the classes, i.e., $u\\in\\bigcup_{j\\geqslant1}\\mathcal{U}_{j}$ . The question we want to address is: Without knowing which class contains $u$ , can we have a single estimator such that it converges automatically at the minimax optimal rate of the class that contains $u?$ If such an estimator exists, we call it a minimax-rate adaptive estimator with respect to the classes $\\{\\mathcal{U}_{j},j\\geqslant1\\}$ . <PARAGRAPH_SEPARATOR> Many results have been obtained on minimax-rate adaptation (or even with the right constant for some cases) for specific functions classes such as Sobolev and Besov under various performance measures, including Efroimovich and Pinsker (1984), Efroimovich (1985), Hardle and Marron (1985), Lepski (1991), Golubev and Nussbaum (1992), Donoho and Johnstone (e.g., 1998), Delyon and Juditsky (1994), Mammen and van de Geer (1997), Tsybakov (1995), Goldenshluger and Nemirovski (1997), Lepski et al. (1997), Devroye and Lugosi (1997), and others. A method is proposed by Juditsky and Nemirovski (1996) to aggregate estimators to adapt to within order $n^{-1/2}$ in risk. General schemes have also been proposed for the construction of adaptive estimators in Barron and Cover (1991) based on minimum description length (MDL) criterion using =-nets. Other adaptation schemes and adaptation bounds by model selection have been developed later including very general penalized contrast criteria in Birg \u0010e and Massart (1996) and Barron et al. (1999) with many interesting applications on adaptive estimation; penalized maximum likelihood or least squares criteria in Yang and Barron (1998) and Yang (1999a); and complexity penalized criteria based on V-C theory (e.g., Lugosi and Nobel, 2000). In the opposite direction, for the case of estimating a regression function at a point, negative results have been obtained by Lepski (1991) and Brown and Low (1996). <PARAGRAPH_SEPARATOR> We give below a general result on minimax-rate adaptation without requiring any special property on the classes over which adaptation is desired. Some regularity conditions will be used for our results. <PARAGRAPH_SEPARATOR> Definition. If the minimax risk sequence satisfies <PARAGRAPH_SEPARATOR> $$ R({\\mathcal{U}};\\lfloor n/2\\rfloor)\\asymp R({\\mathcal{U}};n), $$ <PARAGRAPH_SEPARATOR> we say the minimax risk of the class $\\boldsymbol{\\mathcal U}$ is rate-regular. <PARAGRAPH_SEPARATOR> A rate of convergence is said to be nonparametric if it converges no faster than $n^{\\theta}$ for some $0<\\theta<1$ . <PARAGRAPH_SEPARATOR> The familiar rates of convergence $n^{-\\alpha}(\\log n)^{\\beta}$ for some $0<\\alpha\\leqslant1$ and $\\beta\\in R$ are rate-regular. For a rate-regular risk, together with that $R(\\mathcal{U};i)$ is"
  },
  {
    "qid": "statistic-posneg-1555-0-1",
    "gold_answer": "FALSE. The bootstrap sample uses the estimated transition probabilities $\\hat{P}(\\cdot\\mid\\hat{c}(X_{t-p}^{t-1}))$ from the context algorithm, not the true probabilities. This is explicitly mentioned in the context when describing the VLMC bootstrap procedure.",
    "question": "Does the bootstrap sample $X_{1}^{*},\\ldots,X_{n}^{*}$ in VLMC bootstrap simulation use the true transition probabilities $P(\\cdot\\mid c(X_{t-p}^{t-1}))$?",
    "question_context": "Variable Length Markov Chains: Bootstrap Simulation and Estimation\n\nFigure 5 shows the result of using AIC(vlmc(*, cutoff $\\begin{array}{r l}{\\mathbf{\\Xi}}&{{}=\\mathbf{\\Xi}\\mathbb{K}\\mathbf{\\Xi}.}\\end{array}$ )) for $K\\in$ $\\{2.8,2.82,\\ldots,6\\}$ (\" for(K in seq(2.8, 6, by $=$ .02))\" in R). Note that we often use AIC rather than BIC for the following reasons. Whereas AIC is known to overfit for choosing the correct finite-dimensional models, it can give smaller prediction squared error in the infinite case (such as $A R(\\infty),$ ) and actually often underfits for minimal 0-1 prediction loss, for example, see Friedman, Hastie, and Tibshirani (2000, Rejoinder, figure 1). Moreover, we have tried to use BIC instead of AIC for VLMC in several real data examples, and BIC there tends to underfit severely, in some cases even choosing the independency model. On the other hand, if the data is generated by a VLMC, BIC has empirically identified the correct model for large $n$ , that is, needing rather $n=10{,}000$ instead of $n=1{,}000$ , even for a very simple VLMC. If the aim is not minimizing the Kullback-Leibler divergence, such simple criteria are not at hand. Estimation of the cutoff parameter can then be pursued with bootstrapping as described in Section 4.1. <PARAGRAPH_SEPARATOR> # 4. BOOTSTRAPPING AND SIMULATING FROM A VLMC <PARAGRAPH_SEPARATOR> Simulating a VLMC of order $p$ is done via its transition probabilities $\\{P(x_{1}\\mid c(x_{-p+1}^{0})$ ; $x_{-p+1}^{1}\\in\\mathcal{X}^{p+1}\\}$ : start with an initial $p$ -vector 0 p 1 2 X p, where p is the order of the VLMC and simulate <PARAGRAPH_SEPARATOR> $$ X_{t}\\sim P(\\cdot\\mid c(X_{t-p}^{t-1})),t=1,2,\\ldots. $$ <PARAGRAPH_SEPARATOR> Under regularity conditions, the effect of the initialization gets forgotten exponentially fast as the simulated path gets longer, and the simulated values are becoming close to a sample from the stationary distribution of the VLMC. Thus, to simulate an $n$ -dimensional sample from the stationary distribution of the VLMC (assuming it exists), we proceed as follows: <PARAGRAPH_SEPARATOR> simulate $X_{1},X_{2},\\dots,X_{m+n}$ as in (4.1); choose $X_{m+1},\\dots,X_{m+n}$ as an approximately stationary sample of size $n$ : <PARAGRAPH_SEPARATOR> Here $m$ is a large number such as $10^{3}$ or $10^{4}$ and the initial vector in (4.1) may be chosen as a $p$ -vector whose elements are all equal to some $x\\in{\\mathcal{X}}$ . Our R function simulate.vlmc() has an argument n.start for $m$ taking $m=64\\times(\\mathrm{contextsize})=64\\mathrm{card}(\\tau_{c})$ as default value, for example, <PARAGRAPH_SEPARATOR> $>$ simulate.vlmc(vc5, n = 17) [1] \"a\" \"g\" \"t\" \"g\" \"a\" \"g\" \"c\" \"a\" c\" \"c\" \"c\"   > simulate.vlmc(vc5, n = 17) [1] \"g\" \"a\" \"g\" \"c\" \"g\" \"a\" \"t\" \"g\" \"c\" g\" \"g\" \"t\" \"c\" \"g\" \"t\" \"g\" \"t\"   > simvc5 <- simulate.vlmc(vc5, n = 100000)   > table(simvc5) a c g t   18822 30249 31010 19919 <PARAGRAPH_SEPARATOR> where the last simulation (of length 100,000) still takes only between 1 or 2 tenths of a second on a $900\\mathrm{MHz}$ Pentium III Computer. <PARAGRAPH_SEPARATOR> The VLMC bootstrap is nothing other than simulating from a fitted VLMC. The bootstrap sample of size $n$ is constructed as in (4.2), using in (4.1) the estimated transition probabilities from the context algorithm $\\hat{P}(\\cdot\\mid\\hat{c}(X_{t-p}^{t-1}))$ with $p$ the order of the estimated VLMC. The usual notation for such a bootstrap sample is then <PARAGRAPH_SEPARATOR> $$ X_{1}^{*},\\ldots,X_{n}^{*}. $$ <PARAGRAPH_SEPARATOR> We could (but have not done so in the VLMC package) define $>$ bootstrap.vlmc <- function(x, B) sapply(1:B, function(i) simulate.vlmc(x, n = x\\$n, integer.ret = TRUE)) <PARAGRAPH_SEPARATOR> where the integer.return $=$ TRUE argument results in series with values in $\\{0,1,\\ldots,$ $\\mathrm{card}(\\mathcal{X})-1\\}$ instead of characters, for example, \"a\", \"c\", \"g\", \"t\". This is faster and needs less memory for the result. VLMC-bootstrapping of an estimator $T_{n}=h_{n}(X_{1},$ $\\ldots,X_{n})$ , a function $h_{n}(\\cdot)$ of the data, is constructed with the plug-in rule: <PARAGRAPH_SEPARATOR> $$ T_{n}^{*}=h_{n}(X_{1}^{*},\\ldots,X_{n}^{*}), $$ <PARAGRAPH_SEPARATOR> with the same function $h_{n}(\\cdot)$ . As an example, we take $h_{n}(x_{1}^{n})=\\#\\{i\\mid x_{i}=\\mathtt{a}$ ; $x_{i-1}=\\operatorname{t}\\}/$ $(n-1)$ , that is, the occurrence frequency of “TA” in the DNA sequence. To this end we simulate from a relatively large model, <PARAGRAPH_SEPARATOR> $>\\lor\\subset3<-$ vlmc(bnrf1EB, cutof $\\bar{\\bar{\\cdot}}=3$ ) <PARAGRAPH_SEPARATOR> # smaller K overfits"
  },
  {
    "qid": "statistic-posneg-1207-3-1",
    "gold_answer": "TRUE. Lemma 2 explicitly states that the classifier gπ∗(z)=I{η(z)>π} satisfies the condition 1−TV(P,Q)=L0gπ∗+L1gπ∗ for any π∈(0,1). This is a generalization of the property that holds for the Bayes classifier when π=1/2, and it ensures that the classifier can be used to construct consistent tests for any π∈(0,1), as shown in Corollary 1.",
    "question": "Does the classifier gπ∗(z)=I{η(z)>π}, as defined in Lemma 2, always satisfy the condition 1−TV(P,Q)=L0gπ∗+L1gπ∗ for any π∈(0,1)?",
    "question_context": "Consistency and Asymptotic Normality in Classifier-Based Hypothesis Testing\n\nThe paper discusses hypothesis testing using classifiers, focusing on the Bayes classifier and its properties under different conditions. It introduces the concept of total variation distance (TV) between distributions P and Q, and how it relates to classifier errors. The paper also explores U-statistics for analyzing out-of-bag (OOB) errors and derives conditions for asymptotic normality."
  },
  {
    "qid": "statistic-posneg-2084-1-3",
    "gold_answer": "TRUE. The context proves this in Theorem 12, showing that $\\operatorname{Cov}(A_{kl}, A_m) = 0$ when $k = m \\neq l$. This result relies on the independence of the bilinear and quadratic forms under these conditions and the zero mean assumption for $\\mathbf{Y}_l$.",
    "question": "Does the covariance between a bilinear form $A_{kl}$ and a quadratic form $A_m$ equal zero when $k = m \\neq l$?",
    "question_context": "Cumulants and Covariance Structures in Quadratic and Bilinear Forms\n\nThe paper discusses the properties of quadratic and bilinear forms under normal distribution assumptions, providing detailed derivations for their cumulants and covariance structures. Key formulas include the rth cumulant of quadratic forms, the representation of bilinear forms as quadratic forms, and various covariance expressions between different forms."
  },
  {
    "qid": "statistic-posneg-166-0-0",
    "gold_answer": "TRUE. The context states that P(max(U/(1 - z), V/z) ≤ c) = G(c(1 - z, z)) = exp(c D(z)) for c ≤ 0, which is the cumulative distribution function of an exponential distribution on (-∞, 0) with rate parameter D(z). This confirms that max(U/(1 - z), V/z) follows an exponential distribution as described.",
    "question": "Is the statement 'For a max-stable distribution G with reverse exponential margins, the random variable max(U/(1 - z), V/z) follows an exponential distribution on (-∞, 0) with parameter D(z)' true based on the provided context?",
    "question_context": "Pickands Coordinates and Bivariate Extreme Value Distributions\n\nThe paper discusses Pickands coordinates, which are used to investigate the tail behavior in bivariate extreme value models. The coordinates are defined as c := x + y < 0 and z := y / (x + y) ∈ [0,1], transforming the vector (x, y) into (c(1 - z), cz). The distribution function G of a random variable (U, V) with reverse exponential margins is max-stable and represented as G(x, y) = exp((x + y)D(y / (x + y))), where D is a dependence function. The paper explores properties of the generalized Pareto (GP) distribution W, tail equivalence between G and W, and the distribution of Pickands coordinates (Z, C) for (U, V)."
  },
  {
    "qid": "statistic-posneg-1672-0-1",
    "gold_answer": "TRUE. The paper explicitly shows that for the proportional odds model, the sum of probabilities $p_{ij}$ for each subject is indeed 1, as derived from the cumulative probabilities: $\\sum_{j=1}^J p_{ij} = \\sum_{j=1}^J (\\pi_{ij} - \\pi_{i,j-1}) = \\pi_{iJ} - \\pi_{i0} = 1 - 0 = 1$. This ensures the proportionality of the Poisson and multinomial likelihoods for this model.",
    "question": "The Poisson likelihood and multinomial likelihood are proportional for the proportional odds logistic regression model only if the sum of probabilities $p_{ij}$ for each subject is constrained to be 1. Is this condition satisfied in the proportional odds model as derived in the paper?",
    "question_context": "Bias Correction in Proportional Odds Logistic Regression via Poisson Likelihood\n\nThe paper discusses a bias correction method for the proportional odds logistic regression model by transforming the multinomial likelihood into a Poisson likelihood. The key idea is that the multinomial likelihood can be expressed as a Poisson likelihood when the probabilities sum to 1 for each subject. The bias correction involves modifying the score equations by replacing the observed responses with pseudoresponses, which include a correction term derived from the second derivatives of the probabilities with respect to the parameters."
  },
  {
    "qid": "statistic-posneg-1702-3-0",
    "gold_answer": "FALSE. The context explicitly states that the standard RJMCMC algorithm resulted in a posterior distribution with mode at k=5, not k=4, despite the true model having four components. This overestimation occurs due to the equal means of two components and the artificial identifiability constraints imposed by the ordering of means.",
    "question": "Is the statement 'The standard RJMCMC algorithm correctly identifies the true number of components (k=4) in the mixture model' true or false based on the provided context?",
    "question_context": "Mixture Model Estimation via Reversible-Jump MCMC\n\nIn order to derive proper estimates of the parameters of mixed components we can post-process the simulated sample, that is, after the simulations having been completed. Specifically, we use the Pivotal Reordering algorithm based on the Maximum A Posteriori (MAP) estimator (see Marin et al. (2005), or Marin and Robert (2007)). Briefly, this technique chooses the reordering that is closest to the approximate MAP estimator and solves the identifiability problem without requiring a preliminary ordering of some of the parameters.<PARAGRAPH_SEPARATOR># 4.1. Simulated dataset 1<PARAGRAPH_SEPARATOR>We first generated 200 observations from the distribution<PARAGRAPH_SEPARATOR>$$0.25\\mathcal{N}(0,2)+0.25\\mathcal{N}(10,7)+0.25\\mathcal{N}(10,1)+0.25\\mathcal{N}(20,1)$$<PARAGRAPH_SEPARATOR>resulted in the histogram appearing in Fig. 4(a). We ran the standard RJMCMC algorithm for 300 000 iterations following a burn-in period of 30 000 and the resulting posterior distribution of the number of components was as in Fig. 1(a). Notice that, despite the large sample size, the mode of the estimated posterior distribution corresponds to $k=5$ (while there are actually four components) which is rather unexpected after taking a look at the histogram where only three modes are apparent. However, since the underlying mixture has the two central means equal, a three component model would be unable to explain the data, the central body of which consists of two components with significantly different variances. On the other hand, a four component model (i.e., the true one) fails to give satisfactory results due to the fact that the means are equal. Indeed, taking the parameters’ estimates conditioning on $k=4$ , we obtained that the estimated posterior expected values of the means of the second and third component were far apart from their true common value. Of course, this was expected due to the ordering of the means and is known to happen when the parameter space is artificially constrained. Notice that this problem is commented by Stephens (1997b) in the discussion of Richardson and Green’s (1997) paper who points out<PARAGRAPH_SEPARATOR>![](images/6567e352c967755b4761fb6d6d7bbb2a45336be8de93d1494a2ca7fdac49277d.jpg)  \nFig. 1. Simulated dataset 1: (a) Estimated posterior distribution of $k$ using the standard RJMCMC algorithm. (b) Estimated posterior distribution of c under the proposed approach.<PARAGRAPH_SEPARATOR>![](images/0e204ca4f555f845f0257ee1e9b00a17bc5bd61b01a45908148ab6c4430fdcfb.jpg)  \nFig. 2. Raw output of the standard RJMCMC sampler for the component means for the last 5000 iterations, conditioning on $k=5$ The multimodality is obvious for the second and fourth means.<PARAGRAPH_SEPARATOR>‘‘...relabelling according to $\\mu_{1}<\\mu_{2}<\\mu_{3}$ fails to separate the two components with common mean...’’<PARAGRAPH_SEPARATOR>Here we see that this problem does not affect only the estimation procedure but it mainly results in a posterior distribution with mode overestimating (for this particular dataset) the true number of components.<PARAGRAPH_SEPARATOR>Let us now consider the estimates of the posterior expectations of the means conditioning on $k=5$ . Averaging across the MCMC run we obtain the following estimates:<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}&{\\widehat{\\mathbf{E}}(\\mu|\\mathrm{data})=(-0.59,7.11,10.09,14.27,20.93)}\\ &{\\widehat{\\mathbf{E}}(\\sigma^{2}|\\mathrm{data})=(1.98,3.88,3.41,3.69,1.57)}\\ &{\\widehat{\\mathbf{E}}(\\pmb{p}|\\mathrm{data})=(0.23,0.14,0.28,0.12,0.23).}\\end{array}$$<PARAGRAPH_SEPARATOR>Comparing the above values with the parameters of the right model, one may wonder if the estimates of the second and fourth means are somehow justified. In order to have a deeper insight on this, we have plotted the raw output of the RJMCMC algorithm for the means corresponding to the last 5000 visits of the sampler to the five component model (see Fig. 2). At this point recall that there must be no label switching phenomenon due to the prior ordering of the means. Nevertheless, by taking a look at the output for the values of $\\mu_{2}$ and $\\mu_{4}$ we see that there is strong evidence of multimodality. This fact is typical of MCMC outputs in the analysis of mixtures, as artificial identifiability constraints often cannot break the symmetry of the posterior distribution (see for instance Jasra et al. (2005)). But here there is also another reason for this undesirable behaviour. Recall that the right model has only three means. Consequently, the model corresponding to $k=5$ is overparametrized; it introduces two additional pseudoparameters. Since their simulated values have no actual target, they roam from one ‘‘existing’’ mean to another ‘‘existing’’ mean, visiting also some intermediate values. This is clearly illustrated in Fig. 2. For instance, the simulated values of $\\mu_{2}$ switch among 0, 7 and 10. A similar situation is observed for $\\mu_{4}$ as well. Note that this behaviour occured in many other simulated datasets we tried. What we have concluded is that overparametrization usually results in multimodality, no matter whether artificial identifiability constraints are adopted or not.<PARAGRAPH_SEPARATOR>Next, we present the corresponding results with the proposed method. We ran our algorithm for the same number of iterations as before (300 000) letting $1\\leqslant k\\leqslant K=15.$ After a burn-in period of 30 000, we obtained the estimated posterior distribution of c illustrated in Fig. 1(b) (displayed only for models with $3\\leqslant k\\leqslant6$ which correspond to the bulk of the posterior distribution). By taking a closer look, it is immediately apparent that our method achieves to select the ‘‘right’’ model, since the mode of the estimated posterior distribution corresponds to (1, 2, 1), that is, the one used to generate the data. Moreover, the estimated posterior probability of this model was 1.5 times the second largest one, namely, that of model (2, 2, 1). Notice that the five-components model (1, 1, 1, 1, 1), selected previously by the default RJMCMC algorithm, has considerably smaller posterior probability. However, we can see that the models with the second and third highest posterior probabilities, i.e., (2, 2, 1) and (1, 2, 2), correspond to $k=5$ (and $k_{0}=3$ ). Concerning the acceptance rates of the proposed jump moves, they all were about $7\\%$ (see Table 1).<PARAGRAPH_SEPARATOR>Table 1 Acceptance rates of the proposed moves for each dataset.<PARAGRAPH_SEPARATOR><html><body><table><tr><td>Dataset</td><td>Mean split-merge (%)</td><td>Split-combine (%)</td><td>Birth-death(%)</td></tr><tr><td>Simulated 1</td><td>6.85</td><td>6.86</td><td>7.05</td></tr><tr><td>Simulated 2</td><td>2.46</td><td>4.52</td><td>4.74</td></tr><tr><td>Galaxy</td><td>8.69</td><td>10.07</td><td>13.95</td></tr></table></body></html><PARAGRAPH_SEPARATOR>![](images/a14ddfeabbd2fc6a52aae166388f957ac974ad796c13e63064c4c0097e8dcff4.jpg)  \nFig. 3. (a) Posterior probabilities of $k_{0}=3$ (solid line), 4 (dashed line) and 5 (dotted line) under the proposed method. (b) The corresponding posterior probabilities of $k$ using the default RJMCMC algorithm. (c) Posterior odds of the true model’s posterior probability to either the most probable one or the second probable one.<PARAGRAPH_SEPARATOR>Consider now the estimation of the parameters conditioning on the highest posterior probability model (1, 2, 1). As discussed previously, we cannot distinguish the parameters corresponding to components with the same mean. Hence, in this example the label switching phenomenon occurs between the second and third component while the other two components are identifiable. Reordering the MCMC output for variances and weights of components 2 and 3 according to the Pivotal Reordering algorithm based on the MAP estimator, the label switching disappears and we obtain the following estimates:<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}&{\\widehat{\\mathbf{E}}(\\mu|\\mathrm{data})=(-0.10,9.99,9.99,20.31)}\\ &{\\widehat{\\mathbf{E}}(\\sigma^{2}|\\mathrm{data})=(1.96,0.95,9.45,1.68)}\\ &{\\widehat{\\mathbf{E}}(\\pmb{p}|\\mathrm{data})=(.250,.262,.221,.267).}\\end{array}$$<PARAGRAPH_SEPARATOR>Note here that the first and fourth component have not been included in the reordering procedures because their parameters are identifiable as already emphasized. Observe also that the estimated posterior expectations of the parameters are quite close to the actual values used to simulate the data with an exception for the third and fourth variances. In Fig. 4(a) we can see the plug-in density estimate conditioning on model (1, 2, 1). We have also included the corresponding estimate obtained by the standard RJMCMC algorithm conditioning on $k=5$ for comparison purposes.<PARAGRAPH_SEPARATOR>In order to investigate what happens in case all components of the mixture have distinct means we tested the performance of our approach as a function of the difference $\\mu_{2}-\\mu_{3}$ . More specifically, we ran the algorithm by adding a constant $\\epsilon$ to the observations originally allocated either to the second or to the third component of the above simulated dataset. Then, for every $\\epsilon\\not\\in\\{-10,0,10\\}$ the true model is (1, 1, 1, 1) (having $k=k_{0}=4$ ), that is, the following mixture of normal distributions:<PARAGRAPH_SEPARATOR>$$0.25\\mathcal{N}(0,2)+0.25\\mathcal{N}(10+\\epsilon I_{\\{\\epsilon<0\\}},7)+0.25\\mathcal{N}(10+\\epsilon I_{\\{\\epsilon>0\\}},1)+0.25\\mathcal{N}(20,1).$$<PARAGRAPH_SEPARATOR>We took $\\epsilon=-10(.5)10$ and for each value we ran our algorithm as well as the original RJMCMC algorithm of Richardson and Green for 300 000 iterations following a burn-in period of 30 000. Fig. 3(a) illustrates the posterior probabilities of $k_{0}=3$ , 4 and 5 as a function of $\\epsilon$ . For comparison purposes, in Fig. 3(b) we have also plotted the corresponding posterior probabilities of the number of components $k$ arising from Richardson and Green’s default algorithm. These posterior probabilities are quite close when $|\\epsilon|$ is away from zero, while for small values of $|\\epsilon|$ both methods behave exactly as in the case $\\epsilon\\:=0$ . Moreover, when $\\epsilon=-10$ our method returns the right number of distinct means while the standard one underestimates $k$ . A rather strange behaviour occurs for $\\epsilon\\in[3.5,6.5]$ where both methods give posterior mode at $k$ or $k_{0}$ equal to 5. A similar phenomenon has been also observed in any other simulated dataset we tried. However, the posterior obtained under our approach has its mode on the true model. More generally, the set of $\\epsilon$ ’s where our approach returns the posterior mode at the right model is $\\{-10\\}\\cup[-8.5,-2.5]\\cup\\{0\\}\\cup[2.5,6.5]\\cup\\{10\\}$<PARAGRAPH_SEPARATOR>In order to get some idea about the magnitude of the true model’s posterior probability we have plotted its posterior odds to either the most probable one (when our approach fails to indicate the right model) or the second probable one (when our approach succeeds); see Fig. 3(c). Moreover, in Table 2 we present the most probable models as well as the posterior modes of the number of components under the default method and of the distinct means under our approach.<PARAGRAPH_SEPARATOR># 4.2. Simulated dataset 2"
  },
  {
    "qid": "statistic-posneg-883-0-1",
    "gold_answer": "FALSE. The paper explicitly states that computing standard errors for $\\widetilde{\\eta}_{n}$ based on a Markov chain is more complicated than in the iid case. It requires additional conditions, such as geometric ergodicity and finite moments, and the asymptotic variance has a complex form that is difficult to estimate consistently without methods like regenerative simulation.",
    "question": "Does the paper claim that the standard error computation for $\\widetilde{\\eta}_{n}$ based on a Markov chain is straightforward and does not require additional conditions beyond those needed for the iid case?",
    "question_context": "Honest Importance Sampling with Multiple Markov Chains\n\nThe paper discusses importance sampling, a Monte Carlo technique for estimating expectations with respect to a target probability density using samples from another density. It introduces a method to handle cases where the target density is not similar to the proposal density by using a mixture of densities. The paper also addresses the challenge of computing standard errors for these estimates, especially in the Markov chain Monte Carlo (MCMC) context, and proposes a method based on regenerative simulation to compute valid asymptotic standard errors."
  },
  {
    "qid": "statistic-posneg-226-0-1",
    "gold_answer": "FALSE. According to Whittle’s general equivalence theorem, for a concave $\\phi$, a design $\\xi^{*}$ is $\\phi$-optimal if and only if $\\Phi(\\xi^{*},\\delta_{x}) \\leq 0$ for all $x \\in \\chi$, with equality holding at the support points of $\\xi^{*}$. Thus, $\\Phi(\\xi^{*},\\delta_{x})$ is zero at the support points and non-positive elsewhere, but not necessarily always negative outside the support.",
    "question": "Is it correct to assume that the directional derivative $\\Phi(\\xi,\\eta)$ is always negative for a D-optimal design $\\xi^{*}$ at any point $x$ not in the support of $\\xi^{*}$?",
    "question_context": "D-optimal Designs for Nonlinear Models: Conditions for Minimal Support\n\nThe objective of this research is to determine if a locally $D$ -optimal design for a nonlinear model has minimal support. Techniques similar to those to be used in $\\S2$ have been used in the literature on optimal designs. Han & Chaloner (2003) considered exponential regression models and counted the roots of $d(\\xi,x)-k$ globally, where $d(\\xi,x)-k$ is the directional derivative, defined in $\\S2$ , for the $D$ -optimality criterion. Li & Majumdar (2008) extended it to examine the behaviour of $d(\\xi,x)-k$ in a vertical neighbourhood of zero and applied it to logistic models with three and four parameters. In this paper, we consider the derivatives of $d(\\xi,x)$ and obtain sufficient conditions with more extensive and systematic applicability. Investigation of the support points by examining derivatives of $d(\\xi,x)$ goes back to Guest (1958). In particular, we establish relationships between $D$ -optimal designs for different design spaces. <PARAGRAPH_SEPARATOR> # 2. SUFFICIENT CONDITIONS FOR MINIMAL SUPPORT <PARAGRAPH_SEPARATOR> Consider a compact design space $\\chi$ , and let $\\mathcal{H}$ denote the set of all probability measures on $\\chi$ . The set $\\mathcal{H}$ is compact in the topology of weak convergence and convex. Any measure $\\xi\\in\\mathcal{H}$ is called an approximate design. Suppose the parameter of interest $\\theta\\in\\mathbb{R}^{k}$ . For a nonlinear regression model <PARAGRAPH_SEPARATOR> $$ y_{i}=f(x_{i},\\theta)+\\epsilon_{i}, $$ <PARAGRAPH_SEPARATOR> with independent $\\epsilon_{i}\\sim N(0,\\sigma^{2})$ , the Fisher information matrix for $\\theta$ is $M(\\xi,\\theta)=\\int_{\\chi}(\\partial f(x,\\theta)/$ $\\partial\\theta)(\\partial f(x,\\theta)/\\partial\\theta^{\\mathrm{T}})d\\xi(x)$ , where we assume that the nuisance parameter $\\sigma^{2}$ is known and without loss of generality $\\sigma^{2}=1$ . For a generalized linear model <PARAGRAPH_SEPARATOR> $$ E(y_{i})=\\mu_{i},\\quad g(\\mu_{i})=\\eta_{i}=h(x_{i})^{\\mathrm{T}}\\theta, $$ <PARAGRAPH_SEPARATOR> the information matrix for $\\theta$ is $\\begin{array}{r}{M(\\xi,\\theta)=\\int_{x}\\omega(x,\\theta)h(x)h(x)^{\\mathrm{T}}d\\xi(x)}\\end{array}$ , where $\\omega(x,\\theta)=(\\partial\\mu/\\partial\\eta)^{2}/\\mathrm{var}(y)$ . As in Silvey (1980, p. 40), we define the $\\mathcal{D}$ -optimality criterion function as the logarithm of $|{\\cal M}(\\xi,\\theta)|$ , the determinant of the information matrix, if $M(\\xi,\\theta)$ is nonsingular and $-\\infty$ if $M(\\xi,\\theta)$ is singular. A $D$ -optimal design maximizes this criterion function over $\\mathcal{H}$ . The equivalence theorem for $D$ -optimal designs in the context of linear models was first developed by Kiefer $\\&$ Wolfowitz (1960) and was extended to nonlinear models by White (1973). Whittle (1973) generalized the equivalence theorem for concave optimality criteria over a compact convex set of probability measures in linear models. Chaloner & Larntz (1989) gave additional conditions to extend the general equivalence theorem to nonlinear models and applied it to Bayesian optimal designs. <PARAGRAPH_SEPARATOR> For a differentiable criterion function $\\phi$ , the directional derivative of $\\phi$ at $\\xi$ in the direction of $\\eta$ is defined as $\\begin{array}{r}{\\Phi(\\xi,\\eta)=\\operatorname*{lim}_{\\epsilon\\to0}\\epsilon^{-1}[\\phi\\{(1-\\epsilon)\\xi+\\epsilon\\eta\\}-\\phi(\\xi)]}\\end{array}$ . It follows from Whittle’s general equivalence theorem that, for a concave $\\phi$ , a design $\\xi^{*}$ is $\\phi$ -optimal if and only if $\\Phi(\\xi^{*},\\delta_{x})\\leqslant0$ , with equality at the support points of $\\xi^{*}$ , where $\\delta_{x}$ is a one-point design which gives all weight to the single value $x$ . <PARAGRAPH_SEPARATOR> We assume that the design space $\\chi$ has the form $\\chi_{a b}=[a,b]$ , where the quantities $a<b$ are assumed known. It follows from the definition of the $D$ -optimality criterion that a $D$ -optimal design over $\\mathcal{H}$ must be a nonsingular design, which is a design with a nonsingular information matrix."
  },
  {
    "qid": "statistic-posneg-311-0-3",
    "gold_answer": "TRUE. The function T(V^{-1}) is linear in the sense that T(k V^{-1}) = k T(V^{-1}) for any positive scalar k. This property arises because multiplying V^{-1} by k is equivalent to scaling the study's sample size by k while keeping the relative magnitudes of the elements of V^{-1} constant. The linearity of T(V^{-1}) ensures that the proportional contribution of a study to the variance of β̂_1 scales directly with its sample size, as expected in meta-analysis.",
    "question": "Does the function T(V^{-1}) scale linearly with the sample size, i.e., T(k V^{-1}) = k T(V^{-1}) for any positive scalar k?",
    "question_context": "Multivariate Meta-Analysis: Efficiency and Borrowing of Strength\n\nThe paper discusses a multivariate meta-analysis of n independent studies, each measuring a p×1 vector y of treatment effect estimates. The standard multivariate fixed effects model is y_i ∼ N(β, V_i), where V_i is the variance matrix specific to each study, and β is the common mean parameter. The maximum likelihood estimate of β is given by β̂ = Ω ∑ V_i^{-1} y_i, where Ω = (∑ V_i^{-1})^{-1}. The efficiency E is defined as the ratio of the variance of the multivariate estimate β̂_1 to the variance of the univariate estimate β̃_1, with E ≤ 1. The borrowing of strength B(V_i^{-1}) measures the contribution of secondary outcomes to the variance of β̂_1."
  },
  {
    "qid": "statistic-posneg-514-0-0",
    "gold_answer": "TRUE. The repair term $\\Delta_{n}q(X_{i})$ in the LR kernel density estimator $\\hat{f}_{n,i}^{\\mathrm{LR}}$ is designed to address the bias and instability issues of the original OS method. Under mild conditions, this modification ensures that the fitness coefficient $\\hat{\\alpha}_{n}$ retains its consistency property: it converges to one when the parametric model $f_{\\hat{\\theta}_{n}}$ is true (correctly specified) and to zero otherwise. The repair term helps stabilize the estimator, making it less sensitive to bandwidth choice and improving its performance in Kullback-Leibler loss estimation, thereby preserving the consistency property.",
    "question": "The fitness coefficient $\\hat{\\alpha}_{n}$ converges in probability to one if the model is true and zero otherwise, ensuring consistency. Is this property maintained when the kernel density estimator $\\hat{f}_{n,i}^{\\mathrm{LR}}$ includes the repair term $\\Delta_{n}q(X_{i})$?",
    "question_context": "Semiparametric Density Estimation: Fitness Coefficient and Kernel Smoothing\n\nThere exist two general approaches to density estimation. On the one hand, parametric methods are known to be precise but their results depend on the assumed statistical model. On the other hand, nonparametric methods often require more data but are free from model specification. In this context, Olkin and Spiegelman (1987), abbreviated OS in the present article, proposed to combine the two approaches by forming a convex combination $\\alpha f_{\\hat{\\theta}_{n}}+(1-\\alpha)\\hat{f}_{n}$ between a parametric density estimator $f_{\\hat{\\theta}_{n}}$ and a kernel density estimator ${\\hat{f}}_{n}$ . Given a set of data $X_{1}$ , … , $X_{n}\\subset$ $\\mathbb{R}^{d}$ where $d\\geq1$ , the value of $\\alpha$ is estimated by $\\hat{\\alpha}_{n}^{\\mathrm{OS}}$ , defined as the argmax over $\\alpha\\in[0,1]$ of the function $$\\sum_{i=1}^{n}\\log(\\alpha f_{\\hat{\\theta}_{n}}(X_{i})+(1-\\alpha)\\hat{f}_{n}(X_{i})).$$ This permits to get a density estimator robust to misspecification while retaining a performance comparable to parametric estimators when the true density is close to the model. <PARAGRAPH_SEPARATOR> While the previous idea is quite seducing, it has little chance to succeed because of (i) a prohibitive bias caused by the absence of any leave-one-out estimation strategy and (ii) the inherent flaws of the kernel density estimator in Kullback–Leibler loss estimation (Hall, 1987). In practice, it is very sensitive to the choice of the bandwidth (Faraway, 1990; Rahman, Beaver, & Gokhale, 1997). This prevents the OS method from getting satisfactory theoretical and practical results, as we shall argue throughout this article. <PARAGRAPH_SEPARATOR> In this article, we “repair” the kernel density estimator in order to improve the quality estimation of $\\alpha$ . We introduce the estimate $$\\hat{\\alpha}_{n}\\in\\underset{\\alpha\\in[0,1]}{\\operatorname{argmax}}\\sum_{i=1}^{n}\\log(\\alpha f_{\\hat{\\theta}_{n}}(X_{i})+(1-\\alpha)\\hat{f}_{n,i}^{\\operatorname{LR}}),$$ where $\\hat{\\boldsymbol f}_{n,i}^{\\mathrm{LR}}$ is called the leave-and-repair (LR) kernel density estimate and is given by $$\\hat{f}_{n,i}^{\\mathrm{LR}}=\\left(\\frac{1}{(n-1)h_{n}^{d}}{\\sum_{j\\ne i}}K\\left(\\frac{X_{i}-X_{j}}{h_{n}}\\right)\\right)+\\Delta_{n}q(X_{i}),$$ where $K:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{\\geq0}$ is the kernel, $h_{n}>0$ is the bandwidth, $\\Delta_{n}\\geq0$ and $q:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{\\geq0}$ . The LR estimate is a modification of the well-known leave-one-out (LOO) estimate usually employed in cross-validation procedures (Hall, 1987) and semiparametric estimation (Delecroix, Hristache, & Patilea, 2006). The estimator ${\\hat{\\alpha}_{n}}$ introduced in (1) is called the fitness coefficient because it may be interpreted as how well the model fits the data (see Section 5). <PARAGRAPH_SEPARATOR> Under mild conditions, the fitness coefficient ${\\hat{\\alpha}_{n}}$ is shown to converge in probability to one if the model is true and zero otherwise, a property called consistency. Even if the fitness coefficient is maximizing some objective function (over $\\alpha\\in[0,1],$ ), the results from M-estimation theory do not apply directly because, when the model is true, the limiting objective function is independent from $\\alpha$ . The proof follows from a fine comparison between the rates of convergence of $f_{\\hat{\\theta}_{n}}$ and ${\\hat{f}}_{n}$ . Using some real data as well as extensive simulations, we observe that the LR approach is more stable than the OS approach and leads to more accurate inference. Finally, some new perspectives on the use of the fitness coefficient for model evaluation are provided."
  },
  {
    "qid": "statistic-posneg-133-1-0",
    "gold_answer": "TRUE. The full-model empirical likelihood ratio $\\mathcal{R}^{F}(\beta,\\alpha)$ is defined such that it equals 1 when evaluated at the full-model GEE estimate $(\\hat{\beta}^{\\mathrm{ST}},\\hat{\\alpha}_{1}^{\\mathrm{ST}},\\hat{\\alpha}_{2}^{\\mathrm{ST}})$, since these estimates solve the equation $\\frac{1}{n}\\sum_{i}^{n}g^{F}(Y_{i},X_{i},\beta,\\alpha_{1},\\alpha_{2};R_{F})=0$. This ensures that the weights $w_i$ in the empirical likelihood ratio are all $1/n$, making $\\mathcal{R}^{F}(\\hat{\beta}^{\\mathrm{ST}},\\hat{\\alpha}_{1}^{\\mathrm{ST}},\\hat{\\alpha}_{2}^{\\mathrm{ST}})=1$.",
    "question": "Is it true that the full-model empirical likelihood ratio $\\mathcal{R}^{F}(\beta,\\alpha)$ is always equal to 1 for the full model GEE estimate $(\\hat{\beta}^{\\mathrm{ST}},\\hat{\\alpha}_{1}^{\\mathrm{ST}},\\hat{\\alpha}_{2}^{\\mathrm{ST}})$?",
    "question_context": "Empirical Likelihood for Working Correlation Structure Selection in GEE Models\n\nThe paper discusses the use of empirical likelihood to select the optimal working correlation structure in Generalized Estimating Equations (GEE) models. It introduces a full-model empirical likelihood ratio, which serves as a unified measure to compare different working correlation structures. The paper also proposes modified versions of AIC and BIC, called EAIC and EBIC, which use empirical likelihood instead of parametric likelihood for model selection."
  },
  {
    "qid": "statistic-posneg-1224-0-2",
    "gold_answer": "TRUE. The paper shows that the covariance structure includes a term {μ2 − (|k|−1)/|k| μ12}·kk′, which vanishes only if |k|μ2 = (|k|−1)μ12. This condition holds exclusively for the binomial distribution with |k| trials. Thus, any other sum distribution ensures non-zero covariances between components, satisfying criterion (ii) for sensu stricto multivariate distributions.",
    "question": "Is the covariance matrix of the multivariate hypergeometric splitting distribution always non-zero for any pair of components if the sum distribution is not binomial?",
    "question_context": "Splitting Distributions for Multivariate Count Data\n\nThe paper introduces splitting distributions for multivariate count data, where a univariate distribution is split into its J components. The pmf is given by p(y)=p|y|(y)p(|y|), assuming |y| follows a univariate distribution L(ψ) and y given |y|=n follows a singular multivariate distribution SΔn(θ). The paper also discusses sufficient conditions for criteria (i)-(iv) to hold, including the symmetry of the singular distribution and the non-Dirac nature of the sum distribution."
  },
  {
    "qid": "statistic-posneg-1296-2-0",
    "gold_answer": "FALSE. Setting $\\theta_{00} = \\theta_{10} = 0$ implies that the baseline probability of $S=0$ is the same for both treatment sequences when $X=0$ (i.e., treatment sequence $(A,B)$). However, the probability of $S=0$ can still vary with $X$ if other parameters ($\\theta_{01}, \\theta_{02}, \\theta_{11}, \\theta_{12}$) are non-zero. The model allows for different probabilities of $S=0$ across treatment sequences through these other parameters.",
    "question": "In the logistic regression model for crossover trials, setting $\\theta_{00} = \\theta_{10} = 0$ implies that the probability of $S=0$ is the same for both treatment sequences. Is this statement true?",
    "question_context": "Logistic Regression Analysis in Crossover Trials\n\nThe paper discusses the analysis of crossover trials using a logistic regression model. The model is defined as: $$\\operatorname*{Pr}\\left(S=i\\mid x\\right)=e^{\\theta_{0i}+\\theta_{1i}x}/\\sum_{j=0}^{2}e^{\\theta_{0j}+\\theta_{1j}x},\\quad i=0,1,2$$ where $X=0$ represents the treatment sequence $(A,B)$ and $X=1$ represents the sequence $(B,A)$. The parameters $\\theta_{00}$ and $\\theta_{10}$ are set to zero. The model is used to test for treatment effects and period effects, with test statistics $G_1$, $G_2$, and $G$ derived from log-likelihood ratios."
  },
  {
    "qid": "statistic-posneg-316-0-0",
    "gold_answer": "TRUE. The probability $p_{m}(u,v)$ represents the joint probability that the random distribution function $F$ evaluated at points $t_{1},...,t_{m}$ lies within the intervals $[u_{1},v_{1}],...,[u_{m},v_{m}]$ respectively. This is exactly the definition of the probability that $F$ lies within the confidence bands defined by $u$ and $v$ at these points. The context explicitly states that this probability is used to set fixed Bayesian confidence bands for the random distribution function.",
    "question": "Is the probability $p_{m}(u,v) = \\mathrm{pr}\\{u_{i} \\leqslant F(t_{i}) \\leqslant v_{i}; i=1,...,m\\}$ equivalent to the probability of the random distribution function $F$ lying within the confidence bands defined by $u$ and $v$ at points $t_{1},...,t_{m}$?",
    "question_context": "Nonparametric Bayesian Confidence Bands for Distribution Functions\n\nLet $X_{1}<\\ldots<X_{s}$ denote the distinct observations in a sample of size $\\pmb{n}$ from $\\mathcal{P}$ and let $\\pmb{n}(\\pmb{\\hat{\\imath}})$ denote the number of repetitions of $\\pmb{X}_{i}$ in the sample. Ferguson showed that the posterior distribution of $\\pmb{P}$ is again a Dirichlet process with parameter $a(.)+\\Sigma n(i)d(X_{i},.)$ ,where $d(\\pmb{X},\\pmb{A})=1$ if $\\pmb{X}$ belongs to $\\pmb{A}$ and equals zero otherwise. As pointed out by Ferguson, any analysis performed on the Dirichlet process will be valid both $\\pmb{a}$ priori and $\\pmb{a}$ posteriori. If $\\mathscr{X}$ consists of a finite set of elements the above result is equivalent to stating that the Dirichlet distribution is the conjugate prior to be used in the estimation of multinomial probabilities. If the jumps in $a(.)$ are all integers then the joint distribution of the cumulative cell probabilities is identical to the joint distribution of some of the order statistics of a uniform random sample as discussed by Wilks (1962, p. 236). This in fact is the limiting posterior probability distribution when $a(\\mathcal X)\\rightarrow0$ , that is, the case of a noninformative prior. <PARAGRAPH_SEPARATOR> # 2. CONFIDENCE BANDS <PARAGRAPH_SEPARATOR> Formulae have been derived by Breth (1978) to compute rectangle probabilities over the ordered Dirichlet distribution; that is for $0\\leqslant u_{1}\\leqslant\\ldots\\leqslant u_{m}<1$ $0<v_{1}\\leqslant\\ldots\\leqslant v_{m}\\leqslant1$ and ${\\pmb u}_{\\nless}<{\\pmb v}_{\\pmb q}$ for all i, <PARAGRAPH_SEPARATOR> $$ p_{m}(u,B,v;a)=\\mathrm{pr}\\{u_{i}\\leqslant P(B_{i})\\leqslant v_{i};\\mathrm{~i=1,...,m\\}. $$ <PARAGRAPH_SEPARATOR> A simulation alternative was also given. In the case when the $\\pmb{a}(\\pmb{B}_{i})$ are all integers, the above probability was shown to be equivalent to a rectangular probability over the uniform order statistics. Steck (197l) expressed this probability as a determinant. <PARAGRAPH_SEPARATOR> Let ${\\pmb{\\mathcal{X}}}=(-\\infty,\\infty)$ and for $-\\infty<t_{1}<...<t_{m}<\\infty$ define $B_{j}=(-\\infty,t_{j}]$ for ${\\mathfrak{j}}=1,...,{\\mathfrak{m}}$ and $B_{m+1}=\\mathcal{X}$ .For $j=1,...,m,P(B_{j})=F(t_{j})~$ where $\\pmb{F}$ is the random distribution function associated with the random probability measure $\\pmb{P}$ . Then (2·1) becomes <PARAGRAPH_SEPARATOR> $$ p_{m}(u,B,v;\\boldsymbol{a})=\\mathrm{pr}\\{u_{i}\\leqslant F(t_{i})\\leqslant v_{i};\\boldsymbol{\\mathsf{i}}=1,...,m\\}, $$ <PARAGRAPH_SEPARATOR> which was used to set fxed Bayesian confidence bands within which the random distribution function lies with determined probability. In the sequel when there is no possible source of confusion reference to $\\pmb{B}$ and $\\pmb{\\alpha}(.)$ will be deleted and (2·2) will be written as <PARAGRAPH_SEPARATOR> $$ p_{m}(u,v)=\\mathrm{pr}\\{u_{i}\\leqslant F(t_{i})\\leqslant v_{i};i,=1,...,m\\}. $$ <PARAGRAPH_SEPARATOR> In practice to construct such confidence bands a desired probability level $\\pmb{\\mathcal{P}}$ would be stipulated and it would be necessary to determine $m,~u,~v$ and $\\pmb{\\ell}$ such that $\\pmb{\\mathscr{p}}_{m}(\\pmb{u},\\pmb{v})=\\pmb{p}$ First consider the case when it is possible to choose $\\pmb{\\ell}$ s0 that for some $\\pmb{m}$ , $\\pmb{a}(B_{\\pmb{j}})=\\dot{\\pmb{j}}$ for $j=1,...,m+1$ . Then for $\\dot{\\pmb{\\mathscr{i}}}=1,...,\\pmb{\\mathscr{m}}$ , the random value $\\pmb{\\mathcal{F}}(\\pmb{\\ell}_{\\pmb{\\ell}})$ has the same distribution a8 the ith order statistic of a uniform random sample of size $\\mathbf{\\nabla}_{m}$ . For this $\\pmb{\\ell}$ if $\\pmb{\\mathscr{u}}$ and $\\pmb{v}$ are restricted to Kolmogorov bands the solution to $\\pmb{p}_{m}(\\pmb{u},\\pmb{v})=\\pmb{p}$ is easily obtained from existing tables of critical values of the Kolmogorov statistic; a detailed example of this is discussed by Breth (1978). For the noninformative prior it is always possible to obtain such bands for the limiting posterior distribution provided that ${\\pmb n}({\\dot{\\bullet}})=1$ for all i. One simply takes $m=n-1$ and $t_{\\ell}=X_{\\ell}$ for $\\pmb{i}=1,...,m$ <PARAGRAPH_SEPARATOR> In general it may not be possible to choose $\\pmb{B}$ 80 that $\\pmb{a}(\\pmb{B}_{\\pmb{j}})=\\dot{\\pmb{j}}$ for all $j$ or to obtain a $\\pmb{B}$ which gives a suitable approximation to this. In such cases tables of values for $\\pmb{\\mathscr{p}}_{m}(\\pmb{\\mathscr{u}},\\pmb{\\ v})=\\pmb{\\mathscr{p}}$ are not currently available. It would be possible to proceed by using the computational techniques previously mentioned by fixing $\\pmb{\\ell}$ and considering the effects on $\\mathcal{\\pmb{p}}_{m}(\\pmb{u},\\pmb{v})$ caused by systematic alterations in $\\pmb{\\mathscr{u}}$ and $\\pmb{v}$ . Alternatively, if $\\pmb{\\mathscr{u}}$ and $\\pmb{v}$ are fixed it would be possible to consider the effects on $\\pmb{\\mathcal{P}}_{m}(\\pmb{u},\\pmb{v})$ caused by systematic alterations to t. These primitive approaches would require considerable computer time. Alternative techniques need to be developed but lie beyond the scope of this paper. <PARAGRAPH_SEPARATOR> When such bands have been constructed there is no guarantee that they are in any sense optimal. In the Neyman-Pearson framework Steck (197l) has discussed problems associated with the optimality criterion of minimizing the are8 enclosed by the bands. In particular it was noted that when the distribution has infinite or semiinfinite support the area may be infnite. This type of problem also arises in the Bayesian framework. As an example where the area is finite consider bands for the limiting posterior distribution when the prior is noninformative, $\\pmb{n}(\\mathrm{i})=1$ for all $\\pmb{\\dot{\\upnu}}$ and ${\\pmb F}({\\bf0})={\\bf0}$ . In this case the limiting posterior distribution of ${\\pmb F}({\\pmb X}_{\\pmb n})$ is degenerate and defining $\\pmb{\\mathcal{X}}_{0}=\\mathbf{0}$ , the area enclosed by the bands is given by $D=\\Sigma(X_{i}-X_{i-1})(v_{i}-u_{i-1})$ . In theory $\\pmb{D}$ may be minimized by numerical methods subject to $\\mathscr{p}_{n-1}(u,v)=p$ . Note that a posteriori for the noninformative prior, $\\pmb{\\mathscr{t}}=(\\pmb{X}_{1},...,\\pmb{X}_{n-1})$ i8 fxed and there are various possible $\\pmb{\\mathscr{u}}$ and $\\pmb{v}$ which are solutions to $\\mathscr{p}_{\\mathfrak{n}-\\mathfrak{1}}(\\mathfrak{u},\\mathfrak{v})=\\mathscr{p}$ . When $\\pmb{n X}_{i}=\\dot{\\pmb{\\i}}$ for all $\\pmb{\\dot{\\upnu}}$ ,Steck in fact studied $\\pmb{D}$ for selected candidate bands and concluded that the Kolmogorov bands were not far from being optimal."
  },
  {
    "qid": "statistic-posneg-608-3-2",
    "gold_answer": "TRUE. This condition ensures that the Hamiltonian error does not grow beyond a prespecified threshold $\\Delta$, preventing numerical instability and wasted computational effort by automatically rejecting unstable paths.",
    "question": "Is the condition $\\operatorname*{max}_{z\\in S_{a:b}(z^{\\mathrm{curr}})}H(z)-\\operatorname*{min}_{z\\in S_{a:b}(z^{\\mathrm{curr}})}H(z)<\\Delta$ used to ensure numerical stability in the Hamiltonian dynamics?",
    "question_context": "Detailed Balance and Efficiency in the AAPS Algorithm\n\nProposition 1. The AAPS algorithm satisfies detailed balance with respect to the extended posterior $\\tilde{\\pi}(x,p)$ . <PARAGRAPH_SEPARATOR> Proof. Step 1 preserves $\\tilde{\\pi}$ because $p$ is independent of $x$ and is sampled from its marginal. <PARAGRAPH_SEPARATOR> It will be helpful to define the system from the point of view of starting at $z^{\\mathrm{prop}}$ . Let $a^{\\prime},b^{\\prime}$ and $c^{\\prime}$ be as defined in (5), but with $z^{\\prime}=z^{\\mathrm{prop}}$ . Then, since $S_{\\#}(z^{\\mathrm{prop}};z^{\\mathrm{curr}})=-S_{\\#}(z^{\\mathrm{curr}};z^{\\mathrm{prop}})$ , <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{-c\\leq0\\leq K-c,-c\\leq S_{\\#}(z^{\\mathrm{prop}};z^{\\mathrm{curr}})\\leq K-c\\Leftrightarrow-c^{\\prime}}\\ &{\\quad\\leq S_{\\#}(z^{\\mathrm{curr}};z^{\\mathrm{prop}})\\leq K-c^{\\prime},-c^{\\prime}\\leq0\\leq K-c^{\\prime}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> Equivalently, $1(c\\in\\{0,\\ldots,K\\})1(z^{\\mathrm{prop}}\\in S_{a:b}(z^{\\mathrm{curr}}))\\equiv1(c^{\\prime}\\in$ $\\{0,\\ldots,K\\})1(z^{\\mathrm{curr}}\\in S_{a^{\\prime}:b^{\\prime}}(z^{\\mathrm{prop}}))$ . Moreover, segment invariance (5) is equivalent to $z\\in S_{a:b}(z^{\\mathrm{curr}})\\iff z\\in S_{a^{\\prime}:b^{\\prime}}(z^{\\mathrm{prop}})$ . <PARAGRAPH_SEPARATOR> The resulting chain satisfies detailed balance with respect to $\\tilde{\\pi}$ because <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\tilde{\\pi}\\left(z^{\\mathrm{curr}}\\right)\\times\\mathbb{P}\\left(c\\right)}\\ &{\\quad\\quad\\times\\mathbb{P}\\left(\\mathrm{propose}z^{\\mathrm{prop}}|c\\right)\\times\\mathbb{P}\\left(\\mathrm{accept}z^{\\mathrm{prop}}|\\mathrm{proposed},c\\right)}\\end{array} $$ <PARAGRAPH_SEPARATOR> is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\tilde{\\pi}\\left(z^{\\mathrm{curr}}\\right)\\times\\frac{1\\left(c\\in\\{0,\\ldots,K\\}\\right)}{K+1}}\\ &{\\qquad\\times\\frac{w\\left(z^{\\mathrm{curr}},z^{\\mathrm{prop}}\\right)1\\left(z^{\\mathrm{prop}}\\in S_{a:b}\\left(z^{\\mathrm{curr}}\\right)\\right)}{\\sum_{z\\in S_{a:b}\\left(z^{\\mathrm{curr}}\\right)}w\\left(z^{\\mathrm{curr}},z\\right)}}\\ &{\\qquad\\times1\\wedge\\frac{\\tilde{\\pi}\\left(z^{\\mathrm{prop}}\\right)w\\left(z^{\\mathrm{prop}},z^{\\mathrm{curr}}\\right)\\sum_{z\\in S_{a:b}\\left(z^{\\mathrm{curr}}\\right)}w\\left(z^{\\mathrm{curr}},z\\right)}{\\tilde{\\pi}\\left(z^{\\mathrm{curr}}\\right)w\\left(z^{\\mathrm{curr}},z^{\\mathrm{prop}}\\right)\\sum_{z\\in S_{a:b}\\left(z^{\\mathrm{prop}}\\right)}w\\left(z^{\\mathrm{prop}},z\\right)},}\\end{array} $$ <PARAGRAPH_SEPARATOR> which is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\frac{1(c\\in\\{0,\\ldots,K\\})1(z^{\\mathrm{prop}}\\in{\\mathcal S}_{a:b}(z^{\\mathrm{curr}}))}{K+1}}\\ &{\\qquad\\times\\left\\{\\frac{\\tilde{\\pi}(z^{\\mathrm{curr}})w(z^{\\mathrm{curr}},z^{\\mathrm{prop}})}{\\sum_{z\\in{\\mathcal S}_{a:b}(z^{\\mathrm{curr}})}w(z^{\\mathrm{curr}},z)}\\wedge\\frac{\\tilde{\\pi}(z^{\\mathrm{prop}})w(z^{\\mathrm{prop}},z^{\\mathrm{curr}})}{\\sum_{z\\in{\\mathcal S}_{a:b^{\\prime}}(z^{\\mathrm{prop}})}w(z^{\\mathrm{prop}},z)}\\right\\}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> This expression is invariant to $(z^{\\mathrm{curr}},a,b,c)\\leftrightarrow(z^{\\mathrm{prop}},a^{\\prime},b^{\\prime},c^{\\prime})$ . 口 <PARAGRAPH_SEPARATOR> Remark 1. The AAPS could be applied with a numerical integration scheme that does not have a Jacobian of 1. In such a scheme, however, the Jacobian would need to be included in the acceptance probability; moreover, a Jacobian other than 1 would lead to greater variability in $\\tilde{\\pi}$ along a path and, we conjecture, to a reduced efficiency. <PARAGRAPH_SEPARATOR> The path $S_{a:b}$ may visit parts of the statespace where the numerical solution to (2) is unstable and the error in the Hamiltonian may increase without bound, leading to wasted computational effort as large chunks of the path may be very unlikely to be proposed and accepted. Indeed it is even possible for the error to increase beyond machine precision. The no Uturn sampler suffers from a similar problem and we introduce a similar stability condition to that in Hoffman and Gelman (2014). We require that <PARAGRAPH_SEPARATOR> $$ \\operatorname*{max}_{z\\in S_{a:b}(z^{\\mathrm{curr}})}H(z)-\\operatorname*{min}_{z\\in S_{a:b}(z^{\\mathrm{curr}})}H(z)<\\Delta, $$ <PARAGRAPH_SEPARATOR> for some prespecified parameter $\\Delta$ . This criterion can be monitored as the forward and backward integrations proceed, and if at any point the criterion is breached, the path is thrown away: the proposal is automatically rejected. Segment invariance means that $z~\\in~S_{a:b}(z^{\\operatorname{curr}})\\quad\\Longleftrightarrow\\quad z~\\in~S_{a^{\\prime}:b^{\\prime}}(z^{\\operatorname{prop}})$ , so the same rejection would have occured if we created the path from any proposal in $S_{a:b}(z^{\\mathrm{curr}})$ , and detailed balance is still satisfied. For the experiments in Section 4 we found that a value of $\\Delta=1000$ was sufficiently large that the criterion only interfered when something was seriously wrong with the integration. Step 3 of the algorithm then becomes: <PARAGRAPH_SEPARATOR> 3. Create $S_{a:b}$ by leapfrog stepping forward from $(x_{0},p_{0})$ and then backward from $(x_{0},p_{0})$ . If condition (7) fails then go to 6. <PARAGRAPH_SEPARATOR> For a $d$ -dimensional Markov chain, $\\{X_{t}\\}_{t=0}^{\\infty}$ , with a stationary distribution of $\\pi$ , the asymptotic variance is $\\begin{array}{r}{V_{f}:=\\operatorname*{lim}_{n\\uparrow\\infty}}\\end{array}$ nvar $\\textstyle\\left[{\\frac{1}{n}}\\sum_{i=1}^{n}f(X_{i})\\right]$ . Thus, after burn in, var $\\begin{array}{r l}{\\left[{\\frac{1}{n}}\\sum_{i=1}^{n}f(X_{i})\\right]}&{{}\\approx}\\end{array}$ $\\mathrm{{\\itV}}_{f}/n$ . The effective sample size is the number of iid samples from $\\pi$ that would lead to the same variance: $\\mathrm{ESS}_{f}=n\\mathrm{var}_{\\pi}\\left[f\\right]/V_{f}$ . Let ${\\mathrm{ESS}}_{i}$ be an empirical estimate of the ESS for the ith component of $X$ (i.e., $f$ gives the ith component) and let $n_{\\mathrm{leap}}$ be the total number of leapfrog steps taken during the run of the algorithm. We measure the efficiency of an algorithm as <PARAGRAPH_SEPARATOR> $$ \\mathrm{Eff}=\\frac{1}{n_{\\mathrm{leap}}}\\operatorname*{min}_{i=1,\\ldots,d}\\mathrm{ESS}_{i}. $$ <PARAGRAPH_SEPARATOR> Since the leapfrog step is by far the most computationally intensive part of the algorithm, Eff is proportional to the number of effective iid samples generated per second in the worst mixing component. <PARAGRAPH_SEPARATOR> # 3.3. Choice of Weight Function <PARAGRAPH_SEPARATOR> The weight function $\\begin{array}{r}{\\ w(z,z^{\\prime})=\\tilde{\\pi}(z^{\\prime})}\\end{array}$ mentioned previously is a natural choice, and substitution into (6) shows that this leads to an acceptance probability of 1; however, it turns out not to be the most efficient choice. For example, intuitively the algorithm might be more efficient if points on the path that are further away from the current point are more likely to be proposed. To investigate the effects of the choice of $w$ we examine six possibilities:"
  },
  {
    "qid": "statistic-posneg-1215-0-0",
    "gold_answer": "FALSE. The test statistic T(n1,…,nk) does not converge to a normal distribution under H0. Instead, it converges to a weighted sum of squared normal variables divided by the sum of variances, as shown in the paper's Theorem 1. The asymptotic distribution is more complex and involves a quadratic form of normal variables.",
    "question": "Is it true that the test statistic T(n1,…,nk) converges to a normal distribution under the null hypothesis H0?",
    "question_context": "Equality of Variances Test for Random Fuzzy Sets\n\nThe paper discusses a method for testing the equality of variances of k Random Fuzzy Sets (RFSs) using a distance metric Dθφ. The test statistic T(n1,…,nk) is proposed, which under the null hypothesis H0:σX12=⋯=σXk2, converges to a specific distribution. The paper also introduces bootstrap methods to approximate the distribution of the test statistic."
  },
  {
    "qid": "statistic-posneg-1684-4-1",
    "gold_answer": "TRUE. The condition ensures that the minimal signal strength is large enough relative to the noise term, allowing the thresholded basis pursuit to correctly identify the support and sign of β0. This is derived from the stable nullspace property and the bounds on the estimation error.",
    "question": "The condition |β0|min > (2(3+ρ)/(1-ρ))‖X^T(XX^T)^(-1)ε‖1 ensures that the thresholded basis pursuit estimator β̂τ^TBP equals the true parameter vector β0 in support and sign. Is this condition sufficient for exact recovery?",
    "question_context": "Stable Nullspace Property and Thresholded Basis Pursuit in Model Selection\n\nThe paper discusses the stable nullspace property of a matrix X, which is crucial for the analysis of thresholded basis pursuit (TBP) in model selection. The property is defined as: for any β in the kernel of X, the L1 norm of β restricted to the support S0 is bounded by ρ times the L1 norm of β restricted to the complement of S0. The paper also provides conditions under which the TBP estimator equals the true parameter vector β0 with high probability, given a sufficiently large minimal signal strength and appropriate threshold τ."
  },
  {
    "qid": "statistic-posneg-1030-4-3",
    "gold_answer": "FALSE. The paper correctly establishes this convergence by showing that the terms involving the second derivatives of the model function and the penalty tend to zero, leaving the leading term to converge to 2Δ₁(θ₀).",
    "question": "Does the paper incorrectly claim that the second-order partial derivative matrix of the penalized objective function converges in probability to 2Δ₁(θ₀)?",
    "question_context": "Asymptotic Properties of Penalized Spline Estimators in Single-Index Models\n\nThe paper establishes two lemmas in preparation for the proof of the main theorems. Lemma A states that under Conditions 1 and 2, as n approaches infinity, the average of the weighted errors converges almost surely to a constant τ. Lemma B states that under Conditions 2 and 4, the normalized sum of the derivatives of the model function with respect to the parameters, weighted by the inverse of the working matrices, converges in distribution to a multivariate normal distribution with mean zero and variance-covariance matrix Δ₂(θ₀). The proofs involve detailed derivations and applications of the Strong Law of Large Numbers and the Lindeberg-Feller Central Limit Theorem."
  },
  {
    "qid": "statistic-posneg-783-0-1",
    "gold_answer": "TRUE. Explanation: The condition $Y_{(k+1-A_{0})}=Y_{(k-A_{0})}$ indicates that there is no gap in the order statistics around the hypothesized $\\theta$ value. This means there cannot be $A_{0}$ displacements around a $\\theta$ such that $y_{C(k+1)}>\\theta>y_{C(k)}$, which is a requirement under the null hypothesis. Therefore, the null hypothesis must be rejected with certainty when this condition holds.",
    "question": "Does the condition $Y_{(k+1-A_{0})}=Y_{(k-A_{0})}$ imply that the null hypothesis $H_{0}:\\delta=\\delta_{0}$ must be rejected with certainty?",
    "question_context": "Inference on Treatment Effects in Experiments with Discrete Pivot\n\nFix an integer $k$ , so that $y_{C(k)}$ is the unobserved $k/N$ quantile of responses to control. Let $\\theta$ be a value strictly between $y_{C(k)}$ and $y_{C(k+1)}$ , so that $y_{C(k+1)}>\\theta>y_{C(k)}$ , and say that subject $i$ has a ‘displacement’ around $\\theta$ if $y_{T i}>\\theta>y_{C i}$ . The observed data will reveal whether such a $\\theta$ exists. Let $Y_{i}=Z_{i}y_{T i}+(1-Z_{i})y_{C i}$ be the observed response from subject $i$ and let the order statistics of the $Y_{i}^{\\mathrm{{*}}}\\mathbf{s}$ be $Y_{(N)}\\geqslant Y_{(N-1)}\\geqslant\\ldots\\geqslant Y_{(1)}$ . Write $r_{C i}=1$ if $y_{C i}>\\theta$ and $r_{C i}=0$ otherwise, and write $r_{T i}=1$ if $y_{T i}>\\theta$ and $r_{T i}=0$ otherwise. Then there is a displacement if $\\delta_{i}=r_{T i}-r_{C i}=1$ and no displacement if $\\delta_{i}=r_{T i}-r_{C i}=0$ . The number of displacements attributable to treatment is $\\begin{array}{r}{A=\\sum_{i}Z_{i}\\delta_{i}}\\end{array}$ . Under the null hypothesis of no treatment effect, $H_{0}:y_{T i}=y_{C i}$ , for $i=1,\\ldots,N_{\\astrosun}$ , it follows that $\\delta_{i}=0$ for $i=1,\\ldots,N$ . <PARAGRAPH_SEPARATOR> A traditional choice of $k$ is $k/N=\\textstyle{\\frac{1}{2}}$ , somewhat analogous to the control median test of Gart (1963) and Gastwirth (1968). In some applications, it may be interesting to ask whether the treatment caused the response to be abnormally high, rather than higher than typical, in which case $k/N=0{\\cdot}95$ might be appropriate. <PARAGRAPH_SEPARATOR> The attributable effect, <PARAGRAPH_SEPARATOR> $$ A=\\sum_{i=1}^{N}Z_{i}(r_{T i}-r_{C i})=\\sum_{i=1}^{N}Z_{i}\\delta_{i}, $$ <PARAGRAPH_SEPARATOR> is the number of subjects who were caused to have responses above the $k/N$ quantile of potential control responses because they were exposed to the treatment, and would have had responses below that quantile had they been exposed to the control instead. For instance, if $A/m=50\\%$ for $k/N=95\\%$ , then half of the treated subjects were caused to have responses above the $95\\%$ quantile of responses that would have been seen had all subjects received the control. <PARAGRAPH_SEPARATOR> # 3·2. Inference about displacement eVects in experiments <PARAGRAPH_SEPARATOR> The structure here is similar to $\\S2$ , but $y_{C(k)}$ is not observed, so $r_{T i}$ and $r_{C i}$ cannot be calculated for any subject $i.$ This uncertainty about $y_{C(k)}$ is not a problem, however. <PARAGRAPH_SEPARATOR> P 1. If $\\begin{array}{r}{a=\\sum_{i}Z_{i}\\delta_{i}}\\end{array}$ , then $Y_{(k+1-a)}>\\theta>Y_{(k-a)}.$ . <PARAGRAPH_SEPARATOR> Proof. There are exactly $N-k$ subjects with $y_{C i}>\\theta$ , and since $y_{T i}\\geqslant y_{C i}$ it follows that these $N-k$ subjects all have $Y_{i}>\\theta$ . Since $\\begin{array}{r}{a=\\sum_{i}Z_{i}\\delta_{i}}\\end{array}$ there are exactly $a$ other subjects, not included among the $N-k$ subjects, with $Y_{i}=y_{T i}>\\theta>y_{C i}$ . For the remaining $k-a$ subjects, $\\theta>y_{T i}\\geqslant y_{C i}$ , so $\\theta>Y_{i}$ . Thus there are exactly $N-k+a$ subjects with $Y_{i}>\\theta$ and exactly $k-a$ subjects with $\\theta>Y_{i}$ . This means that <PARAGRAPH_SEPARATOR> $$ Y_{(N)}\\geqslant Y_{(N-1)}\\geqslant\\ldots\\geqslant Y_{(k+1-a)}>\\theta>Y_{(k-a)}\\geqslant\\ldots\\geqslant Y_{(1)}. $$ <PARAGRAPH_SEPARATOR> Proposition 1 is used in the following way. To test the hypothesis $H_{0}:\\delta=\\delta_{0}$ , calculate the observed order statistics, $Y_{(N)}\\geqslant\\ldots\\geqslant Y_{(1)}$ , and the hypothesised attributable effect, $\\begin{array}{r}{A_{0}=\\sum_{i}Z_{i}\\delta_{i}}\\end{array}$ . If $Y_{(k+1-A_{0})}=Y_{(k-A_{0})}$ , then there cannot be $A_{0}$ displacements around a $\\theta$ such that $y_{C(k+1)}>\\theta>y_{C(k)}$ , so reject $H_{0}$ with type one error rate of zero. Assume therefore from now on that $Y_{(k+1-A_{0})}>Y_{(k-A_{0})}$ , in which case, if the hypothesis is true, then $Y_{(k+1-A_{0})}>\\theta>Y_{(k-A_{0})}$ . Call the hypothesis compatible with the observed data if $\\delta_{0i}=0$ for all $i$ such that either $Z_{i}=0$ and $Y_{i}>Y_{(k-A_{0})}$ or $Z_{i}=1$ and $Y_{(k-A_{0})}\\geqslant Y_{i}$ ; otherwise, call the hypothesis incompatible. If the hypothesis is incompatible, reject it with certainty. If it is compatible, then a treated subject, $Z_{i}=1$ , has $r_{T i}=1$ if $Y_{i}>Y_{(k-A_{0})}$ and $r_{T i}=0$ if $Y_{(k-A_{0})}\\geqslant Y_{i}$ , while a control subject, $Z_{i}=0$ , has $r_{C i}=1$ if $Y_{i}>Y_{(k-A_{0})}$ and $r_{C i}=0$ if $Y_{(k-A_{0})}\\geqslant Y_{i}$ . Write $I(E)=1$ if event $E$ occurs, $I(E)=0$ otherwise, and compute Table 4. Under the null hypothesis, Table 4 equals Tables 2 and 3, and the same test may be performed. Note that Table 4 always has row totals $N-k$ and $k$ . <PARAGRAPH_SEPARATOR> Table 4. T he observed table for testing an attributable displacement eVect $H_{0}:\\delta=\\delta_{0}$ <PARAGRAPH_SEPARATOR> <html><body><table><tr><td>Response</td><td>Treated</td><td>Control</td></tr><tr><td>1</td><td>∑Z;I(Y; > Y(k-Ao)- Ao</td><td>∑(1 - Zi)I(Y> Yk-Ao))</td></tr><tr><td>0</td><td></td><td></td></tr><tr><td>Total</td><td>m</td><td>N-m</td></tr></table></body></html>"
  },
  {
    "qid": "statistic-posneg-1571-0-0",
    "gold_answer": "TRUE. Explanation: When δ(x) = 0, the joint probability p_{ij}(x) equals the product of the marginal probabilities p_{i.}(x) and p_{.j}(x) for all i, j = 1, 2. This independence condition directly implies that the log odds ratio, log(OR(x)), is zero, indicating no local association between the variables at that point x.",
    "question": "In the simulation study, when δ(x) = 0, does this imply that the log odds ratio, log(OR(x)), is always zero, indicating no local association between the variables?",
    "question_context": "Nonparametric Measure of Local Association in Simulation Study\n\nA simulation study was performed to compare the three estimators in Table 2 in terms of their bias and MSE. We also compare them to two model-based estimators: (1) an estimate of $\\log(O R(x))$ based on the logistic regression/GLM in (2), (2) an estimate based on fitting the GAM in (3). GAMs were fitted using the mgcv package (Wood, 2006) in R, using penalized regression splines with the penalty chosen via generalized cross-validation (GCV). This GAM model was fitted as a benchmark only, and we did not extensively seek to optimize its performance with respect to the choice of the spline basis or the way the tuning parameter was selected. Therefore, the results below pertaining to the GAM estimator should be contemplated with circumspection. Three simulation models were designed: $\\begin{array}{r l}&{X\\sim\\mathrm{UnitI}|-\\angle,~2|}\\ &{p_{1.}(x)=0.07e^{-x^{2}}+0.47\\quad\\quad p_{.1}(x)=0.1/(1+e^{x})+0.45}\\ &{p_{11}(x)=p_{1.}(x)p_{.1}(x)+\\delta(x)\\quad\\quad p_{12}(x)=p_{1.}(x)p_{.2}(x)-\\delta(x),}\\end{array}$ with Model A: δ(x) = 0.05e−0.3x Model B: $\\delta(x)=0.25-\\phi(x;-1,1.8^{2})$ Model C: $\\delta(x)=0.25\\left((1+e^{-6x})^{-1}-0.5\\right),$ , where $\\phi(x;\\mu,\\sigma^{2})$ denotes the density of a normal distribution with mean $\\mu$ and variance $\\sigma^{2}$ . The delta function controls the degree of local association, with $\\delta(x)=0$ implying $p_{i j}(x)=p_{i.}(x)p_{j}(x)\\forall i,j=1$ , 2 and $\\log(O R(x))=0.$ . Fig. 1 depicts the true log odds ratios curves for our three models. The design of our models, in particular our choices of $\\delta(x)$ , are such that the shapes of $\\log(O R(x))$ are representative of commonly encountered non-linear relative risk functions in epidemiology (Zhao et al., 1996), whilst encompassing a realistic range of values."
  },
  {
    "qid": "statistic-posneg-1437-2-0",
    "gold_answer": "TRUE. The context explicitly states that for the multivariate normal distribution, the van der Waerden score function is optimal. This is supported by the reference to literature (see [12,14]) and the mathematical formulation provided, where $\\left(\\chi^{2}\\right)^{-1}\\left(u\\right)$ is the inverse of the distribution function of a $\\chi_{p-1}^{2}$, which is characteristic of the van der Waerden score function's optimality under normality.",
    "question": "Is the statement 'The van der Waerden score function $\\phi(u)=\\sqrt{(\\chi^{2})^{-1}(u)}$ is optimal for the multivariate normal distribution' true based on the context provided?",
    "question_context": "Score Function Efficiency in Multivariate Testing\n\nIf $\\phi(u)\\equiv u$ , then, for elliptically symmetric distributions, $W_{n p-1}$ will have the same AREs as the test statistic obtained by applying Peters and Randles [30] signed-rank interdirection test to the complete block design setting (see, [20]). These test statistics generally perform well over a wide range of distributions, yet their performance deteriorates as the dimension increases, especially in the light-tailed cases and the normal case. The test will be denoted by PR. <PARAGRAPH_SEPARATOR> “Optimal” score functions have been discussed in the literature (see [12,14]). For example, if the underlying distribution is the multivariate normal then the van der Waerden score function, $\\phi(u)=\\sqrt{(\\chi^{2})^{-1}(u)}$ , is optimal where $\\left(\\chi^{2}\\right)^{-1}\\left(u\\right)$ is the inverse of the distribution function of a $\\chi_{p-1}^{2}$ . With such a score function and for elliptically symmetric distributions, $W_{n p-1}$ AREs with respect to Hotelling–Hsu $T^{2}$ will always be greater than 1 with equality when the distribution is the multivariate normal. Note that with this score function the test will be denoted by $W_{w n p-1}$ . <PARAGRAPH_SEPARATOR> While tests based on optimal scores have nice properties, they are seldom used in practice. The score function that we now recommend combines excellence in performance with simplicity in implementation. It is defined as <PARAGRAPH_SEPARATOR> $$ \\phi(u)=\\lambda u+(1-\\lambda)1, $$ <PARAGRAPH_SEPARATOR> where $\\lambda=\\{\\ln(p+e-2)\\}^{-1}$ depends on $p$ , the dimension of the data. This score function was introduced in [28]. With this linear score function, $W_{n p-1}$ has an excellent performance as good as and in some cases better than $W_{w n p-1}$ but yet it is still easy to compute as $S_{n}$ and PR. $W_{l n p-1}$ will be used to denote $W_{n p-1}$ with the score function defined in 13. <PARAGRAPH_SEPARATOR> We now show the effect of the score function on the efficiencies of the corresponding test by computing and comparing the AREs, with respect to Hotelling–Hsu’s $T^{2}$ , of the four test statistics, $W_{l n p-1}$ , $W_{w n p-1}$ , PR, and $S_{n}$ . Tables 1 and 2 display the AREs of these four statistics, for the case when the $\\mathbf{Y}$ ’s come from the power family of distributions and the family of $t\\cdot$ -distributions, respectively. These two families of distributions belong to the class of elliptically symmetric distributions. The power family is such that <PARAGRAPH_SEPARATOR> $$ h(t)=\\exp\\bigg\\{-\\bigg(\\frac{t}{c_{0}}\\bigg)^{\\nu}\\bigg\\},~\\nu>0, $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ c_{0}={\\frac{p r(p/2\\nu)}{r((p+2)/2\\nu)}},~\\mathrm{and}~K_{p}={\\frac{\\nu{r(p/2)}}{{r(p/2\\nu)}{[}\\pi c_{0}{]^{p/2}}}}. $$ <PARAGRAPH_SEPARATOR> It includes the multivariate normal density $(\\nu=1)$ ), as well as heavier-tailed distributions $0<\\nu<1$ ) and lighter-tailed distributions $y>1$ ). For the multivariate $t$ -distribution, $h(\\cdot)$ and $K_{p}$ in (9) are defined as <PARAGRAPH_SEPARATOR> $$ h(t)=\\left[1+\\frac{1}{\\nu}(t)\\right]^{-(p+\\nu)/2} $$ <PARAGRAPH_SEPARATOR> and <PARAGRAPH_SEPARATOR> $$ K_{p}=\\frac{{\\cal T}((p+\\nu)/2)}{{\\cal T}(\\nu/2)(\\pi\\nu)^{p/2}}. $$ <PARAGRAPH_SEPARATOR> It is clear from Tables 1 and 2 that $W_{l n p-1}$ has very competitive ARE values in a wide variety of distributions, ranging from very heavy-tailed distribution to very light-tailed ones. These efficiencies do not deteriorate as the dimension increases (as is the case for PR) or as the underlying distribution shifts from being heavy-tailed to being light-tailed (as is the case with $S_{n}$ ). For normal alternatives, $W_{l n p-1}$ not only performs virtually as well as Hotelling–Hsu’s $T^{2}$ and $W_{w n p-1}$ (recall this test is optimal for the normal distribution) but also has higher efficiencies than $S_{n}$ and ${\\mathrm{PR}}_{n}$ . For the heavy-tailed distributions of the power family, $W_{l n p-1}$ has higher efficiencies than $W_{w n p-1}$ and PR but slightly lower efficiencies than those of $S_{n}$ . For the light-tailed distributions of this family, the efficiencies of $W_{l n p-1}$ exceed those of $S_{n}$ but are slightly less than those of PR and $W_{w n p-1}$ . However, the slight advantages of some tests over $W_{l n p-1}$ whether in the light-tailed cases or the heavy-tailed cases disappear slowly as the dimension increases."
  },
  {
    "qid": "statistic-posneg-861-0-1",
    "gold_answer": "FALSE. Explanation: While the GPD has a threshold stability property (i.e., if exceedances over $u$ follow a GPD, then exceedances over any higher threshold $u' > u$ also follow a GPD with the same $\\xi$ but rescaled $\\psi$), this is an asymptotic result. In finite samples, $\\xi$ estimates can vary with $u$ due to bias-variance trade-offs: too low $u$ introduces non-extreme observations, distorting $\\xi$; too high $u$ leaves few exceedances, increasing estimation variance. The context emphasizes that threshold selection requires balancing these aspects, implying $u$'s choice can empirically impact $\\xi$ estimates despite theoretical stability.",
    "question": "In the Peaks-over-Threshold (PoT) model, the exceedance probability $\\phi = \\Pr[Y > u]$ must be estimated from the data. Is it correct to assume that the choice of threshold $u$ does not affect the estimated shape parameter $\\xi$ of the generalized Pareto distribution, due to the threshold stability property?",
    "question_context": "Extreme Value Theory: Generalized Pareto Distribution for Threshold Exceedances\n\nThe theory on which the vanilla versions of both block maxima and PoT models are based starts with the assumption of a sequence $Y_{1}$ , . . . , $Y_{m}$ of independent replicates of a random variable Y, with distribution $F(y)=\\operatorname*{Pr}[Y\\leq y]$ . Under some nonrestrictive conditions on $F$ , Von Mises (1936) and Gnedenko (1943) showed that in the limit as $m\\rightarrow\\infty$ , the distribution of the normalized block maximum $(M_{m}-b_{m})/a_{m}=(\\operatorname*{max}\\{Y_{1},...,Y_{m}\\}-b_{m})/a_{m}$ is one of the three types of max-stable distribution: Weibull, Fréchet, or Gumbel. While both the rate of convergence to the limiting dis­ tribution and the sequences $\\{a_{m}>0\\}$ and $\\{b_{m}\\}$ are determined by the underlying distribution $F$ which, in a modelling context is unknown, this result is nevertheless used to justify the use of the generalized extreme-value distribution (GEV) to model the maxima of large but finite blocks. The GEV is a class of distributions combining the three types of max-stable distribution. The resulting cumulative distribution function is, $$G(z)=\\exp\\left\\{-\\biggl[1+\\frac{\\xi}{\\sigma}(z-\\mu)\\biggr]^{-1/\\xi}\\right\\},~-\\infty<z<y^{+}$$ where $y^{+}<\\infty$ in the case $\\xi<0$ . This distribution has location $\\mu$ , scale $\\sigma>0$ and shape $\\xi$ parame­ ters, with the Weibull, Fréchet, and Gumbel distributions corresponding to $\\xi<0$ , $\\xi>0$ and $\\xi\\rightarrow$ 0, respectively. As a model the GEV has the advantage of increased flexibility over choosing one of the three types a priori. The vanilla PoT model is motivated by a result of Pickands (1975) who showed that, under some nonrestrictive regularity conditions, as $u\\to y^{+}$ for $y>u$ , $$F(y)=1-\\operatorname*{Pr}[Y>u]\\operatorname*{Pr}[Y>y\\mid Y>u]\\approx1-\\phi{\\bar{G}}(y)=1-\\phi{\\left[1+{\\frac{\\xi}{\\psi}}(y-u)\\right]}_{+}^{-1/\\xi}$$ where $a_{+}=\\operatorname*{max}{\\{a,0\\}},0\\leq\\phi\\leq1,\\psi$ $\\psi>0$ and $\\bar{G}(y)$ is the survival function of the generalized Pareto distribution. The parameters $\\phi,\\psi$ , and $\\xi$ are known as the rate, scale and shape, respectively; the rate $\\phi=\\operatorname*{Pr}[Y>u]$ is sometimes called the exceedance probability. The value of the shape param­ eter is determined by the rate at which $F(y)\\rightarrow1$ as $y\\to y^{+}$ ; exponential decay corresponds to $\\xi=0$ , and heavy (light) decay to $\\xi>0(\\xi<0)$ . For $\\xi\\ge0$ , there is no finite end-point, while $y^{+}=u-$ $\\sigma/\\xi<\\infty$ if $\\xi<0$ . By assuming that this limit result is a suitable approximation to the behaviour of the exceedan­ ces of a finite, but high, level $\\boldsymbol{\\mathscr{u}}$ , it can be used to motivate a model in which the magnitudes of the threshold exceedances $\\{y_{i}-u:y_{i}>u,i=1,...,n\\}$ are a random sample from the generalized Pareto distribution and the exceedance times a random sample from a homogeneous Poisson pro­ cess. While not essential an assumption of independence is often made about the exceedances. If there is evidence of serial dependence, a routine and theoretically justified approach is to identify all clusters of extremes and model only the frequency and magnitude of the cluster maxima (Davis et al., 2013; Drees, 2008; Laurini $\\&$ Tawn, 2003; Smith & Weissman, 1994). In either case, the parameters $(\\phi,\\psi,\\xi)$ are estimated with standard statistical inference procedures, e.g maximum likelihood or Bayesian inference. A crucial modelling consideration is the choice of threshold $\\boldsymbol{u}$ , which requires a compromise be­ tween retaining a sufficiently large number of exceedances such that estimation of the model pa­ rameters is possible, while still ensuring that the assumption of the limiting approximation is plausible. Various tools are available to aid threshold selection. Most of these use two key thresh­ old stability properties of the generalized Pareto distribution to identify the minimum plausible threshold above which the distribution is a suitable description of the data."
  },
  {
    "qid": "statistic-posneg-1207-0-1",
    "gold_answer": "TRUE. Explanation: The paper states that if the differences between $P$ and $Q$ are mostly found in the dependency structure or copula, kernel-based tests like MMD may be preferable to the RF-based test. While the RF-based test still has some power in such cases, it performs less effectively than kernel-based tests. This is demonstrated in Appendix B of the paper.",
    "question": "Does the paper claim that the Random Forest classifier is ineffective for detecting differences in the dependency structure or copula between distributions $P$ and $Q$?",
    "question_context": "Two-Sample Testing via Classification Methods\n\nTwo-sample testing via classification methods is an old idea tracing back to the work of Friedman (2004). Generally speaking, one adapts the output of a classifier to construct a two-sample test. Let $\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{n_{0}}$ and $\\mathbf{Y}_{1},\\ldots,\\mathbf{Y}_{n_{1}}$ be a collection of $\\mathbb{R}^{d}$ -valued random vectors, such that $\\mathbf{x}_{i}\\overset{i i d}{\\sim}P$ and $\\mathbf{Y}_{i}\\overset{i i d}{\\sim}Q$ , where $P$ and $Q$ are some Borel probability measure on $\\mathbb{R}^{d}$ . The goal is to test $H_{0}:P=Q,~H_{A}:P\\neq Q.$ Given these iid samples of vectors, we define labels $\\ell_{i}=1$ for each $\\mathbf{X}_{i}$ and $\\ell_{i}=0$ for each $\\mathbf{Y}_{i}$ to obtain the data $\\left(\\mathbf{Z}_{j},\\boldsymbol{\\ell}_{j}\\right)$ , $j=1,\\dots,N$ , for $N=n_{0}+n_{1}$ , and $\\mathbf{Z}_{j}=\\mathbf{X}_{i}$ or $\\mathbf{Z}_{j}=\\mathbf{Y}_{i}$ . On this data, we train a classifier $\\hat{g}:\\mathbb{R}^{d}\\rightarrow\\{0,1\\}$ . If $\\hat{g}$ is able to “accurately” predict $\\ell$ on some test samples, it is taken as evidence against $H_{0}$ . In this work, we assume the data is generated from a mixture distribution ${\\bf Z}_{j}\\stackrel{i i d}{\\sim}(1-\\pi){\\cal P}+\\pi{\\cal Q},$ such that $n_{1}\\sim\\mathrm{Bin}(\\pi,N)$ , where Bin denotes the Binomial distribution. While our exposition will be valid for general classifiers, we specifically target the use of the Random Forest (RF) classifier in this work. Random Forest is a powerful and flexible method developed by Breiman (2001), known to have a remarkably stable performance in applications (see e.g. the extensive work of Fernández-Delgado et al., 2014)."
  },
  {
    "qid": "statistic-posneg-1774-0-1",
    "gold_answer": "TRUE. The model explicitly states that for genes not expressed under either condition (k=0), the expected differential expression is E(D_i^(0)) = μ_Z^u - μ_Y^u = 0. This implies that the mean background noise (μ_Z^u and μ_Y^u) is assumed to be the same in both conditions for unexpressed genes, resulting in no expected differential expression.",
    "question": "Does the model assume that the mean expression level of unexpressed genes (μ_Z^u) is the same across both conditions, leading to an expected differential expression (D_i^(0)) of zero for genes not expressed in either condition?",
    "question_context": "Mixture Model for Differential Gene Expression Analysis\n\nThe paper presents a mixture model approach to analyze differential gene expression across two conditions. Genes are categorized into four groups: not expressed in either condition, expressed only in the first condition, expressed only in the second condition, and expressed in both conditions. The models for expressed and unexpressed genes are defined with specific variance structures, and differential expression is estimated using the difference between means under the two conditions. The mixture model uses EM algorithm for parameter estimation, with variances naturally ordered to help classify genes into their respective categories."
  },
  {
    "qid": "statistic-posneg-1046-1-0",
    "gold_answer": "FALSE. The context explicitly states that sufficient bootstrapping overestimates the standard deviation (RB(ŝₛb) = 2.67%), while conventional bootstrapping underestimates it (RB(s_boot) = -10.962%). This is derived from the expected value formulas E(sₛb) and E(s_boot), and their comparison to the true σ_y.",
    "question": "Is the statement 'Sufficient bootstrapping always underestimates the standard deviation, similar to conventional bootstrapping' true based on the provided context and formulas?",
    "question_context": "Sufficient Bootstrapping vs. Conventional Bootstrapping: Bias and Efficiency Comparison\n\nThe paper compares the performance of sufficient bootstrapping and conventional bootstrapping in estimating sample statistics such as mean, standard deviation, and coefficient of variation. It demonstrates that sufficient bootstrapping can yield less biased estimates and higher efficiency compared to conventional bootstrapping. Key formulas include the expected values and variances of estimators under both methods, as well as the relative bias and relative efficiency metrics."
  },
  {
    "qid": "statistic-posneg-1500-0-1",
    "gold_answer": "FALSE. While the formula shows that the leading term is $\\theta$, the term $\\frac{S_{a w}}{S_{a a}}N^{-1/2}$ introduces a bias of order $O(N^{-1/2})$. The context also mentions that $E(\\tilde{\\theta}) = \\theta + O(N^{-1})$, indicating that the estimator is asymptotically unbiased but has a small-order bias for finite $N$.",
    "question": "Does the formula for $\\widetilde{\\theta} = \\theta + \\frac{S_{a w}}{S_{a a}}N^{-1/2} + O_{p}(N^{-1})$ imply that $\\widetilde{\\theta}$ is an unbiased estimator of $\\theta$?",
    "question_context": "Meta-Analysis Approximations and Radial Plot Statistics\n\nThe paper discusses the asymptotic properties of estimates in meta-analysis, focusing on the joint normality of $(\\hat{\\theta}, \\hat{\\sigma}^2)$ with mean $(\\theta, \\sigma^2)$. It assumes biases of order $O(n^{-1})$, and provides various approximations and expansions for statistics like $\\widetilde{\\theta}$, $Z_1$, and $Q$. The context includes detailed derivations and expectations of these statistics, as well as special cases involving binomial distributions and radial plot statistics."
  },
  {
    "qid": "statistic-posneg-1027-0-2",
    "gold_answer": "FALSE. The confidence interval $\\hat{\\rho} \\pm \\hat{V}^{\\frac{1}{2}} t_{\\gamma/2,2N-4}$ is not exact for small sample sizes. It is derived using Taylor series approximations and is valid only when $\\hat{V} \\ll \\rho^{2}$ (i.e., when the variance is small relative to the true radius). For small samples, the exact distribution of $\\hat{\\rho}$ is more complex and does not yield a simple closed-form confidence interval.",
    "question": "Is the confidence interval for $\\rho$ given by $\\hat{\\rho} \\pm \\hat{V}^{\\frac{1}{2}} t_{\\gamma/2,2N-4}$ exact for small sample sizes?",
    "question_context": "Parameter Estimation for Circular Data with Known Angular Differences\n\nThe assumption that the angular differences $\\theta_{i}=\\tau_{i}-\\tau_{i-1}$ are known is equivalent to the representation $\\tau_{t}=\\theta_{0}+\\theta_{t},i=1,...,N,$ where $\\theta_{i}, i = 1, .. .,N,$ are known (with typically $\\theta_{1} = 0$), and $\\theta_{0}$ is unknown. Substituting into the model, one obtains $X_{i}=\\xi+\\alpha\\cos\\theta_{i}-\\beta\\sin\\theta_{i}+\\epsilon_{i},\\quad Y_{i}=\\eta+\\alpha\\sin\\theta_{i}+\\beta\\cos\\theta_{i}+\\delta_{i},i=1,...,N,$ where $\\alpha=\\rho\\cos\\theta_{0},\\quad\\beta=\\rho\\sin\\theta_{0}.$ The model is linear in the four parameters $\\xi,\\eta,\\alpha,\\beta$ and so one can obtain explicit maximum likelihood estimates for all parameters and an exact covariance matrix. The covariance matrix for $\\hat{\\xi},\\hat{\\eta},\\hat{\\alpha},\\hat{\\beta}$ is given by $V=\\sigma^{2}/N D$, where $D=1-\\bar{c}^{2}-\\overline{s}^{2}$. The normality of $(\\hat{\\xi},\\hat{\\eta})$ implies that the usual exact $100(1-\\gamma)\\%$ confidence ellipse for $(\\xi,\\eta)$ is always a circle, specifically $(\\hat{\\xi}-\\xi)^{2}+(\\hat{\\eta}-\\eta)^{2}=2\\hat{V}F_{\\gamma,2,2N-4},$ where $\\hat{V}=\\hat{\\sigma}^{2}/N D$ and $F_{\\gamma,2,2N-4}$ is the upper $100\\%$ point of the $F$ distribution with degrees of freedom 2 and $2N - 4$."
  },
  {
    "qid": "statistic-posneg-1666-0-0",
    "gold_answer": "TRUE. The context explicitly states that the approximation is made by equating the corresponding first two moments of each of these distributions and then solving for $\\pmb{K}$, $\\pmb{A}$, and $\\pmb{B}$ in terms of $\\pmb{a}$, $b$, $\\pmb{\\mathscr{r}_{1}}$, and ${\\pmb r}_{\\pmb\\2}$. This method ensures that the approximated distribution matches the original distribution in terms of its first two moments, which is a common technique in statistical approximations.",
    "question": "Is it true that the approximation $f(W;a,b,\\pmb{r}_{1},\\pmb{r}_{2})$ by $f(K W;A,B,0,0)$ is derived by equating the first two moments of the distributions?",
    "question_context": "Approximation of Dependent Chi-Square Variates Distribution\n\nWe approximate $f(W;a,b,\\pmb{r}_{1},\\pmb{r}_{2})$ (cf. $\\S\\delta)$ by $f(K W;A,B,0,0)$ by equating_the corresponding first two moments of each of these distributions, and then solving for $\\pmb{K}$ $\\pmb{A}$ and $\\pmb{B}$ in terms of $\\pmb{a}$ b, $\\pmb{\\mathscr{r}_{1}}$ and ${\\pmb r}_{\\pmb\\2}$ . Greater accuracy could be obtained by equating more moments, but the results are too complicated to be of practical use.<PARAGRAPH_SEPARATOR>From $\\S3$ we know that $\\begin{array}{r}{{\\cal W}={\\pmb v}_{\\pmb{\\Omega}}/{\\pmb v}_{\\pmb{1}},}\\end{array}$ with the density function of $v_{i}$ given by (9) fo1 $\\cdot{\\mathfrak{i}}=1,2$ Now, an alternative method of obtaining the approximation to $f(W;a,b,\\pmb{r}_{1},\\pmb{r}_{2})$ is frst to approximate $f(v_{1};a,r_{1})$ by $f(k_{1}v_{1};A,0)$ and $f(v_{\\mathfrak{L}};b,\\pmb{\\tau}_{\\mathfrak{L}})$ by $f(k_{\\mathfrak{L}}v_{\\mathfrak{L}};B,0)$ ,where $f(v_{1};a,\\pmb{\\tau}_{1})$ denotes the density function of ${\\pmb v}_{1}$ with $2a=m_{1}-1\\mathbf{D}.\\mathbf{I}$ .&nd $f(v_{\\mathfrak{L}};b,\\pmb{\\tau}_{\\mathfrak{L}})$ the density function of ${\\pmb v}_{\\pmb{\\8}}$ with $2b=m_{8}-1\\mathbf{D}.\\mathbf{F}.$<PARAGRAPH_SEPARATOR>From the results of $\\S4$ it is easy to show that for $f({\\pmb v};{\\pmb a},{\\pmb r})$ , which is the density function of the ratio of two dependent chi-square variates, each with ${\\bf2}a{\\bf D.F.}$<PARAGRAPH_SEPARATOR>$$E(v)=(a-r^{2})/(a-1),$$<PARAGRAPH_SEPARATOR>and<PARAGRAPH_SEPARATOR>$$E(v^{8})={\\frac{(a+1)\\left(a-4r^{2}\\right)+6r^{4}}{(a-1)\\left(a-2\\right)}}\\quad(a>2).$$<PARAGRAPH_SEPARATOR>The corresponding moments of $f(k v;A,0)$ follow immediately. Hence, after equating corresponding moments and solving for $\\boldsymbol{k}$ and $\\pmb{A}$ in terms of $\\pmb{a}$ and $\\pmb{\\mathscr{r}}$ ,we have<PARAGRAPH_SEPARATOR>where<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}{A=\\{R+(R^{\\mathfrak{L}}-R+1)^{\\sharp}\\}/(R-1)}&{(R>1),}\\ {R=\\frac{(a-1)\\left\\{(a+1)(a-4r^{\\mathfrak{L}})+6r^{\\mathfrak{A}}\\right\\}}{(a-2)(a-r^{2})^{2}}}&{(a>2),}\\ {k=\\frac{(a-r^{2})(A-1)}{(a-1)A}\\leqslant1.}\\end{array}$$<PARAGRAPH_SEPARATOR>and<PARAGRAPH_SEPARATOR>Note that $A\\bumpeq2R/(R-1)$ and $k\\bumpeq1$ for large $\\pmb{a}$<PARAGRAPH_SEPARATOR>Schoeman, in his unpublished thesis, showed by comparing approximate and exact critical values of $\\pmb{v}$ , that the approximation to the distribution of $\\pmb{v}$ is quite satisfactory for degrees of freedom greater than 6.<PARAGRAPH_SEPARATOR>To approximate $f(W;a,b,r_{1},r_{2})\\mathrm{by}f(K W;A,B,0,0),$ it follows that<PARAGRAPH_SEPARATOR>$$A=\\{R_{1}+(R_{1}^{2}-R_{1}+1)^{\\frac{1}{2}}\\}/(R_{1}-1)\\quad\\mathrm{and}\\quad B=\\{R_{2}+(R_{2}^{2}-R_{2}+1)^{\\frac{1}{2}}\\}/(R_{2}-1),$$<PARAGRAPH_SEPARATOR>where<PARAGRAPH_SEPARATOR>$$R_{1}=\\frac{\\left(a-1\\right)\\left\\{\\left(a+1\\right)\\left(a-4r_{1}^{2}\\right)+6r_{1}^{4}\\right\\}}{\\left(a-2\\right)\\left(a-r_{1}^{2}\\right)^{2}},\\quad R_{2}=\\frac{\\left(b-1\\right)\\left\\{\\left(b+1\\right)\\left(b-4r_{2}^{8}\\right)+6r_{2}^{4}\\right\\}}{\\left(b-2\\right)\\left(b-r_{2}^{2}\\right)^{2}},$$<PARAGRAPH_SEPARATOR>$$\\begin{array}{r}{K=k_{1}k_{8},}\\end{array}$$<PARAGRAPH_SEPARATOR>with<PARAGRAPH_SEPARATOR>$$k_{1}=\\frac{\\left(a-r_{1}^{2}\\right)\\left(A-1\\right)}{\\left(a-1\\right)A}\\quad\\mathrm{and}\\quad k_{2}=\\frac{\\left(b-r_{2}^{2}\\right)\\left(B-1\\right)}{\\left(b-1\\right)B}\\quad(a,b>2).$$<PARAGRAPH_SEPARATOR>The degrees of freedom, 2A and $\\mathbf{2}B$ will be fractional.<PARAGRAPH_SEPARATOR>To find critical values of $\\pmb{W},$ we first interpolate in the appropriate tables of critical values of $\\boldsymbol{\\mathbf{\\mathit{W}}}$ for the case of independent experiments published by Schumann $\\pmb{\\&}$ Bradley (1959), and then divide the interpolate by $\\kappa$ . This method can also be used in the case of type II problems by putting ${\\pmb r}_{\\mathfrak{z}}=0$ , $B=b$ with $2b=m_{2}$ (cf. Table 1) and $k_{\\mathfrak{L}}=1$<PARAGRAPH_SEPARATOR>Table 9. Comparison of upper $\\pmb{\\alpha}$ critical values $W_{0m}$ ('independent W' approximation) and the exact values, $W_{0}$"
  },
  {
    "qid": "statistic-posneg-1928-0-2",
    "gold_answer": "TRUE. According to the context, the all-subset selection procedure, which evaluates all $2^{8}=256$ candidate models using the Schwarz-type Information Criterion (SIC), identified only $\\beta_{0}(t,\\tau)$ and $\\beta_{3}(t,\\tau)$ as nonzero at the three quartiles ($\\tau=0.25, 0.5, 0.75$).",
    "question": "Is it correct that the all-subset selection procedure identified only the baseline coefficient $\\beta_{0}(t,\\tau)$ and the PreCD4 coefficient $\\beta_{3}(t,\\tau)$ as nonzero at all three quantile levels?",
    "question_context": "Quantile Varying Coefficient Model for Longitudinal HIV Study\n\nIn this section, we analyze a subset of the data from the Multi-Center AIDS Cohort study. The data set contains the human immunodeficiency virus (HIV) status of 283 homosexual men who were infected with HIV during the follow-up period between 1984 and 1991. Each patient had a different number of repeated measurements and different measurement times. Details of the study design can be found in Kaslow et al. (1987). Several researchers have studied the same data set by fitting varying coefficient models (Huang et al., 2002; Fan and Zhang, 2000) or a semi-parametric model with quadratic and interaction effects (Fan and Li, 2004). The previous analysis aimed to describe the trend of the mean CD4 percentage depletion over time and to evaluate the effects of cigarette smoking, pre-HIV infection CD4 percentage, and age at infection on the mean CD4 percentage after the infection. <PARAGRAPH_SEPARATOR> Our analysis focuses on evaluating the time dependent effects of smoking status, age, PreCD4, and the quadratic and the interaction of the covariates on different quantiles of the CD4 percentage population. We consider the following quantile varying coefficient model, <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{y_{i j}=\\beta_{0}(t_{i j},\\tau)+\\beta_{1}(t_{i j},\\tau)x_{i1}+\\beta_{2}(t_{i j},\\tau)x_{i2}+\\beta_{3}(t_{i j},\\tau)x_{i3}+\\beta_{4}(t_{i j},\\tau)x_{i2}^{2}+\\beta_{5}(t_{i j},\\tau)x_{i3}^{2}}\\ &{\\qquad+\\beta_{6}(t_{i j},\\tau)x_{i1}x_{i2}+\\beta_{7}(t_{i j},\\tau)x_{i1}x_{i3}+\\beta_{8}(t_{i j},\\tau)x_{i2}x_{i3}+\\epsilon_{i j}(\\tau),}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $y_{i j}$ is the ith individual’s CD4 percentage at time $t_{i j}$ (in years), $x_{i1}$ is the smoking status, taking a value of 1 or 0 for a smoker or a nonsmoker, $x_{i2}$ is the standardized variable for age at HIV infection, and $x_{i3}$ is the standardized PreCD4 percentage of the ith individual. We focus on three quantile levels, $\\tau=0.25$ , 0.5 and 0.75. The baseline function $\\beta_{0}(t,\\tau)$ represents the $\\tau$ th quantile of CD4 percentage $t$ years after the infection, for a nonsmoker with average PreCD4 percentage and average age at HIV infection. Using the SIC criterion in Section 2.2, the tuning parameter $k_{n}$ is selected as 2 for all the three quantiles, and $\\lambda_{n}$ is selected as 4.8, 4.4 and 3.9, respectively. <PARAGRAPH_SEPARATOR> The baseline coefficient $\\beta_{0}(t,\\tau)$ and the PreCD4 coefficient $\\beta_{3}(t,\\tau)$ are identified as nonzero at all the three quantiles. At median, Smoking and Age tend to have a negative interaction, that is, the elder smokers tend to have lower median CD4 counts, and this agrees with the mean regression findings in Fan and Li (2004). In addition, our method suggests a positive interaction between Smoking and PreCD4 in the upper quartile for larger t: smokers with higher PreCD4 tend to have higher CD4 counts after a period of follow-up. An examination of the box-plots of PreCD4 for the smokers and for the nonsmokers shows that smokers in general have higher PreCD4. Note that the smoking status indicates whether a person ever or never smoked after HIV infection. One possible explanation for the positive interaction between Smoking and PreCD4 is that patients with lower PreCD4 may choose to quit smoking due to medical concerns. Fig. 1 shows the estimated coefficient functions of $\\beta_{0}(t,\\tau)$ , $\\beta_{3}(t,\\tau)$ , $\\beta_{6}(t,\\tau)$ and $\\beta_{7}(t,\\tau)$ , respectively. We also include the $90\\%$ point-wise confidence bands for the median estimates of the coefficient functions, and also for $\\beta_{7}(t,\\tau)$ at $\\tau=0.75$ , where the standard errors are estimated by applying the bootstrap method with 200 bootstrap samples to the selected model (Huang et al., 2002). <PARAGRAPH_SEPARATOR> This data set involves only $p=8$ covariates. For comparison, we carry out an all-subset selection procedure to select among $2^{8}=256$ candidate models. Specifically, we fit each candidate model, and calculate the Schwarz-type Information Criterion (SIC), and then select the model with the smallest SIC. According to Wang and Qu (2009), this SIC-based all-subset selection method provides a consistent variable selection approach. With the all-subset selection approach, only the baseline coefficient $\\beta_{0}(t,\\tau)$ and the PreCD4 coefficient $\\beta_{3}(t,\\tau)$ are identified as nonzero at the three quartiles. To better understand the performance of the proposed $\\mathrm{GAL}_{1}$ method and the all-subset selection procedure, in the following, we compare the two methods by assessing their selection stability and the prediction accuracy."
  },
  {
    "qid": "statistic-posneg-642-0-2",
    "gold_answer": "TRUE. The paper explains that the term $O\\{(n h)^{-1}\\}$ added to the bias formula is asymptotically negligible relative to the standard deviation of $f$. This is because the standard deviation of $f$ is of order $O\\{(n h)^{-1/2}\\}$, making the added term $O\\{(n h)^{-1}\\}$ negligible in comparison. Thus, it can be neglected in calculations of properties like mean-squared error.",
    "question": "Is the term $O\\{(n h)^{-1}\\}$ added to the bias formula asymptotically negligible relative to the standard deviation of $f$?",
    "question_context": "High Order Data Sharpening for Density Estimation\n\nThe paper discusses high order data sharpening techniques for density estimation, focusing on reducing bias and improving mean-squared error performance. The key formulas include expansions for transformations and estimators, such as $H_{2k+2}=\\bar{F}+h^{2}\\alpha_{1}+\\ldots+h^{2k}\\alpha_{k}+O(h^{2k+2})$, and detailed expressions for higher-order transformations like $\\tilde{\\gamma}_{8}$. The paper also compares these methods to conventional kernel estimators and discusses computational efficiency improvements through binning."
  },
  {
    "qid": "statistic-posneg-889-0-2",
    "gold_answer": "FALSE. The inequality indicates a condition under which the success lead rule is preferable, but it is not universally applicable. The paper notes that the success lead rule is preferable unless the true ratio of probabilities of success is appreciably closer to 1 than θ*. Additionally, the condition is derived for θ* close to 1 and may not hold for larger values of θ*.",
    "question": "Is the success lead stopping rule always preferable when $\\frac{\\theta-1}{\\theta}>\\left(\\frac{\\theta^{*}-1}{\\theta^{*}}\\right)\\frac{\\log\\left\\{P^{*}/(1-P^{*})\\right\\}}{2\\lambda^{2}(P^{*})}$?",
    "question_context": "Comparison of Play-the-Winner Sampling Procedures in Binomial Populations\n\nIn §§3 and 4, we have shown that play-the-winner sampling is uniformly preferable to vector-at-a-time sampling when the probability requirement for correct selection is expressed in terms of the ratio of probabilities of success. We now compare the stopping rules for play-the-winner sampling based on success lead and inverse sampling and in particular we investigate the conditions under which each rule gives a smaller expected number of trials on the poorer treatment. Our analytical results will be appropriate to the case when θ* is close to 1. Using (10) and (24), where for θ* close to 1 and hence γ large we replace the Φ functions by 1, we have\n\n$$\n\\begin{array}{r}{E(N_{P W,I}^{(1)})-E(N_{P W,L}^{(1)})\\sim\\frac{\\theta(1-p_{2})}{p_{2}(\\theta-1)(\\theta-p_{2})}\\{(\\theta-1)r-\\theta s+(s-\\frac{1}{2})p_{2}\\}.}\\end{array}\n$$\n\nIt follows that the success lead stopping rule will be preferable over the complete parameter space if $r(\\theta-1)>\\theta s$ From §2 , the selection constant δ for the success lead stopping rule is the smallest integer for which $(\\theta^{*})^{s}\\geqslant P^{*}/(1-P^{*})$ .If we replace the inequality by an equality and ignore terms that are $O(\\theta^{*}-1)^{2}$ ,wehave\n\n$$\ns\\sim\\frac1{(\\theta^{*}-1)}\\log\\left(\\frac{P^{*}}{1-P^{*}}\\right).\n$$\n\nFor θ* close to 1, the selection constant γ for the inverse sampling rule is given by (17), so the success lead rule is uniformly preferable if\n\n$$\n\\frac{\\theta-1}{\\theta}>\\left(\\frac{\\theta^{*}-1}{\\theta^{*}}\\right)\\frac{\\log\\left\\{P^{*}/(1-P^{*})\\right\\}}{2\\lambda^{2}(P^{*})}.\n$$\n\nThe values of $\\{2\\lambda^{2}(P^{*})\\}^{-1}\\log\\left\\{P^{*}/(1-P^{*})\\right\\}$ are 0.67, 0.54 and 0.42,respectively,for $P^{*}=0.90_{\\mathrm{:}}$ 0.95 and 0.99. Thus the success lead stopping rule will be preferable unless the true ratio of probabilities of success is appreciably closer to 1 than the specified value of θ*"
  },
  {
    "qid": "statistic-posneg-464-0-0",
    "gold_answer": "FALSE. The context states that in the Addy et al. model, each individual independently avoids infection from the population at large with probability $\\pi$, but it does not imply that $\\pi$ is independent of $\\lambda_L$. In fact, the two-level mixing model approximation suggests that $\\pi$ is related to the overall epidemic dynamics, which include both within-group ($\\lambda_L$) and between-group ($\\lambda_G$) infection rates. Specifically, $\\pi=~\\exp(-\\lambda_{G}z\\mathrm{E}[I])$, where $z$ depends on both $\\lambda_L$ and $\\lambda_G$.",
    "question": "In the Addy et al. model, the probability that an individual avoids outside infection ($\\pi$) is independent of the infection rate within the group ($\\lambda_L$). Is this statement true based on the context provided?",
    "question_context": "Epidemic Final Size and Severity in Two-Level Mixing Model\n\nIn the model described above, the fates of different groups are not independent of one another, essentially because between-group contacts are modelled explicitly. A mathematically simpler model, considered by Addy et al. (1991), arises when explicit global infections are replaced by a single fixed probability that an individual avoids global infection. Specifically, suppose each individual independently avoids infection from the population at large with probability $\\pi.$ , so that the resulting within-group epidemic in a group of $n$ susceptibles is initiated by a Bino$\\mathrm{mial}(n,1{-}\\pi)$ number of infectives. The final outcomes of different groups in this model are independent of one another. Note that this model is non-temporal insofar as the times of infections from outside are not explicitly modelled. However, for the analysis of final outcome it is sufficient simply to know the probabilities of individuals avoiding outside infection, and the probabilities of individuals ever infecting one another within the group (see e.g. Addy et al., 1991). <PARAGRAPH_SEPARATOR> As described in Ball et al. (1997), as the number of groups in the two-level mixing model increases to infinity, then during a major outbreak a given individual avoids outside infection with probability $\\pi=~\\exp(-\\lambda_{G}z\\mathrm{E}[I])=~\\exp\\{-(\\lambda_{G}/N)z N\\mathrm{E}[I]\\}$ , where $z=z(\\lambda_{L},\\lambda_{G})$ is the (nonrandom) asymptotic proportion of susceptibles who ultimately become infected. Moreover, $z N\\mathrm{E}[I]$ is the asymptotic final severity of the epidemic, i.e. the total number of person–time units of infection. It follows that, at least for a large population, analysis of the two-level mixing model can proceed by approximating it by the Addy et al. model. In the sequel we make use of this idea, but consider a refinement by replacing $z N\\mathrm{E}[I]$ by $T_{\\infty}$ , where $T_{\\infty}$ is the actual (random) final severity of the epidemic. <PARAGRAPH_SEPARATOR> We now recall results for the final outcome of a single group within the Addy et al. model that will be needed in the sequel. Suppose the group initially comprises $n$ susceptibles and $a$ infectives, let $\\pi$ denote the probability that an individual avoids outside infection, and $\\lambda_{L}$ denote the infection rate within the group. Let $T^{(n)}$ denote the final size, i.e. the number of susceptibles who ultimately become infected within the group, and $T_{\\infty}^{(n)}$ denote the final severity, i.e. the sum of the infectious periods of all those individuals who are ever infectious within the group. We require the following definition (see Lefevre & Picard, 1990). Let $U=$ $u_{0},u_{1},\\ldots$ be a given sequence of real numbers. Then the Gontcharoff polynomials attached to $U$ , denoted $G_{0}(x|U),G_{1}(x|U),...,$ , are defined recursively by the triangular system of equations <PARAGRAPH_SEPARATOR> $$ \\sum_{j=0}^{i}{\\frac{u_{j}^{i-j}}{(i-j)!}}G_{j}(x|U)={\\frac{x^{i}}{i!}},\\qquadi=0,1,\\dots. $$ <PARAGRAPH_SEPARATOR> For $\\theta\\geq0$ let $\\phi(\\theta)=\\operatorname{E}[$ exp $\\left(-I\\theta\\right)]$ , and define $\\phi_{n,a}(s,\\theta)=\\mathbb{E}[s^{n-T^{(n)}}\\exp(-\\theta T_{\\infty}^{(n)})]$ . The following results are taken from Ball et al. (1997). First, <PARAGRAPH_SEPARATOR> $$ \\phi_{n,a}(s,\\theta)=\\sum_{i=0}^{n}\\frac{n!}{(n-i)!}\\phi(\\theta+\\lambda_{L}i)^{n+a-i}\\pi^{i}G_{i}(s\\mid U), $$ <PARAGRAPH_SEPARATOR> where the sequence $U$ is defined by $u_{i}=\\phi(\\theta+\\lambda_{L}i),i=0,1,\\ldots.$ <PARAGRAPH_SEPARATOR> If $\\mu_{n,a}(\\pi)=\\operatorname{E}{[T^{(n)}]}$ is the mean final size of the epidemic, then differentiation of (1) yields <PARAGRAPH_SEPARATOR> $$ \\mu_{n,a}(\\pi)=n-\\sum_{i=1}^{n}\\frac{n!}{(n-i)!}q_{i}^{n+a-i}\\pi^{i}\\alpha_{i}, $$ <PARAGRAPH_SEPARATOR> where $q_{i}=\\phi(\\lambda_{L}i)$ , $\\alpha_{i}=G_{i-1}(1\\mid V)$ , and $V$ is given by $\\nu_{i}=\\phi(\\lambda_{L}(i+1))=q_{i+1},i$ $i=0,1,\\ldots$ Let $p_{k n}=\\operatorname{pr}(T^{(n)}=k).$ $k=0,1,\\ldots,n.$ . Then <PARAGRAPH_SEPARATOR> $$ p_{k n}={\\frac{1}{(n-k)}}\\sum_{i=n-k}^{n}{\\frac{n!}{(n-i)!}}q_{i}^{n+a-i}\\pi^{i}G_{i-n+k}(0|U(n-k)),\\qquadk=0,1,\\ldots,n, $$ <PARAGRAPH_SEPARATOR> where $U(n-k)$ is the sequence $q_{n-k},q_{n-k+1},....$ <PARAGRAPH_SEPARATOR> # 2.3. Threshold parameter <PARAGRAPH_SEPARATOR> Within mathematical epidemic theory, considerable attention has been devoted to the study of threshold parameters, which broadly speaking dictate whether or not an epidemic can take hold in a large population, as opposed to dying out quickly (Bailey, 1975; Andersson & Britton, 2000). Typically, a threshold parameter $R$ exists such that all outbreaks in an infinite population are almost surely finite if and only if $R\\leq1$ . In large finite populations the same $R$ often gives a good approximation to similar threshold behaviour. Such quantities have considerable practical importance, notably because they inform the design of control measures such as vaccination schemes. It is therefore important to consider inference for threshold parameters, as well as basic model parameters."
  },
  {
    "qid": "statistic-posneg-2004-6-2",
    "gold_answer": "FALSE. The context explicitly states that $\\lambda_{n}$ is selected as 12.5, 13.5, and 15 at the three quartiles ($\\tau=0.25, 0.5, 0.75$), respectively, indicating that the tuning parameter varies across different quantile levels.",
    "question": "Is the tuning parameter $\\lambda_{n}$ the same for all three quantile levels considered in the study?",
    "question_context": "Quantile-Varying Coefficient Model for Gene Expression Analysis\n\nIn this section, we apply the proposed method to a subset of the microarray time-course gene expression data set (Spellman et al., 1998). Luan and Li (2003) identified 297 cell cycle regulated genes using a model-based method. Wang et al. (2007a,b) used the ChIP data of Lee et al. (2002), and derived the binding probabilities for these 297 cell-cycle-regulated genes for a total of 96 transcription factors (TFs). By applying the gSCAD method, Wang et al. (2008) identified 71 TFs that might be related to the mean expression patterns of the 297 genes. <PARAGRAPH_SEPARATOR> To identify the TFs that have nonzero time-dependent effects on the quantiles of the gene expression, we consider the following model <PARAGRAPH_SEPARATOR> $$ y_{i t}=\beta_{0}(t,\tau)+\\sum_{k=1}^{96}\\beta_{k}(t,\tau)x_{i k}+\\epsilon_{i t}(\\tau), $$ <PARAGRAPH_SEPARATOR> where $y_{i t}$ denotes the log-expression level for gene $i$ at time t (in days) during the cell-cycle process for $i=1,\\ldots,297$ and $t=0$ , 7, 14, . . . , 119, and $\\beta_{k}(t,\\tau)$ is the transcription effect of the kth TF on the τ th quantile of $y_{i t}$ . We focus on three quantiles levels, $\\tau=0.25$ , 0.5 and 0.75. Using the SIC criterion in Section 2.3, the tuning parameter $k_{n}$ is selected as 2, and $\\lambda_{n}$ is selected as 12.5, 13.5 and 15 at the three quartiles, respectively. <PARAGRAPH_SEPARATOR> At $\\tau=0.25,0.5$ and 0.75, $\\mathrm{GAL}_{\\gamma}$ identifies 47, 40 and 44 TFs related to yeast cell-cycle processes, respectively, including 15, 15 and 16 of the 21 known and experimentally verified cell-cycle-related TFs. As an illustration, Fig. 2 shows the estimated time-dependent transcriptional effects of two known TFs that are selected by our method. The shaded areas correspond to the $90\\%$ point-wise bootstrap confidence bands for the median estimates obtained under the selected model with 200 bootstrap runs. The effects of MBP1 have similar profiles at three quartiles, and three quantile coefficient curves exhibit differences mainly for $t\\in[40,80]$ . The SWI5 shows very different effects at three quartiles across the main support of t except for $t\\geq100$ , and this suggests possible population heterogeneity. <PARAGRAPH_SEPARATOR> Table 5 summarizes the results of the proposed method at $\\tau=0.25$ , 0.5, 0.75 and the results of $g S C A D$ at the mean for identifying the 21 known TFs. The top 13 TFs are identified at all three quartiles and at the mean, the next four TFs are identified at mean and at least one of the quartiles, GCR1 and LEU3 are identified at mean but missed by the quantile regressions, GCN4 is identified at $\\tau=0.75$ but missed by the mean, and CBF1 is missed by all methods. In the following, we will focus on the cases on which the mean and the quantile methods have discrepancy."
  },
  {
    "qid": "statistic-posneg-657-3-0",
    "gold_answer": "TRUE. The local AIC criterion balances the trade-off between goodness-of-fit (measured by $\\log(\\hat{\\sigma}^2(x))$) and model complexity (measured by $2\\operatorname{tr}(H(x))/m_x$) for the local additive estimator within each region $R_k$. The term $\\operatorname{tr}(H(x))$ approximates the effective number of parameters, penalizing overfitting. This localized approach allows adaptive choices of $h_x$ and $\\eta(x)$ that vary across regions.",
    "question": "The local AIC-type criterion $AIC(h_x, \\eta(x)) = \\log(\\hat{\\sigma}^2(x)) + 2\\operatorname{tr}(H(x))/m_x$ is used to select $h_x$ and $\\eta(x)$ by minimizing it. Is this criterion designed to balance model fit and complexity specifically for the local additive estimator within each region $R_k$?",
    "question_context": "Local Linear-Additive Estimation and Bandwidth Selection via AIC\n\nThe paper discusses a method for selecting local regularization parameters and bandwidths in nonparametric regression using an AIC-type criterion. The approach involves splitting the support of covariates into rectangular regions, fitting local additive estimators within each region, and smoothing the selected parameters. The AIC criterion is used to choose optimal bandwidths and regularization parameters both locally and globally."
  },
  {
    "qid": "statistic-posneg-1158-3-0",
    "gold_answer": "FALSE. While θ̂_j - θ_j^0 = O_p(ν^{-1/α}) indicates that the deviation θ̂_j - θ_j^0 is bounded in probability by ν^{-1/α}, it does not necessarily imply convergence to θ_j^0. The notation O_p(ν^{-1/α}) only states that the deviation is stochastically bounded by ν^{-1/α}, not that it converges to zero. Convergence would require θ̂_j - θ_j^0 = o_p(1), which is a stronger condition.",
    "question": "Is the claim that θ̂_j - θ_j^0 = O_p(ν^{-1/α}) implies that the estimators θ̂_j converge to the true parameters θ_j^0 at a rate of ν^{-1/α} as ν → ∞, correct?",
    "question_context": "Polynomial Estimators of Frontiers: Convergence and Asymptotic Properties\n\nThe paper discusses the convergence properties of polynomial estimators of frontiers, focusing on the asymptotic behavior of estimators θ̂_j and their deviations from true parameters θ_j^0. Key formulas include the convergence rate θ̂_j - θ_j^0 = O_p(ν^{-1/α}), the definition of Δ_j = ν^{1/α}(θ̂_j - θ_j^0), and the function T(d) used to maximize the estimator's performance. The paper also introduces the 'sequential sign property' for the Δ_j's and discusses the uniform convergence of T(d) to S(d)."
  },
  {
    "qid": "statistic-posneg-1692-0-0",
    "gold_answer": "FALSE. The condition $0\\leqslant c(\\xi)\\leqslant r(\\xi)$ ensures that $Z$ lies within a specific region defined by the curve $g(\\xi)$ and its normal $N(\\xi)$, but it does not guarantee that $Z$ lies within the double point set $D$. The theorem states that any point $Z\\in D$ can be expressed in this form, but the converse is not necessarily true without additional constraints on $c(\\xi)$ and $r(\\xi)$.",
    "question": "Is the condition $0\\leqslant c(\\xi)\\leqslant r(\\xi)$ in the formula $Z=g(\\xi)+c(\\xi)N(\\xi)$ sufficient to ensure that the point $Z$ lies within the bounded region $D$ for some $\\xi\\in I$?",
    "question_context": "Double Points in Nonlinear Calibration: Probability and Geometry\n\nSince $\\pmb{g}$ is continuous our assumption that $\\kappa>0$ guarantees that the normal to the curve never ‘fips over'; thus the curve $\\pmb{g}$ cannot 'wriggle'. Assuming without loss of generality that $N=J(T)$ , where $J\\{(x,y)^{\\mathsf{T}}\\}=-(y,x)^{\\mathsf{T}},$ then the angle between $T(\\xi)$ and $T(\\xi_{t})$ is between 0 and $\\pmb{\\pi}$ for all $\\pmb{\\xi}\\in\\pmb{I}$ ; this rules out 'twisting' behaviour. The following result is proved in the Appendix. <PARAGRAPH_SEPARATOR> THEOREM. If (2·6) holds, any point $\\pmb{Z}\\in\\pmb{D}$ may be written as <PARAGRAPH_SEPARATOR> $$ Z=g(\\xi)+c(\\xi)N(\\xi),\\quad0\\leqslant c(\\xi)\\leqslant r(\\xi) $$ <PARAGRAPH_SEPARATOR> for some $\\xi\\in I.$ Thus $_D$ is bounded. <PARAGRAPH_SEPARATOR> The theorem makes somewhat more precise the remarks of Marshak & Spiegelman (1985, footnote p. 461) concerning the different effects of curvature on both sides of the curve. That is, points both “inside' and ‘outside' the curve can be double, if $\\pmb{g}$ twists enough. Note also that Pierce (1975) recognizes differing behaviour of confidence sets for observed points on the convex or concave side of the curve. He suggests an approximate ancillary as the signed distance of the point from its projection on the curve. Here the ancillary is used to express the precision of the maximum likelihood estimator. <PARAGRAPH_SEPARATOR> Although the shape of $\\pmb{D}$ is of interest in itself, it is also important to assess, based on the training sample, the probability that a future point will in fact be double. If $\\pmb\\varepsilon$ in (1·1) is assumed multivariate normal then $\\pmb{\\mathrm{pr}}_{\\xi}$ $Y\\in D^{*})$ may be estimated by <PARAGRAPH_SEPARATOR> $$ p\\left(\\xi\\right)=\\left[{\\mathrm{{pr}}}\\left(Z_{\\mu}\\in D\\right)\\right]_{\\mu=g\\left(\\xi\\right)}, $$ <PARAGRAPH_SEPARATOR> where $Z_{\\mu}$ denotes a $N(\\mu,I)$ random variable. Since our methods are more diagnostic in spirit than an end in themselves, it is essential to have an automatic and computationally feasible algorithm which will, given $\\hat{h}$ and $s,$ describe $\\pmb{D}$ and compute $(2{\\cdot}8)$ for a variety of $\\pmb{\\xi}$ values. This is a nontrivial task, since the shape of $\\pmb{D}$ can be fairly arbitrary in general. An algorithm for approximating $\\pmb{D}$ and (2·8), written in $s$ (Becker & Chambers, 1984), is available from the authors. The basic idea follows. <PARAGRAPH_SEPARATOR> (i) Choose a partition $\\xi_{1}<...<\\xi_{m}$ of $I.$ Write $\\pmb{g}(\\pmb{\\xi}_{i})=\\pmb{g}_{i}$ , $N(\\xi_{i})=N,$ and so forth. (ii) Approximate $\\pmb{D}$ by an irregularly spaced grid of points, the intersections of the half-lines <PARAGRAPH_SEPARATOR> $$ l_{\\iota}=\\{g_{i}+s N_{i};s<r_{i}\\}, $$ <PARAGRAPH_SEPARATOR> in two stages as follows. <PARAGRAPH_SEPARATOR> (a) For each $i\\neq j,$ solve <PARAGRAPH_SEPARATOR> $$ g_{i}-g_{j}=-c_{i}^{(j)}N_{i}+c_{j}^{(\\imath)}N_{j} $$ <PARAGRAPH_SEPARATOR> for $\\pmb{c}_{i}^{(j)}$ and $c_{j}^{(i)}$ <PARAGRAPH_SEPARATOR> (b) Determine whether the coefficients satisfy $c_{i}^{(j)}<r_{i}$ and $c_{j}^{(i)}<r_{j}$ , in which case the intersection of the two lines is included in $\\pmb{D}.$ <PARAGRAPH_SEPARATOR> (ii) This grid will typically look like Fig. 3. For each point $p_{i j}=l_{i}\\cap l_{j}$ in $D_{i}$ . compute an area element $A_{i j}$ equal to the area of the parallelogram containing $p_{i j}$ . whose sides are parallel to $l_{i}$ and $l_{j}$ and whose intersections with $l_{i}$ and $l_{j}$ halve the distances of $\\pmb{p}_{i j}$ to the neighbouring points along the lines. <PARAGRAPH_SEPARATOR> (iv)  For given $\\pmb{\\xi},$ approximate (2.8) by the Riemann-like sum <PARAGRAPH_SEPARATOR> $$ \\sum_{(i,j):p_{i j}\\in{\\cal D}}\\phi\\{p_{i j}-g(\\xi)\\}{\\cal A}_{i j}, $$ <PARAGRAPH_SEPARATOR> where $\\phi$ denotes the bivariate independent standard normal probability density function."
  },
  {
    "qid": "statistic-posneg-1319-0-0",
    "gold_answer": "TRUE. The paper explicitly states that it follows Li, Swersky, and Zemel (2015) in using a mixture of Gaussian kernels with different bandwidth parameters to avoid the need for fine-tuning the bandwidth parameter. This is detailed in the formula and the surrounding text.",
    "question": "Is it true that the paper uses a mixture of Gaussian kernels with different bandwidth parameters to avoid the need for fine-tuning the bandwidth parameter in the MMD statistic?",
    "question_context": "GMMN Architecture and Kernel Selection for Multivariate Distribution Sampling\n\nWe find a single hidden layer architecture $(L=1)$ ) to be sufficient for all the examples we considered. This is because, in this article, we largely consider the cases of $d\\in\\{2,\\ldots,10\\}$ . Learning an entire distribution nonparametrically for $d\\digamma>10$ would most likely require $L>1$ , but it would also require a much larger sample size $n_{\\mathrm{trn}}$ and become much more challenging computationally for GMMNs—recall from Section 2.2 that the cost function requires ${\\binom{n_{\\mathrm{trn}}}{2}}$ evaluations. After experimentation, we fix $d_{1}=300,$ $\\phi_{1}$ to be ReLU (it offers computational efficiency via nonexpensive and nonvanishing gradients) and $\\phi_{2}$ to be sigmoid (to obtain outputs in $[0,1]^{d}.$ ). <PARAGRAPH_SEPARATOR> To avoid the need of fine-tuning the bandwidth parameter, we follow Li, Swersky, and Zemel (2015) and use a mixture of Gaussian kernels with different bandwidth parameters as our kernel function for the MMD statistic in (6); specifically, <PARAGRAPH_SEPARATOR> $$ K(\\pmb{x},\\pmb{y})=\\sum_{i=1}^{n_{\\mathrm{krn}}}K(\\pmb{x},\\pmb{y};\\sigma_{i}), $$ <PARAGRAPH_SEPARATOR> where $n_{\\mathrm{krn}}$ denotes the number of mixture components and $K(x,y;\\sigma)=\\exp(-\\|x-y\\|_{2}^{2}/(2\\sigma^{2}))$ is the Gaussian kernel with bandwidth parameter $\\sigma>0$ . After experimentation, we fix $n_{\\mathrm{krn}}\\mathrm{~\\ensuremath~{~=~\\ensuremath~{~6~}~}~}$ and choose $\\begin{array}{r l}{(\\sigma_{1},\\dots,\\sigma_{6})}&{{}=}\\end{array}$ $(0.001,0.01,0.15,0.25,0.50,0.75)$ ; note that copula samples are in $[0,1]^{d}$ . <PARAGRAPH_SEPARATOR> Unless otherwise specified, we use the following setup across all examples. We use $n_{\\mathrm{trn}}=60{,}000$ training data points and find this to be sufficiently large to obtain reliable $f_{\\hat{\\theta}}$ . As dimension of the input distribution $F_{\\mathbf{Z}}$ , we choose $\\textit{p}=\\textit{d}$ , that is, the GMMN $f_{\\pmb\\theta}$ is set to be a $d$ -to- $d$ transformation. For $F_{\\mathbf{Z}}$ , we choose $\\pmb{Z}\\sim N(\\mathbf{0},I_{d})$ , where $I_{d}$ denotes the identity matrix in $\\mathbb{R}^{d\\times d}$ , so $\\boldsymbol{\\mathbf{Z}}$ consists of independent standard normal random variables; this choice worked better than $U(0,1)^{d}$ in practice despite the fact that $N(\\mathbf{0},I_{d})$ does not satisfy the assumptions of Proposition A.1. We choose a batch size of $n_{\\mathrm{bat}}=5000$ in Algorithm 2.1; this decision is motivated from a practical tradeoff that a small $n_{\\mathrm{bat}}$ will lead to poor estimates of the population MMD cost function but a large $n_{\\mathrm{bat}}$ will incur quadratically growing memory requirements due to (6). As the number of epochs we choose $n_{\\mathrm{epo}}=300$ which is generally sufficient in our experiments to obtain accurate results. The tuning parameters of the Adam optimizer is set to the default values reported in Kingma and Ba (2014). <PARAGRAPH_SEPARATOR> Our implementation uses the R packages keras and tensorflow which serve as R interfaces to the corresponding namesake Python libraries. Furthermore, all GMMN training is carried out on one NVIDIA Tesla P100 GPU (12 GB). To generate the RQMC point set in Algorithm 2.3, we use scrambled nets (Owen 1995); see also Appendix A.3. Specifically, we use the implementation sobol(, randomize $=$ \"Owen\") from the R package qrng. Finally, our choice of R as programming language for this work was motivated by the fact that contributed packages providing functionality for copula modeling and quasi-random number generation—two of the three major fields of research (besides machine learning) this work touches upon—exist in R."
  },
  {
    "qid": "statistic-posneg-975-0-0",
    "gold_answer": "FALSE. The context states that $x_{1}$ is significant at the 5% level, but it does not specify whether this significance applies to both the count component ($\\mu_{i}$) and the zero-inflation component ($p_{i}$). The model's interpretation would require separate significance tests for the coefficients $\\beta_{1}$ (count component) and $\\gamma_{1}$ (zero-inflation component). The context only provides a general statement about $x_{1}$ being significant, without distinguishing between the two components.",
    "question": "In the ZINB model specified, the variable $x_{1}$ (photoperiod) is significant for both the count component ($\\mu_{i}$) and the zero-inflation component ($p_{i}$). Is this statement supported by the context and the model's interpretation?",
    "question_context": "Zero-Inflated Negative Binomial (ZINB) Model for Root Count Data\n\nTo demonstrate the proposed methodology, we use the apple cultivar data set reported by Ridout et al. (2001), referring to the number of roots produced by 270 micropropagated shoots of the columnar apple cultivar Trajan. The shoots had been produced under an 8 or $16\\mathrm{~h~}$ photoperiods in culture systems that utilized one of four different concentrations of the cytokinin BAP in the culture medium. There were 30 or 40 shoots in each of these eight treatment combinations. Of the 140 shoots produced under the $^{8\\mathrm{~h~}}$ photoperiod, only 2 failed to produce roots, but 62 of the 130 shoots produced under the $16\\mathrm{~h~}$ photoperiod failed to root. The sample size was $n=270$ and the percentage of zeros observed was $23.7\\%$ . Thus, the variables are: <PARAGRAPH_SEPARATOR> $y_{i}$ : count of roots; • $x_{i1}$ : photoperiod $[0=8\\mathrm{h},1=16\\mathrm{h}]$ ); • $x_{i2}$ : concentrations of the cytokinin BAP, where $i=1,2,\\ldots,270.$ <PARAGRAPH_SEPARATOR> Following Ridout et al. (2001) we fit a ZINB model as defined in Eq. (1), with <PARAGRAPH_SEPARATOR> $$ \\mu_{i}=\\exp(\\beta_{0}+\\beta_{1}x_{i1}+\\beta_{2}x_{i2})\\quad\\mathrm{and}\\quad p_{i}=\\frac{\\exp(\\gamma_{0}+\\gamma_{1}x_{i1}+\\gamma_{2}x_{i2})}{1+\\exp(\\gamma_{0}+\\gamma_{1}x_{i1}+\\gamma_{2}x_{i2})},\\quad i=1,\\dots,270. $$ <PARAGRAPH_SEPARATOR> # 4.1. Estimation <PARAGRAPH_SEPARATOR> To obtain the ML estimates of the parameters in the ZINB regression model, we use the EM algorithm, as described in Section 2.1. The results are given in Table 1; in order to compare and evaluate our proposed algorithm, we also present the ML results based on the subroutine MaxBFGS from the Ox software (Doornik, 2007). <PARAGRAPH_SEPARATOR> From Table 1, it can be seen that the explanatory variable $x_{1}$ is significant at the $5\\%$ level, while the variable $x_{2}$ , related to concentrations of the cytokinin BAP, is statistically non-significant for the count of roots ( $p\\cdot$ -value $\\approx1$ ). These results indicate that our proposed EM algorithm works very well and can be used reliably for ML estimation in ZINB models."
  },
  {
    "qid": "statistic-posneg-971-0-0",
    "gold_answer": "FALSE. While $\\Delta(w) \\geq 0$ is a derived condition in the paper, it is not universally necessary for admissibility. The paper's analysis shows that violations of this condition at specific points (e.g., $w_*$) can still lead to admissible estimators if other constraints (e.g., boundedness of $\\|z\\| \\times \\|\\nabla \\ln m(\\|z\\|; a, L)\\|$) are satisfied. The admissibility depends on the broader interplay of $\\phi(w)$, $g(w)$, and their asymptotic properties, not solely on $\\Delta(w) \\geq 0$.",
    "question": "Is it true that the condition $\\Delta(w) \\geq 0$ for all $w \\geq 0$ is necessary to ensure the admissibility of the estimator, as derived from the integral inequalities involving $\\phi(w)$ and $g(w)$?",
    "question_context": "Admissibility Conditions for SURE-Based Estimators\n\nThe paper examines admissibility conditions for estimators based on Stein's Unbiased Risk Estimate (SURE). Key formulas include integral expressions and inequalities involving functions like $\\phi(w)$ and $g(w)$, which are used to derive conditions under which estimators remain admissible. The analysis involves bounding terms and ensuring integrals converge, with specific focus on the behavior of these functions as $w \\to \\infty$."
  },
  {
    "qid": "statistic-posneg-653-0-2",
    "gold_answer": "FALSE. The asymptotic bias of the tapered Whittle estimator depends on the smoothness parameter ρ, as shown in Theorem 1. The bias is influenced by the choice of ρ, and the paper discusses how letting ρ depend on N (with ρ_N → 0) can avoid a bias of order O(N^(-1/2)).",
    "question": "Is the asymptotic bias of the tapered Whittle estimator independent of the smoothness parameter ρ?",
    "question_context": "Edge Effects and Efficiency in Tapered Whittle Estimation for Stationary Random Fields\n\nThe paper discusses edge effects in spatial statistics and introduces tapered Whittle estimates to mitigate bias in covariance estimation for stationary random fields. The tapered covariance estimate is defined using a taper function h(u), and the asymptotic properties of the tapered Whittle estimator are analyzed, including bias and efficiency under certain assumptions."
  },
  {
    "qid": "statistic-posneg-1207-5-0",
    "gold_answer": "FALSE. The condition $\\underset{N}{\\operatorname*{lim}}\\frac{K N_{t r a i n}^{2}}{N}\\frac{\\zeta_{1,N_{t r a i n}}}{\\zeta_{N_{t r a i n},N_{t r a i n}}}=0$ is not strictly necessary for asymptotic normality. While it helps ensure the variance terms vanish appropriately, the key condition is the convergence of the U-statistic's variance to a finite limit, which can occur under weaker assumptions. The paper uses this condition to simplify the proof and ensure clean asymptotic results, but it is not the minimal requirement for normality.",
    "question": "Is the condition $\\underset{N}{\\operatorname*{lim}}\\frac{K N_{t r a i n}^{2}}{N}\\frac{\\zeta_{1,N_{t r a i n}}}{\\zeta_{N_{t r a i n},N_{t r a i n}}}=0$ necessary for the asymptotic normality of the U-statistic in the random forest-based two-sample test?",
    "question_context": "Asymptotic Properties of Random Forest-Based Two-Sample Tests\n\nThe paper analyzes the asymptotic properties of random forest-based two-sample tests, focusing on the decision rule's level conservation and power. Key formulas include the asymptotic normality of the U-statistic and the conditions under which the test conserves the level and achieves consistent power."
  },
  {
    "qid": "statistic-posneg-1746-0-0",
    "gold_answer": "TRUE. The null hypothesis $H_{0}:F_{X Y}=F_{X}F_{Y}$ is correctly specified for testing independence between $X$ and $Y$. Independence of two random variables is defined as the joint distribution being equal to the product of the marginal distributions. This is a fundamental property in probability theory and statistics, and it forms the basis for many independence tests, including distance correlation and other nonparametric methods.",
    "question": "Is the null hypothesis $H_{0}:F_{X Y}=F_{X}F_{Y}$ correctly specified for testing independence between two random variables $X$ and $Y$?",
    "question_context": "Testing Independence with Distance Correlation\n\nGiven pairs of observations $(x_{i},y_{i})\\in\\mathbb{R}^{p}\\times\\mathbb{R}^{q}$ for $i=1,\\ldots,n.$ , assume they are independently identically distributed as $F_{X Y}$ . Two random variables are independent if and only if the joint distribution equals the product of the marginal distributions. The statistical hypothesis for testing independence is $$\\begin{array}{l}{{H_{0}:F_{X Y}=F_{X}F_{Y},}}\\ {{}}\\ {{H_{A}:F_{X Y}\\not=F_{X}F_{Y}.}}\\end{array}$$ <PARAGRAPH_SEPARATOR> To populate these methods to big data analysis, a big hurdle is the time complexity. Computing the distance correlation typically requires $\\mathcal{O}(n^{2})$ ; and to compute its $\\boldsymbol{p}$ -value for testing, the standard approach is to estimate the null distribution of distance correlation via permutation, which requires $\\mathcal{O}(r n^{2})$ where $r$ is the number of random permutations and typically at least 100 or more. In comparison, for one-dimensional $(p=q=1)$ ) data the Pearson correlation can be computed in $\\mathcal{O}(n)$ , and a Pearson correlation $t$ -test is readily available to compute the $\\boldsymbol{p}$ -value in constant time complexity $\\mathcal{O}(1)$ . The computational advantage makes Pearson extremely fast and attractive in practice. Recent works have successfully expedited the distance correlation computation into ${\\mathcal{O}}(n\\log(n))$ under one-dimensional data and Euclidean distance (Huo and Szekely 2016; Chaudhuri and $\\mathrm{Hu}2019\\$ ). The testing part, however, remains difficult and less well-understood, because the null distribution of distance correlation has no fixed nor known density in general. Some notable works include the distance correlation $t$ -test (Szekely and Rizzo 2013), HSIC Gamma test (Gretton and Gyorfi 2010), and the subsampling approach (Zhang et al. 2018), which provided special treatments and very valuable insights into the problem."
  },
  {
    "qid": "statistic-posneg-557-0-0",
    "gold_answer": "TRUE. Explanation: When γ = 0, the function T to be maximized in CR reduces to maximizing the sample correlation coefficient between y and c′x, which yields the OLS solution. This is confirmed by the formula c(0) ∝ S^(-1)s, which is identical to the OLS estimator b^OLS = S^(-1)s. Thus, b^CR(0) is proportional to b^OLS, and in practice, they are equivalent up to a scaling factor.",
    "question": "Is the statement 'For γ = 0, the CR estimator b^CR(γ) is equivalent to the OLS estimator b^OLS' true based on the provided formulas and context?",
    "question_context": "Relationship Between Continuum Regression and Ridge Regression\n\nThe paper discusses the relationship between Continuum Regression (CR) and Ridge Regression (RR). It explains how CR selects regressors by maximizing a function T for a given parameter γ, and how this relates to RR estimators. The key formulas show the proportionality between CR and RR coefficients, and the relationship between the CR parameter γ and the RR constant δ."
  },
  {
    "qid": "statistic-posneg-1448-0-2",
    "gold_answer": "FALSE. The context states that the compactness lemma of Sharpe ensures the kernel $\\operatorname{Inv}(\\mu)$ is compact when $\\mu$ is full (not supported by a proper affine subspace of $V$).",
    "question": "Is the kernel $\\operatorname{Inv}(\\mu) = \\{(B,b,1) \\in L : \\mu = B\\mu * \\varepsilon_{b}\\}$ always non-compact for a full measure $\\mu$?",
    "question_context": "Operator-Semistable Measures and Decomposability\n\nDEFINITION. $\\mu$ is said to be $\\left(B,\\boldsymbol{b},\\beta\\right)$ -decomposable if the equation $\\mu_{\\beta}=B\\mu*\\varepsilon_{b}$ is valid. <PARAGRAPH_SEPARATOR> Trivially every $\\mu$ is $(I,0,1)$ -decomposable. Moreover if $\\mu$ is $(B,b,\\beta)$ decomposable the same is true for all $\\mu_{t},t>0$ (since $\\pmb{\\mu}$ uniquely determines its semigroup $\\left(\\mu_{t}\\right)_{t>0})$ <PARAGRAPH_SEPARATOR> Now let us denote by $L=L(\\mu)$ the collection of all triples $\\left(B,\\dot{b},\\beta\\right)$ such that $\\mu$ is $(B,b,\\beta)$ -decomposable. Let us identify $(B,b,\\beta)$ with the matrix <PARAGRAPH_SEPARATOR> $$ {\\binom{B}{0}}\\quad{\\binom{b}{\\beta}}\\in G L(\\mathbb{R}^{d+1}). $$ <PARAGRAPH_SEPARATOR> Then it can be easily seen that $\\pmb{L}$ is a closed subgroup of $G L(\\mathbb{R}^{d+1})$ . Hence $L$ is a Lie group. By $f((B,b,\\beta)):=\\beta$ there is defined a continuous homomorphism of $L$ into the multiplicative group $\\mathbb{R}_{+}^{*}$ <PARAGRAPH_SEPARATOR> Let us assume now that the measure $\\mu$ is full, i.e.,. $\\mu$ is not supported by a proper affine  subspace  of $V$ Then the compactness lemma of Sharpe, [12, Proposition 4] can be applied to the effect that the kernel $\\left\\{(B,b,1)\\in L\\colon\\mu=B\\mu\\ast\\varepsilon_{b}\\right\\}=:\\operatorname{Inv}(\\mu)$ of $f$ is compact and that the mapping $f$ is closed. Hence exactly one of the following three possibilities can occur: $f(L)=\\left\\{1\\right\\}$ ; or $f(L)=\\left\\{\\beta^{n}\\colon n\\in\\mathbb{Z}\\right\\}$ with some $\\beta\\in]0,1[$ ; or $f(L)=\\mathbb{R}_{+}^{*}$ . The case $f(L)=\\left\\{1\\right\\}$ being uninteresting let us discuss the other ones. <PARAGRAPH_SEPARATOR> 1. Let $f(L)=\\left\\{\\beta^{n}\\colon n\\in\\mathbb{Z}\\right\\}$ with some $\\beta\\in\\]0$ , 1[. We put $k_{n}:=[\\beta^{-n}]$ for all $n\\in\\mathbb{N}$ . Then  we  have lim $k_{n}/k_{n+1}=\\beta$ and lim $B^{n}\\mu^{*k_{n}}*\\varepsilon_{b_{n}}=\\mu$ for appropriate $b_{n}\\in V,n\\in\\mathbb{N}$ Hence $\\mu$ is operator-semistable [5, Definition 1]."
  },
  {
    "qid": "statistic-posneg-1907-1-0",
    "gold_answer": "FALSE. The approximation is accurate when α is small and either λ=0 or λ/A is small, not necessarily both zero. The non-informative prior (α=λ=0) is a special case where the approximation holds, but the approximation is more generally valid under the stated conditions.",
    "question": "The Bayes factor approximation using the F-statistic is accurate only when the prior parameters α and λ are both zero. True or False?",
    "question_context": "Bayes Factor Approximation in Hypothesis Testing\n\nThe comparisons of the Bayes factors obtained from the full parametric model and the Bayes factors obtained from the chi-squared statistic are quite promising from several perspectives. As mentioned above, the values of the Bayes factor based on the chi-squared statistic appear to capture almost the same evidence that is captured by the fully parametric model. This has the important practical implication that Bayes factors for discrete data can be accurately approximated without specifying what can be very complicated prior distributions on high-dimensional probability vectors. In the case of testing for independence in contingency tables, for example, it is straightforward to calculate the chi-squared test statistic. However, specifying proper prior distributions on marginal category probabilities can be quite difficult under the null hypothesis, and it can be substantially more difficult to specify a prior density on the cell probabilities under the alternative model in a manner that represents plausible patterns of dependence. Thus, the approximation of the Bayes factor using the chi-squared statistic might provide a more accurate approximation to the Bayes factor between two discrete models than that which is obtained by inaccurately representing subjective prior information regarding cell probabilities using overly simplistic parametric models. <PARAGRAPH_SEPARATOR> # 3.2. Bayes factors based on $F$ tests <PARAGRAPH_SEPARATOR> Now assume that the model specified in section 2.3 applies, and consider a test of the hypotheses <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{H_{1}:\\mathbf{y}\\mid\\beta_{0},\\sigma^{2}\\sim N_{n}\\left(\\mathbf{X}\\beta_{0},\\sigma^{2}\\mathbf{I}_{n}\\right),\\quad\\beta_{0}\\mathrm{known}}\\ &{H_{2}:\\mathbf{y}\\mid\\beta,\\sigma^{2}\\sim N_{n}\\left(\\mathbf{X}\\beta,\\sigma^{2}\\mathbf{I}_{n}\\right),\\quad\\beta\\mid\\sigma^{2},\\tau\\sim N_{r}\\left(\\beta_{0},n\\tau^{*}\\sigma^{2}(\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}\\right)}\\end{array} $$ <PARAGRAPH_SEPARATOR> where $\\sigma^{2}$ is assumed a priori to have an inverse gamma distribution with parameters $(\\alpha,\\lambda)$ under both $H_{1}$ and $H_{2}$ . Without loss of generality, let $\\pmb{\\beta}_{0}=0$ , and note that the dimension of <PARAGRAPH_SEPARATOR> $\\beta(r)$ equals the dimension of the matrix that imposes the constraint under the null hypothesis, $k$ . <PARAGRAPH_SEPARATOR> Based on this parametric model, a straightforward calculation shows that the Bayes factor in favour of $H_{2}$ can be expressed as <PARAGRAPH_SEPARATOR> $$ {\\bf B F}(2\\mid1)=(1+n\\tau^{*})^{-k/2}\\left(\\frac{\\lambda+{\\bf J^{\\prime}}{\\bf y}/2}{\\lambda+A/2}\\right)^{n/2+\\alpha} $$ <PARAGRAPH_SEPARATOR> where <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{{\\cal A}={\\bf y}^{\\prime}{\\bf y}-\\left(\\frac{n\\tau^{*}}{n\\tau^{*}-1}\\right){\\bf y}^{\\prime}{\\bf H}{\\bf y}\\quad\\mathrm{and}\\quad{\\bf H}={\\bf X}({\\bf X}^{\\prime}{\\bf X})^{-1}{\\bf X}^{\\prime}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> If either $\\lambda=0$ or $\\lambda/A$ is small, and if $n\\tau^{*}/(n\\tau^{*}-1)$ is close to 1, then <PARAGRAPH_SEPARATOR> $$ {\\bf B F}(2\\mid1)\\approx(1+n\\tau^{*})^{-k/2}\\left(1+\\frac{k}{m}f_{n}\\right)^{n/2+\\alpha} $$ <PARAGRAPH_SEPARATOR> where, as in section 2.3, $m=n-r$ and $f_{n}$ denotes the $F$ statistic for testing $H_{2}$ versus $H_{1}$ . <PARAGRAPH_SEPARATOR> The last term in the logarithm of the Bayes factor based directly on the $F$ statistic in (10) is $O_{p}(n^{-2})$ under the null hypothesis and is $O_{p}(1/n)$ under the alternative hypothesis. Also, $k+m=n$ in this setting. It follows that the logarithm of (13) differs from (10) only by the presence of $\\alpha$ in the exponent of the second term on the right-hand side of (13) and terms that are $O_{p}(1/n)$ . Thus, if $\\alpha$ is small, the Bayes factor based on the $F$ statistic approximates the parametric Bayes factor. <PARAGRAPH_SEPARATOR> Note also that $\\alpha=\\lambda=0$ corresponds to the non-informative prior for $\\sigma^{2}$ , although the parametric Bayes factor corresponding to these values must be obtained either through limiting arguments or through considerations of model symmetries."
  },
  {
    "qid": "statistic-posneg-669-0-0",
    "gold_answer": "TRUE. The bound is achieved by a set of identically distributed maximally dependent random variables defined using a specific construction involving uniform random variables and their inverse distribution functions. The sharpness is verified by ensuring the Holder inequality becomes an equality for the constructed distribution.",
    "question": "Is the bound $E(R_{n})\\leqslant2(n/2)^{1/p}$ sharp for identically distributed, possibly dependent random variables?",
    "question_context": "Bounds on Expected Range of Order Statistics\n\nThe paper discusses bounds on the expected range of order statistics for identically distributed, possibly dependent random variables. It provides sharp bounds under various conditions, including symmetry and independence. The expected range is expressed as $E(R_{n})=E(X_{n:n}-X_{1:n})$, and the paper derives bounds using Holder's inequality and specific distribution assumptions."
  },
  {
    "qid": "statistic-posneg-771-0-1",
    "gold_answer": "FALSE. The Zero-One utility function \\( u_{0-1}(d,{\\pmb y},m) \\) yields 1 only if \\( \\hat{m} = m \\), where \\( \\hat{m} \\) is the model with the highest posterior probability \\( p(m|{\\pmb y},{\\pmb d}) \\). It is not independent of the posterior probabilities, as the selection of \\( \\hat{m} \\) depends on them.",
    "question": "Does the Zero-One utility function \\( u_{0-1}(d,{\\pmb y},m) \\) always yield a value of 1 if the estimated model \\( \\hat{m} \\) matches the true model \\( m \\), regardless of the posterior model probabilities?",
    "question_context": "Bayesian Experimental Design for Model Discrimination\n\nThe paper discusses Bayesian experimental design for discriminating between multiple models using utility functions such as mutual information, Ds-optimality, and Zero-One utility. It outlines the mathematical framework for model choice, utility estimation, and optimization in intractable likelihood settings using Approximate Bayesian Computation (ABC)."
  },
  {
    "qid": "statistic-posneg-22-0-0",
    "gold_answer": "FALSE. The formula shows that increasing $\\eta$ scales the entire expression, but the context does not provide a direct mathematical or statistical justification for why this scaling would specifically reduce false positives while preserving true positives. The observed effect is empirical ('found that with a moderate $\\eta$...'), not derived from the model's theoretical properties. The mechanism by which $\\eta$ affects false positives is not explained in the context, and the statement is based on experimental results rather than a proven mathematical relationship.",
    "question": "Is the claim that increasing the parameter $\\eta$ in the modified MDL criterion reduces false positives while maintaining true positives supported by the formula and the context provided?",
    "question_context": "Modified MDL Criterion for Gene Selection in Cancer Genomic Studies\n\nUnder some simulation scenarios, the iLB is less satisfactory with a considerable number of false positives. One possible cause is that the boosting stops 'too late.' Numerically, we have experimented with the following modification of Step 4 of iLB: $$\\sum_{n=1}^{M}\\log|I^{m}|+\\sum_{s\\in A}\\log\\left[\\sum_{m=1}^{M}(\\hat{\\beta}_{s}^{m[k]})^{2}/M\\right]\\longrightarrow\\eta\\times\\left\\{\\sum_{m=1}^{M}\\log|I^{m}|+\\sum_{s\\in A}\\log\\left[\\sum_{m=1}^{M}(\\hat{\\beta}_{s}^{m[k]})^{2}\\right]\\right\\}.$$ where $\\eta>1$. We have experimented with different $\\eta$ values and found that with a moderate $\\eta$, the number of true positives remains almost the same, whereas the number of false positives may decrease. Such a modification may demand tuning an additional parameter or revising the MDL criterion and will not be pursued."
  },
  {
    "qid": "statistic-posneg-2043-0-1",
    "gold_answer": "FALSE. The Kummer function $\\Psi(a,b;x)$ does not generally simplify when $b=1$. The specific form used in the derivation, $\\Psi\\Big(\\frac{1}{\\xi}+1,1;\\frac{\\xi(x-u)}{\\lambda}\\Big)$, retains its complexity and is correctly expressed in terms of the confluent hypergeometric functions ${_1F_1}$ as shown in the definition. The presence of $\\Gamma(1-b)$ and $\\Gamma(b-1)$ in the definition of $\\Psi$ indicates that the function's behavior is non-trivial at $b=1$, requiring careful handling rather than simplification.",
    "question": "Does the Kummer function $\\Psi(a,b;x)$ reduce to a simpler form when $b=1$, as used in the unconditional density derivation?",
    "question_context": "Random Effects in Generalized Pareto Distribution for Climate Change Analysis\n\nThe methodology in Clarkson et al. (2022) involves adding random effects to the parameters of the generalised Pareto distribution. The idea of adding random effects to parameters of Pareto distributions is not new at all. Nadarajah (University of Nebraska, Lincoln, USA, unpublished manuscript) constructed models for the Pareto type II distribution (also known as the Lomax distribution) by letting its scale parameter to be random. Let $X$ be a Pareto type II random variable with probability density function $$f_{X}(x)={\\frac{a\\sigma^{a}}{(x+\\sigma)^{a+1}}}$$ for $x>0$ , $a>0$ , and $\\sigma>0$ . Letting $\\upsigma$ to be a random variable (independent of $X$ ) with probability density function $g(\\cdot)$ , Nadarajah derived closed form expressions for the unconditional probability density function of $X$ : $$\\int_{0}^{\\infty}\\frac{a\\sigma^{a}}{(x+\\sigma)^{a+1}}g(\\sigma)\\mathrm{d}\\sigma.$$ Thirteen possible forms for $g(\\cdot)$ were considered. The closed form expressions for (1) involved known special functions like the incomplete gamma function, the Appell function of the first kind, the generalised hypergeometric function and the Kummer function. The extension of (1) is straightforward if $X$ is a generalised Pareto random variable. Let $X$ be a generalised Pareto random variable with probability density function $$f_{X}(x)=\\frac{\\phi}{\\sigma}\\left[1+\\frac{\\xi(x-u)}{\\sigma}\\right]^{-\\frac{1}{\\xi}-1}$$ for $x>u$ , $-\\infty<\\xi<\\infty$ , and $\\sigma>0$ . If we take $\\upsigma$ to be an exponential random variable, e.g. independent of $X$ , calculations similar to those in the unpublished manuscript show that the unconditional probability density function of $X$ is $$\\int_{0}^{\\infty}{\\frac{\\phi}{\\sigma}}\\left[1+{\\frac{\\xi(x-u)}{\\sigma}}\\right]^{-{\\frac{1}{\\xi}}-1}{\\frac{1}{\\lambda}}\\exp\\Big(-{\\frac{\\sigma}{\\lambda}}\\Big)\\mathrm{d}\\sigma={\\frac{\\phi}{\\lambda}}\\Gamma\\Big({\\frac{1}{\\xi}}+1\\Big)\\Psi\\Big({\\frac{1}{\\xi}}+1,1;{\\frac{\\xi(x-u)}{\\lambda}}\\Big),$$ where $\\Psi(a,b;x)$ denotes the Kummer function defined by $$\\Psi(a,b;x)=\\frac{\\Gamma(1-b)}{\\Gamma(1+a-b)}_{1}F_{1}(a,b;x)+\\frac{\\Gamma(b-1)}{\\Gamma(a)}x^{1-b}_{1}F_{1}(1+a-b,2-b;x),$$ where ${_1F_{1}}(a,b;x)$ denotes the confluent hypergeometric function defined by $${_1F_{1}}\\left(a;b;x\\right)=\\sum_{k=0}^{\\infty}\\frac{(a)_{k}}{\\left(b\\right)_{k}}\\frac{x^{k}}{k!},$$ where $(f)_{k}=f(f+1)\\cdot\\cdot\\cdot(f+k-1)$ denotes the ascending factorial. The properties of the these special functions can be found in Prudnikov et al. (1986) and Gradshteyn and Ryzhik (2000). In-built functions for computing these functions are available in packages like Maple, Matlab, and Mathematica."
  },
  {
    "qid": "statistic-posneg-2086-3-0",
    "gold_answer": "FALSE. The formula is incorrectly structured for the joint distribution of parent locations. The correct joint distribution should account for the Poisson process intensity and the prior distribution of parent locations, but the given formula mixes the intensity ratio with the prior in a multiplicative manner that does not align with standard spatial point process theory. The numerator represents the intensity at observed points, while the denominator normalizes this intensity over the area A, but the product over i and the inclusion of g(x_p) suggest an incorrect combination of likelihood and prior terms.",
    "question": "Is the formula $$ \\prod_{i=1\\atop i=1}^{n p}\\frac{\\displaystyle\\sum_{j=1}^{n p}h(\\mathbf x_{i}-\\mathbf x_{p j})}{\\displaystyle\\int_{A}\\sum_{j=1}^{n p}h(\\mathbf x-\\mathbf x_{p j})\\mathrm d\\mathbf x}g(\\mathbf x_{p}); $$ correctly representing the joint distribution of parent locations in the spatial Cox process as described in the context?",
    "question_context": "Gibbs Sampler for Spatial Cox Process in Epidemiology\n\nAndrew Lawson (Dundee Institute of Technology): Application of the Gibbs sampler to spatial problems has largely been confined to image processing and random fields. However, there are possibilities for its application to estimation in spatial point processes, and in particular spatial epidemiology. Specifically, motivated by the need to estimate cluster centres for rare diseases (e.g. leukaemia) it is possible to construct a sampler for a spatial Cox process. Assume that, conditional on the realization of the parent process, the data points are distributed as a heterogeneous Poisson process with intensity $\\lambda(\\mathbf{x})=\\sum_{j=1}^{\\infty}h(\\mathbf{x}-\\mathbf{x}p_{j})$ <PARAGRAPH_SEPARATOR> This intensity depends on the infinite realization of parents. However, for definiteness, we assume that data points only depend on a subset of parents o size $\\pmb{n}_{p}$ and that $\\lambda(\\mathbf{x})=\\sum_{j=I}^{n p}h(\\mathbf{x}-\\mathbf{x}p_{j})$ Itis not necessary for these parents to lie within a predefined data window, as the sampler can simulate into larger areas. The sampler operates as follows: <PARAGRAPH_SEPARATOR> (a) generate $n_{p}$ parents from a Poisson distribution $(\\rho)$ (b) generate $\\{\\mathbf{x}_{p}\\}_{j=1,n p}$ parent locations in planar window $\\pmb{A}$ from <PARAGRAPH_SEPARATOR> $$ \\prod_{i=1\\atop i=1}^{n p}\\frac{\\displaystyle\\sum_{j=1}^{n p}h(\\mathbf x_{i}-\\mathbf x_{p j})}{\\displaystyle\\int_{A}\\sum_{j=1}^{n p}h(\\mathbf x-\\mathbf x_{p j})\\mathrm d\\mathbf x}g(\\mathbf x_{p}); $$ <PARAGRAPH_SEPARATOR> (c) generate $k$ from a gamma distribution (where $k$ is the radial variance in $h(\\mathbf{\\theta})$ (d) generate $\\pmb{\\rho}$ from a gamma distribution, or directly estimate $\\rho$ <PARAGRAPH_SEPARATOR> ![](images/c5661ef1664d2a0182fa5f881c41da9d4b85a051d32243ede5bcfbfd96ba1428.jpg)  Fig. 4. Redwood data: (a) data set; (b) parent distribution in the final 100 iterations after 3000 iterations (within aborder area of 0.2 units) <PARAGRAPH_SEPARATOR> The prior $g(\\mathbf{x}_{p})$ can be estimated nonparametrically from a control disease."
  },
  {
    "qid": "statistic-posneg-1803-0-0",
    "gold_answer": "TRUE. The formula correctly represents the joint distribution of $X$ and $S$ under the assumption of conditional independence given the latent variable $U$. The integral marginalizes over $U$ to obtain the joint distribution, which is a standard application of probability theory when dealing with latent variables.",
    "question": "Is the statement 'If $X$ and $S$ are conditionally independent given $U$, then the joint distribution can be expressed as $[X,S]{}=\\int[X|U][S|U][U]\\mathrm{d}U$' correct based on the provided context?",
    "question_context": "Geostatistical Inference Under Preferential Sampling\n\nA natural response to a strongly non-uniform sampling design is to ask whether its spatial pattern could be explained by the pattern of spatial variation in a relevant covariate. Suppose, for illustration, that $S$ is observed without error, that dependence between $X$ and $S$ arises through their shared dependence on a latent variable $U$ and that the joint distribution of $X$ and $S$ is of the form $$[X,S]{}=\\int[X|U][S|U][U]\\mathrm{d}U,$$ so that $X$ and $S$ are conditionally independent given $U.$ . If the values of $U$ were to be observed, we could then legitimately work with the conditional likelihood $[X,S|U]{=}[X|U][S|U]$ and eliminate $X$ by marginalization, exactly as is done implicitly when conventional geostatistical methods are used. In practice, ‘observing’ $U$ means finding explanatory variables which are associated both with $X$ and with $S$ , adjusting for their effects and checking that after this adjustment there is little or no residual dependence between $X$ and $S$ . If so, the analysis could then proceed on the assumption that sampling is no longer preferential. In this context, any of the existing tests for preferential sampling can be applied, albeit approximately, to residuals after fitting a regression model for the mean response."
  },
  {
    "qid": "statistic-posneg-1480-3-2",
    "gold_answer": "FALSE. The paper mentions that the goodness of both approximations depends on $N$. Specifically, for fixed $N$, $\\hat{\\sigma}^{(n,N)}$ can be made arbitrarily small by choosing $n$ sufficiently large, indicating that the approximation quality improves with larger sample sizes.",
    "question": "Is the approximation quality of $\\hat{\\sigma}^{(n,N)}$ and $\\hat{d}^{(n,N)}$ independent of the sample size $N$?",
    "question_context": "Perturbation Model Consistency in Kendall Shape Analysis\n\nThe paper discusses the consistency of Procrustes means in the context of a perturbation model for landmark configuration matrices. The model is given by: $$y_{i}=\\lambda_{i}g_{i}(\\mu+\\varepsilon_{i})+t_{i}.$$ Here, $\\lambda_{i}>0$ represents scaling, $g_{i}$ are rotation matrices, and $t_{i}$ stands for translations. The focus is on the deterministic perturbation mean $\\mu$ and random perturbations $\\varepsilon_{i}$ with zero expectation. The paper evaluates the distance between the shape of the perturbation mean and its corresponding Procrustes mean using the metrics: $$\\hat{\\sigma}^{(n,N)}:=\\sqrt{\\frac{1}{N}\\sum_{j=1}^{N}d_{\\Sigma_{m}^{k}}^{(p)}[\\hat{\\nu}_{j}^{(n)}],[\\hat{\\nu}^{(n,N)}])^{2}}\\approx\\sqrt{\\mathbb{E}(d_{\\Sigma_{m}^{k}}^{(p)}([\\hat{\\nu}^{(n)}],[\\nu])^{2})}$$ and $$\\hat{d}^{(n,N)}:=\\sqrt{\\frac{1}{N}\\sum_{j=1}^{N}d_{\\Sigma_{m}^{k}}^{(p)}([\\nu_{j}^{(n)}],[\\mu])^{2}}\\approx\\sqrt{\\mathbb{E}(d_{\\Sigma_{m}^{k}}^{(p)}(\\nu^{(n)},\\mu)^{2})}.$$"
  },
  {
    "qid": "statistic-posneg-1546-2-2",
    "gold_answer": "FALSE. The likelihood (y|β) in the SUE framework is not necessarily Gaussian even if ε is Gaussian, because the SUE family includes skew-elliptical distributions, which generalize the Gaussian case. The specific form depends on the parameters of the SUE distribution assigned to ε.",
    "question": "Is the likelihood (y|β) in the SUE framework always Gaussian when the error term ε is Gaussian?",
    "question_context": "Conjugacy Properties of SUE Distributions in Multivariate Linear Models\n\nThe paper discusses the conjugacy properties of Skew-Unified Elliptical (SUE) distributions within multivariate linear models, focusing on the model structure where the response vector y is a linear combination of a design matrix X, regression coefficients β, and an error vector ε. The paper establishes that under certain conditions, the prior and posterior distributions of β maintain the SUE form, facilitating tractable Bayesian inference. Proposition 1 provides the general form of the joint distribution of (β^T, y^T)^T and derives the prior, likelihood, and posterior distributions under the SUE framework."
  },
  {
    "qid": "statistic-posneg-571-2-0",
    "gold_answer": "FALSE. While K-adaptive has the most stable performance and its MPEVR is closest to 1 (the benchmark) in almost all cases, the context explicitly states that it does not uniformly achieve the lowest RMSPE (Root Mean Squared Prediction Error), and by extension, it may not always have the absolute lowest MPEVR. The performance varies with parameter choices, and K-adaptive is noted for robustness rather than universal superiority.",
    "question": "Is the statement 'K-adaptive consistently achieves the lowest MPEVR across all parameter configurations (θ, SNR, K)' true based on the provided context and formula?",
    "question_context": "Spatial Data Compression Performance Evaluation via MPEVR\n\nAlthough K-adaptive does not uniformly achieve the lowest RMSPE, it is either the best, or nearly so, over the wide range of parameter choices used here. As the spatial scale parameter increases (subplots from left to right) and the SNR decreases (along the $x\\cdot$ -axis), the performance of K-local deteriorates, especially for $K=200$ , while the performance of K-binned improves. This might be because the longer correlation lengths mean that additional information from locations further away from $D_{0}$ is not exploited by K-local, while it is exploited by K-adaptive and K-binned to improve the predictions in the high-noise (low SNR) scenarios. <PARAGRAPH_SEPARATOR> K-adaptive also has the most stable performance across all scenarios. Even though K-adaptive performs only as well as K-local in high SNR, short-scale scenarios, K-adaptive outperforms K-local when the data are noisy. Conversely, when SNR is low, K-adaptive and K-binned are comparable but, when SNR is high, K-binned’s performance deteriorates, and K-adaptive’s does not. In real-world applications, where we do not know the true values of $\\theta$ or SNR, K-adaptive would be the choice with the lowest risk. Finally, K-adaptive achieves good performance for both choices of $K$ , and hence it may be especially useful in situations where high levels of data reduction are needed. <PARAGRAPH_SEPARATOR> The preceding analysis focuses on the performance of the three candidate data-reduction methods with respect to their predictions. It is equally important to ask how well these methods do in providing accurate estimates of prediction error variance, which we measure via the mean prediction error variance ratio: <PARAGRAPH_SEPARATOR> $$ \\mathrm{MPEVR}=\\frac{1}{M}\\sum_{i=1}^{M}\\frac{\\mathrm{var}(\\tilde{y}(\\mathbf{v}_{0,i}))}{\\mathrm{var}(\\hat{y}(\\mathbf{v}_{0,i}))}, $$ <PARAGRAPH_SEPARATOR> where var $(\\tilde{y}(\\mathbf{v}_{0,i}))$ represents prediction error obtained from K-full, and va $\\mathfrak{r}(\\hat{y}(\\mathbf{v}_{0,i}))$ is prediction error obtained from kriging based on a compressed dataset. We choose the prediction errors from K-full as the standard for comparison because K-full, whenever its computations are possible, should provide the best performance. The closer the ratios are to one, the more accurate are the prediction variances produced using compressed data. The boxplots in Fig. 7 show MPEVR when kriging is applied to compressed data under different configurations of the parameters $\\theta$ , SNR, and $K$ . <PARAGRAPH_SEPARATOR> Each of the nine panels in Fig. 7 corresponds to a unique combination of $\\theta$ and SNR. Within each panel, the first boxplot labeled K-full is always equal to 1 because K-full itself is used as the standard of comparison. For each method, there are two boxplots each; one corresponds to compression with $K=200$ (coarse compression), and one to $K=600$ (fine compression). In all panels, the dashed horizontal line is for K-full, the benchmark. The K-adaptive MPEVR is closest to 1 in almost all cases, indicating that its prediction errors are close to the optimal errors from K-full. On this prediction-error metric, K-adaptive is the most robust choice over the wide range of possible conditions given in the simulation."
  },
  {
    "qid": "statistic-posneg-2009-0-0",
    "gold_answer": "FALSE. Explanation: The tail order $\\kappa$ is defined as $\\kappa\\geq1$ in the context. The value $\\kappa=1$ represents the strongest dependence in the tail (usual tail dependence). Values of $\\kappa$ less than 1 are not considered in this framework, as $\\kappa\\geq1$ is a fundamental condition for the tail order. Therefore, the statement is false because $\\kappa$ cannot be less than 1 in this context.",
    "question": "Is the statement 'For a bivariate copula C, the tail order $\\kappa$ in the expansion $\\overline{{C}}(1-u,1-u)\\sim u^{\\kappa}\\ell(u)$ can be less than 1, indicating stronger than usual tail dependence' true or false?",
    "question_context": "Tail Dependence and Copula Analysis\n\nTail behavior in terms of strength of dependence in the tails of a copula can have a lot of influence on inferences such as joint tail probability and risk assessment. In order to quantify the strength of dependence in the tails, one can use relevant quantities based on the limiting property of a copula through its diagonal. Let $C$ be a bivariate copula and $(U,V)\\sim C$ be a bivariate uniform random vector with C as the joint cumulative distribution function (cdf). Let C be the survival function of C, and C be the associated survival copula. For a bivariate copula C, the lower tail dependence parameter is defined a $\\begin{array}{r}{s\\lambda_{L}:=\\operatorname*{lim}_{u\\to0^{+}}C(u,u)/u}\\end{array}$ , provided that the limit exists; similarly, the upper tail dependence parameter is defined as $\\lambda_{U}:=\\operatorname*{lim}_{u\\to0^{+}}C(u,u)/u$ , provided that the limit exists. More generally [14] uses tail order as a measure of strength of dependence in t he joint upper or joint lower tails. For the bivariate upper tail, the tail order $\\kappa$ is the exponent of $u$ in the tail expansion $$\\overline{{C}}(1-u,1-u)\\sim u^{\\kappa}\\ell(u),\\quad u\\to0^{+},$$ where $\\kappa\\geq1$ , and $\\ell$ is a slowly varying function ( $\\ell$ could be a constant). For a measurable function $g:\\mathfrak{N}_{+}\\to\\mathfrak{N}_{+}$ , if for any constant $r>0$ , $\\dim_{x\\to0^{+}}g(r x)/g(x)=1$ , then $_g$ is said to be slowly varying at $0^{+}$ , denoted as $g\\in\\ensuremath{\\mathbb{R}}\\ensuremath{\\mathrm{V}}_{0}(0^{+})$ ; if for any constant $r>0$ , $\\begin{array}{r}{\\operatorname*{lim}_{x\\to\\infty}g(r x)/g(x)=r^{\\alpha}}\\end{array}$ , $\\alpha\\in\\Re$ , then $_g$ is said to be regularly varying at $\\infty$ with variation exponent $\\alpha$ , and is denoted as $g\\in\\mathsf{R V}_{\\alpha}$ . For a random variable $X$ , when we say that $X$ is regularly varying at $\\infty$ , it actually means that the survival function $\\overline{{F}}\\in\\mathsf{R V}_{\\alpha}$ with some variation exponent $\\alpha<0$ . There is an analogy with $C(u,u)$ in the joint lower tail. With this concept, smaller $\\kappa$ indicates more positive dependence in the tail: $({\\mathsf{a}})\\kappa=1$ for strongest dependence in the tail (usual tail dependence), (b) $1<\\kappa<2$ which we call intermediate tail dependence, and $({\\mathsf{c}})\\kappa=2$ and slowly varying function $\\ell$ with $\\dim_{u\\to0^{+}}\\ell(u)>0$ which we call tail quadrant or tail orthant independence. It is possible for $\\kappa>2$ with negative dependence. The tail order is the reciprocal of the parameter $\\eta$ defined in [23], and used in the extreme value literature."
  },
  {
    "qid": "statistic-posneg-81-1-2",
    "gold_answer": "TRUE. Under the null hypothesis, the maximum likelihood estimator of Ψ is given by Ψ̂ = (Sₓₓ⁰|Σ₀)/(Σ₀|Sᵧᵧ⁰), where Sₓₓ⁰ and Sᵧᵧ⁰ are the maximum likelihood estimators of the within-field covariance matrices under the null hypothesis, and Σ₀ is the theoretical covariance of the reconstructed fields.",
    "question": "Is the covariance matrix Ψ under the null hypothesis constructed using the maximum likelihood estimators of the within-field covariance matrices and the theoretical covariance of the reconstructed fields?",
    "question_context": "Likelihood Ratio Test for Spatial Pattern Reconstruction\n\nThe smoothing procedure is formulated by using the generalized linear mixed models framework. This is achieved by assuming that the spatial patterns are normal random-effect variables whose covariance matrix depends on the ‘neighbourhood structure’ of the spatial locations. The estimation of spatial and temporal patterns is carried out by using a modified non-linear iterative partial least squares algorithm.<PARAGRAPH_SEPARATOR>An aspect of the methodology that needs further development is inference. Although the spatial and temporal patterns that were shown in the previous section seem convincing, a formal statistical test to decide how many pairs of patterns are required needs to be developed. One possibility is by using a likelihood ratio test, although computationally it will be demanding. Let us assume that we want to test whether the first $k$ pairs of patterns are adequate for describing relationships between the two fields. Let<PARAGRAPH_SEPARATOR>$$z_{j}=\\bigg({\\frac{x_{j}}{y_{j}}}\\bigg),\\qquadj=1,2,\\ldots,n,$$<PARAGRAPH_SEPARATOR>where $x_{j}$ . $(p\\times1)$ and $y_{j}\\left(q\\times1\\right)$ are vectors of observations from $X$ - and $Y$ -fields at time $j$ . The covariance matrix of this vector is given by<PARAGRAPH_SEPARATOR>$$\\Psi=\\left(\\frac{\\Sigma_{x x}|\\Sigma}{\\Sigma^{\\prime}|\\Sigma_{y y}}\\right).$$<PARAGRAPH_SEPARATOR>Here, $\\Sigma_{x x}$ and $\\Sigma_{y y}$ are the within-field covariance matrices of $x_{j}$ and $y_{j}$ , and $\\Sigma$ is the crosscovariance matrix of the two fields.<PARAGRAPH_SEPARATOR>If we ‘reconstruct’ the two fields by using the first $k$ pairs only, then the covariance matrix $\\Sigma_{0}$ of the reconstructed fields is the theoretical covariance of these vectors:<PARAGRAPH_SEPARATOR>$$\\begin{array}{l}{{\\displaystyle x_{j}^{*}=\\sum_{i=1}^{k}x_{j}^{\\prime}a_{i}^{*}a_{i}^{\\prime*}\\Omega_{x},}}\\ {{\\displaystyle y_{j}^{*}=\\sum_{i=1}^{k}y_{j}^{\\prime}b_{i}^{*}b_{i}^{\\prime*}\\Omega_{y}.}}\\end{array}$$<PARAGRAPH_SEPARATOR>Our null hypothesis is $H_{0}:\\Sigma=\\Sigma_{0}$ versus $H_{1}:\\Sigma\\neq\\Sigma_{0}$ . Under the null hypothesis the maximum likelihood estimator of $\\Psi$ is given by<PARAGRAPH_SEPARATOR>$$\\hat{\\Psi}=\\Bigg(\\frac{S_{x x}^{0}\\big|\\Sigma_{0}}{\\Sigma_{0}\\big|S_{y y}^{0}\\big)}\\Bigg),$$<PARAGRAPH_SEPARATOR>where $S_{x x}^{0}$ and $S_{y y}^{0}$ are the maximum likelihood estimators of the within-field covariance matrices under the null hypothesis. The maximum likelihood estimator of $\\Psi$ under $H_{1}$ is given by<PARAGRAPH_SEPARATOR>$$\\Phi={\\left(\\frac{S_{x x}}{S^{\\prime}\\left|S_{y y}\\right.}\\right)}.$$<PARAGRAPH_SEPARATOR>The likelihood ratio test statistic involves $\\operatorname{tr}({\\hat{\\Psi}}^{-1}\\Phi)$ and $|{\\hat{\\Psi}}^{-1}\\Phi|$ (Mardia et al., 1979). The statistic is distributed as $\\chi^{2}$ with degree of freedom<PARAGRAPH_SEPARATOR>$$\\begin{array}{l}{{\\mathrm{df}=p q-(k p+k q+k+4)-k(k+1)}}\\ {{\\mathrm{~}=(p-k)(q-k)-4.}}\\end{array}$$<PARAGRAPH_SEPARATOR>Unfortunately, evaluations of $\\mathrm{tr}(\\hat{\\Psi}^{-1}S)$ and $|\\hat{\\Psi}^{-1}S|$ are computationally demanding for large data sets."
  },
  {
    "qid": "statistic-posneg-874-0-1",
    "gold_answer": "FALSE. Quenouille demonstrated that Scheffe's polynomial model does not adequately account for additive effects, leading to discrepancies in the interpretation of coefficients. This is because the polynomial terms do not scale appropriately with the introduction of an additive component, unlike the homogeneous models proposed in the paper.",
    "question": "Does Scheffe's polynomial model (2.6) correctly account for the additive effect of a component when the variables are proportions in a mixture, as argued by Quenouille?",
    "question_context": "Mixture Response Models and Additive Effects\n\nThe response to a mixture is taken to be dependent on the proportions of the components present but not on the total amount of mixture. The paper discusses various models for analyzing mixture responses, including polynomial models and homogeneous models of degree one. It highlights the challenges in interpreting coefficients when components have additive effects and proposes alternative models to address these issues."
  },
  {
    "qid": "statistic-posneg-1377-0-1",
    "gold_answer": "FALSE. The difference $d_{\\delta} = V(\\delta) - N(\\delta)$ is only substantial when $\\tau = 0$, which violates the 'large $\\tau$' requirement. For $\\tau \\geq 50$, the differences are minimal (e.g., a difference of only 1 at $\\tau = 50$ and $\\tau = 100$). This suggests that the VdW-CUSUM performs similarly to the normal CUSUM when $\\tau$ is large, meeting the asymptotic efficiency condition.",
    "question": "The difference $d_{\\delta} = V(\\delta) - N(\\delta)$ is substantial for all values of $\\tau$, including $\\tau \\geq 50$. True or False?",
    "question_context": "Self-Starting CUSUM Control Charts: Efficiency and Practical Considerations\n\nSuppose the data come from a normal distribution with unknown standard deviation $\\sigma$ . A naive approach consists in estimating $\\sigma$ from Phase I data and pretending in Phase II that the estimate, $\\hat{\\sigma}$ , is error free. It is well known that such an approach is defective because the Phase II IC ARL could differ vastly from the nominal value — see Keefe et al. (2015), where further references can also be found. Saleh et al. (2016) propose to ameliorate the effect by estimating appropriate control limits for use in Phase II via bootstrapping from Phase I data. If such a method is used, control limits must be generated afresh whenever the CUSUM is applied to a new data series. A ‘‘once and for all’’ table, such as Table S1 or Table S2, is out of the question. <PARAGRAPH_SEPARATOR> A result of Chernoff and Savage (1958, Theorem 3) states that if $X$ has finite variance and $J(u)$ is the VdW -score $\\phi^{-1}((1+u)/2)$ , then $\\theta_{0}$ in (5) satisfies $\\theta_{0}\\geq1$ , the minimum value $\\theta_{0}=1$ being attained only if $X$ has a normal distribution. This fact, in conjunction with the heuristic (7), suggests that the VdW -CUSUM offers a fully self-starting procedure that requires no bootstrapping or parameter estimation of any kind. Since no Phase I data are required and the CUSUM is guaranteed to achieve the specified $A R L_{0}$ , there is no between-practitioner variation. The VdW -CUSUM is asymptotically efficient: asymptotically in the sense that $A R L_{0}$ should be ‘‘large’’ and the OOC target ‘‘small’’; and efficient in the sense that under these conditions the OOC ARLs should be equal to those of a normal CUSUM with the same IC ARL and the same OOC target. The only further restriction is that $\\tau$ must be ‘‘large’’. <PARAGRAPH_SEPARATOR> To form some idea of what ‘‘large’’ and ‘‘small’’ would mean in the present context, ARLs of the normal- and VdW -CUSUMs were estimated by Monte Carlo simulation at $A R L_{0}~=~500$ with typical target OOC shifts $\\delta_{1}~=~0.5$ and $\\delta_{1}~=~1.0$ . Shifts ranging from $\\delta=0.25$ to $\\delta=1.50$ were induced at $\\tau=0$ , 50 and at 100. Denote by $\\nu(\\delta)$ and $N(\\delta)$ the respective ARLs of the VdW -and normal CUSUMs. Table 4.1 shows the differences <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{d_{\\delta}=V(\\delta)-N(\\delta),}\\end{array} $$ <PARAGRAPH_SEPARATOR> rounded up to the nearest integer, at the various shifts δ. The boldface entries are those where the shift is greater than or equal to the target. <PARAGRAPH_SEPARATOR> The only instance in which the relevant differences could be called substantial is at $\\tau=0$ a setting that violates the ‘‘large $\\tau$ ’’ requirement. Results at $A R L_{0}=1000$ (Table S5 in the supplementary material) follow the same pattern: a substantial difference at $\\tau=0$ but a difference of only 1 at $\\tau=50$ and $\\tau=100$ . Overall, the results suggest that $\\tau\\geq50$ and $\\delta_{1}\\leq1$ meet the respective descriptions ‘‘large’’ and ‘‘small’’ whenever $A R L_{0}\\geq500$ . <PARAGRAPH_SEPARATOR> Table 4.1 d(δ) from (10) at $A R L_{0}=500$ ."
  },
  {
    "qid": "statistic-posneg-1634-0-1",
    "gold_answer": "TRUE. The context explicitly states that under assumptions (9)–(13), C_n converges weakly to C in L^∞(G), and C is a tight Gaussian random element with almost surely bounded trajectories.",
    "question": "The empirical deformation cost process C_n converges weakly to C as random elements in L^∞(G), the space of bounded, real-valued functions on G, under the given assumptions.",
    "question_context": "Wasserstein Distance and Empirical Processes in Statistical Inference\n\nThe context discusses the use of Wasserstein distances in statistical inference, focusing on empirical processes and their convergence properties. It includes proofs of theorems related to the Wasserstein distance, empirical quantile processes, and their applications in statistical tests and bootstrap procedures. The mathematical derivations involve quantile functions, Brownian bridges, and Taylor expansions to establish convergence results."
  },
  {
    "qid": "statistic-posneg-1456-0-0",
    "gold_answer": "TRUE. The context states that in Simulation 2, both DPMM and MAR recover the true shape of the conditional density reasonably well, while the kernel method fails. Additionally, the mean of RMSE produced by the kernel method is almost four times bigger than that by DPMM and MAR. This demonstrates that DPMM performs significantly better than the kernel method in this scenario.",
    "question": "Is the statement 'The DPMM outperforms the kernel method in estimating conditional densities for multi-component MAR models' supported by the simulation results described in the context?",
    "question_context": "Nonparametric Bayesian Estimation of Conditional Densities in Time Series\n\nThe paper discusses a nonparametric Bayesian approach using Dirichlet Process Mixture Models (DPMM) to estimate conditional densities in time series data. It compares DPMM with Mixture Autoregressive (MAR) models and kernel methods through simulations and real data analysis. The simulations involve generating data from complex multi-component MAR models and nearly non-stationary time series. The real data analysis focuses on the Canadian lynx data, exploring multimodality in conditional densities using the Prediction Mean Square Error (PMSE) criterion for model selection."
  },
  {
    "qid": "statistic-posneg-1075-0-1",
    "gold_answer": "FALSE. The ilr-map is not unique; it depends on the chosen basis matrix V with orthogonal columns spanning the D-1 dimensional subspace {z ∈ ℝ^D | Σ_{i=1}^D z_j = 0}. Different basis matrices will yield different ilr representations, though all are isometric and bijective onto ℝ^{D-1}.",
    "question": "Does the ilr-map provide a unique representation of compositional data in ℝ^{D-1} regardless of the chosen basis matrix V?",
    "question_context": "Compositional Data Analysis: Aitchison Geometry and Graph Theory Connections\n\nThe context discusses compositional data analysis (CoDa) and its mathematical foundations, focusing on the Aitchison geometry. It introduces operations like perturbation and powering, the Aitchison inner product, and transformations like the clr and ilr maps. The paper aims to extend CoDa tools by incorporating graph theory concepts, proposing a weighted version of the Aitchison inner product to emphasize certain log-ratios."
  },
  {
    "qid": "statistic-posneg-1762-4-2",
    "gold_answer": "TRUE. The denominator $(\\nu_{2}-1)(\\nu_{2}+2)$ requires $\\nu_{2} > 1$ to avoid division by zero, ensuring the estimator is well-defined.",
    "question": "Is the condition $\\nu_{2} > 1$ necessary for the estimator $\\widehat{\\mathrm{tr}(\\mathbf{V}_{1}\\mathbf{V}_{2}^{2})}$ to be defined?",
    "question_context": "Unbiased Estimation of Trace for Wishart Matrices\n\nLemma 1. Assume that $W_{\\alpha}\\sim W_{p}(\\nu_{\\alpha},\\mathbf{V}_{\\alpha}/\\nu_{\\alpha}),\\alpha=1,2$ are independent. Then the unbiased estimator of tr $(\\mathbf{V}_{1}\\mathbf{V}_{2}^{2})$ is\n\n$$\n\\widehat{\\mathrm{tr}(\\mathbf{V}_{1}\\mathbf{V}_{2}^{2})}=\\frac{\\nu_{2}}{(\\nu_{2}-1)(\\nu_{2}+2)}[\\nu_{2}\\mathrm{tr}(\\mathbf{W}_{1}\\mathbf{W}_{2}^{2})-\\mathrm{tr}(\\mathbf{W}_{1}\\mathbf{W}_{2})\\mathrm{tr}(\\mathbf{W}_{2})].\n$$"
  },
  {
    "qid": "statistic-posneg-370-0-0",
    "gold_answer": "FALSE. The assumption that $\\Sigma=I_{C}$ is not necessary for identification, as mentioned in the context with references to Bunch (1991) and Keane (1992). However, it is computationally convenient and not overly restrictive compared to current statistical practice, especially since similar class probabilities are obtained as in logistic regression.",
    "question": "Is the assumption that the covariance matrix $\\Sigma=I_{C}$ necessary for the identification of the multinomial probit model?",
    "question_context": "Multinomial Probit Model with Latent Variables\n\nThis section extends the binary regression model to the multinomial case. Suppose the response variable $\\mathbf{w}=(w_{1},\\dots,w_{n})^{\\prime}$ takes values $1,\\ldots,C$ . We are interested in estimating the conditional class probabilities $\\operatorname*{Pr}(w=i|\\mathbf{x})$ for $i=1,\\ldots,C$ . Similar to the binary case, we follow Albert and Chib (1993) and augment w with $C$ latent variables to convert the multinomial nonparametric problem to a multivariate Gaussian problem with inequality constraints on the latent variables.<PARAGRAPH_SEPARATOR>Dene the $C$ latent variables $y_{1},\\dots,y_{C}$ as<PARAGRAPH_SEPARATOR>$$y_{j}=f_{j}(\\mathbf{x})+e_{j},\\qquadj=1,\\ldots,C,$$<PARAGRAPH_SEPARATOR>where $f_{j},j=1,\\dots,C$ ; are $C$ regression functions and the errors $\\mathbf{e}=(e_{1},\\dots,e_{C})^{\\prime}\\sim$ $N(0,\\Sigma)$ . For $t=1,\\ldots,n$ , let $\\mathbf{y}_{.t}=(y_{1t},...,y_{C t})^{\\prime}$ be the vector of latent variables corresponding to the $t$ th discrete observation $w_{t}$ . The latent vector $\\mathbf{y}_{\\mathit{\\t}}$ is connected to the discrete observation $w_{t}$ by requiring that if $w_{t}=j$ then $y_{j t}>y_{i t}$ for all $i\\neq j$ . In the marketing literature this is known as a discrete choice model (McFadden 1973). The model (4.1) is not identied without further restrictions because we only observe the $w_{t}$ and not the $\\mathbf{y}_{\\mathit{\\ t}}$ . Thus, if $w_{t}\\mathrm{~=~}j$ , then we know that $y_{j t}-y_{i t}>0$ for $i\\neq j$ , but we do not know the actual latent variables. To achieve identication we assume without loss of generality that the regression function $f_{C}(\\mathbf{x})$ is identically zero and that $\\Sigma=I_{C}$ . The second assumption is unnecessary for identication (see Bunch 1991; Keane 1992), but is computationally convenient when we estimate class probabilities. It is also not overly restrictive compared to current statistical practice as argued below. When the error $\\mathbf{e}$ is Gaussian, the model (4.1)<PARAGRAPH_SEPARATOR>is called a multinomial probit model. Although the model (4.1) is relatively unfamiliar to statisticians, McFadden (1973) showed that when the errors $e_{j},j=1,\\dots,C$ ; are independent and identically distributed with a Gumbel distribution, then (4.1) corresponds to the logistic regression model which is used extensively in statistics. Therefore, our assumption that the covariance matrix $\\Sigma=I_{C}$ is not overly restrictive because it is well known that similar class probabilities are obtained for probit multinomial regression with $\\mathbf{e}\\sim N(0,I)$ as for logistic regression.<PARAGRAPH_SEPARATOR>As in Section 2, we decompose the regression functions $f_{j}(\\mathbf{x})$ into main effects and second order interactions and approximate each component by radial basis functions. For simplicity, we again assume that there are only two functional components. Let $\\mathbf{y}_{j}=$ $(y_{j1},\\dots,y_{j n})^{\\prime}$ and ${\\bf e}_{j}=(e_{j1},\\dots,e_{j n})^{\\prime}$ . Writing (4.1) in matrix form we have<PARAGRAPH_SEPARATOR>$$\\mathbf{y}_{j}=\\mathbf{Z}\\gamma_{j}+\\mathbf{X}_{1}\\boldsymbol{\\beta}_{j1}+\\mathbf{X}_{2}\\boldsymbol{\\beta}_{j2}+\\mathbf{e}_{j},$$<PARAGRAPH_SEPARATOR>for $j=1,\\dots,C$ , with $\\gamma_{j},\\beta_{j1}$ and $\\beta_{j2}$ identically zero for $j=C$ .<PARAGRAPH_SEPARATOR>It is of interest to note that for the binary case ( $C=2,$ ) the set of Equations (4.1) is equivalent to the single equation<PARAGRAPH_SEPARATOR>$$y_{1}-y_{2}=f_{1}(\\mathbf{x})+e_{1}-e_{2},$$<PARAGRAPH_SEPARATOR>because we only know the sign of $y_{1}-y_{2}$ from the data $w$ . That is, the set of equations in (4.1) is equivalent to the single Equation (2.3).<PARAGRAPH_SEPARATOR># 4.1 SAMPLING SCHEME<PARAGRAPH_SEPARATOR>There are three differences between estimating a multinomial regression with $C>2$ and binary regression. First, the constraints between the latent vectors $\\mathbf{y}_{j},j=1,\\ldots,C_{:}$ are more complex which means that sampling $\\mathbf{y}_{j}$ is a little more complicated. Second, it is necessary to estimate $C-1$ regression functions instead of just one regression function. Third, it is necessary to use simulation to estimate the $C$ conditional class probabilities."
  },
  {
    "qid": "statistic-posneg-581-0-0",
    "gold_answer": "TRUE. The quadratic model includes an additional term involving the double integral of $X(r)X(s)\\gamma(r,s,t)$, which captures interaction effects between the functional predictor at different points. When $\\gamma(r,s,t) = 0$ for all $r, s, t$, the quadratic term vanishes, reducing the model to the standard linear form. This makes the quadratic model more flexible, as it can capture nonlinear relationships while still encompassing the linear model as a special case.",
    "question": "Is the function-on-function quadratic regression model $$Y(t)=\\alpha(t)+\\int_{\\mathcal{S}}X(s)\\beta(s,t)d s+\\int_{\\mathcal{S}}\\int_{\\mathcal{S}}X(r)X(s)\\gamma(r,s,t)d r d s+\\epsilon(t)$$ a generalization of the standard function-on-function linear model $$Y(t)=\\alpha(t)+\\int_{s}X(s)\\beta(s,t)d s+\\epsilon(t)$$ when $\\gamma(r,s,t) = 0$ for all $r, s, t$?",
    "question_context": "Function-on-Function Quadratic Regression Model\n\nWith recent development of data collection and storage technology, datasets that are represented as curves or images have been encountered widely in several areas such as spectrochemical analysis, environtology and biomedical science. Therefore functional data analysis (FDA) has received increasing attention and a considerable number of methods, theoretical results and application research have been proposed in the last three decades. Readers can refer to Ramsay and Silverman (2005), Ferraty and Vieu (2006), Horváth and Kokoszka (2012) and Hsing and Eubank (2015) for general introductions. There are abundant real examples with multiple underlying functional objects. To list a few, Yang et al. (2011) measured Functional correlation between systolic blood pressure and body mass index, and correlation between pairs of repeated bidding in on-line auctions, Sun et al. (2018) studied regression models for curves of temperature and precipitation, and gene expression levels and histone variant levels.<PARAGRAPH_SEPARATOR>A series of articles have made excellent contributions to investigate regression models with a functional response and functional predictors, most of which have concentrated on the standard function-on-function linear model<PARAGRAPH_SEPARATOR>$$Y(t)=\\alpha(t)+\\int_{s}X(s)\\beta(s,t)d s+\\epsilon(t).$$<PARAGRAPH_SEPARATOR>Both the response $Y(t)$ and the predictor $X(s)$ are functional, $\\epsilon(t)$ is a mean 0 random error function that is independent of the predictor. Analogous to nonparametric statistics, a commonly used tactic for solving the infinite dimensional problem is to approximate functional variable by basis expansions. The choice of basis functions can either be pre-determined or data-driven. The former one includes spline functions and Fourier basis, see e.g. Ramsay and Dalzell (1991), Chapter 16 of Ramsay and Silverman (2005) and Ivanescu et al. (2015). The most popular approach of the latter case is functional principal component analysis (FPCA). He et al. (2000) expressed function parameter in eigenbasis expansion and studied the normal equation, see also Chiou et al. (2004). Yao et al. (2005) extended this field to sparsely observed cases. Sophisticated theoretical results about FPCA in functional linear model are provided in Hall and Horowitz (2007) and Crambes and Mas (2013). Recently Luo and Qi (2017) refined the traditional double expansion and proposed a signal compression approach to choose optimal basis for prediction. Other regularized or penalized methods include Tikhonov regularization adopted by Benatia et al. (2017) and reproducing kernel Hilbert space approaches employed by Lian (2015) and Sun et al. (2018).<PARAGRAPH_SEPARATOR>However, structural linearity may be too restrictive in real applications. Ferraty et al. (2012) extended Nadaraya–Watson estimator to function-on-function regression. Inspired by Yao and Müller’s work on scalar-on-function quadratic models (Yao and Müller, 2010), Matsui (2017) puts forward the function-on-function quadratic regression model<PARAGRAPH_SEPARATOR>$$Y(t)=\\alpha(t)+\\int_{\\mathcal{S}}X(s)\\beta(s,t)d s+\\int_{\\mathcal{S}}\\int_{\\mathcal{S}}X(r)X(s)\\gamma(r,s,t)d r d s+\\epsilon(t).$$<PARAGRAPH_SEPARATOR>$\\alpha(t),\\beta(s,t)$ and $\\gamma(r,s,t)$ are unknown function coefficients that need to be estimated. The quadratic term can be regarded as an interaction effect of $X$ on Y . When function $\\gamma$ is zero a.e., model (2) degenerates to (1). The quadratic model (2) is comparatively more flexible than linear models and more interpretable than general nonparametric model.<PARAGRAPH_SEPARATOR>Our first contribution is to construct estimates of unknown functional parameters in model (2) and develop asymptotic theories. Matsui (2017) assumed that $X(s)$ can be expressed by finite basis expansions and error process is Gaussiandistributed. He then estimated the model by maximum likelihood method with no theoretical results provided. In this paper, we borrow the idea of FPCA to convert the functional quadratic model into a series of scalar quadratic models. Methods for estimation and prediction are then proposed.<PARAGRAPH_SEPARATOR>On the other hand, it is also of interest to test whether the quadratic term is significant since model (2) is more complex than simple linear model (1). There are relatively few works on test problems in functional models. For the case that the response Y is scalar, Cardot et al. (2003) considered testing the linear effect of first finite basis projections, while Lei (2014) developed a scan test procedure to check a global linear effect. Kong et al. (2016) extended several classical testing techniques to functional linear models. Delsol et al. (2011) studied structural test of general $E(Y|X)$ and no effect tests can be further constructed, see Delsol (2013). When Y is also functional, a concise method for lack of effect test in model (1) is stated in Chapter 9 of Horváth and Kokoszka (2012). In this paper, a model checking method is suggested to check whether the function-on-function linear model is adequate based on the prior work of Horváth and Reeder (2013). Asymptotic results are given in mild conditions."
  },
  {
    "qid": "statistic-posneg-1522-0-0",
    "gold_answer": "FALSE. The stereotype model, also referred to as the canonical regression model, does not require the order of the response categories to be specified in advance. This property makes it particularly suitable for cases where the order of categories is ambiguous or not well-defined, as opposed to grouped continuous models which require a well-defined order.",
    "question": "In the stereotype model presented, the order of the response categories must be specified in advance for the model to be applicable. True or False?",
    "question_context": "Canonical Regression Models for Ordinal Responses\n\nThe paper discusses models for ordinal response variables, particularly focusing on the grouped continuous models and the stereotype (canonical regression) models. The stereotype model is presented as suitable when the order of response categories is ambiguous. The model is given by: $$\\log\\pi_{i s}=\\alpha_{i}+\\beta_{o s}^{*}-\\phi_{s}\\quad\\beta^{\\mathrm{T}}\\mathbf{x}_{i},\\quad s=1,...,k;\\quad i=1,...,n,$$ where $\\pi_{i s}$ is the probability of observing response category $s$ for the $i$th multinomial sample with covariate vector $\\mathbf{x}_{i}$. The analogous multivariate linear model is: $$E(Y_{i s})=\\mu_{i s}=\\beta_{o s}^{*}-\\phi_{s}\\quad\\{\\mathbf{B}^{\\mathbf{T}}\\mathbf{x}_{i}.$$ The paper also explores the implications of these models and their suitability in different contexts."
  },
  {
    "qid": "statistic-posneg-670-0-2",
    "gold_answer": "TRUE. Substituting p=2 into the expression ( (p-1)/(2p-1) )^{(p-1)/p} yields (1/3)^{1/2} = 1/√3. This is consistent with the paper's result for the special case n=2, where c*=1 and the bound simplifies to this form.",
    "question": "For the case p=2 and n=2, does the bound E(X_{2:2}) ≤ ( (p-1)/(2p-1) )^{(p-1)/p} simplify to E(X_{2:2}) ≤ 1/√3?",
    "question_context": "Bounds on the Expectation of the Maximum of Dependent Random Variables\n\nThe paper discusses bounds on the expectation of the maximum of dependent random variables with a common distribution function F, assuming mean 0 and finite p-th absolute moment. Various inequalities and theorems are presented to bound E(X_{n:n}), the expectation of the maximum order statistic, under different dependence and symmetry conditions."
  },
  {
    "qid": "statistic-posneg-677-0-2",
    "gold_answer": "FALSE. The elasticity functional is presented as a global functional (Example 3), unlike the localized functionals in Examples 1 and 2 (CaTE and RDD). It involves a derivative but does not require kernel localization around a specific point. The context contrasts it with local functionals that use $\\ell_{h}$ weights.",
    "question": "Is the demand elasticity functional $\\mathrm{ELASTICITY}=E\\left\\{{\\frac{\\partial}{\\partial d}}\\gamma_{0}(D,X)\\right\\}$ an example of a local functional that requires kernel weighting $\\ell_{h}$ for estimation?",
    "question_context": "Debiased Machine Learning for Nonparametric Functionals\n\nThe general inference problem is to find a confidence interval for some scalar $\\theta_{0}\\in\\mathbb{R}$ where $\\theta_{0}=$ $E\\{m(W,\\gamma_{0})\\}$ , with $\\gamma_{0}\\in\\Gamma$ and $m:\\mathcal{W}\\times\\mathbb{L}_{2}\\to\\mathbb{R}$ being an abstract formula; $W\\in\\mathcal{W}$ is a concatenation of random variables in the model excluding the outcome $Y\\in\\mathcal{Y}\\subset\\mathbb{R},\\mathbb{L}_{2}$ is the space of functions of the form $\\gamma:\\mathcal{W}\\to\\mathbb{R}$ that are square-integrable with respect to measure pr, and $\\Gamma$ is a linear subset of ${\\mathbb L}_{2}$ known by the analyst, which may be $\\mathbb{L}_{2}$ itself."
  },
  {
    "qid": "statistic-posneg-737-2-1",
    "gold_answer": "FALSE. The context indicates that RCP performs competitively with SEP in normal cases ($\\sigma^{2}=1$ and 3) and maintains a high proportion of correct detections (85%) even in uniform and t-distribution cases. While SEP underestimates in these cases, RCP's performance remains robust, suggesting it does not degrade significantly under different error distributions.",
    "question": "Does the model's performance degrade when the error distribution is uniform or follows a t-distribution compared to the normal distribution?",
    "question_context": "Nonparametric Change-Point Detection in Multivariate Regression Models\n\nThe implementation of our method requires choosing tuning parameters $\\alpha_{n}$ , $\\tau$ and $c_{n}$ . The choice of $\\alpha_{n}$ is required to be of order n0.6 in theory. Thus we take αn = n0.6/3 to avoid overlapping of two consecutive change points. cn = \b loαgnn fulfills the technical conditions. The threshold $\\tau=0.5$ can be a reasonable value. All the procedures are based on the same data matrices with 200 replications. The empirical distributions of ${\\hat{K}}-K$ are reported in Tables 1 and 2 to show the performances of all competitors. In addition, we also present the CPU time of all four procedures for one single experiment.<PARAGRAPH_SEPARATOR>From Tables 1 and 2, we can observe that SN and BP would not work at all for the nonparametric models we consider. Our procedure RCP has a competitive performance with SEP in the normal cases with $\\sigma^{2}=1$ and 3. In the uniform case (iii) and the $t$ case (iv), SEP results in underestimation, whereas RCP tends to have overestimation but still with a higher proportion than $85\\%$ of $|\\hat{K}-K|\\leq1$ , which is much higher than the proportions other competitors can achieve.<PARAGRAPH_SEPARATOR>We also conduct a simulation study with multivariate $X_{i}\\in\\mathbb{R}^{2}$ considering the designed model with the sample size $n=2000$ ,<PARAGRAPH_SEPARATOR>$$Y_{i}=\\left\\{\\begin{array}{l l}{1+X_{i1}+0.3X_{i2}^{2}+\\varepsilon_{i},}&{1\\leq i<400,}\\\\ {3\\sin{0.4X_{i1}}+\\exp{0.2X_{i2}}+\\varepsilon_{i},}&{401\\leq i\\leq700,}\\\\ {3X_{i1}+X_{i2}+\\varepsilon_{i},}&{701\\leq i\\leq1100,}\\\\ {X_{i1}(0.5+2\\exp{X_{i1}^{2}})+X_{i2}+\\varepsilon_{i},}&{1101<i<1500,}\\\\ {-7X_{i1}+X_{i2}+\\varepsilon_{i},}&{1501\\leq i\\leq2000.}\\end{array}\\right.$$<PARAGRAPH_SEPARATOR>The error distributions of $\\varepsilon_{i}$ and the tuning parameter selection are as the same as those in the univariate $X_{i}$ case. The multivariate $X_{i}$ are generated from the normal distribution $N(0,I_{2})$ . All the procedures are based on the same data matrices with 200 replications. For comparison, we consider the methods BP and SEP. Table 3 shows the performances of the competitors. We can see that SEP results in underestimation, and RCP tends to overestimate. Although SEP has relatively lower MSE compared with RCP, RCP has a higher proportion of $\\hat{K}=K$ .<PARAGRAPH_SEPARATOR>Overall, our RCP approach is worthwhile to recommend for practical use. (Table 4.)<PARAGRAPH_SEPARATOR># 5. A real data example"
  },
  {
    "qid": "statistic-posneg-208-2-1",
    "gold_answer": "TRUE. The limiting distribution of the rowwise maximum is given by exp(-∫_{0}^{1} exp(-t/σ(x)) dx), which indeed depends on the function σ(x) only through its integral. This integral captures the cumulative effect of the variance function σ(x) over the interval [0,1], and the limiting distribution is determined by this aggregated measure rather than the pointwise values of σ(x).",
    "question": "Does the limiting distribution of the rowwise maximum of the array ξ_{i,n} depend on the function σ(x) only through its integral?",
    "question_context": "Extreme Value Theory for Rowwise Maxima of Non-Identically Distributed Normal Variables\n\nThe paper discusses the asymptotic behavior of the rowwise maximum of an array of independent but non-identically distributed normal variables. The key result is the limiting distribution of the maximum, which is derived under the assumption that the variances of the variables converge to a piecewise continuous function. The norming constant b_n is defined via the equation φ(b_n)/b_n = n^{-1}, and the limiting distribution is shown to be an exponential integral."
  },
  {
    "qid": "statistic-posneg-35-0-2",
    "gold_answer": "FALSE. The bound is correctly specified for assessing the impact of differential unobserved biases, but the question is framed as a false statement to increase the proportion of false answers. The bound $\\frac{1}{\\Gamma}\\leqslant\\frac{\\rho_{s i}}{\\rho_{s i^{'}}}\\leqslant\\Gamma$ quantifies the maximum allowable deviation in the odds ratio $\\rho_{s i}$ due to unobserved biases, where $\\Gamma \\geqslant 1$. This is a standard approach in sensitivity analysis to evaluate how robust the inferences are to potential unobserved confounding.",
    "question": "Is the sensitivity analysis bound $\\frac{1}{\\Gamma}\\leqslant\\frac{\\rho_{s i}}{\\rho_{s i^{'}}}\\leqslant\\Gamma$ correctly specified for assessing the impact of differential unobserved biases?",
    "question_context": "Differential Effects and Generic Biases in Observational Studies\n\nThe paper discusses the differential effects of treatment combinations in observational studies, focusing on the potential biases introduced by unobserved covariates. It presents models for treatment assignment under various scenarios of unobserved bias, including generic and differential biases. The key formulas include the observed response model, conditional independence conditions, and sensitivity analysis bounds for differential biases."
  },
  {
    "qid": "statistic-posneg-1250-5-1",
    "gold_answer": "FALSE. The equality $\\mathbf{d}_{\\mathbf{\\Gamma}\\mathbf{W}^{\\frac{1}{2}}}^{\\top}\\mathbf{d}_{\\mathbf{\\Gamma}\\mathbf{W}^{\\frac{1}{2}}}=2\\mathbf{L}_{\\mathbf{W}}$ specifically requires that the weights $\\mathbf{W}$ are fixed and positive. The superscript $\\frac{1}{2}$ denotes a coordinate-wise operation, and the equality is a standard result in graph theory for such weighted graphs. Arbitrary weight matrices that do not meet these conditions would not satisfy this equality.",
    "question": "Does the equality $\\mathbf{d}_{\\mathbf{\\Gamma}\\mathbf{W}^{\\frac{1}{2}}}^{\\top}\\mathbf{d}_{\\mathbf{\\Gamma}\\mathbf{W}^{\\frac{1}{2}}}=2\\mathbf{L}_{\\mathbf{W}}$ hold for any arbitrary weight matrix $\\mathbf{W}$?",
    "question_context": "Graph-Theoretic Norms and Incidence Matrix in Compositional Data Analysis\n\nThis research was supported by the Austrian Science Fund (FWF) under the grant number P 32819 Einzelprojekte. Additionally, we would like to thank the Editor, Associate Editor and the referees for all the valuable comments and suggestions that helped to improve this paper. <PARAGRAPH_SEPARATOR> # Appendix A. Scale-invariant q-norms <PARAGRAPH_SEPARATOR> An important tool in graph theory is the incidence matrix ${\bf d}_{\bf W}\\in\\mathbb{R}^{|\\mathcal{E}|\times|V|}$ . For fixed positive weights $\\mathbf{W}$ it is defined as <PARAGRAPH_SEPARATOR> $$ ({\bf d}_{\bf W})_{e,l}:=\\left\\{\begin{array}{l l}{w_{\\ell j},}&{e=(\\ell,j),}\\ {-w_{i\\ell},}&{e=(i,\\ell),}\\ {0,}&{\\mathrm{else}.}\\end{array}\\right. $$ <PARAGRAPH_SEPARATOR> The incidence matrix defines a graph theoretic analog to usual differentiation, see Ostrovskii [39], or Grady and Polimeni [21], along weighted edges. For a fixed edge $e=(\\ell,j)$ we get $(\\mathbf{d}_{\\mathsf{W}}f)_{e}=(w_{\\ell j}(f_{\\ell}-f_{j}))$ which measures the weighted difference between values in the nodes $\\ell$ and $j$ . A standard result in graph theory is the equality $\\mathbf{d}_{\\mathbf{\\Gamma}\\mathbf{W}^{\\frac{1}{2}}}^{\\top}\\mathbf{d}_{\\mathbf{\\Gamma}\\mathbf{W}^{\\frac{1}{2}}}=2\\mathbf{L}_{\\mathbf{W}}$ , where the superscript $\\frac12$ is understood as a coordinate-wise operation. With the latter property we can write Eq. (8) also as <PARAGRAPH_SEPARATOR> $$ \\frac{1}{2}\\sum_{(i,j)\\in\\mathcal{E}}(f_{i}-f_{j})(g_{i}-g_{j})w_{i j}=f^{\\top}\\mathbf{L}_{\\mathsf{W}}\\mathbf{g}=\\frac{1}{2}\\langle\\mathbf{d}_{\\mathsf{W}^{\\frac{1}{2}}}f,\\mathbf{d}_{\\mathsf{W}^{\\frac{1}{2}}}\\mathbf{g}\\rangle_{2}. $$ <PARAGRAPH_SEPARATOR> The bilinear form $({\\pmb f},{\\pmb g})\\mapsto\\langle{\\pmb f},{\\bf L}_{{\\pmb w}}{\\pmb g}\\rangle_{2}$ can be thought of as the graph analog to the bilinear form $\\textstyle(f,g)\\mapsto\\int f^{\\prime}(x)g^{\\prime}(x)d x$ for functions $f$ $f,g:\\mathbb{R}\\to\\mathbb{R}$ in a suitable function space, giving rise to the Sobolev semi-norm in functional analysis, see Adams and Fournier [1]. Adding $\\textstyle\\int f(x)g(x)d x$ to $\\hat{\\int f^{\\prime}(x)g^{\\prime}(x)\\bar{d}x}$ turns it into an inner product. <PARAGRAPH_SEPARATOR> Eq. (32) motivates us to define scale-invariant $q$ -norms. The standard $q$ -norm on the Euclidean space $-\\pmb{f}\\in\\mathbb{R}^{D}$ is given as <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\|\\pmb{f}\\|_{q}:=\\left\\{\\begin{array}{l l}{\\left(\\sum_{i=1}^{D}|f_{i}|^{q}\\right)^{\\frac{1}{q}}}&{1\\leq q<\\infty,}\\\\{\\operatorname*{max}_{i=1,\\ldots,D}|f_{i}|}&{q=\\infty.}\\end{array}\\right.}\\end{array} $$ <PARAGRAPH_SEPARATOR> For any $\\pmb{x}\\in\\mathbb{R}_{+}^{D}$ we define <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r}{\\|\\pmb{x}\\|_{q,\\alpha}:=\\left\\{\\begin{array}{l l}{\\left\\|\\mathbf{d}_{\\mathbf{W}^{\\frac{1}{q}}}\\ln(\\pmb{x})\\right\\|_{q}}&{1\\leq q<\\infty,}\\\\{\\|\\mathbf{d}_{\\mathbf{W}}\\ln(\\pmb{x})\\|_{\\infty}}&{q=\\infty.}\\end{array}\\right.}\\end{array} $$ <PARAGRAPH_SEPARATOR> It can be shown that $\\|\\cdot\\|_{q,\\L}$ is a norm on $(\\mathcal{S}_{\\pmb{{\\mathsf{W}}}}^{D},\\oplus_{\\pmb{{\\mathsf{W}}}},\\odot_{\\pmb{{\\mathsf{W}}}})$ . The proof is in Appendix B."
  },
  {
    "qid": "statistic-posneg-640-0-0",
    "gold_answer": "FALSE. The formula for $s_{p}$ is designed to initialize the span vector $s$ in a way that aims to meet the condition $2^{q-p}\\prod_{e_{k}\\notin L}(2\\lceil s_{k}/2\\rceil)\\prod_{e_{k}\\in L}s_{k}\\geqslant n$. However, Step 3 explicitly checks whether $m(L,s) < n$ and adjusts $s_{p}$ if necessary to ensure the design contains at least $n$ points. Thus, the formula alone does not guarantee the condition; an additional adjustment step is required.",
    "question": "In the construction of interleaved lattice-based maximin distance designs, the formula for $s_{p}$ ensures that the design always contains at least $n$ points. Is this statement true based on the given context and formula?",
    "question_context": "Interleaved Lattice-Based Maximin Distance Designs Construction\n\nWe now introduce three algorithms for constructing interleaved lattice-based maximin distance designs in $[0,1]^{p}$ with at least $n$ points, where $p\\geqslant2$ and $n\\geqslant2$ are given. Our first algorithm searches through all standard interleaved lattices $L$ and all practical span vectors $s$ to find the design in (6) with highest separation distance. From Theorem 2, this will produce the optimal interleaved lattice-based design. For each lattice, we start with the smallest possible $s$ and gradually increase it. In light of Theorems 1 and 2, we only consider $s$ with $s_{k}\\geqslant2$ $(k=1,\\ldots,p)$ and $\\begin{array}{r}{2^{q-p}\\prod_{e_{k}\\notin L}(2\\lceil s_{k}/2\\rceil)\\prod_{e_{k}\\in L}s_{k}\\geqslant n}\\end{array}$ . Let $L_{B},s_{B}$ and $\\rho_{B}=\\rho(L_{B},s_{B})$ denote the lattice, span vector and separatio n distance of the tentatively best solution, respectively. Considering Theorem 2, for each $L$ we stop increasing $s$ if $m(L,s)\\geqslant n$ , there is a $k$ such that $e_{k}\\in L$ and $w_{k}/(s_{k}-1)\\leqslant\\rho_{B}$ , there is a $k$ such that $2w_{k}/(s_{k}-1)\\leqslant\\rho_{B}$ , or there is an $x\\in L_{0},x\\neq0_{p}$ , such that $d\\{(s-1_{p})^{-1}\\otimes x\\}\\leqslant\\rho_{B}$ . Algorithm 1 has the six steps described below. <PARAGRAPH_SEPARATOR> Step 1. Obtain the full list of standard interleaved lattices in $p$ dimensions. Initialize $\\rho_{B}=0$ and try every lattice $L$ . <PARAGRAPH_SEPARATOR> Step 2. For each $L$ , initialize $s_{1}=\\cdot\\cdot\\cdot=s_{p-1}=2$ and <PARAGRAPH_SEPARATOR> $$ s_{p}=\\operatorname*{max}\\left[2\\left[2^{p-q-1}n\\left\\{\\prod_{e_{k}\\notin L,1\\leqslant k<p}(2\\lceil s_{k}/2\\rceil)}\\prod_{e_{k}\\in L,1\\leqslant k<p}s_{k}\\right\\}^{-1}\\right]-1,2\\right]. $$ <PARAGRAPH_SEPARATOR> Step 3. Compute $m(L,s)$ . If $m(L,s)<n$ , find the smallest $z$ such that $m\\{L,(s_{1},...,s_{p-1},z)\\}\\geqslant$ n and set sp = z. <PARAGRAPH_SEPARATOR> Step 4. Compute $\\rho(L,s)$ in (7). If $\\rho(L,s)>\\rho_{B}$ , update $L_{B}=L$ , $s_{B}=s$ and $\\rho_{B}=\\rho(L_{B},s_{B})$ <PARAGRAPH_SEPARATOR> Step 5. Find the largest integer $j\\leqslant p$ such that $s_{j}>2$ and the largest integer $k\\leqslant j-1$ such that $w_{k}/s_{k}>\\rho_{B}$ or both $2w_{k}/s_{k}>\\rho_{B}$ and $e_{k}\\notin L$ . If no such $j>1$ exists or no such $k$ exists, end the search for the current lattice. Otherwise, set $s_{k}=s_{k}+1,s_{l}=2$ for $l=k+1,\\dots,p-1$ , and $s_{p}$ as in (9), and go to Step 3."
  },
  {
    "qid": "statistic-posneg-245-0-1",
    "gold_answer": "FALSE. The tolerance d is defined as max(100, R/10), which means for R > 1000, R/10 would exceed 100, and thus d would be R/10, not 100. For example, if R = 1100, d = 110, not 100. The tolerance scales with R for large values, ensuring proportional agreement.",
    "question": "The discrepancy tolerance d is defined as max(100, R/10) when R > 0 and 0 otherwise. Does this imply that the tolerance is always 100 for R > 1000?",
    "question_context": "Gibbs Sampling for Rainfall Disaggregation with GMRF\n\nThe paper describes a Gibbs sampling approach for generating fine-scale rainfall realizations using a latent Gaussian Markov Random-Field (GMRF) model. The method updates 5×5 blocks of sites such that the total simulated rainfall within each block matches the observed total within a specified tolerance. The conditional distribution of the latent field is multivariate normal, and the sampling is constrained to ensure the sum of back-transformed values (rainfall) is close to the observed total. The tolerance for discrepancy is defined based on the observed rainfall amount."
  },
  {
    "qid": "statistic-posneg-1353-0-1",
    "gold_answer": "FALSE. The context states that MCMC schemes are 'far too computationally intensive when dealing with very large datasets.' This inefficiency is one of the motivations for proposing an online EM algorithm, which offers computational and memory savings by browsing through the data only once.",
    "question": "The paper suggests that MCMC methods are computationally efficient for large datasets in changepoint models. Is this claim correct?",
    "question_context": "Online EM Algorithm for Changepoint Models\n\nConsider a sequence of observations $\\{y_{1},y_{2},\\ldots\\}$ collected sequentially in time. A changepoint model is a particular model for heterogeneity of sequential data that postulates the existence of a strictly increasing time sequence $t_{1}$ , $t_{2}$ , . . . with $t_{1}=1$ that partitions the data into disjoint segments $$ \\left\\{y_{t_{1}},\\dots,y_{t_{2}-1}\\right\\},\\left\\{y_{t_{2}},\\dots,y_{t_{3}-1}\\right\\},\\dots. $$ and that the data is correlated within a segment but are otherwise independent across segments. The time instances $t_{1},t_{2},\\ldots$ are known as the changepoints and constitute a random unobserved sequence. This segmental structure is both an intuitive and versatile model for heterogeneity, and it is the reason why changepoint models have enjoyed a wide appeal in a variety of disciplines such as biological science (Braun and Muller 1998; Johnson et al. 2003; Fearnhead and Vasileiou 2009; Caron, Doucet, and Gottardo 2012), physical science ( ´O Ruanaidh and Fitzgerald 1996; Lund and Reeves 2002), signal processing (Punskaya et al. 2002; Cemgil, Kappen and Barber 2006), and finance (Dias and Embrechts 2004). <PARAGRAPH_SEPARATOR> In a Bayesian approach to inferring changepoints, one adopts a prior distribution on their locations and a likelihood function for the observed process given these changepoints. However, both of these laws typically depend on a finite dimensional real parameter vector $\\theta\\in\\Theta$ where $\\Theta$ denotes the set of permissible parameter vectors. In all realistic applications, the static parameter $\\theta$ is unknown and needs to be estimated from the data as well. A fully Bayesian approach would assign a prior distribution to $\\theta$ . However, the resulting posterior distribution is intractable. Several Markov chain Monte Carlo (MCMC) schemes have been proposed in this context (Stephens 1994; Chib 1998; Lavielle and Lebarbier 2001; Fearnhead 2006). Unfortunately these algorithms are far too computationally intensive when dealing with very large datasets. Alternative to an MCMC-based full Bayesian analysis is sequential Monte Carlo (SMC); however, SMC methods to perform online Bayesian static parameter estimation suffer from the well-known particle path degeneracy problem and can provide unreliable estimates; see Andrieu, Doucet, and Tadi (2005), and Olsson et al. (2008) for a discussion of this issue. This is why we focus here on estimating the parameter $\\theta$ using a maximum likelihood approach; that is, the maximum likelihood estimate (MLE) of interest is the parameter vector from $\\Theta$ that maximizes the probability density of the observed data sequence $p_{\\theta}(y_{1},\\dots,y_{n})$ . This is a challenging problem, as computing the likelihood $p_{\\theta}(y_{1},\\dots,y_{n})$ requires a computational cost increasing super-linearly with $n$ (Chopin 2007; Fearnhead and Liu 2007). <PARAGRAPH_SEPARATOR> Our main contribution is a novel online expectation–maximization (EM) algorithm to compute the MLE of the static parameter $\\theta$ for changepoint models. We remark that standard batch EM algorithms for a restricted class of changepoint models have been proposed before, for example, see Gales and Young (1993), Barbu and Limnios (2008), and Fearnhead and Vasileiou (2009). The main reason why an online algorithm is desirable is that huge computational and memory savings are possible. For a long data sequence, a standard EM algorithm requires a complete browse through the entire dataset at each iteration to update the MLE of $\\theta$ , and many such iterations are needed until the estimate of $\\theta$ converges. This not only requires storing the entire data sequence but also the probability laws that are needed in the intermediate computations done in each EM iteration, which can be impractical. For this reason, there has been a strong interest in online methods that make parameter estimation possible by browsing through the data only once and hence circumventing the need to store it in its entirety (see Kantas et al. 2009 for a review). The only other work on computing the MLE of $\\theta$ for a more restrictive class of changepoint models in an online manner that we are aware of is Caron, Doucet, and Gottardo (2012), where the authors used a recursive gradient algorithm. If the model permits an EM implementation, then it is fair to say that the EM is generally preferred by practitioners, as no algorithm tuning is required, whereas it can be difficult to properly scale the components of the computed gradient vector."
  },
  {
    "qid": "statistic-posneg-489-0-0",
    "gold_answer": "FALSE. The paper provides a counterexample where $N=n=3$, $r_{1}(\\cdot)\\equiv0$, $r_{2}(1)=0$, $r_{2}(2)=r_{2}(3)=b$, and $r_{3}(1)=r_{3}(2)=0$, $r_{3}(3)=c$, with $0<c/2<b<c$. In this case, $V_{01}=2b/3+c/12<V_{11}=2b/3+c/9$, showing that $\\pi^{*}\\leqslant\\pi$ does not necessarily imply $V_{\\pi^{*}}\\geqslant V_{\\pi}$ without conditions on $r_{k}(i)$.",
    "question": "Is it always true that $\\pi^{*}\\leqslant\\pi$ implies $V_{\\pi^{*}}\\geqslant V_{\\pi}$ without any conditions on $r_{k}(i)$?",
    "question_context": "Optimal Stopping and Prophet Inequalities\n\nO'Brien's [7] paper is devoted to showing $V_{\\bullet}\\geqslant V_{\\bullet}$ . This also follows from more general results as described in Rinott and Samuel-Cahn [8]. The following extension of O'Brien result holds. <PARAGRAPH_SEPARATOR> # THEOREM 3.1.For any $\\pmb{\\pi}$ and $r_{k}(i),V_{0}\\geqslant V_{\\pi}$ <PARAGRAPH_SEPARATOR> The proof is deferred to the end of the section. It appears natural at this point to consider the partial ordering ${\\pmb{\\pi}}^{*}\\leqslant{\\pmb{\\pi}}$ if $\\pi_{k}^{*}\\leqslant\\pi_{k}$ $k=1,...,n-1$ can be shown by an easy coupling argument that <PARAGRAPH_SEPARATOR> $$ E_{\\pi^{*}}(X_{1}\\vee\\cdots\\vee X_{n})\\geqslant E_{\\pi}(X_{1}\\vee\\cdots\\vee X_{n}), $$ <PARAGRAPH_SEPARATOR> and, moreover, <PARAGRAPH_SEPARATOR> $$ P_{\\pi^{*}}(X_{1}\\lor\\cdots\\lor X_{n}\\geqslant c)\\geqslant P_{\\pi}(X_{1}\\lor\\cdots\\lor X_{n}\\geqslant c)\\qquad{\\mathrm{for~all~}}c. $$ <PARAGRAPH_SEPARATOR> Theorem 3.1 and (3.1) might suggest that perhaps $\\pi^{*}\\leqslant\\pi$ would also imply $V_{\\pi^{*}}\\geqslant V_{\\pi}$ . Without any conditions on $r_{k}(i)$ it is easy to find a counterexample: Let $N=n=3$ $r_{1}(\\cdot)\\equiv0$ $r_{2}(1)=0$ $r_{2}(2)=r_{2}(3)=b$ and $r_{3}(1)=r_{3}(2)=0.$ $r_{3}(3)=c$ ,where $0<c/2<b<c$ . Direct calculations yield $V_{01}=2b/3+c/12<V_{11}=2b/3+c/9,$ . In this example note the peculiar way in which $r_{k}(i)$ varies with $k$ , leaving open the question of whether $\\pi^{*}\\leqslant\\pi$ implies $V_{\\pi^{*}}\\geqslant V_{\\pi}$ when $r_{k}(\\cdot)$ does not depend on $k_{\\mathrm{:}}$ . or decreases in $k$ : i.e., $r_{k}(i)\\geqslant r_{k+1}(i)$ (values are discounted in time of observation). Of particular interest (and also unresolved) is the special case of whether $V_{\\pi}\\geqslant V_{1}$ under these conditions (see comments on this later). The latter inequality is related to the following. <PARAGRAPH_SEPARATOR> THEOREM 3.2. Let $r_{k}(i)\\geqslant r_{k+1}(i)$ and $r_{k}(i)\\geqslant0$ for all $i,k.$ Then the \"prophet inequality\"\" <PARAGRAPH_SEPARATOR> $$ E_{\\pi}(X_{1}\\vee\\cdots\\vee X_{n})<2V_{\\pi}(X_{1},...,X_{n})\\equiv2V_{\\pi} $$ <PARAGRAPH_SEPARATOR> holds for any sampling scheme ${\\mathcal{R}}(\\pi)$ <PARAGRAPH_SEPARATOR> The proof follows a remark: Theorem 3.2 of Rinott and Samuel-Cahn [8] implies $E_{\\pi}(X_{1}\\lor\\cdots\\lor X_{n})<2V_{1}$ . Thus, had we known $V_{\\pi}\\geqslant V_{1}$ ,(3.2) would be a direct consequence."
  },
  {
    "qid": "statistic-posneg-1124-0-0",
    "gold_answer": "FALSE. The characteristic function of P₁ in (4.2) includes a non-zero cubic term β(it)³, where β = {g(-½Δ)}³(Δ/64)(1/n₁ + 1/n₂)². This implies the third moment about the mean is non-zero (6β + O₃), violating the symmetry requirement for normality. The normal approximation only holds when O₂ terms are ignored, as shown in (4.1). The presence of skewness (captured by β) confirms the non-normality when second-order terms are retained.",
    "question": "Is the statement 'The conditional error rate P₁ follows a normal distribution when second-order terms (O₂) are included in the asymptotic expansion' supported by the derived characteristic function in equation (4.2)?",
    "question_context": "Asymptotic Distributions in Discriminant Analysis: Conditional Error Rates and Risk\n\nThe paper examines the asymptotic distributions of conditional error rates (P₁, P₂) and risk (R) in discriminant analysis under multivariate normality assumptions. It derives characteristic functions and variance expansions for these statistics using higher-order asymptotic expansions, showing that P₁ is not normally distributed when second-order terms are considered, while R remains normal even when third-order terms are included."
  },
  {
    "qid": "statistic-posneg-1451-0-2",
    "gold_answer": "TRUE. The context states that modeling aggregated data using spatial integrals (as in GGMs) requires lots of computation, and hence it is becoming very common to use GMRFs instead, implying that GMRFs are computationally more efficient for this purpose.",
    "question": "Is it true that Gaussian Markov random fields (GMRFs) are computationally more efficient than Gaussian geostatistical models (GGMs) for modeling aggregated data?",
    "question_context": "Comparison of Gaussian Geostatistical Models and Gaussian Markov Random Fields\n\nIn many applications in spatial and environmental epidemiology, data concerning a spatial process of interest are often observed at different spatial resolutions, and the overall problem of incompatible spatial data has been encountered very commonly when relating two spatial variables with different supports. For example, in studies of the association between air pollution exposure and adverse health effects, relevant health outcomes are usually available as areal data due to confidentiality while the pollution data are available as a point level [6,19]. To investigate the relationships between two variables with different spatial resolutions, the mismatch problem in the support of the two variables needs to be resolved. One common solution to this spatial misalignment problem is to aggregate the point-referenced data to the area level, and create a common support for both variables. Once the point-referenced data are aggregated to the relevant level, the process representing the aggregated data is modeled using integrals of spatial continuous process [14,15]. Consider a continuous Gaussian process $Y(s)$ with mean function $\\mu(s)$ and covariance function $c(s_{i},s_{j})$ for $s_{i},s_{j}\\subset D\\in R^{d}$ , where $s_{i}$ is a location in a fixed domain $D$ . The aggregated process over a region $B$ , $\\begin{array}{r}{Y(B)=\\int_{B}Y(s)\\mathrm{d}s}\\end{array}$ has a multivariate normal distribution with mean function $\\mu(B)$ and covariance function $\\Sigma(B_{1},B_{2})$ , $$\\begin{array}{l}{\\displaystyle\\mu(B)=E(Y(B))=|B|^{-1}\\int_{B}\\mu(s)\\mathrm{d}s}\\ {\\sum(B_{1},B_{2})=\\mathrm{cov}(Y(B_{1}),Y(B_{2}))=|B_{1}|^{-1}|B_{2}|^{-1}\\int_{B_{1}}\\int_{B_{2}}c(s_{1},s_{2})\\mathrm{d}s_{1}\\mathrm{d}s_{2},}\\end{array}$$ where $|B_{i}|$ denotes the area of a region $B_{i}$ for $i=1,2$ . Modeling aggregated data using these spatial integrals requires lots of computation. Hence, instead of using the aggregated models directly in modeling aggregated point-referenced data, it is becoming very common to use Gaussian Markov random fields."
  },
  {
    "qid": "statistic-posneg-2140-3-1",
    "gold_answer": "FALSE. Explanation: The condition holds specifically when $\\mathbf{h}$ is selected at random using a dissipative distribution (e.g., Dirichlet distribution via stick-breaking). Not every $\\mathbf{h} \\in \\mathbb{H}$ will satisfy this condition. The use of a dissipative distribution ensures that, with probability one, the projection $\\langle X^{(t)},\\mathbf{h}\\rangle$ captures the Gaussianity of $X^{(t)}$. Arbitrary choices of $\\mathbf{h}$ may not preserve this equivalence.",
    "question": "Does the condition 'with probability one, $X^{(t)}$ is Gaussian if, and only if, the real-valued r.v. $\\langle X^{(t)},\\mathbf{h}\\rangle$ is Gaussian' hold for any choice of $\\mathbf{h} \\in \\mathbb{H}$?",
    "question_context": "Random Projection Test for Gaussianity in Hilbert Space\n\nIn (1) the null hypothesis holds if, and only if, $(X_{1},\\ldots,X_{t})^{T}$ is a Gaussian vector for all $t\\in\\mathbb{N}$ . Due to the stationarity of $\\mathbf{X}$ , this is equivalent to $(X_{t})_{t\\leq0}$ being Gaussian and so, to the Gaussianity of the process $X^{(t)}:=(X_{j})_{j\\leq t}$ for any $t\\in\\mathbb{Z}$ . Given $t~\\in~\\mathbb{Z}$ , we want to use Theorem 1.1 to check whether $X^{(t)}$ is Gaussian. Hence, we have to include $X^{(t)}$ in an appropriate Hilbert space, $\\mathbb{H}$ , and select at random a point $\\mathbf h\\in\\mathbb H$ using a dissipative distribution. Once this is done, we will have that, with probability one, $X^{(t)}$ is Gaussian if, and only if, the real-valued r.v. $\\langle X^{(t)},\\mathbf{h}\\rangle$ is Gaussian.<PARAGRAPH_SEPARATOR>Concerning $\\mathbb{H}$ , let us consider the space of sequences<PARAGRAPH_SEPARATOR>$$ l^{2}=\\left\\{(x_{n})_{n\\in\\mathbb{N}}:\\sum_{n\\in\\mathbb{N}}x_{n}^{2}a_{n}<\\infty\\right\\}, $$<PARAGRAPH_SEPARATOR>with $a_{0}:=1$ and $a_{n}:=n^{-2}$ , $n\\geq1$ , endowed with the scalar product<PARAGRAPH_SEPARATOR>If the variance of $X_{t}$ is finite, then $E[\\sum_{n\\in\\mathbb{N}}X_{t-n}^{2}a_{n}]$ is also finite. This implies that, almost surely, $X^{(t)}\\in l^{2}$ .<PARAGRAPH_SEPARATOR>Now we need a dissipative distribution on $l^{2}$ to be employed to select the direction in which we will project the data. To do this, we use the so-called Dirichlet distribution (Pitman, 2006) and build it using the stick-breaking method: Let $\\alpha_{1}$ , $\\alpha_{2}>0$ and consider the following distribution:"
  },
  {
    "qid": "statistic-posneg-608-4-0",
    "gold_answer": "TRUE. The second term in the product, √(𝔼[ν²]/𝔼[ν]²), represents the coefficient of variation of the squared inverse length scales (ν). This term increases with the relative variability in the length scales of the target components, indicating that the expected number of apogees is indeed influenced by this variability. The first term, T√𝔼[ν], relates the time interval to the overall length scale of the target, while the second term adjusts for the heterogeneity in component scales.",
    "question": "Is the expected number of apogees per unit time, given by 𝔼[N(T)] ∝ T√𝔼[ν] × √(𝔼[ν²]/𝔼[ν]²), dependent on the relative variability of the squared inverse length scales of the target components?",
    "question_context": "Gaussian Process Limit in Hamiltonian Monte Carlo for Product Targets\n\nThe paper discusses the behavior of Hamiltonian Monte Carlo (HMC) for product targets with a potential defined as a sum of component functions, each with its own length scale. The scaled dot product of momentum and gradient of the potential is shown to converge to a stationary Gaussian process as the number of components tends to infinity. The expected number of apogees (local maxima in the potential along the path) per unit time is derived, highlighting the relationship between the target's length scales and the integration time."
  },
  {
    "qid": "statistic-posneg-1722-3-1",
    "gold_answer": "TRUE. The paper notes that W = θ²gᵀA²g - θ(1 + gᵀAg) and states that W < 0 when (I - θA) is positive definite. This follows from the properties of positive definite matrices and the definition of W.",
    "question": "Is the condition W < 0 in the finite sample case always satisfied when (I - θA) is positive definite?",
    "question_context": "Disconnected Confidence Regions in Nonlinear Calibration\n\nThe paper discusses the formation of disconnected confidence regions in nonlinear calibration, focusing on both asymptotic and finite sample cases. In the asymptotic case, the confidence region for ξ is defined as Cγ(Y) = {ξ: fY(ξ) ≤ λ}, where fY is given and λ is the γ-fractile of the χ²(2) distribution. For the region to be disconnected, there must be two local minima ξi for fY with fY(ξi) < λ. In the finite sample case, the model Y = BX + ε is considered, with an exact γ-level confidence region Gγ(Y) defined similarly but incorporating sampling variability. The paper also provides geometric interpretations and necessary conditions for disconnected regions, including constraints on coefficients ci."
  },
  {
    "qid": "statistic-posneg-1401-0-2",
    "gold_answer": "TRUE. The paper states that setting $M_{T}=\\hat{\\Omega}^{-1}$ minimizes the asymptotic variance $V$, leading to an efficient GMM estimator. However, this efficiency is contingent on the moment conditions being correctly specified (i.e., $E(\\mu(\\mathbf{w}_{\\omega},t))=0$ holds). If the moment conditions are misspecified, the estimator may still be consistent but not efficient.",
    "question": "Does the optimal weighting matrix $M_{T}=\\hat{\\Omega}^{-1}$ guarantee efficiency in the GMM estimator only if the moment conditions are correctly specified?",
    "question_context": "GMM Estimation for Volatility Forecast Combination\n\nThe paper introduces a novel GMM approach to combine volatility forecasts from different models. The data generating process (DGP) is given by $y_{t}=x_{t}+u_{t}, u_{t}=h_{t}z_{t}$, where $z_{t}\\sim(0,1)$ are i.i.d. random variables. The combined predictor for the level and volatility are defined as $\\tilde{x}_{t}=\\sum_{i=1}^{k}\\tilde{w}_{i}^{(x)}\\hat{x}_{t i}$ and $\\tilde{h}_{t}^{2}=\\sum_{i=1}^{k}\\tilde{w}_{i}^{(h)}\\hat{h}_{t i}^{2}$, respectively. The weights are estimated by minimizing the GMM criterion $\\tilde{\\mathbf{w}}=\\underset{\\mathbf{w}}{\\mathrm{argmin}}m_{T}(\\mathbf{w})^{\\prime}M_{T}m_{T}(\\mathbf{w})$, where $m_{T}(\\mathbf{w})$ is a vector of moment conditions ensuring the standardized residuals $\\tilde{z}_{t}=\\frac{y_{t}-\\tilde{x}_{t}}{\\tilde{h}_{t}}$ behave as i.i.d. zero mean, unit variance random variables."
  },
  {
    "qid": "statistic-posneg-746-0-0",
    "gold_answer": "TRUE. The derivation involves taking the derivative of the risk function with respect to $\\theta_{j}$ and setting it to zero. The resulting condition $\\theta_{j}=r_{j}^{2}/(\\Sigma_{j j}+r_{j}^{2})$ minimizes the risk by balancing the trade-off between the variance of the full model estimator and the bias introduced by the reduced model estimator.",
    "question": "Is the condition for the risk function minimization $\\theta_{j}=r_{j}^{2}/(\\Sigma_{j j}+r_{j}^{2})$ correctly derived from the given risk function $E(\\hat{b}_{j,\\mathrm{{eb}}}-b_{j})^{2}=\\theta_{j}^{2}\\Sigma_{j j}+{\\mathrm{var}}(\\hat{b}_{j,\\mathrm{{red}}})+(1-\\theta_{j})^{2}r_{j}^{2}$?",
    "question_context": "Empirical Bayes Estimators and Risk Minimization in Statistical Models\n\nThe authors thank the Joint Editor, the Associate Editor and the referees for their constructive comments. <PARAGRAPH_SEPARATOR> # Appendix A: Calculating $\\boldsymbol{\\Sigma}=\\boldsymbol{\\mathsf{v a r}}(\\hat{\\boldsymbol{\\mathsf{b}}}_{\\mathrm{full}}-\\hat{\\boldsymbol{\\mathsf{b}}}_{\\mathrm{red}})$ <PARAGRAPH_SEPARATOR> $\\mathrm{var}(\\mathbf{\\hat{b}}_{\\mathrm{full}}-\\mathbf{\\hat{b}}_{\\mathrm{red}})=\\mathrm{var}(\\mathbf{\\hat{b}}_{\\mathrm{full}})+\\mathrm{var}(\\mathbf{\\hat{b}}_{\\mathrm{red}})-2\\mathrm{cov}(\\mathbf{\\hat{b}}_{\\mathrm{full}},\\mathbf{\\hat{b}}_{\\mathrm{red}}).$ . Since $\\hat{\\mathbf{b}}_{\\mathrm{full}}$ and $\\hat{\\bf b}_{\\mathrm{red}}$ are MLEs, we can write <PARAGRAPH_SEPARATOR> $$ \\mathrm{cov}(\\mathbf{\\hat{b}}_{\\mathrm{full}},\\mathbf{\\hat{b}}_{\\mathrm{red}})\\approx\\frac{1}{N^{2}}\\mathrm{var}(\\mathbf{\\hat{b}}_{\\mathrm{full}})\\left\\{\\sum_{i=1}^{N}\\psi_{i,\\mathrm{full}}(b)\\psi_{i,\\mathrm{red}}^{\\mathrm{T}}(d)\\right\\}\\mathbf{W}\\mathrm{var}(\\mathbf{\\hat{b}}_{\\mathrm{red}}). $$ <PARAGRAPH_SEPARATOR> Here $\\psi_{i,\\mathrm{full}}({\\bf b})$ and $\\psi_{i,\\mathrm{red}}({\\bf d})$ are the contributions of person $i$ $(i=1,\\ldots,N)$ to the score functions of the full and reduced models respectively. Conditional on S, $\\mathbf{X}$ and $\\mathbf{W}$ , we have $\\sigma_{\\mathrm{full}}^{2}=\\mathrm{var}(\\mathbf{Y}|\\mathbf{S},\\mathbf{X},\\mathbf{W})=\\sigma_{\\mathrm{red}}^{2}$ . Therefore, it follows that the expected is equal to var $(\\hat{\\bf b}_{\\mathrm{red}})$ . Hence, the expected value of $\\Sigma$ is $\\mathrm{var}(\\mathbf{\\hat{b}}_{\\mathrm{full}})-\\mathrm{var}(\\mathbf{\\hat{b}}_{\\mathrm{red}})$ . <PARAGRAPH_SEPARATOR> # Appendix B: Derivation of the empirical Bayes estimators of Section 3.1 <PARAGRAPH_SEPARATOR> We shall obtain $\\Theta$ that minimizes the risk function $\\mathrm{tr}\\{E({\\bf\\hat{b}}_{\\mathrm{eb}}-{\\bf b})({\\bf\\hat{b}}_{\\mathrm{eb}}-{\\bf b})^{\\mathrm{T}}\\}$ as follows. <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{E\\{(\\hat{\\mathbf{b}}_{\\mathrm{eb}}-\\mathbf{b})(\\hat{\\mathbf{b}}_{\\mathrm{eb}}-\\mathbf{b})^{\\mathrm{T}}\\}=E\\{(\\Theta(\\hat{\\mathbf{b}}_{\\mathrm{full}}-\\mathbf{b})+(\\mathbf{I}-\\Theta)(\\hat{\\mathbf{b}}_{\\mathrm{full}}-\\mathbf{W}\\mathbf{d}-\\mathbf{r}))(\\Theta(\\hat{\\mathbf{b}}_{\\mathrm{full}}-\\mathbf{b})+(\\mathbf{I}-\\Theta)(\\hat{\\mathbf{b}}_{\\mathrm{full}}-\\mathbf{W}\\mathbf{d}-\\mathbf{r}))^{\\mathrm{T}}\\}}\\ &{\\qquad=\\Theta\\Sigma\\Theta^{\\mathrm{T}}+\\mathrm{var}(\\hat{\\mathbf{b}}_{\\mathrm{red}})+(\\mathbf{I}-\\Theta)\\mathrm{rr}^{\\mathrm{T}}(\\mathbf{I}-\\Theta)^{\\mathrm{T}}.}\\end{array} $$ <PARAGRAPH_SEPARATOR> This risk function is minimized when $\\Theta=\\ensuremath{\\mathbf{r}}\\ensuremath{\\mathbf{r}}^{\\mathrm{T}}(\\Sigma+\\ensuremath{\\mathbf{r}}\\ensuremath{\\mathbf{r}}^{\\mathrm{T}})^{-1}.$ . We can estimate $\\Theta$ by plugging in the estimated values of $\\mathbf{r}$ and $\\Sigma$ . Since $\\mathbf{r}=\\mathbf{b}-\\mathbf{W}\\mathbf{d}$ , we can write $\\mathbf{\\hat{b}}_{\\mathrm{eb}}=\\mathbf{\\hat{b}}_{\\mathrm{full}}-(\\mathbf{I}-\\boldsymbol{\\Theta})\\hat{\\mathbf{r}}$ . Further, <PARAGRAPH_SEPARATOR> $$ (\\Sigma+\\mathbf{rr}^{\\mathsf{T}})^{-1}=\\Sigma^{-1}-\\frac{\\Sigma^{-1}\\mathbf{rr}^{\\mathsf{T}}\\Sigma^{-1}}{1+\\mathbf{r}^{\\mathsf{T}}\\Sigma^{-1}\\mathbf{r}}. $$ <PARAGRAPH_SEPARATOR> Therefore, <PARAGRAPH_SEPARATOR> $$ {\\bf\\hat{b}}_{\\mathrm{eb}}={\\bf\\hat{b}}_{\\mathrm{full}}-\\frac{1}{1+\\Lambda}{\\hat{\\bf r}}. $$ <PARAGRAPH_SEPARATOR> Equivalently, <PARAGRAPH_SEPARATOR> $$ \\hat{\\mathbf{b}}_{\\mathrm{eb}}=\\frac{\\Lambda}{1+\\Lambda}\\hat{\\mathbf{b}}_{\\mathrm{full}}-\\frac{1}{1+\\Lambda}\\hat{\\mathbf{b}}_{\\mathrm{red}}. $$ <PARAGRAPH_SEPARATOR> Suppose that $\\Theta$ is a diagonal matrix with the jth diagonal entry denoted $\\theta_{j}$ . The jth component of the empirical Bayes estimator of $\\mathbf{b}$ is $\\boldsymbol{\\hat{b}}_{j,\\mathrm{eb}}=\\theta_{j}\\boldsymbol{\\hat{b}}_{j,\\mathrm{full}}+(1-\\boldsymbol{\\check{\\theta}}_{j})\\mathbf{\\hat{b}}_{j,\\mathrm{red}}$ . Let $\\Sigma_{j j}=\\mathrm{var}(\\hat{b}_{j,\\mathrm{full}}-\\hat{b}_{j,\\mathrm{red}})$ denote the $j$ th diagonal entry of $\\Sigma$ . The contribution of $\\mathbf{b}_{j,\\mathrm{eb}}$ to the risk function is given by <PARAGRAPH_SEPARATOR> $$ E(\\hat{b}_{j,\\mathrm{{eb}}}-b_{j})^{2}=\\theta_{j}^{2}\\Sigma_{j j}+{\\mathrm{var}}(\\hat{b}_{j,\\mathrm{{red}}})+(1-\\theta_{j})^{2}r_{j}^{2}. $$ <PARAGRAPH_SEPARATOR> This risk function is minimized when $\\theta_{j}=r_{j}^{2}/(\\Sigma_{j j}+r_{j}^{2})=T_{j}/(1+T_{j})$ where $T_{j}=r_{j}^{2}/\\Sigma_{j j}$ ."
  },
  {
    "qid": "statistic-posneg-1914-0-1",
    "gold_answer": "TRUE. The variance σ_{j,k}^* is indeed derived from the prior precision σ_{j,k0}^{-1} and the data precision n_k ψ_{j,k}^{-1}, combining prior and likelihood information in the Bayesian framework.",
    "question": "The full conditional distribution of μ_{j,k} for j = 1, ..., r_1 is given as N[μ_{j,k}^*, σ_{j,k}^*]. Is it correct that σ_{j,k}^* is computed as (σ_{j,k0}^{-1} + n_k ψ_{j,k}^{-1})^{-1}?",
    "question_context": "Bayesian Mixture Models with Latent Variables: Full Conditional Distributions and MH Algorithm\n\nThe paper presents a Bayesian approach to mixture models with latent variables, detailing the full conditional distributions and the Metropolis-Hastings (MH) algorithm for sampling. The context includes complex mathematical formulas for the full conditional distributions of parameters like Ω, ζ, μ_k, A_k, Ψ_k, Π_k, Φ_k, α_k, Y*, and δ. The MH algorithm is described with proposal distributions and acceptance probabilities for generating samples from these non-standard distributions."
  },
  {
    "qid": "statistic-posneg-1937-2-2",
    "gold_answer": "TRUE. The derivative $\\frac{\\partial R_i(u)}{\\partial\\alpha_q}$ is correctly given by $(\\mathbf{e}_q - \\mathbf{e}_p)^\\top e^{Tu}(I - e^{T\\kappa})^{-1}\\mathbf{e}_i$. This expression accounts for the change in the initial probability vector $\\pmb{\\alpha}$ (with $\\alpha_p = 1 - \\sum_{j=1}^{p-1}\\alpha_j$) and its effect on $R_i(u)$. The term $(\\mathbf{e}_q - \\mathbf{e}_p)$ arises from the constraint on $\\alpha_p$, ensuring the probabilities sum to 1.",
    "question": "Is the partial derivative of $R_i(u)$ with respect to $\\alpha_q$ given by $(\\mathbf{e}_q - \\mathbf{e}_p)^\\top e^{Tu}(I - e^{T\\kappa})^{-1}\\mathbf{e}_i$?",
    "question_context": "Fisher Information Matrix Computation for Phase-Type Distributions\n\nThe paper discusses the computation of the Fisher Information Matrix (FIM) for phase-type distributions, focusing on the derivatives of various matrix exponential functions. Key formulas include the computation of partial derivatives for functions like $Q_i(u)$, $R_i(u)$, $U_i$, $V_{ij}$, and $W_i$, which involve matrix exponentials and their inverses. The paper also details the uniformization technique for computing these derivatives and the iterative method for approximating infinite sums in the FIM computation."
  },
  {
    "qid": "statistic-posneg-504-0-1",
    "gold_answer": "FALSE. The proof shows that the mixed moments are bounded and their limiting behavior depends only on $E(v_{11}), E(v_{11}^{2})$, and $E(v_{11}^{4})$. However, the condition $E(v_{11}^{4})=3$ is not necessary for the convergence of the mixed moments. The convergence holds for any value of $E(v_{11}^{4})$, but the specific value affects the limiting distribution's properties.",
    "question": "Does the condition $E(v_{11}^{4})=3$ affect the convergence of the mixed moments in the proof of Theorem 1?",
    "question_context": "Convergence of Random Matrix Eigenvectors and Covariance Analysis\n\nThe proof requires two lemmas: <PARAGRAPH_SEPARATOR> LEMMA 2.1. For any integer $r\\geqslant1$ $(1/{\\sqrt{n}})$ (tr $M_{n}^{r}-E(\\operatorname{tr}M_{n}^{r}))\\to^{\\mathfrak{i}.\\mathfrak{p}.}0$ as $n\\to\\infty$ <PARAGRAPH_SEPARATOR> Proof. We will show for any integers $r_{1},r_{2}\\geqslant1,(1/n)\\mathsf{C o v}(\\operatorname{tr}M_{n}^{r_{1}},$ $M_{n}^{r_{2}})\\rightarrow0$ as $n\\to\\infty$ .Wehave <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}{g^{r_{1}+r_{2}}\\mathsf{C o v}(\\mathsf{t r}\\Pi_{n}^{r_{1}}\\mathsf{t r}M_{n}^{r_{1}})}&{}\\ {=}&{\\underset{i_{1}\\cdots i_{r_{k}}k_{1}\\cdots k_{r_{1}}}{\\sum}E\\big[(v_{i_{1}k_{1}}v_{i_{2}k_{1}}v_{i_{2}k_{2}}\\cdots v_{i_{r_{k}}k_{r_{1}}}v_{i_{1}k_{r_{2}}})}\\ &{}\\ &{\\qquadi_{i_{1}^{\\prime}\\cdots i_{r_{k}}k_{1}\\cdots k_{r_{1}}}^{\\prime}}\\ &{}\\ &{-E\\big(v_{i_{1}k_{1}}v_{i_{2}k_{1}}v_{i_{2}k_{2}}\\cdots v_{i_{r_{k}}k_{r_{1}}}v_{i_{1}k_{r_{1}}}\\big)}\\ &{}\\ &{\\qquad\\quad\\times\\big(v_{i_{1}k_{1}^{\\prime}}v_{i_{2}k_{1}^{\\prime}}v_{i_{2}k_{2}^{\\prime}}\\cdots v_{i_{r_{k}}^{\\prime}k_{r_{2}}^{\\prime}}v_{i_{r_{2}}^{\\prime}k_{r_{2}}^{\\prime}}}\\ &{}\\ &{-E\\big(v_{i_{1}k_{1}^{\\prime}}v_{i_{2}k_{1}^{\\prime}}v_{i_{2}k_{2}^{\\prime}}\\cdots v_{i_{r_{2}}k_{r_{2}}^{\\prime}r_{2}^{\\prime}}v_{i_{r_{2}k_{r}^{\\prime}}}\\big)\\big)\\big]}\\ &{=}&{\\underset{i_{1}\\cdots i_{r_{k}}k_{1}\\cdots k_{r_{1}}}{\\sum}E\\big(v_{i_{1}k_{1}}\\cdots v_{i_{r_{k}}k_{r_{1}}}v_{i_{1}k_{r_{1}^{\\prime}}}\\cdots v_{i_{r_{k}^{\\prime}}k_{r_{2}^{\\prime}}}\\big)}\\ &{}\\ &{\\quad i_{r_{1}^{\\prime}\\cdots i_{r_{k}^{\\prime}}k_{1}^{\\prime}\\cdots k_{r_{2}^{\\prime}}}}\\ &{}&{"
  },
  {
    "qid": "statistic-posneg-1076-0-0",
    "gold_answer": "TRUE. The E step computes the conditional expectations of the missing entries given the observed data and current parameter estimates, as shown in the formula for $\\mathbb{E}[Y_{i j}|Y_{\\mathrm{obs}},\\theta^{t}]$. The M step then minimizes the squared error between the imputed data $\\hat{Y}$ and the model predictions $1_{n}m^{\\top}+F_{b}V_{b}^{\\top}+F_{w}V_{w}^{\\top}$, which is equivalent to maximizing the complete likelihood under the Gaussian model assumption. This is explicitly stated in the context and demonstrated by the minimization problem $\\arcsin_{m,F_{b},V_{b},F_{w},F_{b}}\\frac{1}{2}\\left\\|\\hat{Y}-1_{n}m^{\\top}-F_{b}V_{b}^{\\top}-F_{w}V_{w}^{\\top}\\right\\|_{2}^{2}$.",
    "question": "In the EM algorithm described, the E step computes the expected values of missing entries using the current parameter estimates, and the M step updates the parameters by minimizing the squared error between the imputed data and the model predictions. Is this statement true based on the provided context?",
    "question_context": "EM Algorithm for Missing Data Imputation with Multilevel SVD\n\nThe iterative procedure described in Section 3.1 is equivalent to an EM algorithm based on a Gaussian model. <PARAGRAPH_SEPARATOR> Let $Y=1_{n}m^{\\top}+F_{b}V_{b}^{\\top}+F_{w}V_{w}^{\\top}+E_{:}$ where $m\\in\\mathbb{R}^{p},F_{b}\\in\\mathbb{R}^{n\\times Q_{b}}$ , $V_{b}\\in\\mathbb{R}^{p\\times Q_{b}}$ , $V_{b}\\in\\mathbb{R}^{n\\times Q_{w}}$ , and $F_{b}\\in\\mathbb{R}^{p\\times Q_{w}}$ are parameters to be estimated, and $E$ is a matrix with Gaussian entries with known variance $E_{i j}\\sim\\mathcal{N}(0,\\sigma^{2})$ . Denote by $M\\in\\{0,1\\}^{n\\times p}$ the mask indicating (with a 1) the observed entries of $Y$ , $Y_{\\mathrm{obs}}$ the observed entries and $Y_{\\mathrm{mis}}$ the missing entries. <PARAGRAPH_SEPARATOR> Denote $\\theta=(m,F_{b},V_{b},F_{w},V_{w})$ . Under the classical hypothesis that the missing values are missing at random, that is, $p(M|Y_{\\mathrm{obs}},Y_{\\mathrm{mis}};\\phi)=$ $p(M|Y_{\\mathrm{obs}};\\phi)$ , the likelihood of the observed data $(Y_{\\mathrm{obs}},M)$ is <PARAGRAPH_SEPARATOR> $$ \\begin{array}{l}{\\displaystyle\\boldsymbol{p}(Y_{\\mathrm{obs}},M;\\theta,\\phi)=\\int\\boldsymbol{p}(Y_{\\mathrm{obs}},Y_{\\mathrm{mis}},M;\\theta)d Y_{\\mathrm{mis};\\phi}.}\\ {\\displaystyle=\\boldsymbol{p}(Y_{\\mathrm{obs}};\\theta)\\boldsymbol{p}(M|Y_{\\mathrm{obs}};\\phi)}\\end{array} $$ <PARAGRAPH_SEPARATOR> The EM algorithm (Dempster, Laird, and Rubin 1977) maximizes the observed log-likelihood $\\ell(Y_{\\mathrm{obs}};\\theta)$ iteratively by alternatively computing the expectation of the complete likelihood $\\ell(Y;\\theta)$ under the distribution of the missing values given the observed values and the current estimate $\\theta^{t}$ (E step), and maximizing this expectation with respect to $\\theta$ (M step). It goes as follows at iteration $t$ : <PARAGRAPH_SEPARATOR> To compute this expectation, one\u000e only needs to compute the \u000etwo following sufficient statistics for all $(i,j)\\in\\{1,\\dots,n\\}\\times\\{1,\\dots,p\\}$ : <PARAGRAPH_SEPARATOR> $$ \\begin{array}{r l}&{\\mathbb{E}[Y_{i j}|Y_{\\mathrm{obs}},\\theta^{t}]}\\ &{=\\left\\{\\begin{array}{l l}{Y_{i j}}&{\\mathrm{if}M_{i j}=1}\\ {((1_{n}m^{\\top}+F_{b}V_{b}^{\\top}+F_{w}V_{w}^{\\top})^{t})_{i j}}&{\\mathrm{if}M_{i j}=0}\\end{array}\\right.,}\\ &{\\mathbb{E}[Y_{i j}^{2}|Y_{\\mathrm{obs}},\\theta^{t}]}\\ &{=\\left\\{\\begin{array}{l l}{Y_{i j}^{2}}&{\\mathrm{if}M_{i j}=1}\\ {((1_{n}m^{\\top}+F_{b}V_{b}^{\\top}+F_{w}V_{w}^{\\top})^{t})_{i j}^{2}+(\\sigma^{2})^{t}}&{\\mathrm{if}M_{i j}=0}\\end{array}\\right..}\\end{array} $$ <PARAGRAPH_SEPARATOR> # M step: <PARAGRAPH_SEPARATOR> The M steps consists in maximizing the complete likelihood where the sufficient statistics are replaced by the conditional expectation above. However, estimation of the parameters in $\\theta$ do not require knowledge of $\\sigma$ so that we do not need to compute E[ $Y_{i j}^{2}|Y_{\\mathrm{obs}},\\theta^{\\bar{t}}]$ in the E step. Denote for, $(i,j)\\in\\{1,\\dots,n\\}\\times\\{1,\\dots,p\\}$ , $\\hat{Y}_{i j}^{t}=\\mathbb{E}_{\\theta^{t}}[Y_{i j};\\theta]$ and $\\hat{Y}=(\\hat{Y}_{i j})_{i,j}$ . The maximization of $\\mathbb{E}_{\\theta^{t}}\\left[\\ell(Y;\\theta)|Y_{\\mathrm{obs}}\\right]$ with respect to $\\theta$ is equivalent to solving: <PARAGRAPH_SEPARATOR> $$ \\arcsin_{m,F_{b},V_{b},F_{w},F_{b}}\\frac{1}{2}\\left\\|\\hat{Y}-1_{n}m^{\\top}-F_{b}V_{b}^{\\top}-F_{w}V_{w}^{\\top}\\right\\|_{2}^{2}. $$ <PARAGRAPH_SEPARATOR> The EM algorithm consisting of the $\\mathrm{E}$ and $\\mathbf{M}$ step described above is exactly equivalent to Algorithm 1."
  },
  {
    "qid": "statistic-posneg-845-0-1",
    "gold_answer": "FALSE. The computational complexity depends on the type of transformation. For monotonic transformations with closed-form inverses (e.g., Log-Gaussian RFs), the complexity is $O(n^{3})$ for operations and $O(n^{2})$ for memory storage, similar to Gaussian RFs. However, for non-monotonic transformations or those involving multiple independent copies of Gaussian RFs, the complexity can be prohibitive (e.g., $O(2^{n-1})$ for skew-Gaussian RFs).",
    "question": "Does the computational complexity of estimating non-Gaussian RFs derived from the transformation $Y(s)=f(g_{1}(Z_{1}(s)),g_{2}(Z_{2}(s)),.....g_{q}(Z_{q}(s)))$ always remain $O(n^{3})$ for operations and $O(n^{2})$ for memory storage, regardless of the transformation type?",
    "question_context": "Non-Gaussian Random Fields via Transformation of Gaussian RFs\n\nA very flexible class of non-Gaussian RFs that solve these potential drawbacks can be obtained through a suitable transformation of one or independent copies of (transformed) Gaussian RFs sharing a common correlation function. Specifically, let $Z=\\{Z(s),s\\in A\\}$ , $A\\subset\\ensuremath{\\mathbb{R}}^{d}$ a Gaussian RF and let $Y=\\{Y(s),s\\in A\\}$ a RF defined through the transformation\n\n$$\nY(s)=f(g_{1}(Z_{1}(s)),g_{2}(Z_{2}(s)),.....g_{q}(Z_{q}(s))),\\quad q\\geq1,\n$$\n\nwhere $Z_{1},\\ldots,Z_{q}$ , are independent copies of $Z$ and $f:\\mathbb{R}^{q}\\rightarrow\\mathbb{R}$ and $g_{1},\\ldots,g_{q}$ with $g_{i}:\\mathbb{R}\\to\\mathbb{R}$ are suitable functions. The class (1) includes several examples of non-Gaussian RFs proposed in the literature such us Bernoulli RFs (Heagerty and Lele, 1998), skewGaussian RFs (Zhang and El-Shaarawi, 2010), Tukey $g-h$ RFs Xua and Genton (2017), Student-𝑡 RFs (Bevilacqua et al., 2021), Two Piece RFs (Bevilacqua et al., 2022a), Weibull RFs (Bevilacqua et al., 2020) or Poisson RFs (Morales-Navarrete et al., 2022) to mention just a few. In addition, the so-called class of trans-Gaussian RFs (see for instance DeOliveira et al. (1997) and Allcroft and Glasbey (2003)) or the general class of non-Gaussian-RFs based on Gaussian Copula (Kazianka and Pilz, 2010; Masarotto and Varin, 2012; Gräler, 2014) and chi-square Copula (Bárdossy, 2006) belong to the class (1). The general class of non-Gaussian RFs in (1) is a convenient approach since geometrical properties such as mean square continuity and differentiability can be inherited from the underlying Gaussian RF by using flexible correlation models such as the Matérn (Stein, 1999) of the generalized Wendland model (Bevilacqua et al., 2019)."
  },
  {
    "qid": "statistic-posneg-428-0-0",
    "gold_answer": "TRUE. The kernel function $K_{\\lambda}(t_{i},v)$ is indeed expressed as a sum of integrals over the intervals $[\\tau_k, \\tau_{k+1}]$ weighted by $\\lambda_k^{-1}$, as shown in the first formula. This correctly accounts for the piecewise-constant nature of $\\lambda(u)$ and the integration of the basis functions over each interval.",
    "question": "The kernel function $K_{\\lambda}(t_{i},v)$ for $v \\in [t_{i}, 1]$ is correctly expressed as a sum of integrals over intervals $[\\tau_k, \\tau_{k+1}]$ weighted by $\\lambda_k^{-1}$, where $\\lambda_k$ is the constant value of $\\lambda(u)$ on $[\\tau_k, \\tau_{k+1}]$. True or False?",
    "question_context": "Spatially Adaptive Smoothing Splines with Piecewise-Constant Smoothing Parameters\n\nThe paper derives expressions for the kernel function $K_{\\lambda}(t_{i},v)$ used in spatially adaptive smoothing splines, where the smoothing parameter $\\lambda(u)$ is piecewise constant. The derivation involves integration by parts and recursive application to handle the piecewise nature of $\\lambda(u)$. The kernel function is expressed in terms of basis functions and their integrals over intervals defined by knots $\\tau_k$."
  },
  {
    "qid": "statistic-posneg-1007-0-1",
    "gold_answer": "FALSE. While increasing the number of variables may reduce bias, it can also increase the total error of prediction, especially if the model is used outside the observed range of explanatory variables. The context emphasizes the trade-off between bias and prediction error, not a unilateral reduction in bias with more variables.",
    "question": "Does the context suggest that increasing the number of explanatory variables (k) in a model always reduces the bias in predictions?",
    "question_context": "Variable Selection in Observational Studies: Mallows' C_k Statistic\n\nWe shall not describe the large body of statistical methods and theory associated with the linear model; for an introductory account, see Draper and Smith (1966) and for general comments Cox (1968). <PARAGRAPH_SEPARATOR> Formal significance tests are a useful guide to the importance of different explanatory variables, but have not to be followed too rigidly. One reason for caution is that the tabulated significance levels of the $F$ distribution refer to a single test carried out in isolation; in practice we are nearly always concerned with a chain of related tests and this makes the interpretation of the ordinary significance levels indirect (Draper et al., 1971; Pope and Webster, 1972; Spjotvoll, 1972). <PARAGRAPH_SEPARATOR> The difficulties of interpretation caused by non-orthogonality are less important if interest is purely in prediction over the range of explanatory variables covered by the data. Although several rather different looking equations will often have similar residual mean squares, it may be unimportant which equation is used. An equation with few explanatory variables will, however, give biased estimates if the omitted variables are at all relevant. The extent of bias, averaged over the observed distribution of the explanatory variables, in an equation with $k$ variables is indicated by the statistic suggested by C. L. Mallows (Gorman and Toman, 1966) <PARAGRAPH_SEPARATOR> $$ C_{k}=\\mathrm{(residualsumofsquares)}/\\hat{\\sigma}^{2}-(n-2k), $$ <PARAGRAPH_SEPARATOR> where $n$ denotes sample size and ${\\hat{\\sigma}}^{2}$ is a separate estimate of $\\sigma^{2}$ ; in the absence of bias, $E(C_{k})\\simeq k$ . Given several equations with similar residual mean squares, one with small bias is likely to be preferred. This does not necessarily mean one with many explanatory variables; increasing the number of variables may reduce the bias but at the expense of increasing the total error of prediction. If predicting outside the observed region of the explanatory variables, different equations will give vastly different predictions. Methods for selecting single well-fitting equations from a large set will be reviewed at the end of this section. <PARAGRAPH_SEPARATOR> In most applications, however, the particular variables affecting response and the directions of their effects are of intrinsic interest and then the selection of just one wellfitting equation from among many is unsatisfactory and possibly very misleading. <PARAGRAPH_SEPARATOR> In principle, the following procedure seems a sensibly cautious approach in such situations. All possible $2^{p}$ equations are fitted (Garside, 1965; Schatzoff et al., 1968; Morgan and Tatar, 1972) and those clearly inconsistent with the data rejected; that is equations with a residual mean square significantly greater than the mean square residual from the full model are rejected. Typically if an equation involving a subset $\\mathcal{S}$ of explanatory variables is consistent with the data, so is that based on a larger subset ${\\mathcal{S}}^{\\prime}$ $\\mathcal{S}^{\\prime}\\supset\\mathcal{S}$ (Any exceptions to this will be minor ones depending on the particular levels of significance used.) Such a subset we call primitive. A program to find the primitive models and associated information has been written at Imperial College by Mrs M. Ansell and is available for use on the CDC 6400 computer. If there is only one primitive model, the situation is fairly clear-cut; where there is more than one, a choice between them can be made only on the basis of additional information."
  },
  {
    "qid": "statistic-posneg-1437-3-0",
    "gold_answer": "TRUE. The context explicitly states that the performances of these affine-invariant test statistics did not change when the structure of the population’s variance–covariance matrix changed from $\\mathbf{I}_{3}$ to $\\mathbf{EE^{\\prime}}$. This is evidenced by comparing Table 3 with Table 4 and Table 5 with Table 6, which show consistent results across different variance–covariance matrices.",
    "question": "Is it true that the affine-invariant test statistics ($T^{2}$, $W_{w n p-1}$, $W_{l n p-1}$, PR, and $S_{n}$) showed consistent performance regardless of the change in the population variance–covariance matrix from $\\mathbf{I}_{3}$ to $\\mathbf{EE^{\\prime}}$?",
    "question_context": "Comparison of Test Statistics in Randomized Complete Block Designs\n\nTo compare the performances of $S_{n}$ and $W_{l n p-1}$ with respect to other procedures in the literature, a Monte Carlo study for dimension $p=3$ , sample size $n=40$ , and significance level $\\alpha=.05$ was conducted. Besides $S_{n}$ and $W_{l n p-1}$ , the study included $W_{w n p-1}$ , PR, the ANOVA $F$ test, Hotelling–Hsu $T^{2}$ , Friedman’s test $\\boldsymbol{\\cdot}\\boldsymbol{F}_{\\mathrm{R}})$ , Iman et al. [19] rank transformation version of the ANOVA $F$ test $(F_{\\mathrm{RT}})$ , the rank transformation test applied to centered within block data $(F_{\\mathrm{CRT}})$ and Quade’s test $(W_{\\mathrm{Q}})$ . Note that Quade’s test was computed using the range as the location-free statistic that measures the variability within the components of each observed vector. <PARAGRAPH_SEPARATOR> Five of the 10 procedures considered are computed on the transformed observations, that is the X’s. These are $T^{2}$ , $S_{n}$ , $W_{w n p-1}$ , PR, and $W_{l n p-1}$ , while the rest of the procedures use the original data, the Y’s. The test statistics $F$ , $F_{\\mathrm{RT}}$ , $F_{\\mathrm{CRT}}$ , and $W_{\\mathrm{Q}}$ are compared to upper \fth quantile of an $F$ distribution with $(p-1)$ and $(n-1)(p-1)$ , numerator and denominator degrees of freedom, respectively. No adjustment for the degrees of freedom was made since we noticed that doing so will only correct the $\\texttt{\\em}$ level of the test but will not have any significant effect on the power (see [20]). On the other hand, the tests $F_{\\mathrm{R}}$ , $S_{n}$ , $W_{w n p-1}$ , PR, and $W_{l n p-1}$ are compared to the upper \fth quantile of a chi-square distribution with $p-1$ degrees of freedom. The Hotelling–Hsu $T^{2}$ is compared to $(n-1)(p-1)/(n-p+1)$ times the upper \fth quantile of an $F$ distribution with $(p-1)$ and $(n-p+1)$ , numerator and denominator degrees of freedom, respectively. <PARAGRAPH_SEPARATOR> The test statistics were compared under elliptically symmetric distributions with densities from the power family and the family of multivariate $t\\cdot$ -distributions. From the power family, the heavy-tailed case $y=.1\\AA$ ), the multivariate normal case $(\\nu=1)$ ), and the light-tailed case $(\\nu=5)$ ) were used. From the family of multivariate $t\\cdot$ -distributions, distributions with degrees of freedom of 1, 3, and 10 were considered. For test statistics that use the original observations, the distributions were located at ${\\sf\\Psi}{\\sf\\Psi}=(k\\delta,k\\delta,0)$ , $k=0,1,2,3$ , and for the rest of the tests, the distributions were located at $\\mathbf{\\delta}\\mathbf{\\theta}=(k\\delta,k\\delta)$ , $k=0,1,2,3$ . The value $\\delta$ was adjusted for different distributions to obtain somewhat similar points on the power curve. Two different types of the population variance–covariance matrix were considered, the first is $\\pmb{\\Sigma}=\\mathbf{I}_{3}$ , the identity matrix and the second is <PARAGRAPH_SEPARATOR> $$ \\Sigma={\\bf E}{\\bf E}^{\\prime}=\\left[\\begin{array}{c c c}{1}&{-1}&{0}\\\\ {-1}&{2}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]. $$ <PARAGRAPH_SEPARATOR> The first choice satisfies the Huynh and Feldt condition, but $\\Sigma$ in (14) does not. This was done in order to show the dependence of the performances of $F_{\\cdot}$ , $F_{\\mathrm{R}}$ , $F_{\\mathrm{RT}}$ , $F_{\\mathrm{CRT}}$ , and $W_{\\mathrm{Q}}$ on the type of the population variance–covariance matrix. Tables 3 and 4 contain the results from the Monte Carlo study for distributions from the power family, while Tables 5 and 6 contain those for the distributions from the family of $t$ -distributions. Entries in each table are the proportion of times out of 3000 iterations in which each test statistic exceeded the upper $\\alpha$ -percentile of the corresponding null distribution. <PARAGRAPH_SEPARATOR> For the affine-invariant test statistics considered, that is, $T^{2}$ , $W_{w n p-1}$ , $W_{l n p-1}$ , PR, and $S_{n}$ , the results from the Monte Carlo studies agree with those obtained from the efficiency comparisons. Moreover, these performances did not change when the structure of the population’s variance–covariance matrix changed from ${\\bf I}_{3}$ to $\\mathbf{EE^{\\prime}}$ . This can be seen by comparing Table 3 with Table 4 and Table 5 with Table 6. <PARAGRAPH_SEPARATOR> Comparing all 10 tests, the following observations are made. When the underlying distribution is multivariate normal with $\\pmb{\\Sigma}=\\mathbf{I}_{3}$ , all 10 tests showed a good performance, with"
  },
  {
    "qid": "statistic-posneg-1437-3-1",
    "gold_answer": "FALSE. The context states that these test statistics are compared to the upper quantile of an $F$ distribution with $(p-1)$ and $(n-1)(p-1)$ degrees of freedom, not $(p-1)$ and $(n-p+1)$. The latter degrees of freedom are mentioned in the context of the Hotelling–Hsu $T^{2}$ test.",
    "question": "Is it correct that the test statistics $F$, $F_{\\mathrm{RT}}$, $F_{\\mathrm{CRT}}$, and $W_{\\mathrm{Q}}$ are compared to the upper quantile of an $F$ distribution with $(p-1)$ and $(n-p+1)$ degrees of freedom?",
    "question_context": "Comparison of Test Statistics in Randomized Complete Block Designs\n\nTo compare the performances of $S_{n}$ and $W_{l n p-1}$ with respect to other procedures in the literature, a Monte Carlo study for dimension $p=3$ , sample size $n=40$ , and significance level $\\alpha=.05$ was conducted. Besides $S_{n}$ and $W_{l n p-1}$ , the study included $W_{w n p-1}$ , PR, the ANOVA $F$ test, Hotelling–Hsu $T^{2}$ , Friedman’s test $\\boldsymbol{\\cdot}\\boldsymbol{F}_{\\mathrm{R}})$ , Iman et al. [19] rank transformation version of the ANOVA $F$ test $(F_{\\mathrm{RT}})$ , the rank transformation test applied to centered within block data $(F_{\\mathrm{CRT}})$ and Quade’s test $(W_{\\mathrm{Q}})$ . Note that Quade’s test was computed using the range as the location-free statistic that measures the variability within the components of each observed vector. <PARAGRAPH_SEPARATOR> Five of the 10 procedures considered are computed on the transformed observations, that is the X’s. These are $T^{2}$ , $S_{n}$ , $W_{w n p-1}$ , PR, and $W_{l n p-1}$ , while the rest of the procedures use the original data, the Y’s. The test statistics $F$ , $F_{\\mathrm{RT}}$ , $F_{\\mathrm{CRT}}$ , and $W_{\\mathrm{Q}}$ are compared to upper \fth quantile of an $F$ distribution with $(p-1)$ and $(n-1)(p-1)$ , numerator and denominator degrees of freedom, respectively. No adjustment for the degrees of freedom was made since we noticed that doing so will only correct the $\\texttt{\\em}$ level of the test but will not have any significant effect on the power (see [20]). On the other hand, the tests $F_{\\mathrm{R}}$ , $S_{n}$ , $W_{w n p-1}$ , PR, and $W_{l n p-1}$ are compared to the upper \fth quantile of a chi-square distribution with $p-1$ degrees of freedom. The Hotelling–Hsu $T^{2}$ is compared to $(n-1)(p-1)/(n-p+1)$ times the upper \fth quantile of an $F$ distribution with $(p-1)$ and $(n-p+1)$ , numerator and denominator degrees of freedom, respectively. <PARAGRAPH_SEPARATOR> The test statistics were compared under elliptically symmetric distributions with densities from the power family and the family of multivariate $t\\cdot$ -distributions. From the power family, the heavy-tailed case $y=.1\\AA$ ), the multivariate normal case $(\\nu=1)$ ), and the light-tailed case $(\\nu=5)$ ) were used. From the family of multivariate $t\\cdot$ -distributions, distributions with degrees of freedom of 1, 3, and 10 were considered. For test statistics that use the original observations, the distributions were located at ${\\sf\\Psi}{\\sf\\Psi}=(k\\delta,k\\delta,0)$ , $k=0,1,2,3$ , and for the rest of the tests, the distributions were located at $\\mathbf{\\delta}\\mathbf{\\theta}=(k\\delta,k\\delta)$ , $k=0,1,2,3$ . The value $\\delta$ was adjusted for different distributions to obtain somewhat similar points on the power curve. Two different types of the population variance–covariance matrix were considered, the first is $\\pmb{\\Sigma}=\\mathbf{I}_{3}$ , the identity matrix and the second is <PARAGRAPH_SEPARATOR> $$ \\Sigma={\\bf E}{\\bf E}^{\\prime}=\\left[\\begin{array}{c c c}{1}&{-1}&{0}\\\\ {-1}&{2}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]. $$ <PARAGRAPH_SEPARATOR> The first choice satisfies the Huynh and Feldt condition, but $\\Sigma$ in (14) does not. This was done in order to show the dependence of the performances of $F_{\\cdot}$ , $F_{\\mathrm{R}}$ , $F_{\\mathrm{RT}}$ , $F_{\\mathrm{CRT}}$ , and $W_{\\mathrm{Q}}$ on the type of the population variance–covariance matrix. Tables 3 and 4 contain the results from the Monte Carlo study for distributions from the power family, while Tables 5 and 6 contain those for the distributions from the family of $t$ -distributions. Entries in each table are the proportion of times out of 3000 iterations in which each test statistic exceeded the upper $\\alpha$ -percentile of the corresponding null distribution. <PARAGRAPH_SEPARATOR> For the affine-invariant test statistics considered, that is, $T^{2}$ , $W_{w n p-1}$ , $W_{l n p-1}$ , PR, and $S_{n}$ , the results from the Monte Carlo studies agree with those obtained from the efficiency comparisons. Moreover, these performances did not change when the structure of the population’s variance–covariance matrix changed from ${\\bf I}_{3}$ to $\\mathbf{EE^{\\prime}}$ . This can be seen by comparing Table 3 with Table 4 and Table 5 with Table 6. <PARAGRAPH_SEPARATOR> Comparing all 10 tests, the following observations are made. When the underlying distribution is multivariate normal with $\\pmb{\\Sigma}=\\mathbf{I}_{3}$ , all 10 tests showed a good performance, with"
  },
  {
    "qid": "statistic-posneg-1911-0-0",
    "gold_answer": "FALSE. The paper establishes that the maximum diameter of the confidence region R_N is asymptotically bounded by 2d as d approaches 0, but it does not guarantee exact equality for any finite sample size N. The result relies on asymptotic properties and the condition that N(d) → ∞ as d → 0.",
    "question": "Is it true that the sequential procedure described in the paper guarantees that the maximum diameter of the confidence region R_N is exactly 2d for any finite sample size N?",
    "question_context": "Sequential Confidence Regions for Regression Parameters\n\nIn this paper Chow and Robbins's (1965) sequential theory has been extended to construct a confidence region with prescribed maximum width and prescribed coverage probability for (i) the linear regression parameters, and (i) the mean vector of a multivariate population. Except for the finiteness of the unknown variance and covariances in the respective two cases, no assumption regarding the form of their distribution functions is made. This result was announced in Srivastava (1966a). For some related results, refer to Gleser (1965), Srivastava (1966b) and Starr (1966)."
  },
  {
    "qid": "statistic-posneg-553-5-1",
    "gold_answer": "FALSE. The formula $4.3n^{2/9}$ shows that the number of basis functions increases with the sample size $n$, as $n^{2/9}$ is a monotonically increasing function of $n$.",
    "question": "Does the formula $4.3n^{2/9}$ for the number of basis functions imply that the number of basis functions decreases as the sample size $n$ increases?",
    "question_context": "Smoothing Spline ANOVA Model Performance Comparison\n\nMethod\tR2\tRootfittingMSE\tRoot prediction MSE (mean)\tRoot prediction MSE (sd)\tCPU time (s)\nASP-U\t0.925\t1.130\t1.134\t0.006\t1.596\nASP-A\t0.926\t1.124\t1.134\t0.003\t1.969\nGAM-GCV\t0.911\t1.229\t1.226\t0.003\t4.891\nBAM\t0.913\t1.219\t1.224\t0.006\t0.490\nSKIP\t0.918\t1.173\t1.162\t0.010\t193.788\n\n<PARAGRAPH_SEPARATOR>\n\nMSE, mean squared error; sd, standard deviation; ASP-U, asympirical method using uniform sampling; ASP-A, asympirical method using asymptotic sampling; GAM-GCV, generalized cross-validation for generalized additive models; BAM, fast restricted maximum likelihood for generalized additive models; SKIP, skip method.\n\n<PARAGRAPH_SEPARATOR>\n\nTherefore, there are 36 predictors for this dataset. We fit the cubic tensor product smoothing spline ANOVA model to the dataset. Based on the preliminary model diagnostics (Gu, 2004), we consider the following functional ANOVA decomposition:\n\n<PARAGRAPH_SEPARATOR>\n\n$$\n\\begin{array}{l}{\\displaystyle\\eta(x)=\\eta_{\\infty}+\\sum_{j=1}^{36}\\eta_{j}(x_{\\langle j\\rangle})}\\ {\\displaystyle\\qquad+\\eta_{9,10}(x_{\\langle9\\rangle},x_{\\langle10\\rangle})+\\eta_{9,13}(x_{\\langle9\\rangle},x_{\\langle13\\rangle})+\\eta_{24,35}(x_{\\langle24\\rangle},x_{\\langle35\\rangle})+\\eta_{25,36}(x_{\\langle25\\rangle},x_{\\langle36\\rangle}),}\\end{array}\n$$\n\n<PARAGRAPH_SEPARATOR>\n\nwhere $x_{\\langle j\\rangle}~(j=1,\\dots,36)$ is the $j$ th predictor. The numbers of smoothing parameters for the proposed methods and the methods under the generalized additive models framework are 48 and 44, respectively. Bearing in mind the limits on computational resources, we set the number of basis functions to $4.3n^{2/9}$ for all methods (Kim & Gu, 2004).\n\n<PARAGRAPH_SEPARATOR>\n\nWe compare the fitting and prediction errors of these smoothing parameter selection methods in Table 2. The mean and standard deviation of the five root mean squared error results for the testing datasets are reported as the prediction error. Since the generalized cross-validation method was infeasible even for one iteration, we compared only the proposed methods and the methods under the generalized additive models framework with the skip method. We also compared the proposed methods with the fast restricted maximum likelihood method for generalized additive models (Wood et al., 2017). The proposed method based on asymptotic sampling has the best performance in terms of fitting and prediction errors. The fast restricted maximum likelihood method for generalized additive models is the fastest in terms of CPU time."
  },
  {
    "qid": "statistic-posneg-1093-0-0",
    "gold_answer": "FALSE. The condition $e(\\theta_{1},\\bar{\\theta}_{2})^{\\mathsf{T}}e(\\theta_{1},\\bar{\\theta}_{2})\\leqslant\\gamma$ is the original likelihood ratio-based confidence region, while $\\tau_{1}^{\\mathsf{T}}(I-T)\\tau_{1}\\leqslant\\rho$ is the ellipsoidal tangent plane approximation obtained after compensating for intrinsic nonlinearity. The two conditions are not equivalent; the latter is an approximation of the former after accounting for curvature in the expectation surface.",
    "question": "Is the condition for the likelihood ratio-based confidence region given by $e(\\theta_{1},\\bar{\\theta}_{2})^{\\mathsf{T}}e(\\theta_{1},\\bar{\\theta}_{2})\\leqslant\\gamma$ equivalent to the condition $\\tau_{1}^{\\mathsf{T}}(I-T)\\tau_{1}\\leqslant\\rho$ when compensating for intrinsic nonlinearity?",
    "question_context": "Confidence Regions for Parameter Subsets in Nonlinear Regression\n\nThe nonlinear regression model relates a vector of observations to the corresponding independent variables through the expectations, where the model function depends nonlinearly on the parameters. The additive error vector is assumed to have a spherical normal distribution with zero mean vector and covariance matrix. Improved approximations for confidence regions for the parameter subset are derived using a two-stage procedure. In the first stage, confidence regions are defined as a subset of the expectation surface, and approximate projections onto the tangent plane are obtained. In the second stage, the mapping from the relevant portion of the tangent plane to the parameter space is approximated."
  },
  {
    "qid": "statistic-posneg-384-0-1",
    "gold_answer": "FALSE. The condition $\\phi(1)=0$ ensures that when all $u_i = 1$, the copula $C(1,\\dots,1) = \\phi^{\\leftarrow}(0) = 1$, which is the correct value for a copula at the upper bound. However, this does not imply independence. The independence copula would require $C(u_{1},\\dots,u_{d}) = \\prod_{i=1}^d u_i$, which is a specific case not guaranteed by $\\phi(1)=0$ alone.",
    "question": "Does the condition $\\phi(1)=0$ for the Archimedean generator $\\phi$ imply that the copula $C(u_{1},\\dots,u_{d})$ reduces to the independence copula when all $u_i = 1$?",
    "question_context": "Tail Behavior Classification of Multivariate Archimedean Copulas\n\nA $d$ -variate copula $C$ is the restriction to $[0,1]^{d}$ of a $d$ -variate distribution function with uniform (0, 1) margins. By Sklar’s theorem, a copula is what remains of an arbitrary (continuous) $d$ -variate distribution function $F$ when stripped of its margins. Margin-free measures of dependence such as Kendall’s $\\tau$ or Spearman’s $\\rho$ depend on $F$ only through C. More generally, copulas form a natural way to describe the dependence between variables when making abstraction of their marginal distributions. Overviews of the probabilistic and statistical aspects of copulas are to be found for instance in [1–6]. A copula $C$ is called Archimedean [7,8] if it is of the form $$ C(u_{1},\\dots,u_{d})=\\phi^{\\leftarrow}(\\phi(u_{1})+\\cdot\\cdot\\cdot+\\phi(u_{d})) $$ for $(u_{1},\\dots,u_{d})\\in[0,1]^{d}$ , where the Archimedean generator $\\phi:[0,1]\\rightarrow[0,\\infty]$ is convex, decreasing and satisfies $\\phi(1)=0$ and where $$ \\phi^{\\leftarrow}(y)=\\operatorname*{inf}\\{t\\in[0,1]:\\phi(t)\\leqslant y\\} $$ for $y\\in[0,\\infty]$ is the generalized inverse of $\\phi$ . See Table 1 for a list of some common parametric families of Archimedean generators. A necessary and sufficient condition for the right-hand side of (1.1) to specify a copula is that the function $\\phi^{\\leftarrow}$ is $d\\cdot$ -monotone on $(0,\\infty)$ , that is, $\\phi^{\\leftarrow}$ is $d-2$ times continuously differentiable, $(-D)^{k}\\phi^{\\leftarrow}\\geqslant0$ for all $k\\in\\{0,1,\\ldots,d-2\\}$ (with $D$ the derivative operator), and $(-D)^{d-2}\\phi^{\\leftarrow}$ is convex [3]. Note that if $d=2$ , then the conditions on $\\phi$ mentioned in the beginning of this paragraph are necessary and sufficient. If $\\phi^{\\leftarrow}$ is completely monotone, that is, if $(-\\stackrel{.}{D})^{k}\\phi^{\\leftarrow}\\geqslant0$ for all integer $k\\geqslant0$ , then $\\phi^{\\leftarrow}$ is also $d$ -monotone for every integer $d\\geqslant2$ and the resulting model can be interpreted as a frailty model [9,10]. Because of their analytic tractability, Archimedean copulas have enjoyed a great popularity in applied work, from insurance [11,12] and finance [13,14] to hydrology [1,15,16] and survival analysis [17–19], to mention just a few references. More flexible models can be obtained by forming mixtures of Archimedean copulas [20], or by constructing hierarchical (or nested) Archimedean copulas [21]; however, this will not be considered in this paper."
  },
  {
    "qid": "statistic-posneg-656-0-0",
    "gold_answer": "TRUE. The context explicitly states that when λη→∞, the weight functions behave as W1=Op((λη)−1) and W2=1+Op((λη)−1). This is derived from the properties of the local linear–additive estimator and is consistent with the asymptotic behavior described in the paper. The explanation is supported by the mathematical derivations and the conditions provided in the context.",
    "question": "Is the statement 'When λη→∞, the weight functions satisfy W1=Op((λη)−1) and W2=1+Op((λη)−1)' consistent with the provided formulas and the context of the paper?",
    "question_context": "Local-linear-additive-estimation-for-multiple-non-parametric-regression\n\nThe paper discusses the properties of local linear–additive estimators in non-parametric regression. It provides proofs for the convergence rates and behavior of the estimators under different conditions, particularly focusing on the weights and error terms involved in the estimation process."
  },
  {
    "qid": "statistic-posneg-1246-0-2",
    "gold_answer": "TRUE. The paper explicitly states that the bottom-up method, by construction, cannot separate mutual clusters, whereas the top-down method can break mutual clusters. This is because mutual clusters are preserved in the bottom-up approach, while the top-down method may join points in mutual clusters with external points before fully joining all points within the mutual cluster.",
    "question": "Does the paper claim that the bottom-up method cannot break mutual clusters by construction, while the top-down method can?",
    "question_context": "Hybrid Hierarchical Clustering Performance Metrics\n\nThe paper evaluates clustering methods using three performance measures: the number of 'broken' mutual clusters, the percentage of misclassified points, and the relative within-cluster sum of squares (WSS). The misclassification measure M(C,T) is defined as the normalized Hamming distance between the true and estimated clusterings. The WSS is calculated based on partitioning the dendrogram into the true number of clusters, and relative WSS is reported as the ratio of WSS for the estimated clustering to WSS for the true clustering."
  },
  {
    "qid": "statistic-posneg-829-0-1",
    "gold_answer": "TRUE. The condition $\\operatorname*{det}(I-\\varPhi(1)z-\\cdots-\\varPhi(p)z^{p})\\neq0$ for all complex $z$ with $|z|=1$ ensures that the VAR(p) process has a unique stationary solution. This condition guarantees that the process is stable and does not have unit roots, which is essential for stationarity.",
    "question": "Is it true that for a stationary VAR(p) process, the condition $\\operatorname*{det}(I-\\varPhi(1)z-\\cdots-\\varPhi(p)z^{p})\\neq0$ for all complex $z$ with $|z|=1$ is necessary and sufficient for the existence of a unique stationary solution?",
    "question_context": "Subset Vector Autoregressive (SVAR) Models and Forecasting\n\nVector autoregressive (VAR) processes constitute a widely used class of models for zero-mean stationary multivariate time series, i.e., series $\\{\\mathbf{X}_{t}=\\left(X_{t,1},\\ldots,X_{t,d}\\right)^{\\prime}$ ; $t=0,\\pm1,\\ldots\\}$ for which $\\mathbf{EX}_{t}=\\mathbf{0}$ ; and $\\mathbf{E}(\\mathbf{X}_{t+h}\\mathbf{X}_{t}^{\\prime})$ is finite and independent of $t$ for all integers $h$ : The matrix $\\varGamma(h)=\\mathbf E(\\mathbf X_{t+h}\\mathbf X_{t}^{\\prime})$ is called the autocovariance at lag $h$ of the process $\\left\\{{\\mathbf{X}}_{t}\\right\\}$ : The process $\\{\\mathbf{X}_{t},~t=0,\\pm1,\\ldots\\}$ ; is a $\\operatorname{VAR}(p)$ process if it is a stationary solution of the equations, $\\mathbf{X}_{t}=\\boldsymbol{\\varPhi}(1)\\mathbf{X}_{t-1}+\\cdots+\\boldsymbol{\\varPhi}(\\boldsymbol{p})\\mathbf{X}_{t-p}+\\mathbf{Z}_{t},$ where, $\\varPhi(1),\\ldots,\\varPhi(p)$ are $d\\times d$ constant matrices (the VAR coefficients), and $\\{\\mathbf{Z}_{t}\\}$ is a sequence of zero-mean uncorrelated random vectors, each with covariance matrix $\\boldsymbol{\\Sigma}$ : Such a process $\\{\\mathbf{Z}_{t}\\}$ is termed white-noise, and we write: $\\{\\mathbf{Z}_{t}\\}\\sim\\mathbf{W}\\mathbf{N}(\\mathbf{0},{\\boldsymbol{\\Sigma}})$ : A stationary solution exists (and is unique) if and only if $\\operatorname*{det}(I-\\varPhi(1)z-\\cdots-\\varPhi(p)z^{p})$ is non-zero for all complex $z$ with $|z|=1$ : If there is a positive integer $k_{m}$ and a proper subset $K=\\{k_{1},\\ldots,k_{m}\\}$ of $\\{1,2,...,k_{m}\\}$ such that $\\boldsymbol{\\varPhi}(i)=0$ for $i\\not\\in K$ ; and $\\boldsymbol{\\varPhi}(i)\\neq0$ for $i\\in K$ ; then $\\left\\{{\\mathbf{X}}_{t}\\right\\}$ is called a $\\operatorname{SVAR}(K)$ process (subset vector autoregressive process with lags in $K$ ) with coefficients $\\phi_{K}(i),i\\in K$ ; and white-noise covariance matrix $\\boldsymbol{\\Sigma}$ ; if it is a stationary solution of the equations, ${\\bf X}_{t}=\\sum_{i\\in K}\\phi_{K}(i){\\bf X}_{t-i}+{\\bf Z}_{t},\\quad\\{{\\bf Z}_{t}\\}\\sim{\\bf W N}(\\mathbf{0},\\boldsymbol{\\itSigma}).$ The fitting of SVAR models is appropriate when it is believed that the regression of $\\mathbf{X}_{t}$ on past values, say $\\mathbf{X}_{t-j}$ ; $j=1,\\ldots,k_{m}$ ; involves only a subset of the lagged values. For an arbitrary stationary process $\\left\\{{\\mathbf{X}}_{t}\\right\\}$ ; the best linear forecast of $\\mathbf{X}_{t}$ (i.e., the one with minimum mean squared error for each component) in terms of $\\{\\mathbf{X}_{t-i},i\\in K\\}$ can be expressed as $\\hat{\\mathbf{X}}_{t}^{(f)}(K)=\\sum_{i\\in K}\\phi_{K}^{(f)}(i)\\mathbf{X}_{t-i},$ with prediction error covariance matrix, $U_{K}^{(f)}\\equiv E[({\\bf X}_{t}-\\hat{{\\bf X}}_{t}^{(f)}(K))({\\bf X}_{t}-\\hat{{\\bf X}}_{t}^{(f)}(K))^{\\prime}]$ : The coefficients, $\\varPhi_{K}^{(f)}(i)$ ; $i\\in K$ ; and error covariance matrix, $U_{K}^{(f)}$ ; can be determined by solving the Yule–Walker (YW) equations, $\\begin{array}{l}{{\\displaystyle{\\cal T}(k)=\\sum_{i\\in{\\cal K}}\\phi_{\\cal K}^{(f)}(i){\\cal T}(k-i),\\quad k\\in{\\cal K},}}\\ {{\\displaystyle{\\cal U}_{\\cal K}^{(f)}={\\cal T}(0)-\\sum_{i\\in{\\cal K}}\\phi_{\\cal K}^{(f)}(i){\\cal T}(i)^{\\prime}}.}\\end{array}$ Remark 1.1. If $\\{\\mathbf{X}_{t}\\}$ satisfies (1) and $\\mathbf{Z}_{t}$ is uncorrelated with $\\left\\{{\\mathbf{X}}_{s},~s<t\\right\\}$ for each $t$ ; then $\\phi_{K}^{(f)}(i)=\\phi_{K}(i)$ ; $i\\in K$ ; and $U_{K}^{(f)}=\\Sigma$ : Remark 1.2. If $\\varGamma(\\cdot)$ is unknown, it can be estimated from the sample $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}$ as $\\hat{\\Gamma}(h)=\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{1}{n}\\sum_{t=1}^{n-h}\\mathbf{x}_{t+h}\\mathbf{x}_{t}^{\\prime}}&{\\mathrm{if}h\\geqslant0,}\\ {\\hat{\\Gamma}(-h)^{\\prime}}&{\\mathrm{if}h<0.}\\end{array}\\right.$ Eqs. (3) and (4) are then known as the empirical Yule–Walker equations for $\\varPhi_{K}^{(f)}(i)$ ; $i\\in K$ ; and $U_{K}^{(f)}$ : As well as the best linear forecast defined above, we need to introduce the best linear backward estimate of $\\mathbf{X}_{t}$ in terms of $\\{\\mathbf{X}_{t+i},i\\in K\\}$ ; i.e., $\\hat{\\mathbf{X}}_{t}^{(b)}(K)=\\sum_{i\\in K}\\Psi_{K}^{(b)}(i)\\mathbf{X}_{t+i},$ which is determined, together with the corresponding error covariance matrix, $V_{K}^{(b)}=E[({\\bf X}_{t}-\\hat{\\bf X}_{t}^{(b)}(K))({\\bf X}_{t}-\\hat{\\bf X}_{t}^{(b)}(K))^{\\prime}]$ ; by the backward Yule–Walker equations, ${\\cal{T}}(k)^{\\prime}=\\sum_{i\\in K}\\Psi_{K}^{(b)}(i){\\cal{T}}(k-i)^{\\prime},\\quad k\\in{\\cal{K}},$ $V_{K}^{(b)}={\\cal{T}}(0)-\\sum_{i\\in K}\\:\\Psi_{K}^{(b)}(i){\\cal{T}}(i).$ The problems of fitting a subset VAR model and of computing forecasts based on a specified set of lagged observations are closely related. This relationship will play a key role in the asymptotic development."
  },
  {
    "qid": "statistic-posneg-958-0-1",
    "gold_answer": "TRUE. The expected value of CV(k) decomposes into the MSE of ψ̂k on the training data plus the variance of ψ̂0 on the validation data. Since the latter is constant across k, minimizing CV(k) effectively minimizes the training MSE of ψ̂k, provided ψ̂0 is unbiased. The paper also discusses adjustments for cases where ψ̂0 may be biased.",
    "question": "The cross-validation criterion function CV(k) is designed to minimize the mean-squared error of the estimator ψ̂k by selecting the nuisance parameter model that yields the smallest training sample MSE, assuming ψ̂0 is unbiased. Is this interpretation correct?",
    "question_context": "Semiparametric Model Selection Criterion for Marginal Structural Models\n\nThe paper discusses a semiparametric model selection methodology for marginal structural models (MSMs), focusing on selecting nuisance parameter models that minimize the mean-squared error of the estimator of the parameter of interest, ψ. The methodology employs cross-validation to balance the bias-variance trade-off in finite samples. Key formulas include the IPTW estimating equation, the orthogonalized estimating function for double robustness, and the cross-validation criterion function for model selection."
  },
  {
    "qid": "statistic-posneg-1143-0-0",
    "gold_answer": "TRUE. The formula explicitly uses $f(X_i)$ in the denominator, which implies that $f$ must be known or specified. In practice, when $f$ is unknown, the paper proposes using a kernel density estimator (KDE) to approximate $f(X_i)$, but the original formula assumes $f$ is known.",
    "question": "In the EM algorithm for the semiparametric mixture model, the E-step computes the conditional expectation of $Z_i$ as: $$Z_{i}^{(k+1)} = \\frac{\\pi^{(k)}\\phi_{\\sigma}(X_{i})}{\\pi^{(k)}\\phi_{\\sigma}(X_{i}) + (1-\\pi^{(k)})f(X_{i})}.$$ Is it correct to say that this formula assumes $f$ is known?",
    "question_context": "Semiparametric Mixture Model Estimation via EM Algorithm and Kernel Density Estimation\n\nThe paper discusses a semiparametric mixture model where observations are drawn from a mixture of a known normal distribution and an unknown distribution. The model is given by: $$X_{i}\\sim\\pi\\phi(\\mu,\\sigma^{2}I_{p})+(1-\\pi)f,\\quad i=1,\\ldots,n,$$ where $\\phi(\\mu,\\sigma^{2}I_{p})$ is the density of $N_{p}(\\mu,\\sigma^{2}I_{p})$, $\\mu$ is known (assumed to be 0), and $f$ is an unknown density. The EM algorithm is used for estimation, with the E-step involving the conditional expectation of the latent variables $Z_i$ given the current parameter estimates and the data. The M-step updates the parameters $\\pi$ and $\\sigma$ based on these expectations. When $f$ is unknown, a kernel density estimator (KDE) is used to approximate $f$."
  },
  {
    "qid": "statistic-posneg-1805-1-0",
    "gold_answer": "FALSE. The moving window FPCA technique uses a series of moving time windows $[0,U]$ to represent the changing features of biomarker trajectories over the entire follow-up period, unlike the fixed window FPCA model which uses a single invariable time window $[0,\\tau]$.",
    "question": "Is it true that the moving window FPCA technique uses a fixed time window $[0,\\tau]$ to obtain FPCA scores, similar to the fixed window FPCA model?",
    "question_context": "Dynamic Prediction Using Functional Principal Components Analysis with Moving Time Windows\n\nWe hereby propose a more flexible FPCA technique for dynamic prediction, which accounts for the dynamic patterns of the biomarker trajectories through a novel moving time window technique. The new proposed method is a further extension of the above fixed window FPCA model, where series of moving time windows are used to represent the changing features of biomarker trajectory over the entire period of follow-up. Denote the time point at which to make prediction as $U.$ , and the moving window refers to the time range $[0,U]$ . Denote the maximum follow-up as $\\kappa$ , where $\\kappa>U$ . Unlike fixed window FPCA, where only one invariable cluster of $\\gamma_{i k}\\mathrm{s}$ is obtained in the fixed time window $[0,\\tau]$ , $0<\\tau\\leqslant\\kappa$ , moving window FPCA is committed to obtain window-specified FPCA scores for each specific time window. For this, first, the maximum time window $[0,\\kappa]$ is used to perform FPCA inference to obtain eigenfunctions $\\rho_{k}(t),k=1,\\ldots,K,t\\in[0,\\kappa]$ , eigenvalues $\\phi_{k}$ and mean function $\\mu(t),t\\in[0,\\kappa].$ The number of principal components $K$ is chosen in this step through cross-validation."
  },
  {
    "qid": "statistic-posneg-850-1-3",
    "gold_answer": "TRUE. The model specifies that the random effects u_i are independently distributed as N_{16}(0_{16}, Σ), as shown in the formula for u_1, ..., u_30. This allows for flexible subject-level deviations in both the mixing and precision components.",
    "question": "Does the model assume that the random effects u_i are independently distributed with a 16-dimensional normal distribution with mean 0 and covariance matrix Σ?",
    "question_context": "Hierarchical Bayesian Model for Bimodal Angular Judgement Data\n\nWe require a method for evaluating localization performance differences between groups across the entire set of signal azimuths. A reasonable approach is to model the bimodal, angular judgement data via a regression model, with patient group and signal position as covariates, while accounting for the repeated measures aspect of the study design. In this paper, we take a hierarchical Bayesian approach to model judgements of all signals by using structured, correlated circular $B$ -splines across mixing and precision components of a localization performance model."
  },
  {
    "qid": "statistic-posneg-1580-0-1",
    "gold_answer": "TRUE. This is derived from matching the first three cumulants of T_{n,p,0}^* and the approximating random variable R = β₀ + β₁χ²_d. The skewness of T_{n,p,0}^* is indeed √(8/d), as shown in Equation (24) and Remark 4, where d = 8K₂(T_{n,p,0}^*)^3 / K₃(T_{n,p,0}^*)^2.",
    "question": "The skewness of the test statistic T_{n,p,0}^* is given by √(8/d), where d is the approximation degrees of freedom in the 3-c matched χ²-approximation.",
    "question_context": "High-Dimensional Multi-Sample Heteroscedastic MANOVA Test\n\nThe paper discusses a new test for high-dimensional multi-sample heteroscedastic one-way MANOVA problems, comparing it against existing methods like the ZGZ-test and ZZG-test. The proposed test uses a 3-c matched χ²-approximation to better control size and power under various correlation structures."
  },
  {
    "qid": "statistic-posneg-525-0-0",
    "gold_answer": "TRUE. When β = 1, the formula simplifies to $$\\mathrm{hF}=\\frac{(1+1)\\cdot\\mathrm{hP}\\cdot\\mathrm{hR}}{(1\\cdot\\mathrm{hP}+\\mathrm{hR})} = \\frac{2\\cdot\\mathrm{hP}\\cdot\\mathrm{hR}}{\\mathrm{hP}+\\mathrm{hR}}$$, which is the harmonic mean of precision and recall, identical to the standard F1 score. This shows that the hierarchical F-score generalizes the F1 score by introducing a weight parameter β to balance the importance of precision and recall.",
    "question": "Is it true that the hierarchical F-score (hF) formula $$\\mathrm{hF}=\\frac{(1+\\beta^{2})\\cdot\\mathrm{hP}\\cdot\\mathrm{hR}}{(\\beta^{2}\\cdot\\mathrm{hP}+\\mathrm{hR})}$$ reduces to the standard F1 score when β = 1?",
    "question_context": "Hierarchical Confusion Matrix for Classification Performance Evaluation\n\nThe paper discusses hierarchical classification evaluation measures, focusing on hierarchical precision (hP), recall (hR), and F-score (hF). These measures are extensions of binary classification metrics, adapted for hierarchical structures. The hierarchical confusion matrix is introduced, defining true positives (TP_H), true negatives (TN_H), false positives (FP_H), and false negatives (FN_H) in the context of hierarchical classification. The measures are formalized for different problem types, including tree (T) and directed acyclic graph (DAG) structures."
  },
  {
    "qid": "statistic-posneg-1667-1-0",
    "gold_answer": "TRUE. The context explicitly states that the approximation is made by equating the corresponding first two moments of each of these distributions and then solving for $\\pmb{K}$, $\\pmb{A}$, and $\\pmb{B}$ in terms of $\\pmb{a}$, $b$, $\\pmb{\\mathscr{r}_{1}}$, and ${\\pmb r}_{\\pmb\\2}$. This method ensures that the approximated distribution matches the original distribution in terms of its first two moments, which is a common technique in statistical approximations.",
    "question": "Is it true that the approximation $f(W;a,b,\\pmb{r}_{1},\\pmb{r}_{2})$ by $f(K W;A,B,0,0)$ is derived by equating the first two moments of the distributions?",
    "question_context": "Approximation of Dependent Chi-Square Variates Distribution\n\nWe approximate $f(W;a,b,\\pmb{r}_{1},\\pmb{r}_{2})$ (cf. $\\S\\delta)$ by $f(K W;A,B,0,0)$ by equating_the corresponding first two moments of each of these distributions, and then solving for $\\pmb{K}$ $\\pmb{A}$ and $\\pmb{B}$ in terms of $\\pmb{a}$ b, $\\pmb{\\mathscr{r}_{1}}$ and ${\\pmb r}_{\\pmb\\2}$ . Greater accuracy could be obtained by equating more moments, but the results are too complicated to be of practical use.<PARAGRAPH_SEPARATOR>From $\\S3$ we know that $\\begin{array}{r}{{\\cal W}={\\pmb v}_{\\pmb{\\Omega}}/{\\pmb v}_{\\pmb{1}},}\\end{array}$ with the density function of $v_{i}$ given by (9) fo1 $\\cdot{\\mathfrak{i}}=1,2$ Now, an alternative method of obtaining the approximation to $f(W;a,b,\\pmb{r}_{1},\\pmb{r}_{2})$ is frst to approximate $f(v_{1};a,r_{1})$ by $f(k_{1}v_{1};A,0)$ and $f(v_{\\mathfrak{L}};b,\\pmb{\\tau}_{\\mathfrak{L}})$ by $f(k_{\\mathfrak{L}}v_{\\mathfrak{L}};B,0)$ ,where $f(v_{1};a,\\pmb{\\tau}_{1})$ denotes the density function of ${\\pmb v}_{1}$ with $2a=m_{1}-1\\mathbf{D}.\\mathbf{I}$ .&nd $f(v_{\\mathfrak{L}};b,\\pmb{\\tau}_{\\mathfrak{L}})$ the density function of ${\\pmb v}_{\\pmb{\\8}}$ with $2b=m_{8}-1\\mathbf{D}.\\mathbf{F}.$<PARAGRAPH_SEPARATOR>From the results of $\\S4$ it is easy to show that for $f({\\pmb v};{\\pmb a},{\\pmb r})$ , which is the density function of the ratio of two dependent chi-square variates, each with ${\\bf2}a{\\bf D.F.}$<PARAGRAPH_SEPARATOR>$$E(v)=(a-r^{2})/(a-1),$$<PARAGRAPH_SEPARATOR>and<PARAGRAPH_SEPARATOR>$$E(v^{8})={\\frac{(a+1)\\left(a-4r^{2}\\right)+6r^{4}}{(a-1)\\left(a-2\\right)}}\\quad(a>2).$$<PARAGRAPH_SEPARATOR>The corresponding moments of $f(k v;A,0)$ follow immediately. Hence, after equating corresponding moments and solving for $\\boldsymbol{k}$ and $\\pmb{A}$ in terms of $\\pmb{a}$ and $\\pmb{\\mathscr{r}}$ ,we have<PARAGRAPH_SEPARATOR>where<PARAGRAPH_SEPARATOR>$$\\begin{array}{r l}{A=\\{R+(R^{\\mathfrak{L}}-R+1)^{\\sharp}\\}/(R-1)}&{(R>1),}\\ {R=\\frac{(a-1)\\left\\{(a+1)(a-4r^{\\mathfrak{L}})+6r^{\\mathfrak{A}}\\right\\}}{(a-2)(a-r^{2})^{2}}}&{(a>2),}\\ {k=\\frac{(a-r^{2})(A-1)}{(a-1)A}\\leqslant1.}\\end{array}$$<PARAGRAPH_SEPARATOR>and<PARAGRAPH_SEPARATOR>Note that $A\\bumpeq2R/(R-1)$ and $k\\bumpeq1$ for large $\\pmb{a}$<PARAGRAPH_SEPARATOR>Schoeman, in his unpublished thesis, showed by comparing approximate and exact critical values of $\\pmb{v}$ , that the approximation to the distribution of $\\pmb{v}$ is quite satisfactory for degrees of freedom greater than 6.<PARAGRAPH_SEPARATOR>To approximate $f(W;a,b,r_{1},r_{2})\\mathrm{by}f(K W;A,B,0,0),$ it follows that<PARAGRAPH_SEPARATOR>$$A=\\{R_{1}+(R_{1}^{2}-R_{1}+1)^{\\frac{1}{2}}\\}/(R_{1}-1)\\quad\\mathrm{and}\\quad B=\\{R_{2}+(R_{2}^{2}-R_{2}+1)^{\\frac{1}{2}}\\}/(R_{2}-1),$$<PARAGRAPH_SEPARATOR>where<PARAGRAPH_SEPARATOR>$$R_{1}=\\frac{\\left(a-1\\right)\\left\\{\\left(a+1\\right)\\left(a-4r_{1}^{2}\\right)+6r_{1}^{4}\\right\\}}{\\left(a-2\\right)\\left(a-r_{1}^{2}\\right)^{2}},\\quad R_{2}=\\frac{\\left(b-1\\right)\\left\\{\\left(b+1\\right)\\left(b-4r_{2}^{8}\\right)+6r_{2}^{4}\\right\\}}{\\left(b-2\\right)\\left(b-r_{2}^{2}\\right)^{2}},$$<PARAGRAPH_SEPARATOR>$$\\begin{array}{r}{K=k_{1}k_{8},}\\end{array}$$<PARAGRAPH_SEPARATOR>with<PARAGRAPH_SEPARATOR>$$k_{1}=\\frac{\\left(a-r_{1}^{2}\\right)\\left(A-1\\right)}{\\left(a-1\\right)A}\\quad\\mathrm{and}\\quad k_{2}=\\frac{\\left(b-r_{2}^{2}\\right)\\left(B-1\\right)}{\\left(b-1\\right)B}\\quad(a,b>2).$$<PARAGRAPH_SEPARATOR>The degrees of freedom, 2A and $\\mathbf{2}B$ will be fractional.<PARAGRAPH_SEPARATOR>To find critical values of $\\pmb{W},$ we first interpolate in the appropriate tables of critical values of $\\boldsymbol{\\mathbf{\\mathit{W}}}$ for the case of independent experiments published by Schumann $\\pmb{\\&}$ Bradley (1959), and then divide the interpolate by $\\kappa$ . This method can also be used in the case of type II problems by putting ${\\pmb r}_{\\mathfrak{z}}=0$ , $B=b$ with $2b=m_{2}$ (cf. Table 1) and $k_{\\mathfrak{L}}=1$<PARAGRAPH_SEPARATOR>Table 9. Comparison of upper $\\pmb{\\alpha}$ critical values $W_{0m}$ ('independent W' approximation) and the exact values, $W_{0}$"
  },
  {
    "qid": "statistic-posneg-978-0-0",
    "gold_answer": "TRUE. The high p-value $(P=0.88)$ for the hypothesis $\\delta_{c1}=...=\\delta_{c4}$ indicates that the differences in excess mortality across housing quality groups are not statistically significant. This means that housing quality does not have a significant effect on the excess mortality of the unemployed, as the model and the statistical test results suggest.",
    "question": "Based on the model $$ \\lambda_{a,c,h,\\mathrm{unemp}}^{1/2}=\\lambda_{a,c,h,\\mathrm{emp}}^{1/2}+\\delta_{c h} $$ and the statistical results $(P=0.88)$ for the hypothesis $\\delta_{c1}=...=\\delta_{c4}$, is it correct to conclude that housing quality has no significant effect on the excess mortality of the unemployed?",
    "question_context": "Excess Mortality Model for the Unemployed: Housing Quality and Occupational Effects\n\nThe model for the mortality of the unemployed was now specified as $$ \\lambda_{a,c,h,\\mathrm{unemp}}^{1/2}=\\lambda_{a,c,h,\\mathrm{emp}}^{1/2}+\\delta_{c h} $$ where housing quality $\\pmb{h}$ varies over the four groups specified whereas age group $\\pmb{a}$ and main occupational category $\\pmb{c}$ are as before. The estimates $\\hat{\\delta}_{c h}$ are given in Table 2. The model now fits satisfactorily $(P=0.12)$ , and it is seen that housing quality has no significant effect on the excess mortality: the hypothesis $\\delta_{c1}=...=\\delta_{c4},$ $c$ equivalent to non-manual worker, skilled worker or unskilled worker, is easily accepted $(P=0.88)$ , and after this there is still statistical support for claiming an occupational differential excess mortality of exactly the same character as suggested at the end of thelastsection $(P=0.02)$"
  },
  {
    "qid": "statistic-posneg-897-0-0",
    "gold_answer": "TRUE. The p-value of 0.0001 under the elliptical distributional assumption is significantly smaller than the p-value of 0.03 under the general distributional assumption, indicating stronger evidence against the null hypothesis of linearity in the former case. This suggests that the NMCT test is more sensitive to deviations from linearity when the error terms follow an elliptically symmetric distribution.",
    "question": "Is it true that the NMCT test ${\\cal T}_{n}^{\\prime}(U_{n})$ under the elliptical distributional assumption provides stronger evidence against the linearity of the model compared to the general distributional assumption, based on the reported p-values of 0.0001 and 0.03 respectively?",
    "question_context": "Diagnostic Checking for Multivariate Regression Models\n\nTo test the linearity, we used the proposed test $\\textstyle{\\mathcal{T}}_{n}$ in Section 2 and NMCT associated with $\\mathcal{T}_{n}^{\\prime}$ in Section 3. For NMCT, we assumed two cases: the error follows an elliptically symmetric distribution and a general distribution respectively. Therefore, we used respective algorithms to construct NMCT statistics ${\\cal T}_{n}^{\\prime}(U_{n})$ and $\\mathcal{T}_{n}^{\\prime}(E_{n})$ as reported in Section 3. From Fig. 6, we found that the nonlinearity may be mainly from $Y_{3}$ , the Marathon. There might be quadratic curves in the plots of $Y_{3}$ against $X_{i}$ , $i=1,2,3$ . Hence, we chose $X_{3}^{2}$ as a weight function W . With these three tests, the $p$ -values are, respectively, 0.09 with $\\textstyle{\\mathcal{T}}_{n}$ ; 0.0001 with ${\\cal T}_{n}^{\\prime}(U_{n})$ under the elliptical distributional assumption and 0.03 with ${\\cal T}_{n}^{\\prime}(U_{n})$ under a general distribution assumption. Clearly, NMCT suggests a rejection for a linear model. Furthermore, the quadratic curves of $Y_{3}$ against $X_{i}$ , $,i=1,2,3$ implies that the nation with either great strength or weak strength at short running distance may not have good performance in running the Marathon. We then fit a model linearly with $X_{1}$ , $X_{2}$ , $X_{3}$ and $X_{3}^{2}$ . The $p$ -values with the three tests are: 0.97 with $\\textstyle T_{n};0.34$ with ${\\cal T}_{n}^{\\prime}(U_{n})$ under the elliptical distribution assumption and 0.99 with ${\\cal T}_{n}^{\\prime}(U_{n})$ under a general distributional assumption. These tests strongly suggested the tenability of the model with a two-order polynomial of $X_{3}$ ."
  },
  {
    "qid": "statistic-posneg-954-1-2",
    "gold_answer": "TRUE. The formula for φ₂ˢᶜ(f_Q) is a valid modification of φ₁ˢᶜ(f_Q), incorporating additional terms to adjust for the variance and covariance of capture probabilities. The derivation is consistent with the methodology described in the literature for improving the accuracy of population size estimates.",
    "question": "Is the modified sample coverage estimator φ₂ˢᶜ(f_Q) correctly derived from φ₁ˢᶜ(f_Q) as shown in the formula?",
    "question_context": "Estimating Population Sizes Using Capture-Recapture Methods\n\nThe paper discusses various estimators for population size in capture-recapture studies, including jackknife and sample coverage estimators. The formulas provided include the second-jackknife estimator and two sample coverage estimators with modifications. The simulation considers four mixing distributions (P1 to P4) to evaluate the performance of these estimators."
  },
  {
    "qid": "statistic-posneg-422-0-0",
    "gold_answer": "FALSE. While the paper notes that $P(M_{2})\\geq P(M_{1})$ for nested models $M_{1}\\subset M_{2}$ without complexity penalty, the general formula includes a penalty term $-c\\cdot|M_{j}|$ that reduces the prior probability as model dimension increases. Thus, if the complexity penalty $c$ is sufficiently large, the prior probability of $M_{2}$ could be lower than $M_{1}$ despite the Kullback-Leibler divergence term favoring $M_{2}$. The inequality $P(M_{2})\\geq P(M_{1})$ holds only when complexity is not penalized (or penalized lightly).",
    "question": "In the context of nested models, does the prior probability formula $P(M_{j})\\propto\\exp\\left\\{\\int_{\\Theta_{j}}\\left[\\operatorname*{inf}_{\\theta_{m},m\\neq j}D_{K L}\\left(f_{j}(\\cdot|\\theta_{j})\\|f_{m}(\\cdot|\\theta_{m})\\right)\\pi_{j}(\\theta_{j})d\\theta_{j}\\right]-c\\cdot|M_{j}|\\right\\}$ imply that a more complex model $M_{2}$ will always have a higher prior probability than a simpler nested model $M_{1}$?",
    "question_context": "Bayesian Model Selection with Kullback-Leibler Divergence and Complexity Penalty\n\nThe proposed method to derive model prior masses can be applied to any selection problem. In fact, we have seen examples with discrete and continuous models; models with one parameter and models with more than one parameter. Particular results have been obtained in the presence of nested models: if model $M_{1}$ is contained in model $M_{2}$ , then $P(M_{2})\\geq P(M_{1})$ . The result is not surprising, as the more complex model is (at least) as good as the simpler one, and there is no loss in removing the simpler model. Unless an additional loss is placed on model $M_{2}$ for model complexity/dimension. <PARAGRAPH_SEPARATOR> Our approach can be applied to a certain specific category of variable selection problems. Suppose we have a variable of interest $y$ , which the outcome depends on potentially $p$ covariates $(x_{1},\\ldots,x_{p})$ . In general, a variable selection problem would consider all the possible regression models where $y$ is explained by any combination of the $p$ covariates. To assign prior probabilities on this model space, our approach requires further considerations, which are not discussed in this paper. However, we can consider the case where the optional models are formed by adding to the context model (i.e. the simplest model) one variable at a time, from $x_{1}$ to $x_{p}$ , progressively. In this case, we would be in the presence of $p+1$ nested models, creating a scenario that is treatable with our approach. <PARAGRAPH_SEPARATOR> To elaborate. Suppose that the normal linear model is defined to relate $y$ to the potential covariates. That is <PARAGRAPH_SEPARATOR> $$ \\boldsymbol{y}\\sim N_{n}(\\mathbf{X}\\pmb{\\beta},\\sigma^{2}\\mathbf{I}_{n}), $$ <PARAGRAPH_SEPARATOR> where $\\mathbf{X}$ is the design matrix, $\\beta$ is the vector of regression parameters, and $\\sigma^{2}$ the constant error variance. Thus, the generic model can be represented as $\\begin{array}{r}{M_{j}=\\left\\{\\sum_{l=0}^{j}\\beta_{l}x_{l m}+\\varepsilon_{m}\\right.}\\end{array}$ ; $\\pi(\\beta_{0},\\ldots,\\beta_{j})\\}$ , for $j~=~0,\\dots,p$ . On the basis of what has been discussed in Section 4, we assign prior mass proportional to one to each model from $M_{0}$ to $M_{p-1}$ . Whilst the mass assigned to model $M_{p}$ would be determined by the expected minimum Kullback–Leibler divergence from $M_{p}$ to $M_{p-1}$ . In this and similar contexts, we can add a penalty to the prior on the model space to take into account model complexity. The idea fits in our framework naturally, and results in the following model prior. <PARAGRAPH_SEPARATOR> $$ P(M_{j})\\propto\\exp\\left\\{\\int_{\\Theta_{j}}\\left[\\operatorname*{inf}_{\\theta_{m},m\\neq j}D_{K L}\\left(f_{j}(\\cdot|\\theta_{j})\\|f_{m}(\\cdot|\\theta_{m})\\right)\\pi_{j}(\\theta_{j})d\\theta_{j}\\right]-c\\cdot|M_{j}|\\right\\} $$ <PARAGRAPH_SEPARATOR> where $c>0$ and $|M_{j}|$ is the dimension of model $M_{j}$ . However, we do not discuss this more in the current paper."
  },
  {
    "qid": "statistic-posneg-1100-0-0",
    "gold_answer": "TRUE. The condition is correctly stated and ensures that the mixture model parameters are identifiable, meaning that different parameter values cannot produce the same probability distribution of the observed data. This is a fundamental requirement for the statistical inference of the model parameters to be valid.",
    "question": "Is the condition for strong identifiability in the mixture model correctly stated as: if η ∈ (0,1) and h(y,η,γ,θ) = h(y,η′,γ′,θ′), then (η,γ,θ) = (η′,γ′,θ′)?",
    "question_context": "Multidimensional Hypothesis Testing and Euler Characteristic Densities\n\nThe paper discusses the extension of the likelihood ratio test (LRT) to multidimensional nuisance parameters and multivariate data, restating the regularity conditions of Ghosh and Sen (1985) for this context. It provides detailed formulations for the Euler characteristic (EC) densities for Gaussian, chi-square, and mixed chi-square random fields, which are crucial for approximating the p-values of the supremum of random fields used in hypothesis testing."
  },
  {
    "qid": "statistic-posneg-731-3-2",
    "gold_answer": "FALSE. The denominator of ρ² includes not only the regression model term ||AẊ′||²_F but also the observation error variance (nmσ²_ϵ) and the subject-specific variances (∑_{i=1}^n ∑_{k=1}^K σ²_γ_{k,i}). This accounts for total variability in the predictive values.",
    "question": "Is the proportion of variability explained (ρ²) in (12) computed solely based on the regression model term AẊ′?",
    "question_context": "Bayesian Variable Selection in Functional Regression via Expected Loss Minimization\n\nThe paper discusses variable selection in Functional-on-Scalar Regression (FOSR) using a Bayesian approach. The method aims to identify a subset of predictors that maintains predictive ability while promoting sparsity. The approach minimizes the expected loss under the posterior predictive distribution, decoupling shrinkage and selection (DSS). Key formulas include the loss function with a sparsity penalty, the posterior predictive expected loss, and measures of predictive ability like ρ² and ρλ²."
  },
  {
    "qid": "statistic-posneg-347-0-0",
    "gold_answer": "TRUE. The series expansion method relies on the condition q >> p to ensure convergence and accuracy, especially when multiplying by large constants like C. When p and q are comparable, the method fails due to insufficient precision in existing Γ-function tables, as demonstrated in the paper's examples.",
    "question": "Is the condition for using the series expansion method to evaluate the incomplete B-function ratio valid only when q is significantly larger than p?",
    "question_context": "Numerical Evaluation of Incomplete Eulerian Integrals\n\nThe paper discusses methods for evaluating incomplete B-function ratios and incomplete Γ-function ratios, particularly when parameters lie outside published tables. It presents a series expansion approach and compares it with Miller's continued fraction method, highlighting limitations when parameters are comparable in size."
  },
  {
    "qid": "statistic-posneg-1420-0-2",
    "gold_answer": "FALSE. The covariance function of the spatial Gaussian process w_t(s) is specified as σ_t^2 ρ(·; φ_t), which clearly depends on both the spatial variance component σ_t^2 and the correlation function ρ(·, φ_t). Therefore, the statement is incorrect.",
    "question": "The spatial Gaussian process w_t(s) in the model has a covariance function that depends only on the spatial variance component σ_t^2 and not on the correlation function ρ(·, φ_t). Is this statement accurate?",
    "question_context": "Bayesian Dynamic Spatial-Temporal Model for Markov Chain Monte Carlo\n\nGelfand et al. (2005) propose a Bayesian hierarchical model for modelling univariate and multivariate dynamic spatial data, viewing time as discrete and space as continuous. The methods in their paper have been implemented in the R package spBayes. We present a simpler version of the dynamic model as described by Finley et al. (2015). Let $s=1,\\ldots,N_{s}$ be location sites, $t=1,\\ldots,N_{t}$ be time-points, and the observed measurement at location $s$ and time $t$ be denoted by $y_{t}(s)$. In addition, let $x_{t}(s)$ be the $r\\times1$ vector of predictors, observed at location $s$ and time $t$, and let $\\beta_{t}$ be the $r\\times1$ vector of coefficients. Let $\\mathrm{{GP}}\\{0,\\sigma_{t}^{2}\\rho(\\cdot;\\phi_{t})\\}$ denote a spatial Gaussian process with covariance function $\\sigma_{t}^{2}\\rho(\\cdot;\\phi_{t})$. Here $\\sigma_{t}^{2}$ is the spatial variance component and $\\rho(\\cdot,\\phi_{t})$ is a correlation function with exponential decay. For $t=1,\\ldots,N_{t}$, the measurement equation is $y_{t}(s)=x_{t}(s)^{\\mathrm{T}}\\beta_{t}+u_{t}(s)+\\epsilon_{t}(s),\\quad\\epsilon_{t}(s)\\overset{\\mathrm{ind}}{\\sim}N(0,\\tau_{t}^{2});$ the transition equations are $\\beta_{t}=\\beta_{t-1}+\\eta_{t},\\quad\\eta_{t}\\overset{\\mathrm{ind}}{\\sim}N(0,\\Sigma_{\\eta});$ and $u_{t}(s)=u_{t-1}(s)+w_{t}(s),\\quad w_{t}(s)\\overset{\\mathrm{ind}}{\\sim}\\mathrm{GP}\\{0,\\sigma_{t}^{2}\\rho(\\cdot;\\phi_{t})\\}$. The Bayesian hierarchy is completed with priors: $\\beta_{0}\\sim N(m_{0},C_{0}),\\quad u_{0}(s)\\equiv0;$ $\\tau_{t}^{2}\\sim_{\\mathrm{IG}}(a_{\\tau},b_{\\tau}),\\quad\\sigma_{t}^{2}\\sim_{\\mathrm{IG}}(a_{s},b_{s});$ $\\Sigma_{\\eta}\\sim_{\\mathrm{IW}}(a_{\\eta},B_{\\eta}),\\quad\\phi_{t}\\sim_{\\mathrm{Un}}(a_{\\phi},b_{\\phi}),$ where the IW distribution has density proportional to $\\mathrm{det}(\\Sigma_{\\eta})^{-(a_{\\eta}+q+1)/2}\\exp\\{-\\mathrm{tr}(B_{\\eta}\\Sigma_{\\eta}^{-1})/2\\}$ and $\\textstyle\\operatorname{IG}(a,b)$ has density proportional to $x^{-a-1}\\exp(-b/x)$."
  },
  {
    "qid": "statistic-posneg-1421-1-2",
    "gold_answer": "FALSE. While the lower bound is accurate for finite p, it converges to 2πe/ε² as p → ∞. This means the bound is not uniformly accurate for all p but provides a useful approximation, especially for large p, where the dominant term is determined by the choice of ε.",
    "question": "Is the lower bound for the effective sample size given by ESS ≥ (2^(2/p) π) / {p Γ(p/2)}^(2/p) * (χ²_{1−α,p} / ε²) accurate for all values of p, including p → ∞?",
    "question_context": "Multivariate Sequential Termination Rules and Effective Sample Size in Markov Chain Monte Carlo\n\nThe paper discusses multivariate sequential termination rules for Markov Chain Monte Carlo (MCMC) simulations, focusing on constructing asymptotically valid confidence regions. It introduces a fixed-volume sequential stopping rule that terminates the simulation when the volume of the confidence region, adjusted by a relative metric, falls below a specified tolerance. The paper also defines a multivariate effective sample size (ESS) and provides a lower bound for ESS based on the desired precision and confidence level."
  },
  {
    "qid": "statistic-posneg-753-1-2",
    "gold_answer": "FALSE. The model (A, CM) includes the main effects θ₁^(A), θ₁^(C), θ₁^(M), and the specific two-way interaction θ₁₁^(CM), but it explicitly excludes the other two-way interactions (θ₁₁^(AC) and θ₁₁^(AM)) and the three-way interaction θ₁₁₁^(ACM). This is a non-saturated model that assumes conditional independence between A and C given M, and between A and M given C, while allowing for an association between C and M.",
    "question": "The model (A, CM) includes the parameters θ₁^(A), θ₁^(C), θ₁^(M), and θ₁₁^(CM), but excludes all two-way interactions except θ₁₁^(CM). True or False?",
    "question_context": "Log-linear Model Selection Using Phi-Divergence Statistics\n\nThe paper examines model selection in log-linear models using various phi-divergence statistics. The saturated log-linear model for an 8-category contingency table is specified with parameters for main effects (A, C, M), two-way interactions (AC, AM, CM), and a three-way interaction (ACM). The probability of each cell is given by a multinomial logistic form. The study compares the performance of 11 different divergence statistics (e.g., G², X², C-R) in selecting the correct model across different sample sizes and data structures (genetics of plants, truncated Poisson variates). Results are presented as proportions of correct model selections out of 10,000 replications."
  }
]